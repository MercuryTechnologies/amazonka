-- Hoogle documentation, generated by Haddock
-- See Hoogle, http://www.haskell.org/hoogle/

@package libZSservicesZSamazonka-rekognitionZSamazonka-rekognition
@version 2.0


module Network.AWS.Rekognition.Types.AgeRange

-- | Structure containing the estimated age range, in years, for a face.
--   
--   Amazon Rekognition estimates an age range for faces detected in the
--   input image. Estimated age ranges can overlap. A face of a 5-year-old
--   might have an estimated range of 4-6, while the face of a 6-year-old
--   might have an estimated range of 4-8.
--   
--   <i>See:</i> <a>newAgeRange</a> smart constructor.
data AgeRange
AgeRange' :: Maybe Natural -> Maybe Natural -> AgeRange

-- | The lowest estimated age.
[$sel:low:AgeRange'] :: AgeRange -> Maybe Natural

-- | The highest estimated age.
[$sel:high:AgeRange'] :: AgeRange -> Maybe Natural

-- | Create a value of <a>AgeRange</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:low:AgeRange'</a>, <a>ageRange_low</a> - The lowest estimated
--   age.
--   
--   <a>$sel:high:AgeRange'</a>, <a>ageRange_high</a> - The highest
--   estimated age.
newAgeRange :: AgeRange

-- | The lowest estimated age.
ageRange_low :: Lens' AgeRange (Maybe Natural)

-- | The highest estimated age.
ageRange_high :: Lens' AgeRange (Maybe Natural)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.AgeRange.AgeRange
instance GHC.Show.Show Network.AWS.Rekognition.Types.AgeRange.AgeRange
instance GHC.Read.Read Network.AWS.Rekognition.Types.AgeRange.AgeRange
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.AgeRange.AgeRange
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.AgeRange.AgeRange
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.AgeRange.AgeRange
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.AgeRange.AgeRange


module Network.AWS.Rekognition.Types.Attribute
newtype Attribute
Attribute' :: Text -> Attribute
[fromAttribute] :: Attribute -> Text
pattern Attribute_ALL :: Attribute
pattern Attribute_DEFAULT :: Attribute
instance Network.AWS.Data.XML.ToXML Network.AWS.Rekognition.Types.Attribute.Attribute
instance Network.AWS.Data.XML.FromXML Network.AWS.Rekognition.Types.Attribute.Attribute
instance Data.Aeson.Types.ToJSON.ToJSONKey Network.AWS.Rekognition.Types.Attribute.Attribute
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.Types.Attribute.Attribute
instance Data.Aeson.Types.FromJSON.FromJSONKey Network.AWS.Rekognition.Types.Attribute.Attribute
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.Attribute.Attribute
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.Types.Attribute.Attribute
instance Network.AWS.Data.Headers.ToHeader Network.AWS.Rekognition.Types.Attribute.Attribute
instance Network.AWS.Data.Log.ToLog Network.AWS.Rekognition.Types.Attribute.Attribute
instance Network.AWS.Data.ByteString.ToByteString Network.AWS.Rekognition.Types.Attribute.Attribute
instance Network.AWS.Data.Text.ToText Network.AWS.Rekognition.Types.Attribute.Attribute
instance Network.AWS.Data.Text.FromText Network.AWS.Rekognition.Types.Attribute.Attribute
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.Attribute.Attribute
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.Attribute.Attribute
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.Attribute.Attribute
instance GHC.Classes.Ord Network.AWS.Rekognition.Types.Attribute.Attribute
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.Attribute.Attribute
instance GHC.Read.Read Network.AWS.Rekognition.Types.Attribute.Attribute
instance GHC.Show.Show Network.AWS.Rekognition.Types.Attribute.Attribute


module Network.AWS.Rekognition.Types.AudioMetadata

-- | Metadata information about an audio stream. An array of
--   <tt>AudioMetadata</tt> objects for the audio streams found in a stored
--   video is returned by GetSegmentDetection.
--   
--   <i>See:</i> <a>newAudioMetadata</a> smart constructor.
data AudioMetadata
AudioMetadata' :: Maybe Text -> Maybe Natural -> Maybe Natural -> Maybe Natural -> AudioMetadata

-- | The audio codec used to encode or decode the audio stream.
[$sel:codec:AudioMetadata'] :: AudioMetadata -> Maybe Text

-- | The sample rate for the audio stream.
[$sel:sampleRate:AudioMetadata'] :: AudioMetadata -> Maybe Natural

-- | The number of audio channels in the segment.
[$sel:numberOfChannels:AudioMetadata'] :: AudioMetadata -> Maybe Natural

-- | The duration of the audio stream in milliseconds.
[$sel:durationMillis:AudioMetadata'] :: AudioMetadata -> Maybe Natural

-- | Create a value of <a>AudioMetadata</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:codec:AudioMetadata'</a>, <a>audioMetadata_codec</a> - The
--   audio codec used to encode or decode the audio stream.
--   
--   <a>$sel:sampleRate:AudioMetadata'</a>, <a>audioMetadata_sampleRate</a>
--   - The sample rate for the audio stream.
--   
--   <a>$sel:numberOfChannels:AudioMetadata'</a>,
--   <a>audioMetadata_numberOfChannels</a> - The number of audio channels
--   in the segment.
--   
--   <a>$sel:durationMillis:AudioMetadata'</a>,
--   <a>audioMetadata_durationMillis</a> - The duration of the audio stream
--   in milliseconds.
newAudioMetadata :: AudioMetadata

-- | The audio codec used to encode or decode the audio stream.
audioMetadata_codec :: Lens' AudioMetadata (Maybe Text)

-- | The sample rate for the audio stream.
audioMetadata_sampleRate :: Lens' AudioMetadata (Maybe Natural)

-- | The number of audio channels in the segment.
audioMetadata_numberOfChannels :: Lens' AudioMetadata (Maybe Natural)

-- | The duration of the audio stream in milliseconds.
audioMetadata_durationMillis :: Lens' AudioMetadata (Maybe Natural)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.AudioMetadata.AudioMetadata
instance GHC.Show.Show Network.AWS.Rekognition.Types.AudioMetadata.AudioMetadata
instance GHC.Read.Read Network.AWS.Rekognition.Types.AudioMetadata.AudioMetadata
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.AudioMetadata.AudioMetadata
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.AudioMetadata.AudioMetadata
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.AudioMetadata.AudioMetadata
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.AudioMetadata.AudioMetadata


module Network.AWS.Rekognition.Types.Beard

-- | Indicates whether or not the face has a beard, and the confidence
--   level in the determination.
--   
--   <i>See:</i> <a>newBeard</a> smart constructor.
data Beard
Beard' :: Maybe Bool -> Maybe Double -> Beard

-- | Boolean value that indicates whether the face has beard or not.
[$sel:value:Beard'] :: Beard -> Maybe Bool

-- | Level of confidence in the determination.
[$sel:confidence:Beard'] :: Beard -> Maybe Double

-- | Create a value of <a>Beard</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:value:Beard'</a>, <a>beard_value</a> - Boolean value that
--   indicates whether the face has beard or not.
--   
--   <a>$sel:confidence:Beard'</a>, <a>beard_confidence</a> - Level of
--   confidence in the determination.
newBeard :: Beard

-- | Boolean value that indicates whether the face has beard or not.
beard_value :: Lens' Beard (Maybe Bool)

-- | Level of confidence in the determination.
beard_confidence :: Lens' Beard (Maybe Double)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.Beard.Beard
instance GHC.Show.Show Network.AWS.Rekognition.Types.Beard.Beard
instance GHC.Read.Read Network.AWS.Rekognition.Types.Beard.Beard
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.Beard.Beard
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.Beard.Beard
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.Beard.Beard
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.Beard.Beard


module Network.AWS.Rekognition.Types.BlackFrame

-- | A filter that allows you to control the black frame detection by
--   specifying the black levels and pixel coverage of black pixels in a
--   frame. As videos can come from multiple sources, formats, and time
--   periods, they may contain different standards and varying noise levels
--   for black frames that need to be accounted for. For more information,
--   see StartSegmentDetection.
--   
--   <i>See:</i> <a>newBlackFrame</a> smart constructor.
data BlackFrame
BlackFrame' :: Maybe Double -> Maybe Double -> BlackFrame

-- | A threshold used to determine the maximum luminance value for a pixel
--   to be considered black. In a full color range video, luminance values
--   range from 0-255. A pixel value of 0 is pure black, and the most
--   strict filter. The maximum black pixel value is computed as follows:
--   max_black_pixel_value = minimum_luminance + MaxPixelThreshold
--   *luminance_range.
--   
--   For example, for a full range video with BlackPixelThreshold = 0.1,
--   max_black_pixel_value is 0 + 0.1 * (255-0) = 25.5.
--   
--   The default value of MaxPixelThreshold is 0.2, which maps to a
--   max_black_pixel_value of 51 for a full range video. You can lower this
--   threshold to be more strict on black levels.
[$sel:maxPixelThreshold:BlackFrame'] :: BlackFrame -> Maybe Double

-- | The minimum percentage of pixels in a frame that need to have a
--   luminance below the max_black_pixel_value for a frame to be considered
--   a black frame. Luminance is calculated using the BT.709 matrix.
--   
--   The default value is 99, which means at least 99% of all pixels in the
--   frame are black pixels as per the <tt>MaxPixelThreshold</tt> set. You
--   can reduce this value to allow more noise on the black frame.
[$sel:minCoveragePercentage:BlackFrame'] :: BlackFrame -> Maybe Double

-- | Create a value of <a>BlackFrame</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:maxPixelThreshold:BlackFrame'</a>,
--   <a>blackFrame_maxPixelThreshold</a> - A threshold used to determine
--   the maximum luminance value for a pixel to be considered black. In a
--   full color range video, luminance values range from 0-255. A pixel
--   value of 0 is pure black, and the most strict filter. The maximum
--   black pixel value is computed as follows: max_black_pixel_value =
--   minimum_luminance + MaxPixelThreshold *luminance_range.
--   
--   For example, for a full range video with BlackPixelThreshold = 0.1,
--   max_black_pixel_value is 0 + 0.1 * (255-0) = 25.5.
--   
--   The default value of MaxPixelThreshold is 0.2, which maps to a
--   max_black_pixel_value of 51 for a full range video. You can lower this
--   threshold to be more strict on black levels.
--   
--   <a>$sel:minCoveragePercentage:BlackFrame'</a>,
--   <a>blackFrame_minCoveragePercentage</a> - The minimum percentage of
--   pixels in a frame that need to have a luminance below the
--   max_black_pixel_value for a frame to be considered a black frame.
--   Luminance is calculated using the BT.709 matrix.
--   
--   The default value is 99, which means at least 99% of all pixels in the
--   frame are black pixels as per the <tt>MaxPixelThreshold</tt> set. You
--   can reduce this value to allow more noise on the black frame.
newBlackFrame :: BlackFrame

-- | A threshold used to determine the maximum luminance value for a pixel
--   to be considered black. In a full color range video, luminance values
--   range from 0-255. A pixel value of 0 is pure black, and the most
--   strict filter. The maximum black pixel value is computed as follows:
--   max_black_pixel_value = minimum_luminance + MaxPixelThreshold
--   *luminance_range.
--   
--   For example, for a full range video with BlackPixelThreshold = 0.1,
--   max_black_pixel_value is 0 + 0.1 * (255-0) = 25.5.
--   
--   The default value of MaxPixelThreshold is 0.2, which maps to a
--   max_black_pixel_value of 51 for a full range video. You can lower this
--   threshold to be more strict on black levels.
blackFrame_maxPixelThreshold :: Lens' BlackFrame (Maybe Double)

-- | The minimum percentage of pixels in a frame that need to have a
--   luminance below the max_black_pixel_value for a frame to be considered
--   a black frame. Luminance is calculated using the BT.709 matrix.
--   
--   The default value is 99, which means at least 99% of all pixels in the
--   frame are black pixels as per the <tt>MaxPixelThreshold</tt> set. You
--   can reduce this value to allow more noise on the black frame.
blackFrame_minCoveragePercentage :: Lens' BlackFrame (Maybe Double)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.BlackFrame.BlackFrame
instance GHC.Show.Show Network.AWS.Rekognition.Types.BlackFrame.BlackFrame
instance GHC.Read.Read Network.AWS.Rekognition.Types.BlackFrame.BlackFrame
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.BlackFrame.BlackFrame
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.BlackFrame.BlackFrame
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.BlackFrame.BlackFrame
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.Types.BlackFrame.BlackFrame


module Network.AWS.Rekognition.Types.BodyPart
newtype BodyPart
BodyPart' :: Text -> BodyPart
[fromBodyPart] :: BodyPart -> Text
pattern BodyPart_FACE :: BodyPart
pattern BodyPart_HEAD :: BodyPart
pattern BodyPart_LEFT_HAND :: BodyPart
pattern BodyPart_RIGHT_HAND :: BodyPart
instance Network.AWS.Data.XML.ToXML Network.AWS.Rekognition.Types.BodyPart.BodyPart
instance Network.AWS.Data.XML.FromXML Network.AWS.Rekognition.Types.BodyPart.BodyPart
instance Data.Aeson.Types.ToJSON.ToJSONKey Network.AWS.Rekognition.Types.BodyPart.BodyPart
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.Types.BodyPart.BodyPart
instance Data.Aeson.Types.FromJSON.FromJSONKey Network.AWS.Rekognition.Types.BodyPart.BodyPart
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.BodyPart.BodyPart
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.Types.BodyPart.BodyPart
instance Network.AWS.Data.Headers.ToHeader Network.AWS.Rekognition.Types.BodyPart.BodyPart
instance Network.AWS.Data.Log.ToLog Network.AWS.Rekognition.Types.BodyPart.BodyPart
instance Network.AWS.Data.ByteString.ToByteString Network.AWS.Rekognition.Types.BodyPart.BodyPart
instance Network.AWS.Data.Text.ToText Network.AWS.Rekognition.Types.BodyPart.BodyPart
instance Network.AWS.Data.Text.FromText Network.AWS.Rekognition.Types.BodyPart.BodyPart
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.BodyPart.BodyPart
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.BodyPart.BodyPart
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.BodyPart.BodyPart
instance GHC.Classes.Ord Network.AWS.Rekognition.Types.BodyPart.BodyPart
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.BodyPart.BodyPart
instance GHC.Read.Read Network.AWS.Rekognition.Types.BodyPart.BodyPart
instance GHC.Show.Show Network.AWS.Rekognition.Types.BodyPart.BodyPart


module Network.AWS.Rekognition.Types.BoundingBox

-- | Identifies the bounding box around the label, face, text or personal
--   protective equipment. The <tt>left</tt> (x-coordinate) and
--   <tt>top</tt> (y-coordinate) are coordinates representing the top and
--   left sides of the bounding box. Note that the upper-left corner of the
--   image is the origin (0,0).
--   
--   The <tt>top</tt> and <tt>left</tt> values returned are ratios of the
--   overall image size. For example, if the input image is 700x200 pixels,
--   and the top-left coordinate of the bounding box is 350x50 pixels, the
--   API returns a <tt>left</tt> value of 0.5 (350/700) and a <tt>top</tt>
--   value of 0.25 (50/200).
--   
--   The <tt>width</tt> and <tt>height</tt> values represent the dimensions
--   of the bounding box as a ratio of the overall image dimension. For
--   example, if the input image is 700x200 pixels, and the bounding box
--   width is 70 pixels, the width returned is 0.1.
--   
--   The bounding box coordinates can have negative values. For example, if
--   Amazon Rekognition is able to detect a face that is at the image edge
--   and is only partially visible, the service can return coordinates that
--   are outside the image bounds and, depending on the image edge, you
--   might get negative values or values greater than 1 for the
--   <tt>left</tt> or <tt>top</tt> values.
--   
--   <i>See:</i> <a>newBoundingBox</a> smart constructor.
data BoundingBox
BoundingBox' :: Maybe Double -> Maybe Double -> Maybe Double -> Maybe Double -> BoundingBox

-- | Height of the bounding box as a ratio of the overall image height.
[$sel:height:BoundingBox'] :: BoundingBox -> Maybe Double

-- | Left coordinate of the bounding box as a ratio of overall image width.
[$sel:left:BoundingBox'] :: BoundingBox -> Maybe Double

-- | Width of the bounding box as a ratio of the overall image width.
[$sel:width:BoundingBox'] :: BoundingBox -> Maybe Double

-- | Top coordinate of the bounding box as a ratio of overall image height.
[$sel:top:BoundingBox'] :: BoundingBox -> Maybe Double

-- | Create a value of <a>BoundingBox</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:height:BoundingBox'</a>, <a>boundingBox_height</a> - Height of
--   the bounding box as a ratio of the overall image height.
--   
--   <a>$sel:left:BoundingBox'</a>, <a>boundingBox_left</a> - Left
--   coordinate of the bounding box as a ratio of overall image width.
--   
--   <a>$sel:width:BoundingBox'</a>, <a>boundingBox_width</a> - Width of
--   the bounding box as a ratio of the overall image width.
--   
--   <a>$sel:top:BoundingBox'</a>, <a>boundingBox_top</a> - Top coordinate
--   of the bounding box as a ratio of overall image height.
newBoundingBox :: BoundingBox

-- | Height of the bounding box as a ratio of the overall image height.
boundingBox_height :: Lens' BoundingBox (Maybe Double)

-- | Left coordinate of the bounding box as a ratio of overall image width.
boundingBox_left :: Lens' BoundingBox (Maybe Double)

-- | Width of the bounding box as a ratio of the overall image width.
boundingBox_width :: Lens' BoundingBox (Maybe Double)

-- | Top coordinate of the bounding box as a ratio of overall image height.
boundingBox_top :: Lens' BoundingBox (Maybe Double)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.BoundingBox.BoundingBox
instance GHC.Show.Show Network.AWS.Rekognition.Types.BoundingBox.BoundingBox
instance GHC.Read.Read Network.AWS.Rekognition.Types.BoundingBox.BoundingBox
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.BoundingBox.BoundingBox
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.BoundingBox.BoundingBox
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.BoundingBox.BoundingBox
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.BoundingBox.BoundingBox
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.Types.BoundingBox.BoundingBox


module Network.AWS.Rekognition.Types.CelebrityRecognitionSortBy
newtype CelebrityRecognitionSortBy
CelebrityRecognitionSortBy' :: Text -> CelebrityRecognitionSortBy
[fromCelebrityRecognitionSortBy] :: CelebrityRecognitionSortBy -> Text
pattern CelebrityRecognitionSortBy_ID :: CelebrityRecognitionSortBy
pattern CelebrityRecognitionSortBy_TIMESTAMP :: CelebrityRecognitionSortBy
instance Network.AWS.Data.XML.ToXML Network.AWS.Rekognition.Types.CelebrityRecognitionSortBy.CelebrityRecognitionSortBy
instance Network.AWS.Data.XML.FromXML Network.AWS.Rekognition.Types.CelebrityRecognitionSortBy.CelebrityRecognitionSortBy
instance Data.Aeson.Types.ToJSON.ToJSONKey Network.AWS.Rekognition.Types.CelebrityRecognitionSortBy.CelebrityRecognitionSortBy
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.Types.CelebrityRecognitionSortBy.CelebrityRecognitionSortBy
instance Data.Aeson.Types.FromJSON.FromJSONKey Network.AWS.Rekognition.Types.CelebrityRecognitionSortBy.CelebrityRecognitionSortBy
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.CelebrityRecognitionSortBy.CelebrityRecognitionSortBy
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.Types.CelebrityRecognitionSortBy.CelebrityRecognitionSortBy
instance Network.AWS.Data.Headers.ToHeader Network.AWS.Rekognition.Types.CelebrityRecognitionSortBy.CelebrityRecognitionSortBy
instance Network.AWS.Data.Log.ToLog Network.AWS.Rekognition.Types.CelebrityRecognitionSortBy.CelebrityRecognitionSortBy
instance Network.AWS.Data.ByteString.ToByteString Network.AWS.Rekognition.Types.CelebrityRecognitionSortBy.CelebrityRecognitionSortBy
instance Network.AWS.Data.Text.ToText Network.AWS.Rekognition.Types.CelebrityRecognitionSortBy.CelebrityRecognitionSortBy
instance Network.AWS.Data.Text.FromText Network.AWS.Rekognition.Types.CelebrityRecognitionSortBy.CelebrityRecognitionSortBy
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.CelebrityRecognitionSortBy.CelebrityRecognitionSortBy
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.CelebrityRecognitionSortBy.CelebrityRecognitionSortBy
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.CelebrityRecognitionSortBy.CelebrityRecognitionSortBy
instance GHC.Classes.Ord Network.AWS.Rekognition.Types.CelebrityRecognitionSortBy.CelebrityRecognitionSortBy
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.CelebrityRecognitionSortBy.CelebrityRecognitionSortBy
instance GHC.Read.Read Network.AWS.Rekognition.Types.CelebrityRecognitionSortBy.CelebrityRecognitionSortBy
instance GHC.Show.Show Network.AWS.Rekognition.Types.CelebrityRecognitionSortBy.CelebrityRecognitionSortBy


module Network.AWS.Rekognition.Types.ComparedSourceImageFace

-- | Type that describes the face Amazon Rekognition chose to compare with
--   the faces in the target. This contains a bounding box for the selected
--   face and confidence level that the bounding box contains a face. Note
--   that Amazon Rekognition selects the largest face in the source image
--   for this comparison.
--   
--   <i>See:</i> <a>newComparedSourceImageFace</a> smart constructor.
data ComparedSourceImageFace
ComparedSourceImageFace' :: Maybe BoundingBox -> Maybe Double -> ComparedSourceImageFace

-- | Bounding box of the face.
[$sel:boundingBox:ComparedSourceImageFace'] :: ComparedSourceImageFace -> Maybe BoundingBox

-- | Confidence level that the selected bounding box contains a face.
[$sel:confidence:ComparedSourceImageFace'] :: ComparedSourceImageFace -> Maybe Double

-- | Create a value of <a>ComparedSourceImageFace</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:boundingBox:ComparedSourceImageFace'</a>,
--   <a>comparedSourceImageFace_boundingBox</a> - Bounding box of the face.
--   
--   <a>$sel:confidence:ComparedSourceImageFace'</a>,
--   <a>comparedSourceImageFace_confidence</a> - Confidence level that the
--   selected bounding box contains a face.
newComparedSourceImageFace :: ComparedSourceImageFace

-- | Bounding box of the face.
comparedSourceImageFace_boundingBox :: Lens' ComparedSourceImageFace (Maybe BoundingBox)

-- | Confidence level that the selected bounding box contains a face.
comparedSourceImageFace_confidence :: Lens' ComparedSourceImageFace (Maybe Double)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.ComparedSourceImageFace.ComparedSourceImageFace
instance GHC.Show.Show Network.AWS.Rekognition.Types.ComparedSourceImageFace.ComparedSourceImageFace
instance GHC.Read.Read Network.AWS.Rekognition.Types.ComparedSourceImageFace.ComparedSourceImageFace
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.ComparedSourceImageFace.ComparedSourceImageFace
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.ComparedSourceImageFace.ComparedSourceImageFace
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.ComparedSourceImageFace.ComparedSourceImageFace
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.ComparedSourceImageFace.ComparedSourceImageFace


module Network.AWS.Rekognition.Types.ContentClassifier
newtype ContentClassifier
ContentClassifier' :: Text -> ContentClassifier
[fromContentClassifier] :: ContentClassifier -> Text
pattern ContentClassifier_FreeOfAdultContent :: ContentClassifier
pattern ContentClassifier_FreeOfPersonallyIdentifiableInformation :: ContentClassifier
instance Network.AWS.Data.XML.ToXML Network.AWS.Rekognition.Types.ContentClassifier.ContentClassifier
instance Network.AWS.Data.XML.FromXML Network.AWS.Rekognition.Types.ContentClassifier.ContentClassifier
instance Data.Aeson.Types.ToJSON.ToJSONKey Network.AWS.Rekognition.Types.ContentClassifier.ContentClassifier
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.Types.ContentClassifier.ContentClassifier
instance Data.Aeson.Types.FromJSON.FromJSONKey Network.AWS.Rekognition.Types.ContentClassifier.ContentClassifier
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.ContentClassifier.ContentClassifier
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.Types.ContentClassifier.ContentClassifier
instance Network.AWS.Data.Headers.ToHeader Network.AWS.Rekognition.Types.ContentClassifier.ContentClassifier
instance Network.AWS.Data.Log.ToLog Network.AWS.Rekognition.Types.ContentClassifier.ContentClassifier
instance Network.AWS.Data.ByteString.ToByteString Network.AWS.Rekognition.Types.ContentClassifier.ContentClassifier
instance Network.AWS.Data.Text.ToText Network.AWS.Rekognition.Types.ContentClassifier.ContentClassifier
instance Network.AWS.Data.Text.FromText Network.AWS.Rekognition.Types.ContentClassifier.ContentClassifier
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.ContentClassifier.ContentClassifier
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.ContentClassifier.ContentClassifier
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.ContentClassifier.ContentClassifier
instance GHC.Classes.Ord Network.AWS.Rekognition.Types.ContentClassifier.ContentClassifier
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.ContentClassifier.ContentClassifier
instance GHC.Read.Read Network.AWS.Rekognition.Types.ContentClassifier.ContentClassifier
instance GHC.Show.Show Network.AWS.Rekognition.Types.ContentClassifier.ContentClassifier


module Network.AWS.Rekognition.Types.ContentModerationSortBy
newtype ContentModerationSortBy
ContentModerationSortBy' :: Text -> ContentModerationSortBy
[fromContentModerationSortBy] :: ContentModerationSortBy -> Text
pattern ContentModerationSortBy_NAME :: ContentModerationSortBy
pattern ContentModerationSortBy_TIMESTAMP :: ContentModerationSortBy
instance Network.AWS.Data.XML.ToXML Network.AWS.Rekognition.Types.ContentModerationSortBy.ContentModerationSortBy
instance Network.AWS.Data.XML.FromXML Network.AWS.Rekognition.Types.ContentModerationSortBy.ContentModerationSortBy
instance Data.Aeson.Types.ToJSON.ToJSONKey Network.AWS.Rekognition.Types.ContentModerationSortBy.ContentModerationSortBy
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.Types.ContentModerationSortBy.ContentModerationSortBy
instance Data.Aeson.Types.FromJSON.FromJSONKey Network.AWS.Rekognition.Types.ContentModerationSortBy.ContentModerationSortBy
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.ContentModerationSortBy.ContentModerationSortBy
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.Types.ContentModerationSortBy.ContentModerationSortBy
instance Network.AWS.Data.Headers.ToHeader Network.AWS.Rekognition.Types.ContentModerationSortBy.ContentModerationSortBy
instance Network.AWS.Data.Log.ToLog Network.AWS.Rekognition.Types.ContentModerationSortBy.ContentModerationSortBy
instance Network.AWS.Data.ByteString.ToByteString Network.AWS.Rekognition.Types.ContentModerationSortBy.ContentModerationSortBy
instance Network.AWS.Data.Text.ToText Network.AWS.Rekognition.Types.ContentModerationSortBy.ContentModerationSortBy
instance Network.AWS.Data.Text.FromText Network.AWS.Rekognition.Types.ContentModerationSortBy.ContentModerationSortBy
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.ContentModerationSortBy.ContentModerationSortBy
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.ContentModerationSortBy.ContentModerationSortBy
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.ContentModerationSortBy.ContentModerationSortBy
instance GHC.Classes.Ord Network.AWS.Rekognition.Types.ContentModerationSortBy.ContentModerationSortBy
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.ContentModerationSortBy.ContentModerationSortBy
instance GHC.Read.Read Network.AWS.Rekognition.Types.ContentModerationSortBy.ContentModerationSortBy
instance GHC.Show.Show Network.AWS.Rekognition.Types.ContentModerationSortBy.ContentModerationSortBy


module Network.AWS.Rekognition.Types.CoversBodyPart

-- | Information about an item of Personal Protective Equipment covering a
--   corresponding body part. For more information, see
--   DetectProtectiveEquipment.
--   
--   <i>See:</i> <a>newCoversBodyPart</a> smart constructor.
data CoversBodyPart
CoversBodyPart' :: Maybe Bool -> Maybe Double -> CoversBodyPart

-- | True if the PPE covers the corresponding body part, otherwise false.
[$sel:value:CoversBodyPart'] :: CoversBodyPart -> Maybe Bool

-- | The confidence that Amazon Rekognition has in the value of
--   <tt>Value</tt>.
[$sel:confidence:CoversBodyPart'] :: CoversBodyPart -> Maybe Double

-- | Create a value of <a>CoversBodyPart</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:value:CoversBodyPart'</a>, <a>coversBodyPart_value</a> - True
--   if the PPE covers the corresponding body part, otherwise false.
--   
--   <a>$sel:confidence:CoversBodyPart'</a>,
--   <a>coversBodyPart_confidence</a> - The confidence that Amazon
--   Rekognition has in the value of <tt>Value</tt>.
newCoversBodyPart :: CoversBodyPart

-- | True if the PPE covers the corresponding body part, otherwise false.
coversBodyPart_value :: Lens' CoversBodyPart (Maybe Bool)

-- | The confidence that Amazon Rekognition has in the value of
--   <tt>Value</tt>.
coversBodyPart_confidence :: Lens' CoversBodyPart (Maybe Double)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.CoversBodyPart.CoversBodyPart
instance GHC.Show.Show Network.AWS.Rekognition.Types.CoversBodyPart.CoversBodyPart
instance GHC.Read.Read Network.AWS.Rekognition.Types.CoversBodyPart.CoversBodyPart
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.CoversBodyPart.CoversBodyPart
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.CoversBodyPart.CoversBodyPart
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.CoversBodyPart.CoversBodyPart
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.CoversBodyPart.CoversBodyPart


module Network.AWS.Rekognition.Types.DetectionFilter

-- | A set of parameters that allow you to filter out certain results from
--   your returned results.
--   
--   <i>See:</i> <a>newDetectionFilter</a> smart constructor.
data DetectionFilter
DetectionFilter' :: Maybe Double -> Maybe Double -> Maybe Double -> DetectionFilter

-- | Sets the minimum height of the word bounding box. Words with bounding
--   box heights lesser than this value will be excluded from the result.
--   Value is relative to the video frame height.
[$sel:minBoundingBoxHeight:DetectionFilter'] :: DetectionFilter -> Maybe Double

-- | Sets the minimum width of the word bounding box. Words with bounding
--   boxes widths lesser than this value will be excluded from the result.
--   Value is relative to the video frame width.
[$sel:minBoundingBoxWidth:DetectionFilter'] :: DetectionFilter -> Maybe Double

-- | Sets the confidence of word detection. Words with detection confidence
--   below this will be excluded from the result. Values should be between
--   50 and 100 as Text in Video will not return any result below 50.
[$sel:minConfidence:DetectionFilter'] :: DetectionFilter -> Maybe Double

-- | Create a value of <a>DetectionFilter</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:minBoundingBoxHeight:DetectionFilter'</a>,
--   <a>detectionFilter_minBoundingBoxHeight</a> - Sets the minimum height
--   of the word bounding box. Words with bounding box heights lesser than
--   this value will be excluded from the result. Value is relative to the
--   video frame height.
--   
--   <a>$sel:minBoundingBoxWidth:DetectionFilter'</a>,
--   <a>detectionFilter_minBoundingBoxWidth</a> - Sets the minimum width of
--   the word bounding box. Words with bounding boxes widths lesser than
--   this value will be excluded from the result. Value is relative to the
--   video frame width.
--   
--   <a>$sel:minConfidence:DetectionFilter'</a>,
--   <a>detectionFilter_minConfidence</a> - Sets the confidence of word
--   detection. Words with detection confidence below this will be excluded
--   from the result. Values should be between 50 and 100 as Text in Video
--   will not return any result below 50.
newDetectionFilter :: DetectionFilter

-- | Sets the minimum height of the word bounding box. Words with bounding
--   box heights lesser than this value will be excluded from the result.
--   Value is relative to the video frame height.
detectionFilter_minBoundingBoxHeight :: Lens' DetectionFilter (Maybe Double)

-- | Sets the minimum width of the word bounding box. Words with bounding
--   boxes widths lesser than this value will be excluded from the result.
--   Value is relative to the video frame width.
detectionFilter_minBoundingBoxWidth :: Lens' DetectionFilter (Maybe Double)

-- | Sets the confidence of word detection. Words with detection confidence
--   below this will be excluded from the result. Values should be between
--   50 and 100 as Text in Video will not return any result below 50.
detectionFilter_minConfidence :: Lens' DetectionFilter (Maybe Double)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.DetectionFilter.DetectionFilter
instance GHC.Show.Show Network.AWS.Rekognition.Types.DetectionFilter.DetectionFilter
instance GHC.Read.Read Network.AWS.Rekognition.Types.DetectionFilter.DetectionFilter
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.DetectionFilter.DetectionFilter
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.DetectionFilter.DetectionFilter
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.DetectionFilter.DetectionFilter
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.Types.DetectionFilter.DetectionFilter


module Network.AWS.Rekognition.Types.EmotionName
newtype EmotionName
EmotionName' :: Text -> EmotionName
[fromEmotionName] :: EmotionName -> Text
pattern EmotionName_ANGRY :: EmotionName
pattern EmotionName_CALM :: EmotionName
pattern EmotionName_CONFUSED :: EmotionName
pattern EmotionName_DISGUSTED :: EmotionName
pattern EmotionName_FEAR :: EmotionName
pattern EmotionName_HAPPY :: EmotionName
pattern EmotionName_SAD :: EmotionName
pattern EmotionName_SURPRISED :: EmotionName
pattern EmotionName_UNKNOWN :: EmotionName
instance Network.AWS.Data.XML.ToXML Network.AWS.Rekognition.Types.EmotionName.EmotionName
instance Network.AWS.Data.XML.FromXML Network.AWS.Rekognition.Types.EmotionName.EmotionName
instance Data.Aeson.Types.ToJSON.ToJSONKey Network.AWS.Rekognition.Types.EmotionName.EmotionName
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.Types.EmotionName.EmotionName
instance Data.Aeson.Types.FromJSON.FromJSONKey Network.AWS.Rekognition.Types.EmotionName.EmotionName
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.EmotionName.EmotionName
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.Types.EmotionName.EmotionName
instance Network.AWS.Data.Headers.ToHeader Network.AWS.Rekognition.Types.EmotionName.EmotionName
instance Network.AWS.Data.Log.ToLog Network.AWS.Rekognition.Types.EmotionName.EmotionName
instance Network.AWS.Data.ByteString.ToByteString Network.AWS.Rekognition.Types.EmotionName.EmotionName
instance Network.AWS.Data.Text.ToText Network.AWS.Rekognition.Types.EmotionName.EmotionName
instance Network.AWS.Data.Text.FromText Network.AWS.Rekognition.Types.EmotionName.EmotionName
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.EmotionName.EmotionName
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.EmotionName.EmotionName
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.EmotionName.EmotionName
instance GHC.Classes.Ord Network.AWS.Rekognition.Types.EmotionName.EmotionName
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.EmotionName.EmotionName
instance GHC.Read.Read Network.AWS.Rekognition.Types.EmotionName.EmotionName
instance GHC.Show.Show Network.AWS.Rekognition.Types.EmotionName.EmotionName


module Network.AWS.Rekognition.Types.Emotion

-- | The emotions that appear to be expressed on the face, and the
--   confidence level in the determination. The API is only making a
--   determination of the physical appearance of a person's face. It is not
--   a determination of the person’s internal emotional state and should
--   not be used in such a way. For example, a person pretending to have a
--   sad face might not be sad emotionally.
--   
--   <i>See:</i> <a>newEmotion</a> smart constructor.
data Emotion
Emotion' :: Maybe Double -> Maybe EmotionName -> Emotion

-- | Level of confidence in the determination.
[$sel:confidence:Emotion'] :: Emotion -> Maybe Double

-- | Type of emotion detected.
[$sel:type':Emotion'] :: Emotion -> Maybe EmotionName

-- | Create a value of <a>Emotion</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:confidence:Emotion'</a>, <a>emotion_confidence</a> - Level of
--   confidence in the determination.
--   
--   <a>$sel:type':Emotion'</a>, <a>emotion_type</a> - Type of emotion
--   detected.
newEmotion :: Emotion

-- | Level of confidence in the determination.
emotion_confidence :: Lens' Emotion (Maybe Double)

-- | Type of emotion detected.
emotion_type :: Lens' Emotion (Maybe EmotionName)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.Emotion.Emotion
instance GHC.Show.Show Network.AWS.Rekognition.Types.Emotion.Emotion
instance GHC.Read.Read Network.AWS.Rekognition.Types.Emotion.Emotion
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.Emotion.Emotion
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.Emotion.Emotion
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.Emotion.Emotion
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.Emotion.Emotion


module Network.AWS.Rekognition.Types.EyeOpen

-- | Indicates whether or not the eyes on the face are open, and the
--   confidence level in the determination.
--   
--   <i>See:</i> <a>newEyeOpen</a> smart constructor.
data EyeOpen
EyeOpen' :: Maybe Bool -> Maybe Double -> EyeOpen

-- | Boolean value that indicates whether the eyes on the face are open.
[$sel:value:EyeOpen'] :: EyeOpen -> Maybe Bool

-- | Level of confidence in the determination.
[$sel:confidence:EyeOpen'] :: EyeOpen -> Maybe Double

-- | Create a value of <a>EyeOpen</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:value:EyeOpen'</a>, <a>eyeOpen_value</a> - Boolean value that
--   indicates whether the eyes on the face are open.
--   
--   <a>$sel:confidence:EyeOpen'</a>, <a>eyeOpen_confidence</a> - Level of
--   confidence in the determination.
newEyeOpen :: EyeOpen

-- | Boolean value that indicates whether the eyes on the face are open.
eyeOpen_value :: Lens' EyeOpen (Maybe Bool)

-- | Level of confidence in the determination.
eyeOpen_confidence :: Lens' EyeOpen (Maybe Double)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.EyeOpen.EyeOpen
instance GHC.Show.Show Network.AWS.Rekognition.Types.EyeOpen.EyeOpen
instance GHC.Read.Read Network.AWS.Rekognition.Types.EyeOpen.EyeOpen
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.EyeOpen.EyeOpen
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.EyeOpen.EyeOpen
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.EyeOpen.EyeOpen
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.EyeOpen.EyeOpen


module Network.AWS.Rekognition.Types.Eyeglasses

-- | Indicates whether or not the face is wearing eye glasses, and the
--   confidence level in the determination.
--   
--   <i>See:</i> <a>newEyeglasses</a> smart constructor.
data Eyeglasses
Eyeglasses' :: Maybe Bool -> Maybe Double -> Eyeglasses

-- | Boolean value that indicates whether the face is wearing eye glasses
--   or not.
[$sel:value:Eyeglasses'] :: Eyeglasses -> Maybe Bool

-- | Level of confidence in the determination.
[$sel:confidence:Eyeglasses'] :: Eyeglasses -> Maybe Double

-- | Create a value of <a>Eyeglasses</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:value:Eyeglasses'</a>, <a>eyeglasses_value</a> - Boolean value
--   that indicates whether the face is wearing eye glasses or not.
--   
--   <a>$sel:confidence:Eyeglasses'</a>, <a>eyeglasses_confidence</a> -
--   Level of confidence in the determination.
newEyeglasses :: Eyeglasses

-- | Boolean value that indicates whether the face is wearing eye glasses
--   or not.
eyeglasses_value :: Lens' Eyeglasses (Maybe Bool)

-- | Level of confidence in the determination.
eyeglasses_confidence :: Lens' Eyeglasses (Maybe Double)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.Eyeglasses.Eyeglasses
instance GHC.Show.Show Network.AWS.Rekognition.Types.Eyeglasses.Eyeglasses
instance GHC.Read.Read Network.AWS.Rekognition.Types.Eyeglasses.Eyeglasses
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.Eyeglasses.Eyeglasses
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.Eyeglasses.Eyeglasses
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.Eyeglasses.Eyeglasses
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.Eyeglasses.Eyeglasses


module Network.AWS.Rekognition.Types.Face

-- | Describes the face properties such as the bounding box, face ID, image
--   ID of the input image, and external image ID that you assigned.
--   
--   <i>See:</i> <a>newFace</a> smart constructor.
data Face
Face' :: Maybe Text -> Maybe BoundingBox -> Maybe Text -> Maybe Double -> Maybe Text -> Face

-- | Unique identifier that Amazon Rekognition assigns to the face.
[$sel:faceId:Face'] :: Face -> Maybe Text

-- | Bounding box of the face.
[$sel:boundingBox:Face'] :: Face -> Maybe BoundingBox

-- | Identifier that you assign to all the faces in the input image.
[$sel:externalImageId:Face'] :: Face -> Maybe Text

-- | Confidence level that the bounding box contains a face (and not a
--   different object such as a tree).
[$sel:confidence:Face'] :: Face -> Maybe Double

-- | Unique identifier that Amazon Rekognition assigns to the input image.
[$sel:imageId:Face'] :: Face -> Maybe Text

-- | Create a value of <a>Face</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:faceId:Face'</a>, <a>face_faceId</a> - Unique identifier that
--   Amazon Rekognition assigns to the face.
--   
--   <a>$sel:boundingBox:Face'</a>, <a>face_boundingBox</a> - Bounding box
--   of the face.
--   
--   <a>$sel:externalImageId:Face'</a>, <a>face_externalImageId</a> -
--   Identifier that you assign to all the faces in the input image.
--   
--   <a>$sel:confidence:Face'</a>, <a>face_confidence</a> - Confidence
--   level that the bounding box contains a face (and not a different
--   object such as a tree).
--   
--   <a>$sel:imageId:Face'</a>, <a>face_imageId</a> - Unique identifier
--   that Amazon Rekognition assigns to the input image.
newFace :: Face

-- | Unique identifier that Amazon Rekognition assigns to the face.
face_faceId :: Lens' Face (Maybe Text)

-- | Bounding box of the face.
face_boundingBox :: Lens' Face (Maybe BoundingBox)

-- | Identifier that you assign to all the faces in the input image.
face_externalImageId :: Lens' Face (Maybe Text)

-- | Confidence level that the bounding box contains a face (and not a
--   different object such as a tree).
face_confidence :: Lens' Face (Maybe Double)

-- | Unique identifier that Amazon Rekognition assigns to the input image.
face_imageId :: Lens' Face (Maybe Text)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.Face.Face
instance GHC.Show.Show Network.AWS.Rekognition.Types.Face.Face
instance GHC.Read.Read Network.AWS.Rekognition.Types.Face.Face
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.Face.Face
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.Face.Face
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.Face.Face
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.Face.Face


module Network.AWS.Rekognition.Types.FaceAttributes
newtype FaceAttributes
FaceAttributes' :: Text -> FaceAttributes
[fromFaceAttributes] :: FaceAttributes -> Text
pattern FaceAttributes_ALL :: FaceAttributes
pattern FaceAttributes_DEFAULT :: FaceAttributes
instance Network.AWS.Data.XML.ToXML Network.AWS.Rekognition.Types.FaceAttributes.FaceAttributes
instance Network.AWS.Data.XML.FromXML Network.AWS.Rekognition.Types.FaceAttributes.FaceAttributes
instance Data.Aeson.Types.ToJSON.ToJSONKey Network.AWS.Rekognition.Types.FaceAttributes.FaceAttributes
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.Types.FaceAttributes.FaceAttributes
instance Data.Aeson.Types.FromJSON.FromJSONKey Network.AWS.Rekognition.Types.FaceAttributes.FaceAttributes
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.FaceAttributes.FaceAttributes
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.Types.FaceAttributes.FaceAttributes
instance Network.AWS.Data.Headers.ToHeader Network.AWS.Rekognition.Types.FaceAttributes.FaceAttributes
instance Network.AWS.Data.Log.ToLog Network.AWS.Rekognition.Types.FaceAttributes.FaceAttributes
instance Network.AWS.Data.ByteString.ToByteString Network.AWS.Rekognition.Types.FaceAttributes.FaceAttributes
instance Network.AWS.Data.Text.ToText Network.AWS.Rekognition.Types.FaceAttributes.FaceAttributes
instance Network.AWS.Data.Text.FromText Network.AWS.Rekognition.Types.FaceAttributes.FaceAttributes
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.FaceAttributes.FaceAttributes
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.FaceAttributes.FaceAttributes
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.FaceAttributes.FaceAttributes
instance GHC.Classes.Ord Network.AWS.Rekognition.Types.FaceAttributes.FaceAttributes
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.FaceAttributes.FaceAttributes
instance GHC.Read.Read Network.AWS.Rekognition.Types.FaceAttributes.FaceAttributes
instance GHC.Show.Show Network.AWS.Rekognition.Types.FaceAttributes.FaceAttributes


module Network.AWS.Rekognition.Types.FaceMatch

-- | Provides face metadata. In addition, it also provides the confidence
--   in the match of this face with the input face.
--   
--   <i>See:</i> <a>newFaceMatch</a> smart constructor.
data FaceMatch
FaceMatch' :: Maybe Double -> Maybe Face -> FaceMatch

-- | Confidence in the match of this face with the input face.
[$sel:similarity:FaceMatch'] :: FaceMatch -> Maybe Double

-- | Describes the face properties such as the bounding box, face ID, image
--   ID of the source image, and external image ID that you assigned.
[$sel:face:FaceMatch'] :: FaceMatch -> Maybe Face

-- | Create a value of <a>FaceMatch</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:similarity:FaceMatch'</a>, <a>faceMatch_similarity</a> -
--   Confidence in the match of this face with the input face.
--   
--   <a>$sel:face:FaceMatch'</a>, <a>faceMatch_face</a> - Describes the
--   face properties such as the bounding box, face ID, image ID of the
--   source image, and external image ID that you assigned.
newFaceMatch :: FaceMatch

-- | Confidence in the match of this face with the input face.
faceMatch_similarity :: Lens' FaceMatch (Maybe Double)

-- | Describes the face properties such as the bounding box, face ID, image
--   ID of the source image, and external image ID that you assigned.
faceMatch_face :: Lens' FaceMatch (Maybe Face)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.FaceMatch.FaceMatch
instance GHC.Show.Show Network.AWS.Rekognition.Types.FaceMatch.FaceMatch
instance GHC.Read.Read Network.AWS.Rekognition.Types.FaceMatch.FaceMatch
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.FaceMatch.FaceMatch
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.FaceMatch.FaceMatch
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.FaceMatch.FaceMatch
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.FaceMatch.FaceMatch


module Network.AWS.Rekognition.Types.FaceSearchSettings

-- | Input face recognition parameters for an Amazon Rekognition stream
--   processor. <tt>FaceRecognitionSettings</tt> is a request parameter for
--   CreateStreamProcessor.
--   
--   <i>See:</i> <a>newFaceSearchSettings</a> smart constructor.
data FaceSearchSettings
FaceSearchSettings' :: Maybe Double -> Maybe Text -> FaceSearchSettings

-- | Minimum face match confidence score that must be met to return a
--   result for a recognized face. Default is 80. 0 is the lowest
--   confidence. 100 is the highest confidence.
[$sel:faceMatchThreshold:FaceSearchSettings'] :: FaceSearchSettings -> Maybe Double

-- | The ID of a collection that contains faces that you want to search
--   for.
[$sel:collectionId:FaceSearchSettings'] :: FaceSearchSettings -> Maybe Text

-- | Create a value of <a>FaceSearchSettings</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:faceMatchThreshold:FaceSearchSettings'</a>,
--   <a>faceSearchSettings_faceMatchThreshold</a> - Minimum face match
--   confidence score that must be met to return a result for a recognized
--   face. Default is 80. 0 is the lowest confidence. 100 is the highest
--   confidence.
--   
--   <a>$sel:collectionId:FaceSearchSettings'</a>,
--   <a>faceSearchSettings_collectionId</a> - The ID of a collection that
--   contains faces that you want to search for.
newFaceSearchSettings :: FaceSearchSettings

-- | Minimum face match confidence score that must be met to return a
--   result for a recognized face. Default is 80. 0 is the lowest
--   confidence. 100 is the highest confidence.
faceSearchSettings_faceMatchThreshold :: Lens' FaceSearchSettings (Maybe Double)

-- | The ID of a collection that contains faces that you want to search
--   for.
faceSearchSettings_collectionId :: Lens' FaceSearchSettings (Maybe Text)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.FaceSearchSettings.FaceSearchSettings
instance GHC.Show.Show Network.AWS.Rekognition.Types.FaceSearchSettings.FaceSearchSettings
instance GHC.Read.Read Network.AWS.Rekognition.Types.FaceSearchSettings.FaceSearchSettings
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.FaceSearchSettings.FaceSearchSettings
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.FaceSearchSettings.FaceSearchSettings
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.FaceSearchSettings.FaceSearchSettings
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.FaceSearchSettings.FaceSearchSettings
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.Types.FaceSearchSettings.FaceSearchSettings


module Network.AWS.Rekognition.Types.FaceSearchSortBy
newtype FaceSearchSortBy
FaceSearchSortBy' :: Text -> FaceSearchSortBy
[fromFaceSearchSortBy] :: FaceSearchSortBy -> Text
pattern FaceSearchSortBy_INDEX :: FaceSearchSortBy
pattern FaceSearchSortBy_TIMESTAMP :: FaceSearchSortBy
instance Network.AWS.Data.XML.ToXML Network.AWS.Rekognition.Types.FaceSearchSortBy.FaceSearchSortBy
instance Network.AWS.Data.XML.FromXML Network.AWS.Rekognition.Types.FaceSearchSortBy.FaceSearchSortBy
instance Data.Aeson.Types.ToJSON.ToJSONKey Network.AWS.Rekognition.Types.FaceSearchSortBy.FaceSearchSortBy
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.Types.FaceSearchSortBy.FaceSearchSortBy
instance Data.Aeson.Types.FromJSON.FromJSONKey Network.AWS.Rekognition.Types.FaceSearchSortBy.FaceSearchSortBy
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.FaceSearchSortBy.FaceSearchSortBy
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.Types.FaceSearchSortBy.FaceSearchSortBy
instance Network.AWS.Data.Headers.ToHeader Network.AWS.Rekognition.Types.FaceSearchSortBy.FaceSearchSortBy
instance Network.AWS.Data.Log.ToLog Network.AWS.Rekognition.Types.FaceSearchSortBy.FaceSearchSortBy
instance Network.AWS.Data.ByteString.ToByteString Network.AWS.Rekognition.Types.FaceSearchSortBy.FaceSearchSortBy
instance Network.AWS.Data.Text.ToText Network.AWS.Rekognition.Types.FaceSearchSortBy.FaceSearchSortBy
instance Network.AWS.Data.Text.FromText Network.AWS.Rekognition.Types.FaceSearchSortBy.FaceSearchSortBy
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.FaceSearchSortBy.FaceSearchSortBy
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.FaceSearchSortBy.FaceSearchSortBy
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.FaceSearchSortBy.FaceSearchSortBy
instance GHC.Classes.Ord Network.AWS.Rekognition.Types.FaceSearchSortBy.FaceSearchSortBy
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.FaceSearchSortBy.FaceSearchSortBy
instance GHC.Read.Read Network.AWS.Rekognition.Types.FaceSearchSortBy.FaceSearchSortBy
instance GHC.Show.Show Network.AWS.Rekognition.Types.FaceSearchSortBy.FaceSearchSortBy


module Network.AWS.Rekognition.Types.GenderType
newtype GenderType
GenderType' :: Text -> GenderType
[fromGenderType] :: GenderType -> Text
pattern GenderType_Female :: GenderType
pattern GenderType_Male :: GenderType
instance Network.AWS.Data.XML.ToXML Network.AWS.Rekognition.Types.GenderType.GenderType
instance Network.AWS.Data.XML.FromXML Network.AWS.Rekognition.Types.GenderType.GenderType
instance Data.Aeson.Types.ToJSON.ToJSONKey Network.AWS.Rekognition.Types.GenderType.GenderType
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.Types.GenderType.GenderType
instance Data.Aeson.Types.FromJSON.FromJSONKey Network.AWS.Rekognition.Types.GenderType.GenderType
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.GenderType.GenderType
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.Types.GenderType.GenderType
instance Network.AWS.Data.Headers.ToHeader Network.AWS.Rekognition.Types.GenderType.GenderType
instance Network.AWS.Data.Log.ToLog Network.AWS.Rekognition.Types.GenderType.GenderType
instance Network.AWS.Data.ByteString.ToByteString Network.AWS.Rekognition.Types.GenderType.GenderType
instance Network.AWS.Data.Text.ToText Network.AWS.Rekognition.Types.GenderType.GenderType
instance Network.AWS.Data.Text.FromText Network.AWS.Rekognition.Types.GenderType.GenderType
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.GenderType.GenderType
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.GenderType.GenderType
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.GenderType.GenderType
instance GHC.Classes.Ord Network.AWS.Rekognition.Types.GenderType.GenderType
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.GenderType.GenderType
instance GHC.Read.Read Network.AWS.Rekognition.Types.GenderType.GenderType
instance GHC.Show.Show Network.AWS.Rekognition.Types.GenderType.GenderType


module Network.AWS.Rekognition.Types.Gender

-- | The predicted gender of a detected face.
--   
--   Amazon Rekognition makes gender binary (male/female) predictions based
--   on the physical appearance of a face in a particular image. This kind
--   of prediction is not designed to categorize a person’s gender
--   identity, and you shouldn't use Amazon Rekognition to make such a
--   determination. For example, a male actor wearing a long-haired wig and
--   earrings for a role might be predicted as female.
--   
--   Using Amazon Rekognition to make gender binary predictions is best
--   suited for use cases where aggregate gender distribution statistics
--   need to be analyzed without identifying specific users. For example,
--   the percentage of female users compared to male users on a social
--   media platform.
--   
--   We don't recommend using gender binary predictions to make decisions
--   that impact  an individual's rights, privacy, or access to services.
--   
--   <i>See:</i> <a>newGender</a> smart constructor.
data Gender
Gender' :: Maybe GenderType -> Maybe Double -> Gender

-- | The predicted gender of the face.
[$sel:value:Gender'] :: Gender -> Maybe GenderType

-- | Level of confidence in the prediction.
[$sel:confidence:Gender'] :: Gender -> Maybe Double

-- | Create a value of <a>Gender</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:value:Gender'</a>, <a>gender_value</a> - The predicted gender
--   of the face.
--   
--   <a>$sel:confidence:Gender'</a>, <a>gender_confidence</a> - Level of
--   confidence in the prediction.
newGender :: Gender

-- | The predicted gender of the face.
gender_value :: Lens' Gender (Maybe GenderType)

-- | Level of confidence in the prediction.
gender_confidence :: Lens' Gender (Maybe Double)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.Gender.Gender
instance GHC.Show.Show Network.AWS.Rekognition.Types.Gender.Gender
instance GHC.Read.Read Network.AWS.Rekognition.Types.Gender.Gender
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.Gender.Gender
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.Gender.Gender
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.Gender.Gender
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.Gender.Gender


module Network.AWS.Rekognition.Types.HumanLoopActivationOutput

-- | Shows the results of the human in the loop evaluation. If there is no
--   HumanLoopArn, the input did not trigger human review.
--   
--   <i>See:</i> <a>newHumanLoopActivationOutput</a> smart constructor.
data HumanLoopActivationOutput
HumanLoopActivationOutput' :: Maybe (NonEmpty Text) -> Maybe Text -> Maybe Text -> HumanLoopActivationOutput

-- | Shows if and why human review was needed.
[$sel:humanLoopActivationReasons:HumanLoopActivationOutput'] :: HumanLoopActivationOutput -> Maybe (NonEmpty Text)

-- | The Amazon Resource Name (ARN) of the HumanLoop created.
[$sel:humanLoopArn:HumanLoopActivationOutput'] :: HumanLoopActivationOutput -> Maybe Text

-- | Shows the result of condition evaluations, including those conditions
--   which activated a human review.
[$sel:humanLoopActivationConditionsEvaluationResults:HumanLoopActivationOutput'] :: HumanLoopActivationOutput -> Maybe Text

-- | Create a value of <a>HumanLoopActivationOutput</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:humanLoopActivationReasons:HumanLoopActivationOutput'</a>,
--   <a>humanLoopActivationOutput_humanLoopActivationReasons</a> - Shows if
--   and why human review was needed.
--   
--   <a>$sel:humanLoopArn:HumanLoopActivationOutput'</a>,
--   <a>humanLoopActivationOutput_humanLoopArn</a> - The Amazon Resource
--   Name (ARN) of the HumanLoop created.
--   
--   
--   <a>$sel:humanLoopActivationConditionsEvaluationResults:HumanLoopActivationOutput'</a>,
--   <a>humanLoopActivationOutput_humanLoopActivationConditionsEvaluationResults</a>
--   - Shows the result of condition evaluations, including those
--   conditions which activated a human review.
newHumanLoopActivationOutput :: HumanLoopActivationOutput

-- | Shows if and why human review was needed.
humanLoopActivationOutput_humanLoopActivationReasons :: Lens' HumanLoopActivationOutput (Maybe (NonEmpty Text))

-- | The Amazon Resource Name (ARN) of the HumanLoop created.
humanLoopActivationOutput_humanLoopArn :: Lens' HumanLoopActivationOutput (Maybe Text)

-- | Shows the result of condition evaluations, including those conditions
--   which activated a human review.
humanLoopActivationOutput_humanLoopActivationConditionsEvaluationResults :: Lens' HumanLoopActivationOutput (Maybe Text)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.HumanLoopActivationOutput.HumanLoopActivationOutput
instance GHC.Show.Show Network.AWS.Rekognition.Types.HumanLoopActivationOutput.HumanLoopActivationOutput
instance GHC.Read.Read Network.AWS.Rekognition.Types.HumanLoopActivationOutput.HumanLoopActivationOutput
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.HumanLoopActivationOutput.HumanLoopActivationOutput
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.HumanLoopActivationOutput.HumanLoopActivationOutput
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.HumanLoopActivationOutput.HumanLoopActivationOutput
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.HumanLoopActivationOutput.HumanLoopActivationOutput


module Network.AWS.Rekognition.Types.HumanLoopDataAttributes

-- | Allows you to set attributes of the image. Currently, you can declare
--   an image as free of personally identifiable information.
--   
--   <i>See:</i> <a>newHumanLoopDataAttributes</a> smart constructor.
data HumanLoopDataAttributes
HumanLoopDataAttributes' :: Maybe [ContentClassifier] -> HumanLoopDataAttributes

-- | Sets whether the input image is free of personally identifiable
--   information.
[$sel:contentClassifiers:HumanLoopDataAttributes'] :: HumanLoopDataAttributes -> Maybe [ContentClassifier]

-- | Create a value of <a>HumanLoopDataAttributes</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:contentClassifiers:HumanLoopDataAttributes'</a>,
--   <a>humanLoopDataAttributes_contentClassifiers</a> - Sets whether the
--   input image is free of personally identifiable information.
newHumanLoopDataAttributes :: HumanLoopDataAttributes

-- | Sets whether the input image is free of personally identifiable
--   information.
humanLoopDataAttributes_contentClassifiers :: Lens' HumanLoopDataAttributes (Maybe [ContentClassifier])
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.HumanLoopDataAttributes.HumanLoopDataAttributes
instance GHC.Show.Show Network.AWS.Rekognition.Types.HumanLoopDataAttributes.HumanLoopDataAttributes
instance GHC.Read.Read Network.AWS.Rekognition.Types.HumanLoopDataAttributes.HumanLoopDataAttributes
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.HumanLoopDataAttributes.HumanLoopDataAttributes
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.HumanLoopDataAttributes.HumanLoopDataAttributes
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.HumanLoopDataAttributes.HumanLoopDataAttributes
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.Types.HumanLoopDataAttributes.HumanLoopDataAttributes


module Network.AWS.Rekognition.Types.HumanLoopConfig

-- | Sets up the flow definition the image will be sent to if one of the
--   conditions is met. You can also set certain attributes of the image
--   before review.
--   
--   <i>See:</i> <a>newHumanLoopConfig</a> smart constructor.
data HumanLoopConfig
HumanLoopConfig' :: Maybe HumanLoopDataAttributes -> Text -> Text -> HumanLoopConfig

-- | Sets attributes of the input data.
[$sel:dataAttributes:HumanLoopConfig'] :: HumanLoopConfig -> Maybe HumanLoopDataAttributes

-- | The name of the human review used for this image. This should be kept
--   unique within a region.
[$sel:humanLoopName:HumanLoopConfig'] :: HumanLoopConfig -> Text

-- | The Amazon Resource Name (ARN) of the flow definition. You can create
--   a flow definition by using the Amazon Sagemaker
--   <a>CreateFlowDefinition</a> Operation.
[$sel:flowDefinitionArn:HumanLoopConfig'] :: HumanLoopConfig -> Text

-- | Create a value of <a>HumanLoopConfig</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:dataAttributes:HumanLoopConfig'</a>,
--   <a>humanLoopConfig_dataAttributes</a> - Sets attributes of the input
--   data.
--   
--   <a>$sel:humanLoopName:HumanLoopConfig'</a>,
--   <a>humanLoopConfig_humanLoopName</a> - The name of the human review
--   used for this image. This should be kept unique within a region.
--   
--   <a>$sel:flowDefinitionArn:HumanLoopConfig'</a>,
--   <a>humanLoopConfig_flowDefinitionArn</a> - The Amazon Resource Name
--   (ARN) of the flow definition. You can create a flow definition by
--   using the Amazon Sagemaker <a>CreateFlowDefinition</a> Operation.
newHumanLoopConfig :: Text -> Text -> HumanLoopConfig

-- | Sets attributes of the input data.
humanLoopConfig_dataAttributes :: Lens' HumanLoopConfig (Maybe HumanLoopDataAttributes)

-- | The name of the human review used for this image. This should be kept
--   unique within a region.
humanLoopConfig_humanLoopName :: Lens' HumanLoopConfig Text

-- | The Amazon Resource Name (ARN) of the flow definition. You can create
--   a flow definition by using the Amazon Sagemaker
--   <a>CreateFlowDefinition</a> Operation.
humanLoopConfig_flowDefinitionArn :: Lens' HumanLoopConfig Text
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.HumanLoopConfig.HumanLoopConfig
instance GHC.Show.Show Network.AWS.Rekognition.Types.HumanLoopConfig.HumanLoopConfig
instance GHC.Read.Read Network.AWS.Rekognition.Types.HumanLoopConfig.HumanLoopConfig
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.HumanLoopConfig.HumanLoopConfig
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.HumanLoopConfig.HumanLoopConfig
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.HumanLoopConfig.HumanLoopConfig
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.Types.HumanLoopConfig.HumanLoopConfig


module Network.AWS.Rekognition.Types.ImageQuality

-- | Identifies face image brightness and sharpness.
--   
--   <i>See:</i> <a>newImageQuality</a> smart constructor.
data ImageQuality
ImageQuality' :: Maybe Double -> Maybe Double -> ImageQuality

-- | Value representing sharpness of the face. The service returns a value
--   between 0 and 100 (inclusive). A higher value indicates a sharper face
--   image.
[$sel:sharpness:ImageQuality'] :: ImageQuality -> Maybe Double

-- | Value representing brightness of the face. The service returns a value
--   between 0 and 100 (inclusive). A higher value indicates a brighter
--   face image.
[$sel:brightness:ImageQuality'] :: ImageQuality -> Maybe Double

-- | Create a value of <a>ImageQuality</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:sharpness:ImageQuality'</a>, <a>imageQuality_sharpness</a> -
--   Value representing sharpness of the face. The service returns a value
--   between 0 and 100 (inclusive). A higher value indicates a sharper face
--   image.
--   
--   <a>$sel:brightness:ImageQuality'</a>, <a>imageQuality_brightness</a> -
--   Value representing brightness of the face. The service returns a value
--   between 0 and 100 (inclusive). A higher value indicates a brighter
--   face image.
newImageQuality :: ImageQuality

-- | Value representing sharpness of the face. The service returns a value
--   between 0 and 100 (inclusive). A higher value indicates a sharper face
--   image.
imageQuality_sharpness :: Lens' ImageQuality (Maybe Double)

-- | Value representing brightness of the face. The service returns a value
--   between 0 and 100 (inclusive). A higher value indicates a brighter
--   face image.
imageQuality_brightness :: Lens' ImageQuality (Maybe Double)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.ImageQuality.ImageQuality
instance GHC.Show.Show Network.AWS.Rekognition.Types.ImageQuality.ImageQuality
instance GHC.Read.Read Network.AWS.Rekognition.Types.ImageQuality.ImageQuality
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.ImageQuality.ImageQuality
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.ImageQuality.ImageQuality
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.ImageQuality.ImageQuality
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.ImageQuality.ImageQuality


module Network.AWS.Rekognition.Types.Instance

-- | An instance of a label returned by Amazon Rekognition Image
--   (DetectLabels) or by Amazon Rekognition Video (GetLabelDetection).
--   
--   <i>See:</i> <a>newInstance</a> smart constructor.
data Instance
Instance' :: Maybe BoundingBox -> Maybe Double -> Instance

-- | The position of the label instance on the image.
[$sel:boundingBox:Instance'] :: Instance -> Maybe BoundingBox

-- | The confidence that Amazon Rekognition has in the accuracy of the
--   bounding box.
[$sel:confidence:Instance'] :: Instance -> Maybe Double

-- | Create a value of <a>Instance</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:boundingBox:Instance'</a>, <a>instance_boundingBox</a> - The
--   position of the label instance on the image.
--   
--   <a>$sel:confidence:Instance'</a>, <a>instance_confidence</a> - The
--   confidence that Amazon Rekognition has in the accuracy of the bounding
--   box.
newInstance :: Instance

-- | The position of the label instance on the image.
instance_boundingBox :: Lens' Instance (Maybe BoundingBox)

-- | The confidence that Amazon Rekognition has in the accuracy of the
--   bounding box.
instance_confidence :: Lens' Instance (Maybe Double)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.Instance.Instance
instance GHC.Show.Show Network.AWS.Rekognition.Types.Instance.Instance
instance GHC.Read.Read Network.AWS.Rekognition.Types.Instance.Instance
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.Instance.Instance
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.Instance.Instance
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.Instance.Instance
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.Instance.Instance


module Network.AWS.Rekognition.Types.KinesisDataStream

-- | The Kinesis data stream Amazon Rekognition to which the analysis
--   results of a Amazon Rekognition stream processor are streamed. For
--   more information, see CreateStreamProcessor in the Amazon Rekognition
--   Developer Guide.
--   
--   <i>See:</i> <a>newKinesisDataStream</a> smart constructor.
data KinesisDataStream
KinesisDataStream' :: Maybe Text -> KinesisDataStream

-- | ARN of the output Amazon Kinesis Data Streams stream.
[$sel:arn:KinesisDataStream'] :: KinesisDataStream -> Maybe Text

-- | Create a value of <a>KinesisDataStream</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:arn:KinesisDataStream'</a>, <a>kinesisDataStream_arn</a> - ARN
--   of the output Amazon Kinesis Data Streams stream.
newKinesisDataStream :: KinesisDataStream

-- | ARN of the output Amazon Kinesis Data Streams stream.
kinesisDataStream_arn :: Lens' KinesisDataStream (Maybe Text)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.KinesisDataStream.KinesisDataStream
instance GHC.Show.Show Network.AWS.Rekognition.Types.KinesisDataStream.KinesisDataStream
instance GHC.Read.Read Network.AWS.Rekognition.Types.KinesisDataStream.KinesisDataStream
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.KinesisDataStream.KinesisDataStream
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.KinesisDataStream.KinesisDataStream
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.KinesisDataStream.KinesisDataStream
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.KinesisDataStream.KinesisDataStream
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.Types.KinesisDataStream.KinesisDataStream


module Network.AWS.Rekognition.Types.KinesisVideoStream

-- | Kinesis video stream stream that provides the source streaming video
--   for a Amazon Rekognition Video stream processor. For more information,
--   see CreateStreamProcessor in the Amazon Rekognition Developer Guide.
--   
--   <i>See:</i> <a>newKinesisVideoStream</a> smart constructor.
data KinesisVideoStream
KinesisVideoStream' :: Maybe Text -> KinesisVideoStream

-- | ARN of the Kinesis video stream stream that streams the source video.
[$sel:arn:KinesisVideoStream'] :: KinesisVideoStream -> Maybe Text

-- | Create a value of <a>KinesisVideoStream</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:arn:KinesisVideoStream'</a>, <a>kinesisVideoStream_arn</a> -
--   ARN of the Kinesis video stream stream that streams the source video.
newKinesisVideoStream :: KinesisVideoStream

-- | ARN of the Kinesis video stream stream that streams the source video.
kinesisVideoStream_arn :: Lens' KinesisVideoStream (Maybe Text)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.KinesisVideoStream.KinesisVideoStream
instance GHC.Show.Show Network.AWS.Rekognition.Types.KinesisVideoStream.KinesisVideoStream
instance GHC.Read.Read Network.AWS.Rekognition.Types.KinesisVideoStream.KinesisVideoStream
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.KinesisVideoStream.KinesisVideoStream
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.KinesisVideoStream.KinesisVideoStream
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.KinesisVideoStream.KinesisVideoStream
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.KinesisVideoStream.KinesisVideoStream
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.Types.KinesisVideoStream.KinesisVideoStream


module Network.AWS.Rekognition.Types.KnownGenderType

-- | A list of enum string of possible gender values that Celebrity
--   returns.
newtype KnownGenderType
KnownGenderType' :: Text -> KnownGenderType
[fromKnownGenderType] :: KnownGenderType -> Text
pattern KnownGenderType_Female :: KnownGenderType
pattern KnownGenderType_Male :: KnownGenderType
instance Network.AWS.Data.XML.ToXML Network.AWS.Rekognition.Types.KnownGenderType.KnownGenderType
instance Network.AWS.Data.XML.FromXML Network.AWS.Rekognition.Types.KnownGenderType.KnownGenderType
instance Data.Aeson.Types.ToJSON.ToJSONKey Network.AWS.Rekognition.Types.KnownGenderType.KnownGenderType
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.Types.KnownGenderType.KnownGenderType
instance Data.Aeson.Types.FromJSON.FromJSONKey Network.AWS.Rekognition.Types.KnownGenderType.KnownGenderType
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.KnownGenderType.KnownGenderType
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.Types.KnownGenderType.KnownGenderType
instance Network.AWS.Data.Headers.ToHeader Network.AWS.Rekognition.Types.KnownGenderType.KnownGenderType
instance Network.AWS.Data.Log.ToLog Network.AWS.Rekognition.Types.KnownGenderType.KnownGenderType
instance Network.AWS.Data.ByteString.ToByteString Network.AWS.Rekognition.Types.KnownGenderType.KnownGenderType
instance Network.AWS.Data.Text.ToText Network.AWS.Rekognition.Types.KnownGenderType.KnownGenderType
instance Network.AWS.Data.Text.FromText Network.AWS.Rekognition.Types.KnownGenderType.KnownGenderType
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.KnownGenderType.KnownGenderType
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.KnownGenderType.KnownGenderType
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.KnownGenderType.KnownGenderType
instance GHC.Classes.Ord Network.AWS.Rekognition.Types.KnownGenderType.KnownGenderType
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.KnownGenderType.KnownGenderType
instance GHC.Read.Read Network.AWS.Rekognition.Types.KnownGenderType.KnownGenderType
instance GHC.Show.Show Network.AWS.Rekognition.Types.KnownGenderType.KnownGenderType


module Network.AWS.Rekognition.Types.KnownGender

-- | The known gender identity for the celebrity that matches the provided
--   ID.
--   
--   <i>See:</i> <a>newKnownGender</a> smart constructor.
data KnownGender
KnownGender' :: Maybe KnownGenderType -> KnownGender

-- | A string value of the KnownGender info about the Celebrity.
[$sel:type':KnownGender'] :: KnownGender -> Maybe KnownGenderType

-- | Create a value of <a>KnownGender</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:type':KnownGender'</a>, <a>knownGender_type</a> - A string
--   value of the KnownGender info about the Celebrity.
newKnownGender :: KnownGender

-- | A string value of the KnownGender info about the Celebrity.
knownGender_type :: Lens' KnownGender (Maybe KnownGenderType)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.KnownGender.KnownGender
instance GHC.Show.Show Network.AWS.Rekognition.Types.KnownGender.KnownGender
instance GHC.Read.Read Network.AWS.Rekognition.Types.KnownGender.KnownGender
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.KnownGender.KnownGender
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.KnownGender.KnownGender
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.KnownGender.KnownGender
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.KnownGender.KnownGender


module Network.AWS.Rekognition.Types.LabelDetectionSortBy
newtype LabelDetectionSortBy
LabelDetectionSortBy' :: Text -> LabelDetectionSortBy
[fromLabelDetectionSortBy] :: LabelDetectionSortBy -> Text
pattern LabelDetectionSortBy_NAME :: LabelDetectionSortBy
pattern LabelDetectionSortBy_TIMESTAMP :: LabelDetectionSortBy
instance Network.AWS.Data.XML.ToXML Network.AWS.Rekognition.Types.LabelDetectionSortBy.LabelDetectionSortBy
instance Network.AWS.Data.XML.FromXML Network.AWS.Rekognition.Types.LabelDetectionSortBy.LabelDetectionSortBy
instance Data.Aeson.Types.ToJSON.ToJSONKey Network.AWS.Rekognition.Types.LabelDetectionSortBy.LabelDetectionSortBy
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.Types.LabelDetectionSortBy.LabelDetectionSortBy
instance Data.Aeson.Types.FromJSON.FromJSONKey Network.AWS.Rekognition.Types.LabelDetectionSortBy.LabelDetectionSortBy
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.LabelDetectionSortBy.LabelDetectionSortBy
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.Types.LabelDetectionSortBy.LabelDetectionSortBy
instance Network.AWS.Data.Headers.ToHeader Network.AWS.Rekognition.Types.LabelDetectionSortBy.LabelDetectionSortBy
instance Network.AWS.Data.Log.ToLog Network.AWS.Rekognition.Types.LabelDetectionSortBy.LabelDetectionSortBy
instance Network.AWS.Data.ByteString.ToByteString Network.AWS.Rekognition.Types.LabelDetectionSortBy.LabelDetectionSortBy
instance Network.AWS.Data.Text.ToText Network.AWS.Rekognition.Types.LabelDetectionSortBy.LabelDetectionSortBy
instance Network.AWS.Data.Text.FromText Network.AWS.Rekognition.Types.LabelDetectionSortBy.LabelDetectionSortBy
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.LabelDetectionSortBy.LabelDetectionSortBy
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.LabelDetectionSortBy.LabelDetectionSortBy
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.LabelDetectionSortBy.LabelDetectionSortBy
instance GHC.Classes.Ord Network.AWS.Rekognition.Types.LabelDetectionSortBy.LabelDetectionSortBy
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.LabelDetectionSortBy.LabelDetectionSortBy
instance GHC.Read.Read Network.AWS.Rekognition.Types.LabelDetectionSortBy.LabelDetectionSortBy
instance GHC.Show.Show Network.AWS.Rekognition.Types.LabelDetectionSortBy.LabelDetectionSortBy


module Network.AWS.Rekognition.Types.LandmarkType
newtype LandmarkType
LandmarkType' :: Text -> LandmarkType
[fromLandmarkType] :: LandmarkType -> Text
pattern LandmarkType_ChinBottom :: LandmarkType
pattern LandmarkType_EyeLeft :: LandmarkType
pattern LandmarkType_EyeRight :: LandmarkType
pattern LandmarkType_LeftEyeBrowLeft :: LandmarkType
pattern LandmarkType_LeftEyeBrowRight :: LandmarkType
pattern LandmarkType_LeftEyeBrowUp :: LandmarkType
pattern LandmarkType_LeftEyeDown :: LandmarkType
pattern LandmarkType_LeftEyeLeft :: LandmarkType
pattern LandmarkType_LeftEyeRight :: LandmarkType
pattern LandmarkType_LeftEyeUp :: LandmarkType
pattern LandmarkType_LeftPupil :: LandmarkType
pattern LandmarkType_MidJawlineLeft :: LandmarkType
pattern LandmarkType_MidJawlineRight :: LandmarkType
pattern LandmarkType_MouthDown :: LandmarkType
pattern LandmarkType_MouthLeft :: LandmarkType
pattern LandmarkType_MouthRight :: LandmarkType
pattern LandmarkType_MouthUp :: LandmarkType
pattern LandmarkType_Nose :: LandmarkType
pattern LandmarkType_NoseLeft :: LandmarkType
pattern LandmarkType_NoseRight :: LandmarkType
pattern LandmarkType_RightEyeBrowLeft :: LandmarkType
pattern LandmarkType_RightEyeBrowRight :: LandmarkType
pattern LandmarkType_RightEyeBrowUp :: LandmarkType
pattern LandmarkType_RightEyeDown :: LandmarkType
pattern LandmarkType_RightEyeLeft :: LandmarkType
pattern LandmarkType_RightEyeRight :: LandmarkType
pattern LandmarkType_RightEyeUp :: LandmarkType
pattern LandmarkType_RightPupil :: LandmarkType
pattern LandmarkType_UpperJawlineLeft :: LandmarkType
pattern LandmarkType_UpperJawlineRight :: LandmarkType
instance Network.AWS.Data.XML.ToXML Network.AWS.Rekognition.Types.LandmarkType.LandmarkType
instance Network.AWS.Data.XML.FromXML Network.AWS.Rekognition.Types.LandmarkType.LandmarkType
instance Data.Aeson.Types.ToJSON.ToJSONKey Network.AWS.Rekognition.Types.LandmarkType.LandmarkType
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.Types.LandmarkType.LandmarkType
instance Data.Aeson.Types.FromJSON.FromJSONKey Network.AWS.Rekognition.Types.LandmarkType.LandmarkType
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.LandmarkType.LandmarkType
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.Types.LandmarkType.LandmarkType
instance Network.AWS.Data.Headers.ToHeader Network.AWS.Rekognition.Types.LandmarkType.LandmarkType
instance Network.AWS.Data.Log.ToLog Network.AWS.Rekognition.Types.LandmarkType.LandmarkType
instance Network.AWS.Data.ByteString.ToByteString Network.AWS.Rekognition.Types.LandmarkType.LandmarkType
instance Network.AWS.Data.Text.ToText Network.AWS.Rekognition.Types.LandmarkType.LandmarkType
instance Network.AWS.Data.Text.FromText Network.AWS.Rekognition.Types.LandmarkType.LandmarkType
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.LandmarkType.LandmarkType
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.LandmarkType.LandmarkType
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.LandmarkType.LandmarkType
instance GHC.Classes.Ord Network.AWS.Rekognition.Types.LandmarkType.LandmarkType
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.LandmarkType.LandmarkType
instance GHC.Read.Read Network.AWS.Rekognition.Types.LandmarkType.LandmarkType
instance GHC.Show.Show Network.AWS.Rekognition.Types.LandmarkType.LandmarkType


module Network.AWS.Rekognition.Types.Landmark

-- | Indicates the location of the landmark on the face.
--   
--   <i>See:</i> <a>newLandmark</a> smart constructor.
data Landmark
Landmark' :: Maybe LandmarkType -> Maybe Double -> Maybe Double -> Landmark

-- | Type of landmark.
[$sel:type':Landmark'] :: Landmark -> Maybe LandmarkType

-- | The x-coordinate of the landmark expressed as a ratio of the width of
--   the image. The x-coordinate is measured from the left-side of the
--   image. For example, if the image is 700 pixels wide and the
--   x-coordinate of the landmark is at 350 pixels, this value is 0.5.
[$sel:x:Landmark'] :: Landmark -> Maybe Double

-- | The y-coordinate of the landmark expressed as a ratio of the height of
--   the image. The y-coordinate is measured from the top of the image. For
--   example, if the image height is 200 pixels and the y-coordinate of the
--   landmark is at 50 pixels, this value is 0.25.
[$sel:y:Landmark'] :: Landmark -> Maybe Double

-- | Create a value of <a>Landmark</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:type':Landmark'</a>, <a>landmark_type</a> - Type of landmark.
--   
--   <a>$sel:x:Landmark'</a>, <a>landmark_x</a> - The x-coordinate of the
--   landmark expressed as a ratio of the width of the image. The
--   x-coordinate is measured from the left-side of the image. For example,
--   if the image is 700 pixels wide and the x-coordinate of the landmark
--   is at 350 pixels, this value is 0.5.
--   
--   <a>$sel:y:Landmark'</a>, <a>landmark_y</a> - The y-coordinate of the
--   landmark expressed as a ratio of the height of the image. The
--   y-coordinate is measured from the top of the image. For example, if
--   the image height is 200 pixels and the y-coordinate of the landmark is
--   at 50 pixels, this value is 0.25.
newLandmark :: Landmark

-- | Type of landmark.
landmark_type :: Lens' Landmark (Maybe LandmarkType)

-- | The x-coordinate of the landmark expressed as a ratio of the width of
--   the image. The x-coordinate is measured from the left-side of the
--   image. For example, if the image is 700 pixels wide and the
--   x-coordinate of the landmark is at 350 pixels, this value is 0.5.
landmark_x :: Lens' Landmark (Maybe Double)

-- | The y-coordinate of the landmark expressed as a ratio of the height of
--   the image. The y-coordinate is measured from the top of the image. For
--   example, if the image height is 200 pixels and the y-coordinate of the
--   landmark is at 50 pixels, this value is 0.25.
landmark_y :: Lens' Landmark (Maybe Double)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.Landmark.Landmark
instance GHC.Show.Show Network.AWS.Rekognition.Types.Landmark.Landmark
instance GHC.Read.Read Network.AWS.Rekognition.Types.Landmark.Landmark
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.Landmark.Landmark
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.Landmark.Landmark
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.Landmark.Landmark
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.Landmark.Landmark


module Network.AWS.Rekognition.Types.ModerationLabel

-- | Provides information about a single type of inappropriate, unwanted,
--   or offensive content found in an image or video. Each type of
--   moderated content has a label within a hierarchical taxonomy. For more
--   information, see Content moderation in the Amazon Rekognition
--   Developer Guide.
--   
--   <i>See:</i> <a>newModerationLabel</a> smart constructor.
data ModerationLabel
ModerationLabel' :: Maybe Double -> Maybe Text -> Maybe Text -> ModerationLabel

-- | Specifies the confidence that Amazon Rekognition has that the label
--   has been correctly identified.
--   
--   If you don't specify the <tt>MinConfidence</tt> parameter in the call
--   to <tt>DetectModerationLabels</tt>, the operation returns labels with
--   a confidence value greater than or equal to 50 percent.
[$sel:confidence:ModerationLabel'] :: ModerationLabel -> Maybe Double

-- | The label name for the type of unsafe content detected in the image.
[$sel:name:ModerationLabel'] :: ModerationLabel -> Maybe Text

-- | The name for the parent label. Labels at the top level of the
--   hierarchy have the parent label <tt>""</tt>.
[$sel:parentName:ModerationLabel'] :: ModerationLabel -> Maybe Text

-- | Create a value of <a>ModerationLabel</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:confidence:ModerationLabel'</a>,
--   <a>moderationLabel_confidence</a> - Specifies the confidence that
--   Amazon Rekognition has that the label has been correctly identified.
--   
--   If you don't specify the <tt>MinConfidence</tt> parameter in the call
--   to <tt>DetectModerationLabels</tt>, the operation returns labels with
--   a confidence value greater than or equal to 50 percent.
--   
--   <a>$sel:name:ModerationLabel'</a>, <a>moderationLabel_name</a> - The
--   label name for the type of unsafe content detected in the image.
--   
--   <a>$sel:parentName:ModerationLabel'</a>,
--   <a>moderationLabel_parentName</a> - The name for the parent label.
--   Labels at the top level of the hierarchy have the parent label
--   <tt>""</tt>.
newModerationLabel :: ModerationLabel

-- | Specifies the confidence that Amazon Rekognition has that the label
--   has been correctly identified.
--   
--   If you don't specify the <tt>MinConfidence</tt> parameter in the call
--   to <tt>DetectModerationLabels</tt>, the operation returns labels with
--   a confidence value greater than or equal to 50 percent.
moderationLabel_confidence :: Lens' ModerationLabel (Maybe Double)

-- | The label name for the type of unsafe content detected in the image.
moderationLabel_name :: Lens' ModerationLabel (Maybe Text)

-- | The name for the parent label. Labels at the top level of the
--   hierarchy have the parent label <tt>""</tt>.
moderationLabel_parentName :: Lens' ModerationLabel (Maybe Text)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.ModerationLabel.ModerationLabel
instance GHC.Show.Show Network.AWS.Rekognition.Types.ModerationLabel.ModerationLabel
instance GHC.Read.Read Network.AWS.Rekognition.Types.ModerationLabel.ModerationLabel
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.ModerationLabel.ModerationLabel
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.ModerationLabel.ModerationLabel
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.ModerationLabel.ModerationLabel
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.ModerationLabel.ModerationLabel


module Network.AWS.Rekognition.Types.ContentModerationDetection

-- | Information about an inappropriate, unwanted, or offensive content
--   label detection in a stored video.
--   
--   <i>See:</i> <a>newContentModerationDetection</a> smart constructor.
data ContentModerationDetection
ContentModerationDetection' :: Maybe ModerationLabel -> Maybe Integer -> ContentModerationDetection

-- | The content moderation label detected by in the stored video.
[$sel:moderationLabel:ContentModerationDetection'] :: ContentModerationDetection -> Maybe ModerationLabel

-- | Time, in milliseconds from the beginning of the video, that the
--   content moderation label was detected.
[$sel:timestamp:ContentModerationDetection'] :: ContentModerationDetection -> Maybe Integer

-- | Create a value of <a>ContentModerationDetection</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:moderationLabel:ContentModerationDetection'</a>,
--   <a>contentModerationDetection_moderationLabel</a> - The content
--   moderation label detected by in the stored video.
--   
--   <a>$sel:timestamp:ContentModerationDetection'</a>,
--   <a>contentModerationDetection_timestamp</a> - Time, in milliseconds
--   from the beginning of the video, that the content moderation label was
--   detected.
newContentModerationDetection :: ContentModerationDetection

-- | The content moderation label detected by in the stored video.
contentModerationDetection_moderationLabel :: Lens' ContentModerationDetection (Maybe ModerationLabel)

-- | Time, in milliseconds from the beginning of the video, that the
--   content moderation label was detected.
contentModerationDetection_timestamp :: Lens' ContentModerationDetection (Maybe Integer)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.ContentModerationDetection.ContentModerationDetection
instance GHC.Show.Show Network.AWS.Rekognition.Types.ContentModerationDetection.ContentModerationDetection
instance GHC.Read.Read Network.AWS.Rekognition.Types.ContentModerationDetection.ContentModerationDetection
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.ContentModerationDetection.ContentModerationDetection
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.ContentModerationDetection.ContentModerationDetection
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.ContentModerationDetection.ContentModerationDetection
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.ContentModerationDetection.ContentModerationDetection


module Network.AWS.Rekognition.Types.MouthOpen

-- | Indicates whether or not the mouth on the face is open, and the
--   confidence level in the determination.
--   
--   <i>See:</i> <a>newMouthOpen</a> smart constructor.
data MouthOpen
MouthOpen' :: Maybe Bool -> Maybe Double -> MouthOpen

-- | Boolean value that indicates whether the mouth on the face is open or
--   not.
[$sel:value:MouthOpen'] :: MouthOpen -> Maybe Bool

-- | Level of confidence in the determination.
[$sel:confidence:MouthOpen'] :: MouthOpen -> Maybe Double

-- | Create a value of <a>MouthOpen</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:value:MouthOpen'</a>, <a>mouthOpen_value</a> - Boolean value
--   that indicates whether the mouth on the face is open or not.
--   
--   <a>$sel:confidence:MouthOpen'</a>, <a>mouthOpen_confidence</a> - Level
--   of confidence in the determination.
newMouthOpen :: MouthOpen

-- | Boolean value that indicates whether the mouth on the face is open or
--   not.
mouthOpen_value :: Lens' MouthOpen (Maybe Bool)

-- | Level of confidence in the determination.
mouthOpen_confidence :: Lens' MouthOpen (Maybe Double)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.MouthOpen.MouthOpen
instance GHC.Show.Show Network.AWS.Rekognition.Types.MouthOpen.MouthOpen
instance GHC.Read.Read Network.AWS.Rekognition.Types.MouthOpen.MouthOpen
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.MouthOpen.MouthOpen
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.MouthOpen.MouthOpen
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.MouthOpen.MouthOpen
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.MouthOpen.MouthOpen


module Network.AWS.Rekognition.Types.Mustache

-- | Indicates whether or not the face has a mustache, and the confidence
--   level in the determination.
--   
--   <i>See:</i> <a>newMustache</a> smart constructor.
data Mustache
Mustache' :: Maybe Bool -> Maybe Double -> Mustache

-- | Boolean value that indicates whether the face has mustache or not.
[$sel:value:Mustache'] :: Mustache -> Maybe Bool

-- | Level of confidence in the determination.
[$sel:confidence:Mustache'] :: Mustache -> Maybe Double

-- | Create a value of <a>Mustache</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:value:Mustache'</a>, <a>mustache_value</a> - Boolean value
--   that indicates whether the face has mustache or not.
--   
--   <a>$sel:confidence:Mustache'</a>, <a>mustache_confidence</a> - Level
--   of confidence in the determination.
newMustache :: Mustache

-- | Boolean value that indicates whether the face has mustache or not.
mustache_value :: Lens' Mustache (Maybe Bool)

-- | Level of confidence in the determination.
mustache_confidence :: Lens' Mustache (Maybe Double)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.Mustache.Mustache
instance GHC.Show.Show Network.AWS.Rekognition.Types.Mustache.Mustache
instance GHC.Read.Read Network.AWS.Rekognition.Types.Mustache.Mustache
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.Mustache.Mustache
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.Mustache.Mustache
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.Mustache.Mustache
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.Mustache.Mustache


module Network.AWS.Rekognition.Types.NotificationChannel

-- | The Amazon Simple Notification Service topic to which Amazon
--   Rekognition publishes the completion status of a video analysis
--   operation. For more information, see api-video. Note that the Amazon
--   SNS topic must have a topic name that begins with
--   <i>AmazonRekognition</i> if you are using the
--   AmazonRekognitionServiceRole permissions policy to access the topic.
--   For more information, see <a>Giving access to multiple Amazon SNS
--   topics</a>.
--   
--   <i>See:</i> <a>newNotificationChannel</a> smart constructor.
data NotificationChannel
NotificationChannel' :: Text -> Text -> NotificationChannel

-- | The Amazon SNS topic to which Amazon Rekognition to posts the
--   completion status.
[$sel:sNSTopicArn:NotificationChannel'] :: NotificationChannel -> Text

-- | The ARN of an IAM role that gives Amazon Rekognition publishing
--   permissions to the Amazon SNS topic.
[$sel:roleArn:NotificationChannel'] :: NotificationChannel -> Text

-- | Create a value of <a>NotificationChannel</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:sNSTopicArn:NotificationChannel'</a>,
--   <a>notificationChannel_sNSTopicArn</a> - The Amazon SNS topic to which
--   Amazon Rekognition to posts the completion status.
--   
--   <a>$sel:roleArn:NotificationChannel'</a>,
--   <a>notificationChannel_roleArn</a> - The ARN of an IAM role that gives
--   Amazon Rekognition publishing permissions to the Amazon SNS topic.
newNotificationChannel :: Text -> Text -> NotificationChannel

-- | The Amazon SNS topic to which Amazon Rekognition to posts the
--   completion status.
notificationChannel_sNSTopicArn :: Lens' NotificationChannel Text

-- | The ARN of an IAM role that gives Amazon Rekognition publishing
--   permissions to the Amazon SNS topic.
notificationChannel_roleArn :: Lens' NotificationChannel Text
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.NotificationChannel.NotificationChannel
instance GHC.Show.Show Network.AWS.Rekognition.Types.NotificationChannel.NotificationChannel
instance GHC.Read.Read Network.AWS.Rekognition.Types.NotificationChannel.NotificationChannel
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.NotificationChannel.NotificationChannel
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.NotificationChannel.NotificationChannel
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.NotificationChannel.NotificationChannel
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.Types.NotificationChannel.NotificationChannel


module Network.AWS.Rekognition.Types.OrientationCorrection
newtype OrientationCorrection
OrientationCorrection' :: Text -> OrientationCorrection
[fromOrientationCorrection] :: OrientationCorrection -> Text
pattern OrientationCorrection_ROTATE_0 :: OrientationCorrection
pattern OrientationCorrection_ROTATE_180 :: OrientationCorrection
pattern OrientationCorrection_ROTATE_270 :: OrientationCorrection
pattern OrientationCorrection_ROTATE_90 :: OrientationCorrection
instance Network.AWS.Data.XML.ToXML Network.AWS.Rekognition.Types.OrientationCorrection.OrientationCorrection
instance Network.AWS.Data.XML.FromXML Network.AWS.Rekognition.Types.OrientationCorrection.OrientationCorrection
instance Data.Aeson.Types.ToJSON.ToJSONKey Network.AWS.Rekognition.Types.OrientationCorrection.OrientationCorrection
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.Types.OrientationCorrection.OrientationCorrection
instance Data.Aeson.Types.FromJSON.FromJSONKey Network.AWS.Rekognition.Types.OrientationCorrection.OrientationCorrection
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.OrientationCorrection.OrientationCorrection
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.Types.OrientationCorrection.OrientationCorrection
instance Network.AWS.Data.Headers.ToHeader Network.AWS.Rekognition.Types.OrientationCorrection.OrientationCorrection
instance Network.AWS.Data.Log.ToLog Network.AWS.Rekognition.Types.OrientationCorrection.OrientationCorrection
instance Network.AWS.Data.ByteString.ToByteString Network.AWS.Rekognition.Types.OrientationCorrection.OrientationCorrection
instance Network.AWS.Data.Text.ToText Network.AWS.Rekognition.Types.OrientationCorrection.OrientationCorrection
instance Network.AWS.Data.Text.FromText Network.AWS.Rekognition.Types.OrientationCorrection.OrientationCorrection
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.OrientationCorrection.OrientationCorrection
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.OrientationCorrection.OrientationCorrection
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.OrientationCorrection.OrientationCorrection
instance GHC.Classes.Ord Network.AWS.Rekognition.Types.OrientationCorrection.OrientationCorrection
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.OrientationCorrection.OrientationCorrection
instance GHC.Read.Read Network.AWS.Rekognition.Types.OrientationCorrection.OrientationCorrection
instance GHC.Show.Show Network.AWS.Rekognition.Types.OrientationCorrection.OrientationCorrection


module Network.AWS.Rekognition.Types.OutputConfig

-- | The S3 bucket and folder location where training output is placed.
--   
--   <i>See:</i> <a>newOutputConfig</a> smart constructor.
data OutputConfig
OutputConfig' :: Maybe Text -> Maybe Text -> OutputConfig

-- | The prefix applied to the training output files.
[$sel:s3KeyPrefix:OutputConfig'] :: OutputConfig -> Maybe Text

-- | The S3 bucket where training output is placed.
[$sel:s3Bucket:OutputConfig'] :: OutputConfig -> Maybe Text

-- | Create a value of <a>OutputConfig</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:s3KeyPrefix:OutputConfig'</a>, <a>outputConfig_s3KeyPrefix</a>
--   - The prefix applied to the training output files.
--   
--   <a>$sel:s3Bucket:OutputConfig'</a>, <a>outputConfig_s3Bucket</a> - The
--   S3 bucket where training output is placed.
newOutputConfig :: OutputConfig

-- | The prefix applied to the training output files.
outputConfig_s3KeyPrefix :: Lens' OutputConfig (Maybe Text)

-- | The S3 bucket where training output is placed.
outputConfig_s3Bucket :: Lens' OutputConfig (Maybe Text)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.OutputConfig.OutputConfig
instance GHC.Show.Show Network.AWS.Rekognition.Types.OutputConfig.OutputConfig
instance GHC.Read.Read Network.AWS.Rekognition.Types.OutputConfig.OutputConfig
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.OutputConfig.OutputConfig
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.OutputConfig.OutputConfig
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.OutputConfig.OutputConfig
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.OutputConfig.OutputConfig
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.Types.OutputConfig.OutputConfig


module Network.AWS.Rekognition.Types.Parent

-- | A parent label for a label. A label can have 0, 1, or more parents.
--   
--   <i>See:</i> <a>newParent</a> smart constructor.
data Parent
Parent' :: Maybe Text -> Parent

-- | The name of the parent label.
[$sel:name:Parent'] :: Parent -> Maybe Text

-- | Create a value of <a>Parent</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:name:Parent'</a>, <a>parent_name</a> - The name of the parent
--   label.
newParent :: Parent

-- | The name of the parent label.
parent_name :: Lens' Parent (Maybe Text)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.Parent.Parent
instance GHC.Show.Show Network.AWS.Rekognition.Types.Parent.Parent
instance GHC.Read.Read Network.AWS.Rekognition.Types.Parent.Parent
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.Parent.Parent
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.Parent.Parent
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.Parent.Parent
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.Parent.Parent


module Network.AWS.Rekognition.Types.Label

-- | Structure containing details about the detected label, including the
--   name, detected instances, parent labels, and level of confidence.
--   
--   <i>See:</i> <a>newLabel</a> smart constructor.
data Label
Label' :: Maybe Double -> Maybe [Parent] -> Maybe Text -> Maybe [Instance] -> Label

-- | Level of confidence.
[$sel:confidence:Label'] :: Label -> Maybe Double

-- | The parent labels for a label. The response includes all ancestor
--   labels.
[$sel:parents:Label'] :: Label -> Maybe [Parent]

-- | The name (label) of the object or scene.
[$sel:name:Label'] :: Label -> Maybe Text

-- | If <tt>Label</tt> represents an object, <tt>Instances</tt> contains
--   the bounding boxes for each instance of the detected object. Bounding
--   boxes are returned for common object labels such as people, cars,
--   furniture, apparel or pets.
[$sel:instances:Label'] :: Label -> Maybe [Instance]

-- | Create a value of <a>Label</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:confidence:Label'</a>, <a>label_confidence</a> - Level of
--   confidence.
--   
--   <a>$sel:parents:Label'</a>, <a>label_parents</a> - The parent labels
--   for a label. The response includes all ancestor labels.
--   
--   <a>$sel:name:Label'</a>, <a>label_name</a> - The name (label) of the
--   object or scene.
--   
--   <a>$sel:instances:Label'</a>, <a>label_instances</a> - If
--   <tt>Label</tt> represents an object, <tt>Instances</tt> contains the
--   bounding boxes for each instance of the detected object. Bounding
--   boxes are returned for common object labels such as people, cars,
--   furniture, apparel or pets.
newLabel :: Label

-- | Level of confidence.
label_confidence :: Lens' Label (Maybe Double)

-- | The parent labels for a label. The response includes all ancestor
--   labels.
label_parents :: Lens' Label (Maybe [Parent])

-- | The name (label) of the object or scene.
label_name :: Lens' Label (Maybe Text)

-- | If <tt>Label</tt> represents an object, <tt>Instances</tt> contains
--   the bounding boxes for each instance of the detected object. Bounding
--   boxes are returned for common object labels such as people, cars,
--   furniture, apparel or pets.
label_instances :: Lens' Label (Maybe [Instance])
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.Label.Label
instance GHC.Show.Show Network.AWS.Rekognition.Types.Label.Label
instance GHC.Read.Read Network.AWS.Rekognition.Types.Label.Label
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.Label.Label
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.Label.Label
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.Label.Label
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.Label.Label


module Network.AWS.Rekognition.Types.LabelDetection

-- | Information about a label detected in a video analysis request and the
--   time the label was detected in the video.
--   
--   <i>See:</i> <a>newLabelDetection</a> smart constructor.
data LabelDetection
LabelDetection' :: Maybe Label -> Maybe Integer -> LabelDetection

-- | Details about the detected label.
[$sel:label:LabelDetection'] :: LabelDetection -> Maybe Label

-- | Time, in milliseconds from the start of the video, that the label was
--   detected.
[$sel:timestamp:LabelDetection'] :: LabelDetection -> Maybe Integer

-- | Create a value of <a>LabelDetection</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:label:LabelDetection'</a>, <a>labelDetection_label</a> -
--   Details about the detected label.
--   
--   <a>$sel:timestamp:LabelDetection'</a>, <a>labelDetection_timestamp</a>
--   - Time, in milliseconds from the start of the video, that the label
--   was detected.
newLabelDetection :: LabelDetection

-- | Details about the detected label.
labelDetection_label :: Lens' LabelDetection (Maybe Label)

-- | Time, in milliseconds from the start of the video, that the label was
--   detected.
labelDetection_timestamp :: Lens' LabelDetection (Maybe Integer)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.LabelDetection.LabelDetection
instance GHC.Show.Show Network.AWS.Rekognition.Types.LabelDetection.LabelDetection
instance GHC.Read.Read Network.AWS.Rekognition.Types.LabelDetection.LabelDetection
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.LabelDetection.LabelDetection
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.LabelDetection.LabelDetection
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.LabelDetection.LabelDetection
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.LabelDetection.LabelDetection


module Network.AWS.Rekognition.Types.PersonTrackingSortBy
newtype PersonTrackingSortBy
PersonTrackingSortBy' :: Text -> PersonTrackingSortBy
[fromPersonTrackingSortBy] :: PersonTrackingSortBy -> Text
pattern PersonTrackingSortBy_INDEX :: PersonTrackingSortBy
pattern PersonTrackingSortBy_TIMESTAMP :: PersonTrackingSortBy
instance Network.AWS.Data.XML.ToXML Network.AWS.Rekognition.Types.PersonTrackingSortBy.PersonTrackingSortBy
instance Network.AWS.Data.XML.FromXML Network.AWS.Rekognition.Types.PersonTrackingSortBy.PersonTrackingSortBy
instance Data.Aeson.Types.ToJSON.ToJSONKey Network.AWS.Rekognition.Types.PersonTrackingSortBy.PersonTrackingSortBy
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.Types.PersonTrackingSortBy.PersonTrackingSortBy
instance Data.Aeson.Types.FromJSON.FromJSONKey Network.AWS.Rekognition.Types.PersonTrackingSortBy.PersonTrackingSortBy
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.PersonTrackingSortBy.PersonTrackingSortBy
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.Types.PersonTrackingSortBy.PersonTrackingSortBy
instance Network.AWS.Data.Headers.ToHeader Network.AWS.Rekognition.Types.PersonTrackingSortBy.PersonTrackingSortBy
instance Network.AWS.Data.Log.ToLog Network.AWS.Rekognition.Types.PersonTrackingSortBy.PersonTrackingSortBy
instance Network.AWS.Data.ByteString.ToByteString Network.AWS.Rekognition.Types.PersonTrackingSortBy.PersonTrackingSortBy
instance Network.AWS.Data.Text.ToText Network.AWS.Rekognition.Types.PersonTrackingSortBy.PersonTrackingSortBy
instance Network.AWS.Data.Text.FromText Network.AWS.Rekognition.Types.PersonTrackingSortBy.PersonTrackingSortBy
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.PersonTrackingSortBy.PersonTrackingSortBy
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.PersonTrackingSortBy.PersonTrackingSortBy
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.PersonTrackingSortBy.PersonTrackingSortBy
instance GHC.Classes.Ord Network.AWS.Rekognition.Types.PersonTrackingSortBy.PersonTrackingSortBy
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.PersonTrackingSortBy.PersonTrackingSortBy
instance GHC.Read.Read Network.AWS.Rekognition.Types.PersonTrackingSortBy.PersonTrackingSortBy
instance GHC.Show.Show Network.AWS.Rekognition.Types.PersonTrackingSortBy.PersonTrackingSortBy


module Network.AWS.Rekognition.Types.Point

-- | The X and Y coordinates of a point on an image. The X and Y values
--   returned are ratios of the overall image size. For example, if the
--   input image is 700x200 and the operation returns X=0.5 and Y=0.25,
--   then the point is at the (350,50) pixel coordinate on the image.
--   
--   An array of <tt>Point</tt> objects, <tt>Polygon</tt>, is returned by
--   DetectText and by DetectCustomLabels. <tt>Polygon</tt> represents a
--   fine-grained polygon around a detected item. For more information, see
--   Geometry in the Amazon Rekognition Developer Guide.
--   
--   <i>See:</i> <a>newPoint</a> smart constructor.
data Point
Point' :: Maybe Double -> Maybe Double -> Point

-- | The value of the X coordinate for a point on a <tt>Polygon</tt>.
[$sel:x:Point'] :: Point -> Maybe Double

-- | The value of the Y coordinate for a point on a <tt>Polygon</tt>.
[$sel:y:Point'] :: Point -> Maybe Double

-- | Create a value of <a>Point</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:x:Point'</a>, <a>point_x</a> - The value of the X coordinate
--   for a point on a <tt>Polygon</tt>.
--   
--   <a>$sel:y:Point'</a>, <a>point_y</a> - The value of the Y coordinate
--   for a point on a <tt>Polygon</tt>.
newPoint :: Point

-- | The value of the X coordinate for a point on a <tt>Polygon</tt>.
point_x :: Lens' Point (Maybe Double)

-- | The value of the Y coordinate for a point on a <tt>Polygon</tt>.
point_y :: Lens' Point (Maybe Double)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.Point.Point
instance GHC.Show.Show Network.AWS.Rekognition.Types.Point.Point
instance GHC.Read.Read Network.AWS.Rekognition.Types.Point.Point
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.Point.Point
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.Point.Point
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.Point.Point
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.Point.Point


module Network.AWS.Rekognition.Types.Geometry

-- | Information about where an object (DetectCustomLabels) or text
--   (DetectText) is located on an image.
--   
--   <i>See:</i> <a>newGeometry</a> smart constructor.
data Geometry
Geometry' :: Maybe BoundingBox -> Maybe [Point] -> Geometry

-- | An axis-aligned coarse representation of the detected item's location
--   on the image.
[$sel:boundingBox:Geometry'] :: Geometry -> Maybe BoundingBox

-- | Within the bounding box, a fine-grained polygon around the detected
--   item.
[$sel:polygon:Geometry'] :: Geometry -> Maybe [Point]

-- | Create a value of <a>Geometry</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:boundingBox:Geometry'</a>, <a>geometry_boundingBox</a> - An
--   axis-aligned coarse representation of the detected item's location on
--   the image.
--   
--   <a>$sel:polygon:Geometry'</a>, <a>geometry_polygon</a> - Within the
--   bounding box, a fine-grained polygon around the detected item.
newGeometry :: Geometry

-- | An axis-aligned coarse representation of the detected item's location
--   on the image.
geometry_boundingBox :: Lens' Geometry (Maybe BoundingBox)

-- | Within the bounding box, a fine-grained polygon around the detected
--   item.
geometry_polygon :: Lens' Geometry (Maybe [Point])
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.Geometry.Geometry
instance GHC.Show.Show Network.AWS.Rekognition.Types.Geometry.Geometry
instance GHC.Read.Read Network.AWS.Rekognition.Types.Geometry.Geometry
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.Geometry.Geometry
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.Geometry.Geometry
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.Geometry.Geometry
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.Geometry.Geometry


module Network.AWS.Rekognition.Types.CustomLabel

-- | A custom label detected in an image by a call to DetectCustomLabels.
--   
--   <i>See:</i> <a>newCustomLabel</a> smart constructor.
data CustomLabel
CustomLabel' :: Maybe Double -> Maybe Text -> Maybe Geometry -> CustomLabel

-- | The confidence that the model has in the detection of the custom
--   label. The range is 0-100. A higher value indicates a higher
--   confidence.
[$sel:confidence:CustomLabel'] :: CustomLabel -> Maybe Double

-- | The name of the custom label.
[$sel:name:CustomLabel'] :: CustomLabel -> Maybe Text

-- | The location of the detected object on the image that corresponds to
--   the custom label. Includes an axis aligned coarse bounding box
--   surrounding the object and a finer grain polygon for more accurate
--   spatial information.
[$sel:geometry:CustomLabel'] :: CustomLabel -> Maybe Geometry

-- | Create a value of <a>CustomLabel</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:confidence:CustomLabel'</a>, <a>customLabel_confidence</a> -
--   The confidence that the model has in the detection of the custom
--   label. The range is 0-100. A higher value indicates a higher
--   confidence.
--   
--   <a>$sel:name:CustomLabel'</a>, <a>customLabel_name</a> - The name of
--   the custom label.
--   
--   <a>$sel:geometry:CustomLabel'</a>, <a>customLabel_geometry</a> - The
--   location of the detected object on the image that corresponds to the
--   custom label. Includes an axis aligned coarse bounding box surrounding
--   the object and a finer grain polygon for more accurate spatial
--   information.
newCustomLabel :: CustomLabel

-- | The confidence that the model has in the detection of the custom
--   label. The range is 0-100. A higher value indicates a higher
--   confidence.
customLabel_confidence :: Lens' CustomLabel (Maybe Double)

-- | The name of the custom label.
customLabel_name :: Lens' CustomLabel (Maybe Text)

-- | The location of the detected object on the image that corresponds to
--   the custom label. Includes an axis aligned coarse bounding box
--   surrounding the object and a finer grain polygon for more accurate
--   spatial information.
customLabel_geometry :: Lens' CustomLabel (Maybe Geometry)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.CustomLabel.CustomLabel
instance GHC.Show.Show Network.AWS.Rekognition.Types.CustomLabel.CustomLabel
instance GHC.Read.Read Network.AWS.Rekognition.Types.CustomLabel.CustomLabel
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.CustomLabel.CustomLabel
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.CustomLabel.CustomLabel
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.CustomLabel.CustomLabel
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.CustomLabel.CustomLabel


module Network.AWS.Rekognition.Types.Pose

-- | Indicates the pose of the face as determined by its pitch, roll, and
--   yaw.
--   
--   <i>See:</i> <a>newPose</a> smart constructor.
data Pose
Pose' :: Maybe Double -> Maybe Double -> Maybe Double -> Pose

-- | Value representing the face rotation on the yaw axis.
[$sel:yaw:Pose'] :: Pose -> Maybe Double

-- | Value representing the face rotation on the roll axis.
[$sel:roll:Pose'] :: Pose -> Maybe Double

-- | Value representing the face rotation on the pitch axis.
[$sel:pitch:Pose'] :: Pose -> Maybe Double

-- | Create a value of <a>Pose</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:yaw:Pose'</a>, <a>pose_yaw</a> - Value representing the face
--   rotation on the yaw axis.
--   
--   <a>$sel:roll:Pose'</a>, <a>pose_roll</a> - Value representing the face
--   rotation on the roll axis.
--   
--   <a>$sel:pitch:Pose'</a>, <a>pose_pitch</a> - Value representing the
--   face rotation on the pitch axis.
newPose :: Pose

-- | Value representing the face rotation on the yaw axis.
pose_yaw :: Lens' Pose (Maybe Double)

-- | Value representing the face rotation on the roll axis.
pose_roll :: Lens' Pose (Maybe Double)

-- | Value representing the face rotation on the pitch axis.
pose_pitch :: Lens' Pose (Maybe Double)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.Pose.Pose
instance GHC.Show.Show Network.AWS.Rekognition.Types.Pose.Pose
instance GHC.Read.Read Network.AWS.Rekognition.Types.Pose.Pose
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.Pose.Pose
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.Pose.Pose
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.Pose.Pose
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.Pose.Pose


module Network.AWS.Rekognition.Types.ProjectStatus
newtype ProjectStatus
ProjectStatus' :: Text -> ProjectStatus
[fromProjectStatus] :: ProjectStatus -> Text
pattern ProjectStatus_CREATED :: ProjectStatus
pattern ProjectStatus_CREATING :: ProjectStatus
pattern ProjectStatus_DELETING :: ProjectStatus
instance Network.AWS.Data.XML.ToXML Network.AWS.Rekognition.Types.ProjectStatus.ProjectStatus
instance Network.AWS.Data.XML.FromXML Network.AWS.Rekognition.Types.ProjectStatus.ProjectStatus
instance Data.Aeson.Types.ToJSON.ToJSONKey Network.AWS.Rekognition.Types.ProjectStatus.ProjectStatus
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.Types.ProjectStatus.ProjectStatus
instance Data.Aeson.Types.FromJSON.FromJSONKey Network.AWS.Rekognition.Types.ProjectStatus.ProjectStatus
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.ProjectStatus.ProjectStatus
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.Types.ProjectStatus.ProjectStatus
instance Network.AWS.Data.Headers.ToHeader Network.AWS.Rekognition.Types.ProjectStatus.ProjectStatus
instance Network.AWS.Data.Log.ToLog Network.AWS.Rekognition.Types.ProjectStatus.ProjectStatus
instance Network.AWS.Data.ByteString.ToByteString Network.AWS.Rekognition.Types.ProjectStatus.ProjectStatus
instance Network.AWS.Data.Text.ToText Network.AWS.Rekognition.Types.ProjectStatus.ProjectStatus
instance Network.AWS.Data.Text.FromText Network.AWS.Rekognition.Types.ProjectStatus.ProjectStatus
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.ProjectStatus.ProjectStatus
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.ProjectStatus.ProjectStatus
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.ProjectStatus.ProjectStatus
instance GHC.Classes.Ord Network.AWS.Rekognition.Types.ProjectStatus.ProjectStatus
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.ProjectStatus.ProjectStatus
instance GHC.Read.Read Network.AWS.Rekognition.Types.ProjectStatus.ProjectStatus
instance GHC.Show.Show Network.AWS.Rekognition.Types.ProjectStatus.ProjectStatus


module Network.AWS.Rekognition.Types.ProjectDescription

-- | A description of a Amazon Rekognition Custom Labels project.
--   
--   <i>See:</i> <a>newProjectDescription</a> smart constructor.
data ProjectDescription
ProjectDescription' :: Maybe ProjectStatus -> Maybe POSIX -> Maybe Text -> ProjectDescription

-- | The current status of the project.
[$sel:status:ProjectDescription'] :: ProjectDescription -> Maybe ProjectStatus

-- | The Unix timestamp for the date and time that the project was created.
[$sel:creationTimestamp:ProjectDescription'] :: ProjectDescription -> Maybe POSIX

-- | The Amazon Resource Name (ARN) of the project.
[$sel:projectArn:ProjectDescription'] :: ProjectDescription -> Maybe Text

-- | Create a value of <a>ProjectDescription</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:status:ProjectDescription'</a>,
--   <a>projectDescription_status</a> - The current status of the project.
--   
--   <a>$sel:creationTimestamp:ProjectDescription'</a>,
--   <a>projectDescription_creationTimestamp</a> - The Unix timestamp for
--   the date and time that the project was created.
--   
--   <a>$sel:projectArn:ProjectDescription'</a>,
--   <a>projectDescription_projectArn</a> - The Amazon Resource Name (ARN)
--   of the project.
newProjectDescription :: ProjectDescription

-- | The current status of the project.
projectDescription_status :: Lens' ProjectDescription (Maybe ProjectStatus)

-- | The Unix timestamp for the date and time that the project was created.
projectDescription_creationTimestamp :: Lens' ProjectDescription (Maybe UTCTime)

-- | The Amazon Resource Name (ARN) of the project.
projectDescription_projectArn :: Lens' ProjectDescription (Maybe Text)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.ProjectDescription.ProjectDescription
instance GHC.Show.Show Network.AWS.Rekognition.Types.ProjectDescription.ProjectDescription
instance GHC.Read.Read Network.AWS.Rekognition.Types.ProjectDescription.ProjectDescription
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.ProjectDescription.ProjectDescription
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.ProjectDescription.ProjectDescription
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.ProjectDescription.ProjectDescription
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.ProjectDescription.ProjectDescription


module Network.AWS.Rekognition.Types.ProjectVersionStatus
newtype ProjectVersionStatus
ProjectVersionStatus' :: Text -> ProjectVersionStatus
[fromProjectVersionStatus] :: ProjectVersionStatus -> Text
pattern ProjectVersionStatus_DELETING :: ProjectVersionStatus
pattern ProjectVersionStatus_FAILED :: ProjectVersionStatus
pattern ProjectVersionStatus_RUNNING :: ProjectVersionStatus
pattern ProjectVersionStatus_STARTING :: ProjectVersionStatus
pattern ProjectVersionStatus_STOPPED :: ProjectVersionStatus
pattern ProjectVersionStatus_STOPPING :: ProjectVersionStatus
pattern ProjectVersionStatus_TRAINING_COMPLETED :: ProjectVersionStatus
pattern ProjectVersionStatus_TRAINING_FAILED :: ProjectVersionStatus
pattern ProjectVersionStatus_TRAINING_IN_PROGRESS :: ProjectVersionStatus
instance Network.AWS.Data.XML.ToXML Network.AWS.Rekognition.Types.ProjectVersionStatus.ProjectVersionStatus
instance Network.AWS.Data.XML.FromXML Network.AWS.Rekognition.Types.ProjectVersionStatus.ProjectVersionStatus
instance Data.Aeson.Types.ToJSON.ToJSONKey Network.AWS.Rekognition.Types.ProjectVersionStatus.ProjectVersionStatus
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.Types.ProjectVersionStatus.ProjectVersionStatus
instance Data.Aeson.Types.FromJSON.FromJSONKey Network.AWS.Rekognition.Types.ProjectVersionStatus.ProjectVersionStatus
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.ProjectVersionStatus.ProjectVersionStatus
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.Types.ProjectVersionStatus.ProjectVersionStatus
instance Network.AWS.Data.Headers.ToHeader Network.AWS.Rekognition.Types.ProjectVersionStatus.ProjectVersionStatus
instance Network.AWS.Data.Log.ToLog Network.AWS.Rekognition.Types.ProjectVersionStatus.ProjectVersionStatus
instance Network.AWS.Data.ByteString.ToByteString Network.AWS.Rekognition.Types.ProjectVersionStatus.ProjectVersionStatus
instance Network.AWS.Data.Text.ToText Network.AWS.Rekognition.Types.ProjectVersionStatus.ProjectVersionStatus
instance Network.AWS.Data.Text.FromText Network.AWS.Rekognition.Types.ProjectVersionStatus.ProjectVersionStatus
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.ProjectVersionStatus.ProjectVersionStatus
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.ProjectVersionStatus.ProjectVersionStatus
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.ProjectVersionStatus.ProjectVersionStatus
instance GHC.Classes.Ord Network.AWS.Rekognition.Types.ProjectVersionStatus.ProjectVersionStatus
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.ProjectVersionStatus.ProjectVersionStatus
instance GHC.Read.Read Network.AWS.Rekognition.Types.ProjectVersionStatus.ProjectVersionStatus
instance GHC.Show.Show Network.AWS.Rekognition.Types.ProjectVersionStatus.ProjectVersionStatus


module Network.AWS.Rekognition.Types.ProtectiveEquipmentSummary

-- | Summary information for required items of personal protective
--   equipment (PPE) detected on persons by a call to
--   DetectProtectiveEquipment. You specify the required type of PPE in the
--   <tt>SummarizationAttributes</tt>
--   (ProtectiveEquipmentSummarizationAttributes) input parameter. The
--   summary includes which persons were detected wearing the required
--   personal protective equipment (<tt>PersonsWithRequiredEquipment</tt>),
--   which persons were detected as not wearing the required PPE
--   (<tt>PersonsWithoutRequiredEquipment</tt>), and the persons in which a
--   determination could not be made (<tt>PersonsIndeterminate</tt>).
--   
--   To get a total for each category, use the size of the field array. For
--   example, to find out how many people were detected as wearing the
--   specified PPE, use the size of the
--   <tt>PersonsWithRequiredEquipment</tt> array. If you want to find out
--   more about a person, such as the location (BoundingBox) of the person
--   on the image, use the person ID in each array element. Each person ID
--   matches the ID field of a ProtectiveEquipmentPerson object returned in
--   the <tt>Persons</tt> array by <tt>DetectProtectiveEquipment</tt>.
--   
--   <i>See:</i> <a>newProtectiveEquipmentSummary</a> smart constructor.
data ProtectiveEquipmentSummary
ProtectiveEquipmentSummary' :: Maybe [Natural] -> Maybe [Natural] -> Maybe [Natural] -> ProtectiveEquipmentSummary

-- | An array of IDs for persons who are wearing detected personal
--   protective equipment.
[$sel:personsWithRequiredEquipment:ProtectiveEquipmentSummary'] :: ProtectiveEquipmentSummary -> Maybe [Natural]

-- | An array of IDs for persons who are not wearing all of the types of
--   PPE specified in the <tt>RequiredEquipmentTypes</tt> field of the
--   detected personal protective equipment.
[$sel:personsWithoutRequiredEquipment:ProtectiveEquipmentSummary'] :: ProtectiveEquipmentSummary -> Maybe [Natural]

-- | An array of IDs for persons where it was not possible to determine if
--   they are wearing personal protective equipment.
[$sel:personsIndeterminate:ProtectiveEquipmentSummary'] :: ProtectiveEquipmentSummary -> Maybe [Natural]

-- | Create a value of <a>ProtectiveEquipmentSummary</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:personsWithRequiredEquipment:ProtectiveEquipmentSummary'</a>,
--   <a>protectiveEquipmentSummary_personsWithRequiredEquipment</a> - An
--   array of IDs for persons who are wearing detected personal protective
--   equipment.
--   
--   
--   <a>$sel:personsWithoutRequiredEquipment:ProtectiveEquipmentSummary'</a>,
--   <a>protectiveEquipmentSummary_personsWithoutRequiredEquipment</a> - An
--   array of IDs for persons who are not wearing all of the types of PPE
--   specified in the <tt>RequiredEquipmentTypes</tt> field of the detected
--   personal protective equipment.
--   
--   <a>$sel:personsIndeterminate:ProtectiveEquipmentSummary'</a>,
--   <a>protectiveEquipmentSummary_personsIndeterminate</a> - An array of
--   IDs for persons where it was not possible to determine if they are
--   wearing personal protective equipment.
newProtectiveEquipmentSummary :: ProtectiveEquipmentSummary

-- | An array of IDs for persons who are wearing detected personal
--   protective equipment.
protectiveEquipmentSummary_personsWithRequiredEquipment :: Lens' ProtectiveEquipmentSummary (Maybe [Natural])

-- | An array of IDs for persons who are not wearing all of the types of
--   PPE specified in the <tt>RequiredEquipmentTypes</tt> field of the
--   detected personal protective equipment.
protectiveEquipmentSummary_personsWithoutRequiredEquipment :: Lens' ProtectiveEquipmentSummary (Maybe [Natural])

-- | An array of IDs for persons where it was not possible to determine if
--   they are wearing personal protective equipment.
protectiveEquipmentSummary_personsIndeterminate :: Lens' ProtectiveEquipmentSummary (Maybe [Natural])
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.ProtectiveEquipmentSummary.ProtectiveEquipmentSummary
instance GHC.Show.Show Network.AWS.Rekognition.Types.ProtectiveEquipmentSummary.ProtectiveEquipmentSummary
instance GHC.Read.Read Network.AWS.Rekognition.Types.ProtectiveEquipmentSummary.ProtectiveEquipmentSummary
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.ProtectiveEquipmentSummary.ProtectiveEquipmentSummary
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.ProtectiveEquipmentSummary.ProtectiveEquipmentSummary
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.ProtectiveEquipmentSummary.ProtectiveEquipmentSummary
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.ProtectiveEquipmentSummary.ProtectiveEquipmentSummary


module Network.AWS.Rekognition.Types.ProtectiveEquipmentType
newtype ProtectiveEquipmentType
ProtectiveEquipmentType' :: Text -> ProtectiveEquipmentType
[fromProtectiveEquipmentType] :: ProtectiveEquipmentType -> Text
pattern ProtectiveEquipmentType_FACE_COVER :: ProtectiveEquipmentType
pattern ProtectiveEquipmentType_HAND_COVER :: ProtectiveEquipmentType
pattern ProtectiveEquipmentType_HEAD_COVER :: ProtectiveEquipmentType
instance Network.AWS.Data.XML.ToXML Network.AWS.Rekognition.Types.ProtectiveEquipmentType.ProtectiveEquipmentType
instance Network.AWS.Data.XML.FromXML Network.AWS.Rekognition.Types.ProtectiveEquipmentType.ProtectiveEquipmentType
instance Data.Aeson.Types.ToJSON.ToJSONKey Network.AWS.Rekognition.Types.ProtectiveEquipmentType.ProtectiveEquipmentType
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.Types.ProtectiveEquipmentType.ProtectiveEquipmentType
instance Data.Aeson.Types.FromJSON.FromJSONKey Network.AWS.Rekognition.Types.ProtectiveEquipmentType.ProtectiveEquipmentType
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.ProtectiveEquipmentType.ProtectiveEquipmentType
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.Types.ProtectiveEquipmentType.ProtectiveEquipmentType
instance Network.AWS.Data.Headers.ToHeader Network.AWS.Rekognition.Types.ProtectiveEquipmentType.ProtectiveEquipmentType
instance Network.AWS.Data.Log.ToLog Network.AWS.Rekognition.Types.ProtectiveEquipmentType.ProtectiveEquipmentType
instance Network.AWS.Data.ByteString.ToByteString Network.AWS.Rekognition.Types.ProtectiveEquipmentType.ProtectiveEquipmentType
instance Network.AWS.Data.Text.ToText Network.AWS.Rekognition.Types.ProtectiveEquipmentType.ProtectiveEquipmentType
instance Network.AWS.Data.Text.FromText Network.AWS.Rekognition.Types.ProtectiveEquipmentType.ProtectiveEquipmentType
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.ProtectiveEquipmentType.ProtectiveEquipmentType
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.ProtectiveEquipmentType.ProtectiveEquipmentType
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.ProtectiveEquipmentType.ProtectiveEquipmentType
instance GHC.Classes.Ord Network.AWS.Rekognition.Types.ProtectiveEquipmentType.ProtectiveEquipmentType
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.ProtectiveEquipmentType.ProtectiveEquipmentType
instance GHC.Read.Read Network.AWS.Rekognition.Types.ProtectiveEquipmentType.ProtectiveEquipmentType
instance GHC.Show.Show Network.AWS.Rekognition.Types.ProtectiveEquipmentType.ProtectiveEquipmentType


module Network.AWS.Rekognition.Types.ProtectiveEquipmentSummarizationAttributes

-- | Specifies summary attributes to return from a call to
--   DetectProtectiveEquipment. You can specify which types of PPE to
--   summarize. You can also specify a minimum confidence value for
--   detections. Summary information is returned in the <tt>Summary</tt>
--   (ProtectiveEquipmentSummary) field of the response from
--   <tt>DetectProtectiveEquipment</tt>. The summary includes which persons
--   in an image were detected wearing the requested types of person
--   protective equipment (PPE), which persons were detected as not wearing
--   PPE, and the persons in which a determination could not be made. For
--   more information, see ProtectiveEquipmentSummary.
--   
--   <i>See:</i> <a>newProtectiveEquipmentSummarizationAttributes</a> smart
--   constructor.
data ProtectiveEquipmentSummarizationAttributes
ProtectiveEquipmentSummarizationAttributes' :: Double -> [ProtectiveEquipmentType] -> ProtectiveEquipmentSummarizationAttributes

-- | The minimum confidence level for which you want summary information.
--   The confidence level applies to person detection, body part detection,
--   equipment detection, and body part coverage. Amazon Rekognition
--   doesn't return summary information with a confidence than this
--   specified value. There isn't a default value.
--   
--   Specify a <tt>MinConfidence</tt> value that is between 50-100% as
--   <tt>DetectProtectiveEquipment</tt> returns predictions only where the
--   detection confidence is between 50% - 100%. If you specify a value
--   that is less than 50%, the results are the same specifying a value of
--   50%.
[$sel:minConfidence:ProtectiveEquipmentSummarizationAttributes'] :: ProtectiveEquipmentSummarizationAttributes -> Double

-- | An array of personal protective equipment types for which you want
--   summary information. If a person is detected wearing a required
--   requipment type, the person's ID is added to the
--   <tt>PersonsWithRequiredEquipment</tt> array field returned in
--   ProtectiveEquipmentSummary by <tt>DetectProtectiveEquipment</tt>.
[$sel:requiredEquipmentTypes:ProtectiveEquipmentSummarizationAttributes'] :: ProtectiveEquipmentSummarizationAttributes -> [ProtectiveEquipmentType]

-- | Create a value of <a>ProtectiveEquipmentSummarizationAttributes</a>
--   with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:minConfidence:ProtectiveEquipmentSummarizationAttributes'</a>,
--   <a>protectiveEquipmentSummarizationAttributes_minConfidence</a> - The
--   minimum confidence level for which you want summary information. The
--   confidence level applies to person detection, body part detection,
--   equipment detection, and body part coverage. Amazon Rekognition
--   doesn't return summary information with a confidence than this
--   specified value. There isn't a default value.
--   
--   Specify a <tt>MinConfidence</tt> value that is between 50-100% as
--   <tt>DetectProtectiveEquipment</tt> returns predictions only where the
--   detection confidence is between 50% - 100%. If you specify a value
--   that is less than 50%, the results are the same specifying a value of
--   50%.
--   
--   
--   <a>$sel:requiredEquipmentTypes:ProtectiveEquipmentSummarizationAttributes'</a>,
--   <a>protectiveEquipmentSummarizationAttributes_requiredEquipmentTypes</a>
--   - An array of personal protective equipment types for which you want
--   summary information. If a person is detected wearing a required
--   requipment type, the person's ID is added to the
--   <tt>PersonsWithRequiredEquipment</tt> array field returned in
--   ProtectiveEquipmentSummary by <tt>DetectProtectiveEquipment</tt>.
newProtectiveEquipmentSummarizationAttributes :: Double -> ProtectiveEquipmentSummarizationAttributes

-- | The minimum confidence level for which you want summary information.
--   The confidence level applies to person detection, body part detection,
--   equipment detection, and body part coverage. Amazon Rekognition
--   doesn't return summary information with a confidence than this
--   specified value. There isn't a default value.
--   
--   Specify a <tt>MinConfidence</tt> value that is between 50-100% as
--   <tt>DetectProtectiveEquipment</tt> returns predictions only where the
--   detection confidence is between 50% - 100%. If you specify a value
--   that is less than 50%, the results are the same specifying a value of
--   50%.
protectiveEquipmentSummarizationAttributes_minConfidence :: Lens' ProtectiveEquipmentSummarizationAttributes Double

-- | An array of personal protective equipment types for which you want
--   summary information. If a person is detected wearing a required
--   requipment type, the person's ID is added to the
--   <tt>PersonsWithRequiredEquipment</tt> array field returned in
--   ProtectiveEquipmentSummary by <tt>DetectProtectiveEquipment</tt>.
protectiveEquipmentSummarizationAttributes_requiredEquipmentTypes :: Lens' ProtectiveEquipmentSummarizationAttributes [ProtectiveEquipmentType]
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.ProtectiveEquipmentSummarizationAttributes.ProtectiveEquipmentSummarizationAttributes
instance GHC.Show.Show Network.AWS.Rekognition.Types.ProtectiveEquipmentSummarizationAttributes.ProtectiveEquipmentSummarizationAttributes
instance GHC.Read.Read Network.AWS.Rekognition.Types.ProtectiveEquipmentSummarizationAttributes.ProtectiveEquipmentSummarizationAttributes
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.ProtectiveEquipmentSummarizationAttributes.ProtectiveEquipmentSummarizationAttributes
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.ProtectiveEquipmentSummarizationAttributes.ProtectiveEquipmentSummarizationAttributes
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.ProtectiveEquipmentSummarizationAttributes.ProtectiveEquipmentSummarizationAttributes
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.Types.ProtectiveEquipmentSummarizationAttributes.ProtectiveEquipmentSummarizationAttributes


module Network.AWS.Rekognition.Types.EquipmentDetection

-- | Information about an item of Personal Protective Equipment (PPE)
--   detected by DetectProtectiveEquipment. For more information, see
--   DetectProtectiveEquipment.
--   
--   <i>See:</i> <a>newEquipmentDetection</a> smart constructor.
data EquipmentDetection
EquipmentDetection' :: Maybe BoundingBox -> Maybe CoversBodyPart -> Maybe Double -> Maybe ProtectiveEquipmentType -> EquipmentDetection

-- | A bounding box surrounding the item of detected PPE.
[$sel:boundingBox:EquipmentDetection'] :: EquipmentDetection -> Maybe BoundingBox

-- | Information about the body part covered by the detected PPE.
[$sel:coversBodyPart:EquipmentDetection'] :: EquipmentDetection -> Maybe CoversBodyPart

-- | The confidence that Amazon Rekognition has that the bounding box
--   (<tt>BoundingBox</tt>) contains an item of PPE.
[$sel:confidence:EquipmentDetection'] :: EquipmentDetection -> Maybe Double

-- | The type of detected PPE.
[$sel:type':EquipmentDetection'] :: EquipmentDetection -> Maybe ProtectiveEquipmentType

-- | Create a value of <a>EquipmentDetection</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:boundingBox:EquipmentDetection'</a>,
--   <a>equipmentDetection_boundingBox</a> - A bounding box surrounding the
--   item of detected PPE.
--   
--   <a>$sel:coversBodyPart:EquipmentDetection'</a>,
--   <a>equipmentDetection_coversBodyPart</a> - Information about the body
--   part covered by the detected PPE.
--   
--   <a>$sel:confidence:EquipmentDetection'</a>,
--   <a>equipmentDetection_confidence</a> - The confidence that Amazon
--   Rekognition has that the bounding box (<tt>BoundingBox</tt>) contains
--   an item of PPE.
--   
--   <a>$sel:type':EquipmentDetection'</a>, <a>equipmentDetection_type</a>
--   - The type of detected PPE.
newEquipmentDetection :: EquipmentDetection

-- | A bounding box surrounding the item of detected PPE.
equipmentDetection_boundingBox :: Lens' EquipmentDetection (Maybe BoundingBox)

-- | Information about the body part covered by the detected PPE.
equipmentDetection_coversBodyPart :: Lens' EquipmentDetection (Maybe CoversBodyPart)

-- | The confidence that Amazon Rekognition has that the bounding box
--   (<tt>BoundingBox</tt>) contains an item of PPE.
equipmentDetection_confidence :: Lens' EquipmentDetection (Maybe Double)

-- | The type of detected PPE.
equipmentDetection_type :: Lens' EquipmentDetection (Maybe ProtectiveEquipmentType)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.EquipmentDetection.EquipmentDetection
instance GHC.Show.Show Network.AWS.Rekognition.Types.EquipmentDetection.EquipmentDetection
instance GHC.Read.Read Network.AWS.Rekognition.Types.EquipmentDetection.EquipmentDetection
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.EquipmentDetection.EquipmentDetection
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.EquipmentDetection.EquipmentDetection
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.EquipmentDetection.EquipmentDetection
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.EquipmentDetection.EquipmentDetection


module Network.AWS.Rekognition.Types.ProtectiveEquipmentBodyPart

-- | Information about a body part detected by DetectProtectiveEquipment
--   that contains PPE. An array of <tt>ProtectiveEquipmentBodyPart</tt>
--   objects is returned for each person detected by
--   <tt>DetectProtectiveEquipment</tt>.
--   
--   <i>See:</i> <a>newProtectiveEquipmentBodyPart</a> smart constructor.
data ProtectiveEquipmentBodyPart
ProtectiveEquipmentBodyPart' :: Maybe [EquipmentDetection] -> Maybe Double -> Maybe BodyPart -> ProtectiveEquipmentBodyPart

-- | An array of Personal Protective Equipment items detected around a body
--   part.
[$sel:equipmentDetections:ProtectiveEquipmentBodyPart'] :: ProtectiveEquipmentBodyPart -> Maybe [EquipmentDetection]

-- | The confidence that Amazon Rekognition has in the detection accuracy
--   of the detected body part.
[$sel:confidence:ProtectiveEquipmentBodyPart'] :: ProtectiveEquipmentBodyPart -> Maybe Double

-- | The detected body part.
[$sel:name:ProtectiveEquipmentBodyPart'] :: ProtectiveEquipmentBodyPart -> Maybe BodyPart

-- | Create a value of <a>ProtectiveEquipmentBodyPart</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:equipmentDetections:ProtectiveEquipmentBodyPart'</a>,
--   <a>protectiveEquipmentBodyPart_equipmentDetections</a> - An array of
--   Personal Protective Equipment items detected around a body part.
--   
--   <a>$sel:confidence:ProtectiveEquipmentBodyPart'</a>,
--   <a>protectiveEquipmentBodyPart_confidence</a> - The confidence that
--   Amazon Rekognition has in the detection accuracy of the detected body
--   part.
--   
--   <a>$sel:name:ProtectiveEquipmentBodyPart'</a>,
--   <a>protectiveEquipmentBodyPart_name</a> - The detected body part.
newProtectiveEquipmentBodyPart :: ProtectiveEquipmentBodyPart

-- | An array of Personal Protective Equipment items detected around a body
--   part.
protectiveEquipmentBodyPart_equipmentDetections :: Lens' ProtectiveEquipmentBodyPart (Maybe [EquipmentDetection])

-- | The confidence that Amazon Rekognition has in the detection accuracy
--   of the detected body part.
protectiveEquipmentBodyPart_confidence :: Lens' ProtectiveEquipmentBodyPart (Maybe Double)

-- | The detected body part.
protectiveEquipmentBodyPart_name :: Lens' ProtectiveEquipmentBodyPart (Maybe BodyPart)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.ProtectiveEquipmentBodyPart.ProtectiveEquipmentBodyPart
instance GHC.Show.Show Network.AWS.Rekognition.Types.ProtectiveEquipmentBodyPart.ProtectiveEquipmentBodyPart
instance GHC.Read.Read Network.AWS.Rekognition.Types.ProtectiveEquipmentBodyPart.ProtectiveEquipmentBodyPart
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.ProtectiveEquipmentBodyPart.ProtectiveEquipmentBodyPart
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.ProtectiveEquipmentBodyPart.ProtectiveEquipmentBodyPart
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.ProtectiveEquipmentBodyPart.ProtectiveEquipmentBodyPart
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.ProtectiveEquipmentBodyPart.ProtectiveEquipmentBodyPart


module Network.AWS.Rekognition.Types.ProtectiveEquipmentPerson

-- | A person detected by a call to DetectProtectiveEquipment. The API
--   returns all persons detected in the input image in an array of
--   <tt>ProtectiveEquipmentPerson</tt> objects.
--   
--   <i>See:</i> <a>newProtectiveEquipmentPerson</a> smart constructor.
data ProtectiveEquipmentPerson
ProtectiveEquipmentPerson' :: Maybe [ProtectiveEquipmentBodyPart] -> Maybe BoundingBox -> Maybe Double -> Maybe Natural -> ProtectiveEquipmentPerson

-- | An array of body parts detected on a person's body (including body
--   parts without PPE).
[$sel:bodyParts:ProtectiveEquipmentPerson'] :: ProtectiveEquipmentPerson -> Maybe [ProtectiveEquipmentBodyPart]

-- | A bounding box around the detected person.
[$sel:boundingBox:ProtectiveEquipmentPerson'] :: ProtectiveEquipmentPerson -> Maybe BoundingBox

-- | The confidence that Amazon Rekognition has that the bounding box
--   contains a person.
[$sel:confidence:ProtectiveEquipmentPerson'] :: ProtectiveEquipmentPerson -> Maybe Double

-- | The identifier for the detected person. The identifier is only unique
--   for a single call to <tt>DetectProtectiveEquipment</tt>.
[$sel:id:ProtectiveEquipmentPerson'] :: ProtectiveEquipmentPerson -> Maybe Natural

-- | Create a value of <a>ProtectiveEquipmentPerson</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:bodyParts:ProtectiveEquipmentPerson'</a>,
--   <a>protectiveEquipmentPerson_bodyParts</a> - An array of body parts
--   detected on a person's body (including body parts without PPE).
--   
--   <a>$sel:boundingBox:ProtectiveEquipmentPerson'</a>,
--   <a>protectiveEquipmentPerson_boundingBox</a> - A bounding box around
--   the detected person.
--   
--   <a>$sel:confidence:ProtectiveEquipmentPerson'</a>,
--   <a>protectiveEquipmentPerson_confidence</a> - The confidence that
--   Amazon Rekognition has that the bounding box contains a person.
--   
--   <a>$sel:id:ProtectiveEquipmentPerson'</a>,
--   <a>protectiveEquipmentPerson_id</a> - The identifier for the detected
--   person. The identifier is only unique for a single call to
--   <tt>DetectProtectiveEquipment</tt>.
newProtectiveEquipmentPerson :: ProtectiveEquipmentPerson

-- | An array of body parts detected on a person's body (including body
--   parts without PPE).
protectiveEquipmentPerson_bodyParts :: Lens' ProtectiveEquipmentPerson (Maybe [ProtectiveEquipmentBodyPart])

-- | A bounding box around the detected person.
protectiveEquipmentPerson_boundingBox :: Lens' ProtectiveEquipmentPerson (Maybe BoundingBox)

-- | The confidence that Amazon Rekognition has that the bounding box
--   contains a person.
protectiveEquipmentPerson_confidence :: Lens' ProtectiveEquipmentPerson (Maybe Double)

-- | The identifier for the detected person. The identifier is only unique
--   for a single call to <tt>DetectProtectiveEquipment</tt>.
protectiveEquipmentPerson_id :: Lens' ProtectiveEquipmentPerson (Maybe Natural)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.ProtectiveEquipmentPerson.ProtectiveEquipmentPerson
instance GHC.Show.Show Network.AWS.Rekognition.Types.ProtectiveEquipmentPerson.ProtectiveEquipmentPerson
instance GHC.Read.Read Network.AWS.Rekognition.Types.ProtectiveEquipmentPerson.ProtectiveEquipmentPerson
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.ProtectiveEquipmentPerson.ProtectiveEquipmentPerson
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.ProtectiveEquipmentPerson.ProtectiveEquipmentPerson
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.ProtectiveEquipmentPerson.ProtectiveEquipmentPerson
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.ProtectiveEquipmentPerson.ProtectiveEquipmentPerson


module Network.AWS.Rekognition.Types.QualityFilter
newtype QualityFilter
QualityFilter' :: Text -> QualityFilter
[fromQualityFilter] :: QualityFilter -> Text
pattern QualityFilter_AUTO :: QualityFilter
pattern QualityFilter_HIGH :: QualityFilter
pattern QualityFilter_LOW :: QualityFilter
pattern QualityFilter_MEDIUM :: QualityFilter
pattern QualityFilter_NONE :: QualityFilter
instance Network.AWS.Data.XML.ToXML Network.AWS.Rekognition.Types.QualityFilter.QualityFilter
instance Network.AWS.Data.XML.FromXML Network.AWS.Rekognition.Types.QualityFilter.QualityFilter
instance Data.Aeson.Types.ToJSON.ToJSONKey Network.AWS.Rekognition.Types.QualityFilter.QualityFilter
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.Types.QualityFilter.QualityFilter
instance Data.Aeson.Types.FromJSON.FromJSONKey Network.AWS.Rekognition.Types.QualityFilter.QualityFilter
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.QualityFilter.QualityFilter
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.Types.QualityFilter.QualityFilter
instance Network.AWS.Data.Headers.ToHeader Network.AWS.Rekognition.Types.QualityFilter.QualityFilter
instance Network.AWS.Data.Log.ToLog Network.AWS.Rekognition.Types.QualityFilter.QualityFilter
instance Network.AWS.Data.ByteString.ToByteString Network.AWS.Rekognition.Types.QualityFilter.QualityFilter
instance Network.AWS.Data.Text.ToText Network.AWS.Rekognition.Types.QualityFilter.QualityFilter
instance Network.AWS.Data.Text.FromText Network.AWS.Rekognition.Types.QualityFilter.QualityFilter
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.QualityFilter.QualityFilter
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.QualityFilter.QualityFilter
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.QualityFilter.QualityFilter
instance GHC.Classes.Ord Network.AWS.Rekognition.Types.QualityFilter.QualityFilter
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.QualityFilter.QualityFilter
instance GHC.Read.Read Network.AWS.Rekognition.Types.QualityFilter.QualityFilter
instance GHC.Show.Show Network.AWS.Rekognition.Types.QualityFilter.QualityFilter


module Network.AWS.Rekognition.Types.Reason
newtype Reason
Reason' :: Text -> Reason
[fromReason] :: Reason -> Text
pattern Reason_EXCEEDS_MAX_FACES :: Reason
pattern Reason_EXTREME_POSE :: Reason
pattern Reason_LOW_BRIGHTNESS :: Reason
pattern Reason_LOW_CONFIDENCE :: Reason
pattern Reason_LOW_FACE_QUALITY :: Reason
pattern Reason_LOW_SHARPNESS :: Reason
pattern Reason_SMALL_BOUNDING_BOX :: Reason
instance Network.AWS.Data.XML.ToXML Network.AWS.Rekognition.Types.Reason.Reason
instance Network.AWS.Data.XML.FromXML Network.AWS.Rekognition.Types.Reason.Reason
instance Data.Aeson.Types.ToJSON.ToJSONKey Network.AWS.Rekognition.Types.Reason.Reason
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.Types.Reason.Reason
instance Data.Aeson.Types.FromJSON.FromJSONKey Network.AWS.Rekognition.Types.Reason.Reason
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.Reason.Reason
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.Types.Reason.Reason
instance Network.AWS.Data.Headers.ToHeader Network.AWS.Rekognition.Types.Reason.Reason
instance Network.AWS.Data.Log.ToLog Network.AWS.Rekognition.Types.Reason.Reason
instance Network.AWS.Data.ByteString.ToByteString Network.AWS.Rekognition.Types.Reason.Reason
instance Network.AWS.Data.Text.ToText Network.AWS.Rekognition.Types.Reason.Reason
instance Network.AWS.Data.Text.FromText Network.AWS.Rekognition.Types.Reason.Reason
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.Reason.Reason
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.Reason.Reason
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.Reason.Reason
instance GHC.Classes.Ord Network.AWS.Rekognition.Types.Reason.Reason
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.Reason.Reason
instance GHC.Read.Read Network.AWS.Rekognition.Types.Reason.Reason
instance GHC.Show.Show Network.AWS.Rekognition.Types.Reason.Reason


module Network.AWS.Rekognition.Types.RegionOfInterest

-- | Specifies a location within the frame that Rekognition checks for
--   text. Uses a <tt>BoundingBox</tt> object to set a region of the
--   screen.
--   
--   A word is included in the region if the word is more than half in that
--   region. If there is more than one region, the word will be compared
--   with all regions of the screen. Any word more than half in a region is
--   kept in the results.
--   
--   <i>See:</i> <a>newRegionOfInterest</a> smart constructor.
data RegionOfInterest
RegionOfInterest' :: Maybe BoundingBox -> RegionOfInterest

-- | The box representing a region of interest on screen.
[$sel:boundingBox:RegionOfInterest'] :: RegionOfInterest -> Maybe BoundingBox

-- | Create a value of <a>RegionOfInterest</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:boundingBox:RegionOfInterest'</a>,
--   <a>regionOfInterest_boundingBox</a> - The box representing a region of
--   interest on screen.
newRegionOfInterest :: RegionOfInterest

-- | The box representing a region of interest on screen.
regionOfInterest_boundingBox :: Lens' RegionOfInterest (Maybe BoundingBox)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.RegionOfInterest.RegionOfInterest
instance GHC.Show.Show Network.AWS.Rekognition.Types.RegionOfInterest.RegionOfInterest
instance GHC.Read.Read Network.AWS.Rekognition.Types.RegionOfInterest.RegionOfInterest
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.RegionOfInterest.RegionOfInterest
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.RegionOfInterest.RegionOfInterest
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.RegionOfInterest.RegionOfInterest
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.Types.RegionOfInterest.RegionOfInterest


module Network.AWS.Rekognition.Types.DetectTextFilters

-- | A set of optional parameters that you can use to set the criteria that
--   the text must meet to be included in your response.
--   <tt>WordFilter</tt> looks at a word’s height, width, and minimum
--   confidence. <tt>RegionOfInterest</tt> lets you set a specific region
--   of the image to look for text in.
--   
--   <i>See:</i> <a>newDetectTextFilters</a> smart constructor.
data DetectTextFilters
DetectTextFilters' :: Maybe [RegionOfInterest] -> Maybe DetectionFilter -> DetectTextFilters

-- | A Filter focusing on a certain area of the image. Uses a
--   <tt>BoundingBox</tt> object to set the region of the image.
[$sel:regionsOfInterest:DetectTextFilters'] :: DetectTextFilters -> Maybe [RegionOfInterest]
[$sel:wordFilter:DetectTextFilters'] :: DetectTextFilters -> Maybe DetectionFilter

-- | Create a value of <a>DetectTextFilters</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:regionsOfInterest:DetectTextFilters'</a>,
--   <a>detectTextFilters_regionsOfInterest</a> - A Filter focusing on a
--   certain area of the image. Uses a <tt>BoundingBox</tt> object to set
--   the region of the image.
--   
--   <a>$sel:wordFilter:DetectTextFilters'</a>,
--   <a>detectTextFilters_wordFilter</a> - Undocumented member.
newDetectTextFilters :: DetectTextFilters

-- | A Filter focusing on a certain area of the image. Uses a
--   <tt>BoundingBox</tt> object to set the region of the image.
detectTextFilters_regionsOfInterest :: Lens' DetectTextFilters (Maybe [RegionOfInterest])

-- | Undocumented member.
detectTextFilters_wordFilter :: Lens' DetectTextFilters (Maybe DetectionFilter)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.DetectTextFilters.DetectTextFilters
instance GHC.Show.Show Network.AWS.Rekognition.Types.DetectTextFilters.DetectTextFilters
instance GHC.Read.Read Network.AWS.Rekognition.Types.DetectTextFilters.DetectTextFilters
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.DetectTextFilters.DetectTextFilters
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.DetectTextFilters.DetectTextFilters
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.DetectTextFilters.DetectTextFilters
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.Types.DetectTextFilters.DetectTextFilters


module Network.AWS.Rekognition.Types.S3Object

-- | Provides the S3 bucket name and object name.
--   
--   The region for the S3 bucket containing the S3 object must match the
--   region you use for Amazon Rekognition operations.
--   
--   For Amazon Rekognition to process an S3 object, the user must have
--   permission to access the S3 object. For more information, see
--   Resource-Based Policies in the Amazon Rekognition Developer Guide.
--   
--   <i>See:</i> <a>newS3Object</a> smart constructor.
data S3Object
S3Object' :: Maybe Text -> Maybe Text -> Maybe Text -> S3Object

-- | Name of the S3 bucket.
[$sel:bucket:S3Object'] :: S3Object -> Maybe Text

-- | S3 object key name.
[$sel:name:S3Object'] :: S3Object -> Maybe Text

-- | If the bucket is versioning enabled, you can specify the object
--   version.
[$sel:version:S3Object'] :: S3Object -> Maybe Text

-- | Create a value of <a>S3Object</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:bucket:S3Object'</a>, <a>s3Object_bucket</a> - Name of the S3
--   bucket.
--   
--   <a>$sel:name:S3Object'</a>, <a>s3Object_name</a> - S3 object key name.
--   
--   <a>$sel:version:S3Object'</a>, <a>s3Object_version</a> - If the bucket
--   is versioning enabled, you can specify the object version.
newS3Object :: S3Object

-- | Name of the S3 bucket.
s3Object_bucket :: Lens' S3Object (Maybe Text)

-- | S3 object key name.
s3Object_name :: Lens' S3Object (Maybe Text)

-- | If the bucket is versioning enabled, you can specify the object
--   version.
s3Object_version :: Lens' S3Object (Maybe Text)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.S3Object.S3Object
instance GHC.Show.Show Network.AWS.Rekognition.Types.S3Object.S3Object
instance GHC.Read.Read Network.AWS.Rekognition.Types.S3Object.S3Object
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.S3Object.S3Object
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.S3Object.S3Object
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.S3Object.S3Object
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.S3Object.S3Object
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.Types.S3Object.S3Object


module Network.AWS.Rekognition.Types.Image

-- | Provides the input image either as bytes or an S3 object.
--   
--   You pass image bytes to an Amazon Rekognition API operation by using
--   the <tt>Bytes</tt> property. For example, you would use the
--   <tt>Bytes</tt> property to pass an image loaded from a local file
--   system. Image bytes passed by using the <tt>Bytes</tt> property must
--   be base64-encoded. Your code may not need to encode image bytes if you
--   are using an AWS SDK to call Amazon Rekognition API operations.
--   
--   For more information, see Analyzing an Image Loaded from a Local File
--   System in the Amazon Rekognition Developer Guide.
--   
--   You pass images stored in an S3 bucket to an Amazon Rekognition API
--   operation by using the <tt>S3Object</tt> property. Images stored in an
--   S3 bucket do not need to be base64-encoded.
--   
--   The region for the S3 bucket containing the S3 object must match the
--   region you use for Amazon Rekognition operations.
--   
--   If you use the AWS CLI to call Amazon Rekognition operations, passing
--   image bytes using the Bytes property is not supported. You must first
--   upload the image to an Amazon S3 bucket and then call the operation
--   using the S3Object property.
--   
--   For Amazon Rekognition to process an S3 object, the user must have
--   permission to access the S3 object. For more information, see Resource
--   Based Policies in the Amazon Rekognition Developer Guide.
--   
--   <i>See:</i> <a>newImage</a> smart constructor.
data Image
Image' :: Maybe S3Object -> Maybe Base64 -> Image

-- | Identifies an S3 object as the image source.
[$sel:s3Object:Image'] :: Image -> Maybe S3Object

-- | Blob of image bytes up to 5 MBs.
[$sel:bytes:Image'] :: Image -> Maybe Base64

-- | Create a value of <a>Image</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:s3Object:Image'</a>, <a>image_s3Object</a> - Identifies an S3
--   object as the image source.
--   
--   <a>$sel:bytes:Image'</a>, <a>image_bytes</a> - Blob of image bytes up
--   to 5 MBs.-- -- <i>Note:</i> This <tt>Lens</tt> automatically encodes
--   and decodes Base64 data. -- The underlying isomorphism will encode to
--   Base64 representation during -- serialisation, and decode from Base64
--   representation during deserialisation. -- This <tt>Lens</tt> accepts
--   and returns only raw unencoded data.
newImage :: Image

-- | Identifies an S3 object as the image source.
image_s3Object :: Lens' Image (Maybe S3Object)

-- | Blob of image bytes up to 5 MBs.-- -- <i>Note:</i> This <tt>Lens</tt>
--   automatically encodes and decodes Base64 data. -- The underlying
--   isomorphism will encode to Base64 representation during --
--   serialisation, and decode from Base64 representation during
--   deserialisation. -- This <tt>Lens</tt> accepts and returns only raw
--   unencoded data.
image_bytes :: Lens' Image (Maybe ByteString)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.Image.Image
instance GHC.Show.Show Network.AWS.Rekognition.Types.Image.Image
instance GHC.Read.Read Network.AWS.Rekognition.Types.Image.Image
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.Image.Image
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.Image.Image
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.Image.Image
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.Types.Image.Image


module Network.AWS.Rekognition.Types.GroundTruthManifest

-- | The S3 bucket that contains an Amazon Sagemaker Ground Truth format
--   manifest file.
--   
--   <i>See:</i> <a>newGroundTruthManifest</a> smart constructor.
data GroundTruthManifest
GroundTruthManifest' :: Maybe S3Object -> GroundTruthManifest
[$sel:s3Object:GroundTruthManifest'] :: GroundTruthManifest -> Maybe S3Object

-- | Create a value of <a>GroundTruthManifest</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:s3Object:GroundTruthManifest'</a>,
--   <a>groundTruthManifest_s3Object</a> - Undocumented member.
newGroundTruthManifest :: GroundTruthManifest

-- | Undocumented member.
groundTruthManifest_s3Object :: Lens' GroundTruthManifest (Maybe S3Object)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.GroundTruthManifest.GroundTruthManifest
instance GHC.Show.Show Network.AWS.Rekognition.Types.GroundTruthManifest.GroundTruthManifest
instance GHC.Read.Read Network.AWS.Rekognition.Types.GroundTruthManifest.GroundTruthManifest
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.GroundTruthManifest.GroundTruthManifest
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.GroundTruthManifest.GroundTruthManifest
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.GroundTruthManifest.GroundTruthManifest
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.GroundTruthManifest.GroundTruthManifest
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.Types.GroundTruthManifest.GroundTruthManifest


module Network.AWS.Rekognition.Types.Asset

-- | Assets are the images that you use to train and evaluate a model
--   version. Assets can also contain validation information that you use
--   to debug a failed model training.
--   
--   <i>See:</i> <a>newAsset</a> smart constructor.
data Asset
Asset' :: Maybe GroundTruthManifest -> Asset
[$sel:groundTruthManifest:Asset'] :: Asset -> Maybe GroundTruthManifest

-- | Create a value of <a>Asset</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:groundTruthManifest:Asset'</a>,
--   <a>asset_groundTruthManifest</a> - Undocumented member.
newAsset :: Asset

-- | Undocumented member.
asset_groundTruthManifest :: Lens' Asset (Maybe GroundTruthManifest)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.Asset.Asset
instance GHC.Show.Show Network.AWS.Rekognition.Types.Asset.Asset
instance GHC.Read.Read Network.AWS.Rekognition.Types.Asset.Asset
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.Asset.Asset
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.Asset.Asset
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.Asset.Asset
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.Asset.Asset
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.Types.Asset.Asset


module Network.AWS.Rekognition.Types.SegmentType
newtype SegmentType
SegmentType' :: Text -> SegmentType
[fromSegmentType] :: SegmentType -> Text
pattern SegmentType_SHOT :: SegmentType
pattern SegmentType_TECHNICAL_CUE :: SegmentType
instance Network.AWS.Data.XML.ToXML Network.AWS.Rekognition.Types.SegmentType.SegmentType
instance Network.AWS.Data.XML.FromXML Network.AWS.Rekognition.Types.SegmentType.SegmentType
instance Data.Aeson.Types.ToJSON.ToJSONKey Network.AWS.Rekognition.Types.SegmentType.SegmentType
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.Types.SegmentType.SegmentType
instance Data.Aeson.Types.FromJSON.FromJSONKey Network.AWS.Rekognition.Types.SegmentType.SegmentType
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.SegmentType.SegmentType
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.Types.SegmentType.SegmentType
instance Network.AWS.Data.Headers.ToHeader Network.AWS.Rekognition.Types.SegmentType.SegmentType
instance Network.AWS.Data.Log.ToLog Network.AWS.Rekognition.Types.SegmentType.SegmentType
instance Network.AWS.Data.ByteString.ToByteString Network.AWS.Rekognition.Types.SegmentType.SegmentType
instance Network.AWS.Data.Text.ToText Network.AWS.Rekognition.Types.SegmentType.SegmentType
instance Network.AWS.Data.Text.FromText Network.AWS.Rekognition.Types.SegmentType.SegmentType
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.SegmentType.SegmentType
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.SegmentType.SegmentType
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.SegmentType.SegmentType
instance GHC.Classes.Ord Network.AWS.Rekognition.Types.SegmentType.SegmentType
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.SegmentType.SegmentType
instance GHC.Read.Read Network.AWS.Rekognition.Types.SegmentType.SegmentType
instance GHC.Show.Show Network.AWS.Rekognition.Types.SegmentType.SegmentType


module Network.AWS.Rekognition.Types.SegmentTypeInfo

-- | Information about the type of a segment requested in a call to
--   StartSegmentDetection. An array of <tt>SegmentTypeInfo</tt> objects is
--   returned by the response from GetSegmentDetection.
--   
--   <i>See:</i> <a>newSegmentTypeInfo</a> smart constructor.
data SegmentTypeInfo
SegmentTypeInfo' :: Maybe Text -> Maybe SegmentType -> SegmentTypeInfo

-- | The version of the model used to detect segments.
[$sel:modelVersion:SegmentTypeInfo'] :: SegmentTypeInfo -> Maybe Text

-- | The type of a segment (technical cue or shot detection).
[$sel:type':SegmentTypeInfo'] :: SegmentTypeInfo -> Maybe SegmentType

-- | Create a value of <a>SegmentTypeInfo</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:modelVersion:SegmentTypeInfo'</a>,
--   <a>segmentTypeInfo_modelVersion</a> - The version of the model used to
--   detect segments.
--   
--   <a>$sel:type':SegmentTypeInfo'</a>, <a>segmentTypeInfo_type</a> - The
--   type of a segment (technical cue or shot detection).
newSegmentTypeInfo :: SegmentTypeInfo

-- | The version of the model used to detect segments.
segmentTypeInfo_modelVersion :: Lens' SegmentTypeInfo (Maybe Text)

-- | The type of a segment (technical cue or shot detection).
segmentTypeInfo_type :: Lens' SegmentTypeInfo (Maybe SegmentType)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.SegmentTypeInfo.SegmentTypeInfo
instance GHC.Show.Show Network.AWS.Rekognition.Types.SegmentTypeInfo.SegmentTypeInfo
instance GHC.Read.Read Network.AWS.Rekognition.Types.SegmentTypeInfo.SegmentTypeInfo
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.SegmentTypeInfo.SegmentTypeInfo
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.SegmentTypeInfo.SegmentTypeInfo
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.SegmentTypeInfo.SegmentTypeInfo
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.SegmentTypeInfo.SegmentTypeInfo


module Network.AWS.Rekognition.Types.ShotSegment

-- | Information about a shot detection segment detected in a video. For
--   more information, see SegmentDetection.
--   
--   <i>See:</i> <a>newShotSegment</a> smart constructor.
data ShotSegment
ShotSegment' :: Maybe Double -> Maybe Natural -> ShotSegment

-- | The confidence that Amazon Rekognition Video has in the accuracy of
--   the detected segment.
[$sel:confidence:ShotSegment'] :: ShotSegment -> Maybe Double

-- | An Identifier for a shot detection segment detected in a video.
[$sel:index:ShotSegment'] :: ShotSegment -> Maybe Natural

-- | Create a value of <a>ShotSegment</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:confidence:ShotSegment'</a>, <a>shotSegment_confidence</a> -
--   The confidence that Amazon Rekognition Video has in the accuracy of
--   the detected segment.
--   
--   <a>$sel:index:ShotSegment'</a>, <a>shotSegment_index</a> - An
--   Identifier for a shot detection segment detected in a video.
newShotSegment :: ShotSegment

-- | The confidence that Amazon Rekognition Video has in the accuracy of
--   the detected segment.
shotSegment_confidence :: Lens' ShotSegment (Maybe Double)

-- | An Identifier for a shot detection segment detected in a video.
shotSegment_index :: Lens' ShotSegment (Maybe Natural)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.ShotSegment.ShotSegment
instance GHC.Show.Show Network.AWS.Rekognition.Types.ShotSegment.ShotSegment
instance GHC.Read.Read Network.AWS.Rekognition.Types.ShotSegment.ShotSegment
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.ShotSegment.ShotSegment
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.ShotSegment.ShotSegment
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.ShotSegment.ShotSegment
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.ShotSegment.ShotSegment


module Network.AWS.Rekognition.Types.Smile

-- | Indicates whether or not the face is smiling, and the confidence level
--   in the determination.
--   
--   <i>See:</i> <a>newSmile</a> smart constructor.
data Smile
Smile' :: Maybe Bool -> Maybe Double -> Smile

-- | Boolean value that indicates whether the face is smiling or not.
[$sel:value:Smile'] :: Smile -> Maybe Bool

-- | Level of confidence in the determination.
[$sel:confidence:Smile'] :: Smile -> Maybe Double

-- | Create a value of <a>Smile</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:value:Smile'</a>, <a>smile_value</a> - Boolean value that
--   indicates whether the face is smiling or not.
--   
--   <a>$sel:confidence:Smile'</a>, <a>smile_confidence</a> - Level of
--   confidence in the determination.
newSmile :: Smile

-- | Boolean value that indicates whether the face is smiling or not.
smile_value :: Lens' Smile (Maybe Bool)

-- | Level of confidence in the determination.
smile_confidence :: Lens' Smile (Maybe Double)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.Smile.Smile
instance GHC.Show.Show Network.AWS.Rekognition.Types.Smile.Smile
instance GHC.Read.Read Network.AWS.Rekognition.Types.Smile.Smile
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.Smile.Smile
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.Smile.Smile
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.Smile.Smile
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.Smile.Smile


module Network.AWS.Rekognition.Types.ComparedFace

-- | Provides face metadata for target image faces that are analyzed by
--   <tt>CompareFaces</tt> and <tt>RecognizeCelebrities</tt>.
--   
--   <i>See:</i> <a>newComparedFace</a> smart constructor.
data ComparedFace
ComparedFace' :: Maybe BoundingBox -> Maybe [Emotion] -> Maybe Pose -> Maybe Double -> Maybe ImageQuality -> Maybe Smile -> Maybe [Landmark] -> ComparedFace

-- | Bounding box of the face.
[$sel:boundingBox:ComparedFace'] :: ComparedFace -> Maybe BoundingBox

-- | The emotions that appear to be expressed on the face, and the
--   confidence level in the determination. Valid values include "Happy",
--   "Sad", "Angry", "Confused", "Disgusted", "Surprised", "Calm",
--   "Unknown", and "Fear".
[$sel:emotions:ComparedFace'] :: ComparedFace -> Maybe [Emotion]

-- | Indicates the pose of the face as determined by its pitch, roll, and
--   yaw.
[$sel:pose:ComparedFace'] :: ComparedFace -> Maybe Pose

-- | Level of confidence that what the bounding box contains is a face.
[$sel:confidence:ComparedFace'] :: ComparedFace -> Maybe Double

-- | Identifies face image brightness and sharpness.
[$sel:quality:ComparedFace'] :: ComparedFace -> Maybe ImageQuality

-- | Indicates whether or not the face is smiling, and the confidence level
--   in the determination.
[$sel:smile:ComparedFace'] :: ComparedFace -> Maybe Smile

-- | An array of facial landmarks.
[$sel:landmarks:ComparedFace'] :: ComparedFace -> Maybe [Landmark]

-- | Create a value of <a>ComparedFace</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:boundingBox:ComparedFace'</a>, <a>comparedFace_boundingBox</a>
--   - Bounding box of the face.
--   
--   <a>$sel:emotions:ComparedFace'</a>, <a>comparedFace_emotions</a> - The
--   emotions that appear to be expressed on the face, and the confidence
--   level in the determination. Valid values include "Happy", "Sad",
--   "Angry", "Confused", "Disgusted", "Surprised", "Calm", "Unknown", and
--   "Fear".
--   
--   <a>$sel:pose:ComparedFace'</a>, <a>comparedFace_pose</a> - Indicates
--   the pose of the face as determined by its pitch, roll, and yaw.
--   
--   <a>$sel:confidence:ComparedFace'</a>, <a>comparedFace_confidence</a> -
--   Level of confidence that what the bounding box contains is a face.
--   
--   <a>$sel:quality:ComparedFace'</a>, <a>comparedFace_quality</a> -
--   Identifies face image brightness and sharpness.
--   
--   <a>$sel:smile:ComparedFace'</a>, <a>comparedFace_smile</a> - Indicates
--   whether or not the face is smiling, and the confidence level in the
--   determination.
--   
--   <a>$sel:landmarks:ComparedFace'</a>, <a>comparedFace_landmarks</a> -
--   An array of facial landmarks.
newComparedFace :: ComparedFace

-- | Bounding box of the face.
comparedFace_boundingBox :: Lens' ComparedFace (Maybe BoundingBox)

-- | The emotions that appear to be expressed on the face, and the
--   confidence level in the determination. Valid values include "Happy",
--   "Sad", "Angry", "Confused", "Disgusted", "Surprised", "Calm",
--   "Unknown", and "Fear".
comparedFace_emotions :: Lens' ComparedFace (Maybe [Emotion])

-- | Indicates the pose of the face as determined by its pitch, roll, and
--   yaw.
comparedFace_pose :: Lens' ComparedFace (Maybe Pose)

-- | Level of confidence that what the bounding box contains is a face.
comparedFace_confidence :: Lens' ComparedFace (Maybe Double)

-- | Identifies face image brightness and sharpness.
comparedFace_quality :: Lens' ComparedFace (Maybe ImageQuality)

-- | Indicates whether or not the face is smiling, and the confidence level
--   in the determination.
comparedFace_smile :: Lens' ComparedFace (Maybe Smile)

-- | An array of facial landmarks.
comparedFace_landmarks :: Lens' ComparedFace (Maybe [Landmark])
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.ComparedFace.ComparedFace
instance GHC.Show.Show Network.AWS.Rekognition.Types.ComparedFace.ComparedFace
instance GHC.Read.Read Network.AWS.Rekognition.Types.ComparedFace.ComparedFace
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.ComparedFace.ComparedFace
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.ComparedFace.ComparedFace
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.ComparedFace.ComparedFace
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.ComparedFace.ComparedFace


module Network.AWS.Rekognition.Types.CompareFacesMatch

-- | Provides information about a face in a target image that matches the
--   source image face analyzed by <tt>CompareFaces</tt>. The <tt>Face</tt>
--   property contains the bounding box of the face in the target image.
--   The <tt>Similarity</tt> property is the confidence that the source
--   image face matches the face in the bounding box.
--   
--   <i>See:</i> <a>newCompareFacesMatch</a> smart constructor.
data CompareFacesMatch
CompareFacesMatch' :: Maybe Double -> Maybe ComparedFace -> CompareFacesMatch

-- | Level of confidence that the faces match.
[$sel:similarity:CompareFacesMatch'] :: CompareFacesMatch -> Maybe Double

-- | Provides face metadata (bounding box and confidence that the bounding
--   box actually contains a face).
[$sel:face:CompareFacesMatch'] :: CompareFacesMatch -> Maybe ComparedFace

-- | Create a value of <a>CompareFacesMatch</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:similarity:CompareFacesMatch'</a>,
--   <a>compareFacesMatch_similarity</a> - Level of confidence that the
--   faces match.
--   
--   <a>$sel:face:CompareFacesMatch'</a>, <a>compareFacesMatch_face</a> -
--   Provides face metadata (bounding box and confidence that the bounding
--   box actually contains a face).
newCompareFacesMatch :: CompareFacesMatch

-- | Level of confidence that the faces match.
compareFacesMatch_similarity :: Lens' CompareFacesMatch (Maybe Double)

-- | Provides face metadata (bounding box and confidence that the bounding
--   box actually contains a face).
compareFacesMatch_face :: Lens' CompareFacesMatch (Maybe ComparedFace)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.CompareFacesMatch.CompareFacesMatch
instance GHC.Show.Show Network.AWS.Rekognition.Types.CompareFacesMatch.CompareFacesMatch
instance GHC.Read.Read Network.AWS.Rekognition.Types.CompareFacesMatch.CompareFacesMatch
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.CompareFacesMatch.CompareFacesMatch
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.CompareFacesMatch.CompareFacesMatch
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.CompareFacesMatch.CompareFacesMatch
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.CompareFacesMatch.CompareFacesMatch


module Network.AWS.Rekognition.Types.Celebrity

-- | Provides information about a celebrity recognized by the
--   RecognizeCelebrities operation.
--   
--   <i>See:</i> <a>newCelebrity</a> smart constructor.
data Celebrity
Celebrity' :: Maybe Double -> Maybe [Text] -> Maybe KnownGender -> Maybe Text -> Maybe Text -> Maybe ComparedFace -> Celebrity

-- | The confidence, in percentage, that Amazon Rekognition has that the
--   recognized face is the celebrity.
[$sel:matchConfidence:Celebrity'] :: Celebrity -> Maybe Double

-- | An array of URLs pointing to additional information about the
--   celebrity. If there is no additional information about the celebrity,
--   this list is empty.
[$sel:urls:Celebrity'] :: Celebrity -> Maybe [Text]
[$sel:knownGender:Celebrity'] :: Celebrity -> Maybe KnownGender

-- | The name of the celebrity.
[$sel:name:Celebrity'] :: Celebrity -> Maybe Text

-- | A unique identifier for the celebrity.
[$sel:id:Celebrity'] :: Celebrity -> Maybe Text

-- | Provides information about the celebrity's face, such as its location
--   on the image.
[$sel:face:Celebrity'] :: Celebrity -> Maybe ComparedFace

-- | Create a value of <a>Celebrity</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:matchConfidence:Celebrity'</a>,
--   <a>celebrity_matchConfidence</a> - The confidence, in percentage, that
--   Amazon Rekognition has that the recognized face is the celebrity.
--   
--   <a>$sel:urls:Celebrity'</a>, <a>celebrity_urls</a> - An array of URLs
--   pointing to additional information about the celebrity. If there is no
--   additional information about the celebrity, this list is empty.
--   
--   <a>$sel:knownGender:Celebrity'</a>, <a>celebrity_knownGender</a> -
--   Undocumented member.
--   
--   <a>$sel:name:Celebrity'</a>, <a>celebrity_name</a> - The name of the
--   celebrity.
--   
--   <a>$sel:id:Celebrity'</a>, <a>celebrity_id</a> - A unique identifier
--   for the celebrity.
--   
--   <a>$sel:face:Celebrity'</a>, <a>celebrity_face</a> - Provides
--   information about the celebrity's face, such as its location on the
--   image.
newCelebrity :: Celebrity

-- | The confidence, in percentage, that Amazon Rekognition has that the
--   recognized face is the celebrity.
celebrity_matchConfidence :: Lens' Celebrity (Maybe Double)

-- | An array of URLs pointing to additional information about the
--   celebrity. If there is no additional information about the celebrity,
--   this list is empty.
celebrity_urls :: Lens' Celebrity (Maybe [Text])

-- | Undocumented member.
celebrity_knownGender :: Lens' Celebrity (Maybe KnownGender)

-- | The name of the celebrity.
celebrity_name :: Lens' Celebrity (Maybe Text)

-- | A unique identifier for the celebrity.
celebrity_id :: Lens' Celebrity (Maybe Text)

-- | Provides information about the celebrity's face, such as its location
--   on the image.
celebrity_face :: Lens' Celebrity (Maybe ComparedFace)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.Celebrity.Celebrity
instance GHC.Show.Show Network.AWS.Rekognition.Types.Celebrity.Celebrity
instance GHC.Read.Read Network.AWS.Rekognition.Types.Celebrity.Celebrity
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.Celebrity.Celebrity
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.Celebrity.Celebrity
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.Celebrity.Celebrity
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.Celebrity.Celebrity


module Network.AWS.Rekognition.Types.StartShotDetectionFilter

-- | Filters for the shot detection segments returned by
--   <tt>GetSegmentDetection</tt>. For more information, see
--   StartSegmentDetectionFilters.
--   
--   <i>See:</i> <a>newStartShotDetectionFilter</a> smart constructor.
data StartShotDetectionFilter
StartShotDetectionFilter' :: Maybe Double -> StartShotDetectionFilter

-- | Specifies the minimum confidence that Amazon Rekognition Video must
--   have in order to return a detected segment. Confidence represents how
--   certain Amazon Rekognition is that a segment is correctly identified.
--   0 is the lowest confidence. 100 is the highest confidence. Amazon
--   Rekognition Video doesn't return any segments with a confidence level
--   lower than this specified value.
--   
--   If you don't specify <tt>MinSegmentConfidence</tt>, the
--   <tt>GetSegmentDetection</tt> returns segments with confidence values
--   greater than or equal to 50 percent.
[$sel:minSegmentConfidence:StartShotDetectionFilter'] :: StartShotDetectionFilter -> Maybe Double

-- | Create a value of <a>StartShotDetectionFilter</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:minSegmentConfidence:StartShotDetectionFilter'</a>,
--   <a>startShotDetectionFilter_minSegmentConfidence</a> - Specifies the
--   minimum confidence that Amazon Rekognition Video must have in order to
--   return a detected segment. Confidence represents how certain Amazon
--   Rekognition is that a segment is correctly identified. 0 is the lowest
--   confidence. 100 is the highest confidence. Amazon Rekognition Video
--   doesn't return any segments with a confidence level lower than this
--   specified value.
--   
--   If you don't specify <tt>MinSegmentConfidence</tt>, the
--   <tt>GetSegmentDetection</tt> returns segments with confidence values
--   greater than or equal to 50 percent.
newStartShotDetectionFilter :: StartShotDetectionFilter

-- | Specifies the minimum confidence that Amazon Rekognition Video must
--   have in order to return a detected segment. Confidence represents how
--   certain Amazon Rekognition is that a segment is correctly identified.
--   0 is the lowest confidence. 100 is the highest confidence. Amazon
--   Rekognition Video doesn't return any segments with a confidence level
--   lower than this specified value.
--   
--   If you don't specify <tt>MinSegmentConfidence</tt>, the
--   <tt>GetSegmentDetection</tt> returns segments with confidence values
--   greater than or equal to 50 percent.
startShotDetectionFilter_minSegmentConfidence :: Lens' StartShotDetectionFilter (Maybe Double)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.StartShotDetectionFilter.StartShotDetectionFilter
instance GHC.Show.Show Network.AWS.Rekognition.Types.StartShotDetectionFilter.StartShotDetectionFilter
instance GHC.Read.Read Network.AWS.Rekognition.Types.StartShotDetectionFilter.StartShotDetectionFilter
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.StartShotDetectionFilter.StartShotDetectionFilter
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.StartShotDetectionFilter.StartShotDetectionFilter
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.StartShotDetectionFilter.StartShotDetectionFilter
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.Types.StartShotDetectionFilter.StartShotDetectionFilter


module Network.AWS.Rekognition.Types.StartTechnicalCueDetectionFilter

-- | Filters for the technical segments returned by GetSegmentDetection.
--   For more information, see StartSegmentDetectionFilters.
--   
--   <i>See:</i> <a>newStartTechnicalCueDetectionFilter</a> smart
--   constructor.
data StartTechnicalCueDetectionFilter
StartTechnicalCueDetectionFilter' :: Maybe BlackFrame -> Maybe Double -> StartTechnicalCueDetectionFilter

-- | A filter that allows you to control the black frame detection by
--   specifying the black levels and pixel coverage of black pixels in a
--   frame. Videos can come from multiple sources, formats, and time
--   periods, with different standards and varying noise levels for black
--   frames that need to be accounted for.
[$sel:blackFrame:StartTechnicalCueDetectionFilter'] :: StartTechnicalCueDetectionFilter -> Maybe BlackFrame

-- | Specifies the minimum confidence that Amazon Rekognition Video must
--   have in order to return a detected segment. Confidence represents how
--   certain Amazon Rekognition is that a segment is correctly identified.
--   0 is the lowest confidence. 100 is the highest confidence. Amazon
--   Rekognition Video doesn't return any segments with a confidence level
--   lower than this specified value.
--   
--   If you don't specify <tt>MinSegmentConfidence</tt>,
--   <tt>GetSegmentDetection</tt> returns segments with confidence values
--   greater than or equal to 50 percent.
[$sel:minSegmentConfidence:StartTechnicalCueDetectionFilter'] :: StartTechnicalCueDetectionFilter -> Maybe Double

-- | Create a value of <a>StartTechnicalCueDetectionFilter</a> with all
--   optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:blackFrame:StartTechnicalCueDetectionFilter'</a>,
--   <a>startTechnicalCueDetectionFilter_blackFrame</a> - A filter that
--   allows you to control the black frame detection by specifying the
--   black levels and pixel coverage of black pixels in a frame. Videos can
--   come from multiple sources, formats, and time periods, with different
--   standards and varying noise levels for black frames that need to be
--   accounted for.
--   
--   <a>$sel:minSegmentConfidence:StartTechnicalCueDetectionFilter'</a>,
--   <a>startTechnicalCueDetectionFilter_minSegmentConfidence</a> -
--   Specifies the minimum confidence that Amazon Rekognition Video must
--   have in order to return a detected segment. Confidence represents how
--   certain Amazon Rekognition is that a segment is correctly identified.
--   0 is the lowest confidence. 100 is the highest confidence. Amazon
--   Rekognition Video doesn't return any segments with a confidence level
--   lower than this specified value.
--   
--   If you don't specify <tt>MinSegmentConfidence</tt>,
--   <tt>GetSegmentDetection</tt> returns segments with confidence values
--   greater than or equal to 50 percent.
newStartTechnicalCueDetectionFilter :: StartTechnicalCueDetectionFilter

-- | A filter that allows you to control the black frame detection by
--   specifying the black levels and pixel coverage of black pixels in a
--   frame. Videos can come from multiple sources, formats, and time
--   periods, with different standards and varying noise levels for black
--   frames that need to be accounted for.
startTechnicalCueDetectionFilter_blackFrame :: Lens' StartTechnicalCueDetectionFilter (Maybe BlackFrame)

-- | Specifies the minimum confidence that Amazon Rekognition Video must
--   have in order to return a detected segment. Confidence represents how
--   certain Amazon Rekognition is that a segment is correctly identified.
--   0 is the lowest confidence. 100 is the highest confidence. Amazon
--   Rekognition Video doesn't return any segments with a confidence level
--   lower than this specified value.
--   
--   If you don't specify <tt>MinSegmentConfidence</tt>,
--   <tt>GetSegmentDetection</tt> returns segments with confidence values
--   greater than or equal to 50 percent.
startTechnicalCueDetectionFilter_minSegmentConfidence :: Lens' StartTechnicalCueDetectionFilter (Maybe Double)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.StartTechnicalCueDetectionFilter.StartTechnicalCueDetectionFilter
instance GHC.Show.Show Network.AWS.Rekognition.Types.StartTechnicalCueDetectionFilter.StartTechnicalCueDetectionFilter
instance GHC.Read.Read Network.AWS.Rekognition.Types.StartTechnicalCueDetectionFilter.StartTechnicalCueDetectionFilter
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.StartTechnicalCueDetectionFilter.StartTechnicalCueDetectionFilter
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.StartTechnicalCueDetectionFilter.StartTechnicalCueDetectionFilter
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.StartTechnicalCueDetectionFilter.StartTechnicalCueDetectionFilter
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.Types.StartTechnicalCueDetectionFilter.StartTechnicalCueDetectionFilter


module Network.AWS.Rekognition.Types.StartSegmentDetectionFilters

-- | Filters applied to the technical cue or shot detection segments. For
--   more information, see StartSegmentDetection.
--   
--   <i>See:</i> <a>newStartSegmentDetectionFilters</a> smart constructor.
data StartSegmentDetectionFilters
StartSegmentDetectionFilters' :: Maybe StartTechnicalCueDetectionFilter -> Maybe StartShotDetectionFilter -> StartSegmentDetectionFilters

-- | Filters that are specific to technical cues.
[$sel:technicalCueFilter:StartSegmentDetectionFilters'] :: StartSegmentDetectionFilters -> Maybe StartTechnicalCueDetectionFilter

-- | Filters that are specific to shot detections.
[$sel:shotFilter:StartSegmentDetectionFilters'] :: StartSegmentDetectionFilters -> Maybe StartShotDetectionFilter

-- | Create a value of <a>StartSegmentDetectionFilters</a> with all
--   optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:technicalCueFilter:StartSegmentDetectionFilters'</a>,
--   <a>startSegmentDetectionFilters_technicalCueFilter</a> - Filters that
--   are specific to technical cues.
--   
--   <a>$sel:shotFilter:StartSegmentDetectionFilters'</a>,
--   <a>startSegmentDetectionFilters_shotFilter</a> - Filters that are
--   specific to shot detections.
newStartSegmentDetectionFilters :: StartSegmentDetectionFilters

-- | Filters that are specific to technical cues.
startSegmentDetectionFilters_technicalCueFilter :: Lens' StartSegmentDetectionFilters (Maybe StartTechnicalCueDetectionFilter)

-- | Filters that are specific to shot detections.
startSegmentDetectionFilters_shotFilter :: Lens' StartSegmentDetectionFilters (Maybe StartShotDetectionFilter)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.StartSegmentDetectionFilters.StartSegmentDetectionFilters
instance GHC.Show.Show Network.AWS.Rekognition.Types.StartSegmentDetectionFilters.StartSegmentDetectionFilters
instance GHC.Read.Read Network.AWS.Rekognition.Types.StartSegmentDetectionFilters.StartSegmentDetectionFilters
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.StartSegmentDetectionFilters.StartSegmentDetectionFilters
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.StartSegmentDetectionFilters.StartSegmentDetectionFilters
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.StartSegmentDetectionFilters.StartSegmentDetectionFilters
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.Types.StartSegmentDetectionFilters.StartSegmentDetectionFilters


module Network.AWS.Rekognition.Types.StartTextDetectionFilters

-- | Set of optional parameters that let you set the criteria text must
--   meet to be included in your response. <tt>WordFilter</tt> looks at a
--   word's height, width and minimum confidence. <tt>RegionOfInterest</tt>
--   lets you set a specific region of the screen to look for text in.
--   
--   <i>See:</i> <a>newStartTextDetectionFilters</a> smart constructor.
data StartTextDetectionFilters
StartTextDetectionFilters' :: Maybe [RegionOfInterest] -> Maybe DetectionFilter -> StartTextDetectionFilters

-- | Filter focusing on a certain area of the frame. Uses a
--   <tt>BoundingBox</tt> object to set the region of the screen.
[$sel:regionsOfInterest:StartTextDetectionFilters'] :: StartTextDetectionFilters -> Maybe [RegionOfInterest]

-- | Filters focusing on qualities of the text, such as confidence or size.
[$sel:wordFilter:StartTextDetectionFilters'] :: StartTextDetectionFilters -> Maybe DetectionFilter

-- | Create a value of <a>StartTextDetectionFilters</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:regionsOfInterest:StartTextDetectionFilters'</a>,
--   <a>startTextDetectionFilters_regionsOfInterest</a> - Filter focusing
--   on a certain area of the frame. Uses a <tt>BoundingBox</tt> object to
--   set the region of the screen.
--   
--   <a>$sel:wordFilter:StartTextDetectionFilters'</a>,
--   <a>startTextDetectionFilters_wordFilter</a> - Filters focusing on
--   qualities of the text, such as confidence or size.
newStartTextDetectionFilters :: StartTextDetectionFilters

-- | Filter focusing on a certain area of the frame. Uses a
--   <tt>BoundingBox</tt> object to set the region of the screen.
startTextDetectionFilters_regionsOfInterest :: Lens' StartTextDetectionFilters (Maybe [RegionOfInterest])

-- | Filters focusing on qualities of the text, such as confidence or size.
startTextDetectionFilters_wordFilter :: Lens' StartTextDetectionFilters (Maybe DetectionFilter)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.StartTextDetectionFilters.StartTextDetectionFilters
instance GHC.Show.Show Network.AWS.Rekognition.Types.StartTextDetectionFilters.StartTextDetectionFilters
instance GHC.Read.Read Network.AWS.Rekognition.Types.StartTextDetectionFilters.StartTextDetectionFilters
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.StartTextDetectionFilters.StartTextDetectionFilters
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.StartTextDetectionFilters.StartTextDetectionFilters
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.StartTextDetectionFilters.StartTextDetectionFilters
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.Types.StartTextDetectionFilters.StartTextDetectionFilters


module Network.AWS.Rekognition.Types.StreamProcessorInput

-- | Information about the source streaming video.
--   
--   <i>See:</i> <a>newStreamProcessorInput</a> smart constructor.
data StreamProcessorInput
StreamProcessorInput' :: Maybe KinesisVideoStream -> StreamProcessorInput

-- | The Kinesis video stream input stream for the source streaming video.
[$sel:kinesisVideoStream:StreamProcessorInput'] :: StreamProcessorInput -> Maybe KinesisVideoStream

-- | Create a value of <a>StreamProcessorInput</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:kinesisVideoStream:StreamProcessorInput'</a>,
--   <a>streamProcessorInput_kinesisVideoStream</a> - The Kinesis video
--   stream input stream for the source streaming video.
newStreamProcessorInput :: StreamProcessorInput

-- | The Kinesis video stream input stream for the source streaming video.
streamProcessorInput_kinesisVideoStream :: Lens' StreamProcessorInput (Maybe KinesisVideoStream)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.StreamProcessorInput.StreamProcessorInput
instance GHC.Show.Show Network.AWS.Rekognition.Types.StreamProcessorInput.StreamProcessorInput
instance GHC.Read.Read Network.AWS.Rekognition.Types.StreamProcessorInput.StreamProcessorInput
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.StreamProcessorInput.StreamProcessorInput
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.StreamProcessorInput.StreamProcessorInput
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.StreamProcessorInput.StreamProcessorInput
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.StreamProcessorInput.StreamProcessorInput
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.Types.StreamProcessorInput.StreamProcessorInput


module Network.AWS.Rekognition.Types.StreamProcessorOutput

-- | Information about the Amazon Kinesis Data Streams stream to which a
--   Amazon Rekognition Video stream processor streams the results of a
--   video analysis. For more information, see CreateStreamProcessor in the
--   Amazon Rekognition Developer Guide.
--   
--   <i>See:</i> <a>newStreamProcessorOutput</a> smart constructor.
data StreamProcessorOutput
StreamProcessorOutput' :: Maybe KinesisDataStream -> StreamProcessorOutput

-- | The Amazon Kinesis Data Streams stream to which the Amazon Rekognition
--   stream processor streams the analysis results.
[$sel:kinesisDataStream:StreamProcessorOutput'] :: StreamProcessorOutput -> Maybe KinesisDataStream

-- | Create a value of <a>StreamProcessorOutput</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:kinesisDataStream:StreamProcessorOutput'</a>,
--   <a>streamProcessorOutput_kinesisDataStream</a> - The Amazon Kinesis
--   Data Streams stream to which the Amazon Rekognition stream processor
--   streams the analysis results.
newStreamProcessorOutput :: StreamProcessorOutput

-- | The Amazon Kinesis Data Streams stream to which the Amazon Rekognition
--   stream processor streams the analysis results.
streamProcessorOutput_kinesisDataStream :: Lens' StreamProcessorOutput (Maybe KinesisDataStream)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.StreamProcessorOutput.StreamProcessorOutput
instance GHC.Show.Show Network.AWS.Rekognition.Types.StreamProcessorOutput.StreamProcessorOutput
instance GHC.Read.Read Network.AWS.Rekognition.Types.StreamProcessorOutput.StreamProcessorOutput
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.StreamProcessorOutput.StreamProcessorOutput
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.StreamProcessorOutput.StreamProcessorOutput
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.StreamProcessorOutput.StreamProcessorOutput
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.StreamProcessorOutput.StreamProcessorOutput
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.Types.StreamProcessorOutput.StreamProcessorOutput


module Network.AWS.Rekognition.Types.StreamProcessorSettings

-- | Input parameters used to recognize faces in a streaming video analyzed
--   by a Amazon Rekognition stream processor.
--   
--   <i>See:</i> <a>newStreamProcessorSettings</a> smart constructor.
data StreamProcessorSettings
StreamProcessorSettings' :: Maybe FaceSearchSettings -> StreamProcessorSettings

-- | Face search settings to use on a streaming video.
[$sel:faceSearch:StreamProcessorSettings'] :: StreamProcessorSettings -> Maybe FaceSearchSettings

-- | Create a value of <a>StreamProcessorSettings</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:faceSearch:StreamProcessorSettings'</a>,
--   <a>streamProcessorSettings_faceSearch</a> - Face search settings to
--   use on a streaming video.
newStreamProcessorSettings :: StreamProcessorSettings

-- | Face search settings to use on a streaming video.
streamProcessorSettings_faceSearch :: Lens' StreamProcessorSettings (Maybe FaceSearchSettings)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.StreamProcessorSettings.StreamProcessorSettings
instance GHC.Show.Show Network.AWS.Rekognition.Types.StreamProcessorSettings.StreamProcessorSettings
instance GHC.Read.Read Network.AWS.Rekognition.Types.StreamProcessorSettings.StreamProcessorSettings
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.StreamProcessorSettings.StreamProcessorSettings
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.StreamProcessorSettings.StreamProcessorSettings
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.StreamProcessorSettings.StreamProcessorSettings
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.StreamProcessorSettings.StreamProcessorSettings
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.Types.StreamProcessorSettings.StreamProcessorSettings


module Network.AWS.Rekognition.Types.StreamProcessorStatus
newtype StreamProcessorStatus
StreamProcessorStatus' :: Text -> StreamProcessorStatus
[fromStreamProcessorStatus] :: StreamProcessorStatus -> Text
pattern StreamProcessorStatus_FAILED :: StreamProcessorStatus
pattern StreamProcessorStatus_RUNNING :: StreamProcessorStatus
pattern StreamProcessorStatus_STARTING :: StreamProcessorStatus
pattern StreamProcessorStatus_STOPPED :: StreamProcessorStatus
pattern StreamProcessorStatus_STOPPING :: StreamProcessorStatus
instance Network.AWS.Data.XML.ToXML Network.AWS.Rekognition.Types.StreamProcessorStatus.StreamProcessorStatus
instance Network.AWS.Data.XML.FromXML Network.AWS.Rekognition.Types.StreamProcessorStatus.StreamProcessorStatus
instance Data.Aeson.Types.ToJSON.ToJSONKey Network.AWS.Rekognition.Types.StreamProcessorStatus.StreamProcessorStatus
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.Types.StreamProcessorStatus.StreamProcessorStatus
instance Data.Aeson.Types.FromJSON.FromJSONKey Network.AWS.Rekognition.Types.StreamProcessorStatus.StreamProcessorStatus
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.StreamProcessorStatus.StreamProcessorStatus
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.Types.StreamProcessorStatus.StreamProcessorStatus
instance Network.AWS.Data.Headers.ToHeader Network.AWS.Rekognition.Types.StreamProcessorStatus.StreamProcessorStatus
instance Network.AWS.Data.Log.ToLog Network.AWS.Rekognition.Types.StreamProcessorStatus.StreamProcessorStatus
instance Network.AWS.Data.ByteString.ToByteString Network.AWS.Rekognition.Types.StreamProcessorStatus.StreamProcessorStatus
instance Network.AWS.Data.Text.ToText Network.AWS.Rekognition.Types.StreamProcessorStatus.StreamProcessorStatus
instance Network.AWS.Data.Text.FromText Network.AWS.Rekognition.Types.StreamProcessorStatus.StreamProcessorStatus
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.StreamProcessorStatus.StreamProcessorStatus
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.StreamProcessorStatus.StreamProcessorStatus
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.StreamProcessorStatus.StreamProcessorStatus
instance GHC.Classes.Ord Network.AWS.Rekognition.Types.StreamProcessorStatus.StreamProcessorStatus
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.StreamProcessorStatus.StreamProcessorStatus
instance GHC.Read.Read Network.AWS.Rekognition.Types.StreamProcessorStatus.StreamProcessorStatus
instance GHC.Show.Show Network.AWS.Rekognition.Types.StreamProcessorStatus.StreamProcessorStatus


module Network.AWS.Rekognition.Types.StreamProcessor

-- | An object that recognizes faces in a streaming video. An Amazon
--   Rekognition stream processor is created by a call to
--   CreateStreamProcessor. The request parameters for
--   <tt>CreateStreamProcessor</tt> describe the Kinesis video stream
--   source for the streaming video, face recognition parameters, and where
--   to stream the analysis resullts.
--   
--   <i>See:</i> <a>newStreamProcessor</a> smart constructor.
data StreamProcessor
StreamProcessor' :: Maybe StreamProcessorStatus -> Maybe Text -> StreamProcessor

-- | Current status of the Amazon Rekognition stream processor.
[$sel:status:StreamProcessor'] :: StreamProcessor -> Maybe StreamProcessorStatus

-- | Name of the Amazon Rekognition stream processor.
[$sel:name:StreamProcessor'] :: StreamProcessor -> Maybe Text

-- | Create a value of <a>StreamProcessor</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:status:StreamProcessor'</a>, <a>streamProcessor_status</a> -
--   Current status of the Amazon Rekognition stream processor.
--   
--   <a>$sel:name:StreamProcessor'</a>, <a>streamProcessor_name</a> - Name
--   of the Amazon Rekognition stream processor.
newStreamProcessor :: StreamProcessor

-- | Current status of the Amazon Rekognition stream processor.
streamProcessor_status :: Lens' StreamProcessor (Maybe StreamProcessorStatus)

-- | Name of the Amazon Rekognition stream processor.
streamProcessor_name :: Lens' StreamProcessor (Maybe Text)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.StreamProcessor.StreamProcessor
instance GHC.Show.Show Network.AWS.Rekognition.Types.StreamProcessor.StreamProcessor
instance GHC.Read.Read Network.AWS.Rekognition.Types.StreamProcessor.StreamProcessor
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.StreamProcessor.StreamProcessor
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.StreamProcessor.StreamProcessor
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.StreamProcessor.StreamProcessor
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.StreamProcessor.StreamProcessor


module Network.AWS.Rekognition.Types.Summary

-- | The S3 bucket that contains the training summary. The training summary
--   includes aggregated evaluation metrics for the entire testing dataset
--   and metrics for each individual label.
--   
--   You get the training summary S3 bucket location by calling
--   DescribeProjectVersions.
--   
--   <i>See:</i> <a>newSummary</a> smart constructor.
data Summary
Summary' :: Maybe S3Object -> Summary
[$sel:s3Object:Summary'] :: Summary -> Maybe S3Object

-- | Create a value of <a>Summary</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:s3Object:Summary'</a>, <a>summary_s3Object</a> - Undocumented
--   member.
newSummary :: Summary

-- | Undocumented member.
summary_s3Object :: Lens' Summary (Maybe S3Object)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.Summary.Summary
instance GHC.Show.Show Network.AWS.Rekognition.Types.Summary.Summary
instance GHC.Read.Read Network.AWS.Rekognition.Types.Summary.Summary
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.Summary.Summary
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.Summary.Summary
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.Summary.Summary
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.Summary.Summary


module Network.AWS.Rekognition.Types.EvaluationResult

-- | The evaluation results for the training of a model.
--   
--   <i>See:</i> <a>newEvaluationResult</a> smart constructor.
data EvaluationResult
EvaluationResult' :: Maybe Summary -> Maybe Double -> EvaluationResult

-- | The S3 bucket that contains the training summary.
[$sel:summary:EvaluationResult'] :: EvaluationResult -> Maybe Summary

-- | The F1 score for the evaluation of all labels. The F1 score metric
--   evaluates the overall precision and recall performance of the model as
--   a single value. A higher value indicates better precision and recall
--   performance. A lower score indicates that precision, recall, or both
--   are performing poorly.
[$sel:f1Score:EvaluationResult'] :: EvaluationResult -> Maybe Double

-- | Create a value of <a>EvaluationResult</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:summary:EvaluationResult'</a>, <a>evaluationResult_summary</a>
--   - The S3 bucket that contains the training summary.
--   
--   <a>$sel:f1Score:EvaluationResult'</a>, <a>evaluationResult_f1Score</a>
--   - The F1 score for the evaluation of all labels. The F1 score metric
--   evaluates the overall precision and recall performance of the model as
--   a single value. A higher value indicates better precision and recall
--   performance. A lower score indicates that precision, recall, or both
--   are performing poorly.
newEvaluationResult :: EvaluationResult

-- | The S3 bucket that contains the training summary.
evaluationResult_summary :: Lens' EvaluationResult (Maybe Summary)

-- | The F1 score for the evaluation of all labels. The F1 score metric
--   evaluates the overall precision and recall performance of the model as
--   a single value. A higher value indicates better precision and recall
--   performance. A lower score indicates that precision, recall, or both
--   are performing poorly.
evaluationResult_f1Score :: Lens' EvaluationResult (Maybe Double)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.EvaluationResult.EvaluationResult
instance GHC.Show.Show Network.AWS.Rekognition.Types.EvaluationResult.EvaluationResult
instance GHC.Read.Read Network.AWS.Rekognition.Types.EvaluationResult.EvaluationResult
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.EvaluationResult.EvaluationResult
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.EvaluationResult.EvaluationResult
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.EvaluationResult.EvaluationResult
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.EvaluationResult.EvaluationResult


module Network.AWS.Rekognition.Types.Sunglasses

-- | Indicates whether or not the face is wearing sunglasses, and the
--   confidence level in the determination.
--   
--   <i>See:</i> <a>newSunglasses</a> smart constructor.
data Sunglasses
Sunglasses' :: Maybe Bool -> Maybe Double -> Sunglasses

-- | Boolean value that indicates whether the face is wearing sunglasses or
--   not.
[$sel:value:Sunglasses'] :: Sunglasses -> Maybe Bool

-- | Level of confidence in the determination.
[$sel:confidence:Sunglasses'] :: Sunglasses -> Maybe Double

-- | Create a value of <a>Sunglasses</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:value:Sunglasses'</a>, <a>sunglasses_value</a> - Boolean value
--   that indicates whether the face is wearing sunglasses or not.
--   
--   <a>$sel:confidence:Sunglasses'</a>, <a>sunglasses_confidence</a> -
--   Level of confidence in the determination.
newSunglasses :: Sunglasses

-- | Boolean value that indicates whether the face is wearing sunglasses or
--   not.
sunglasses_value :: Lens' Sunglasses (Maybe Bool)

-- | Level of confidence in the determination.
sunglasses_confidence :: Lens' Sunglasses (Maybe Double)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.Sunglasses.Sunglasses
instance GHC.Show.Show Network.AWS.Rekognition.Types.Sunglasses.Sunglasses
instance GHC.Read.Read Network.AWS.Rekognition.Types.Sunglasses.Sunglasses
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.Sunglasses.Sunglasses
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.Sunglasses.Sunglasses
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.Sunglasses.Sunglasses
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.Sunglasses.Sunglasses


module Network.AWS.Rekognition.Types.FaceDetail

-- | Structure containing attributes of the face that the algorithm
--   detected.
--   
--   A <tt>FaceDetail</tt> object contains either the default facial
--   attributes or all facial attributes. The default attributes are
--   <tt>BoundingBox</tt>, <tt>Confidence</tt>, <tt>Landmarks</tt>,
--   <tt>Pose</tt>, and <tt>Quality</tt>.
--   
--   GetFaceDetection is the only Amazon Rekognition Video stored video
--   operation that can return a <tt>FaceDetail</tt> object with all
--   attributes. To specify which attributes to return, use the
--   <tt>FaceAttributes</tt> input parameter for StartFaceDetection. The
--   following Amazon Rekognition Video operations return only the default
--   attributes. The corresponding Start operations don't have a
--   <tt>FaceAttributes</tt> input parameter.
--   
--   <ul>
--   <li>GetCelebrityRecognition</li>
--   <li>GetPersonTracking</li>
--   <li>GetFaceSearch</li>
--   </ul>
--   
--   The Amazon Rekognition Image DetectFaces and IndexFaces operations can
--   return all facial attributes. To specify which attributes to return,
--   use the <tt>Attributes</tt> input parameter for <tt>DetectFaces</tt>.
--   For <tt>IndexFaces</tt>, use the <tt>DetectAttributes</tt> input
--   parameter.
--   
--   <i>See:</i> <a>newFaceDetail</a> smart constructor.
data FaceDetail
FaceDetail' :: Maybe AgeRange -> Maybe Sunglasses -> Maybe MouthOpen -> Maybe BoundingBox -> Maybe [Emotion] -> Maybe EyeOpen -> Maybe Pose -> Maybe Double -> Maybe Gender -> Maybe ImageQuality -> Maybe Eyeglasses -> Maybe Beard -> Maybe Mustache -> Maybe Smile -> Maybe [Landmark] -> FaceDetail

-- | The estimated age range, in years, for the face. Low represents the
--   lowest estimated age and High represents the highest estimated age.
[$sel:ageRange:FaceDetail'] :: FaceDetail -> Maybe AgeRange

-- | Indicates whether or not the face is wearing sunglasses, and the
--   confidence level in the determination.
[$sel:sunglasses:FaceDetail'] :: FaceDetail -> Maybe Sunglasses

-- | Indicates whether or not the mouth on the face is open, and the
--   confidence level in the determination.
[$sel:mouthOpen:FaceDetail'] :: FaceDetail -> Maybe MouthOpen

-- | Bounding box of the face. Default attribute.
[$sel:boundingBox:FaceDetail'] :: FaceDetail -> Maybe BoundingBox

-- | The emotions that appear to be expressed on the face, and the
--   confidence level in the determination. The API is only making a
--   determination of the physical appearance of a person's face. It is not
--   a determination of the person’s internal emotional state and should
--   not be used in such a way. For example, a person pretending to have a
--   sad face might not be sad emotionally.
[$sel:emotions:FaceDetail'] :: FaceDetail -> Maybe [Emotion]

-- | Indicates whether or not the eyes on the face are open, and the
--   confidence level in the determination.
[$sel:eyesOpen:FaceDetail'] :: FaceDetail -> Maybe EyeOpen

-- | Indicates the pose of the face as determined by its pitch, roll, and
--   yaw. Default attribute.
[$sel:pose:FaceDetail'] :: FaceDetail -> Maybe Pose

-- | Confidence level that the bounding box contains a face (and not a
--   different object such as a tree). Default attribute.
[$sel:confidence:FaceDetail'] :: FaceDetail -> Maybe Double

-- | The predicted gender of a detected face.
[$sel:gender:FaceDetail'] :: FaceDetail -> Maybe Gender

-- | Identifies image brightness and sharpness. Default attribute.
[$sel:quality:FaceDetail'] :: FaceDetail -> Maybe ImageQuality

-- | Indicates whether or not the face is wearing eye glasses, and the
--   confidence level in the determination.
[$sel:eyeglasses:FaceDetail'] :: FaceDetail -> Maybe Eyeglasses

-- | Indicates whether or not the face has a beard, and the confidence
--   level in the determination.
[$sel:beard:FaceDetail'] :: FaceDetail -> Maybe Beard

-- | Indicates whether or not the face has a mustache, and the confidence
--   level in the determination.
[$sel:mustache:FaceDetail'] :: FaceDetail -> Maybe Mustache

-- | Indicates whether or not the face is smiling, and the confidence level
--   in the determination.
[$sel:smile:FaceDetail'] :: FaceDetail -> Maybe Smile

-- | Indicates the location of landmarks on the face. Default attribute.
[$sel:landmarks:FaceDetail'] :: FaceDetail -> Maybe [Landmark]

-- | Create a value of <a>FaceDetail</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:ageRange:FaceDetail'</a>, <a>faceDetail_ageRange</a> - The
--   estimated age range, in years, for the face. Low represents the lowest
--   estimated age and High represents the highest estimated age.
--   
--   <a>$sel:sunglasses:FaceDetail'</a>, <a>faceDetail_sunglasses</a> -
--   Indicates whether or not the face is wearing sunglasses, and the
--   confidence level in the determination.
--   
--   <a>$sel:mouthOpen:FaceDetail'</a>, <a>faceDetail_mouthOpen</a> -
--   Indicates whether or not the mouth on the face is open, and the
--   confidence level in the determination.
--   
--   <a>$sel:boundingBox:FaceDetail'</a>, <a>faceDetail_boundingBox</a> -
--   Bounding box of the face. Default attribute.
--   
--   <a>$sel:emotions:FaceDetail'</a>, <a>faceDetail_emotions</a> - The
--   emotions that appear to be expressed on the face, and the confidence
--   level in the determination. The API is only making a determination of
--   the physical appearance of a person's face. It is not a determination
--   of the person’s internal emotional state and should not be used in
--   such a way. For example, a person pretending to have a sad face might
--   not be sad emotionally.
--   
--   <a>$sel:eyesOpen:FaceDetail'</a>, <a>faceDetail_eyesOpen</a> -
--   Indicates whether or not the eyes on the face are open, and the
--   confidence level in the determination.
--   
--   <a>$sel:pose:FaceDetail'</a>, <a>faceDetail_pose</a> - Indicates the
--   pose of the face as determined by its pitch, roll, and yaw. Default
--   attribute.
--   
--   <a>$sel:confidence:FaceDetail'</a>, <a>faceDetail_confidence</a> -
--   Confidence level that the bounding box contains a face (and not a
--   different object such as a tree). Default attribute.
--   
--   <a>$sel:gender:FaceDetail'</a>, <a>faceDetail_gender</a> - The
--   predicted gender of a detected face.
--   
--   <a>$sel:quality:FaceDetail'</a>, <a>faceDetail_quality</a> -
--   Identifies image brightness and sharpness. Default attribute.
--   
--   <a>$sel:eyeglasses:FaceDetail'</a>, <a>faceDetail_eyeglasses</a> -
--   Indicates whether or not the face is wearing eye glasses, and the
--   confidence level in the determination.
--   
--   <a>$sel:beard:FaceDetail'</a>, <a>faceDetail_beard</a> - Indicates
--   whether or not the face has a beard, and the confidence level in the
--   determination.
--   
--   <a>$sel:mustache:FaceDetail'</a>, <a>faceDetail_mustache</a> -
--   Indicates whether or not the face has a mustache, and the confidence
--   level in the determination.
--   
--   <a>$sel:smile:FaceDetail'</a>, <a>faceDetail_smile</a> - Indicates
--   whether or not the face is smiling, and the confidence level in the
--   determination.
--   
--   <a>$sel:landmarks:FaceDetail'</a>, <a>faceDetail_landmarks</a> -
--   Indicates the location of landmarks on the face. Default attribute.
newFaceDetail :: FaceDetail

-- | The estimated age range, in years, for the face. Low represents the
--   lowest estimated age and High represents the highest estimated age.
faceDetail_ageRange :: Lens' FaceDetail (Maybe AgeRange)

-- | Indicates whether or not the face is wearing sunglasses, and the
--   confidence level in the determination.
faceDetail_sunglasses :: Lens' FaceDetail (Maybe Sunglasses)

-- | Indicates whether or not the mouth on the face is open, and the
--   confidence level in the determination.
faceDetail_mouthOpen :: Lens' FaceDetail (Maybe MouthOpen)

-- | Bounding box of the face. Default attribute.
faceDetail_boundingBox :: Lens' FaceDetail (Maybe BoundingBox)

-- | The emotions that appear to be expressed on the face, and the
--   confidence level in the determination. The API is only making a
--   determination of the physical appearance of a person's face. It is not
--   a determination of the person’s internal emotional state and should
--   not be used in such a way. For example, a person pretending to have a
--   sad face might not be sad emotionally.
faceDetail_emotions :: Lens' FaceDetail (Maybe [Emotion])

-- | Indicates whether or not the eyes on the face are open, and the
--   confidence level in the determination.
faceDetail_eyesOpen :: Lens' FaceDetail (Maybe EyeOpen)

-- | Indicates the pose of the face as determined by its pitch, roll, and
--   yaw. Default attribute.
faceDetail_pose :: Lens' FaceDetail (Maybe Pose)

-- | Confidence level that the bounding box contains a face (and not a
--   different object such as a tree). Default attribute.
faceDetail_confidence :: Lens' FaceDetail (Maybe Double)

-- | The predicted gender of a detected face.
faceDetail_gender :: Lens' FaceDetail (Maybe Gender)

-- | Identifies image brightness and sharpness. Default attribute.
faceDetail_quality :: Lens' FaceDetail (Maybe ImageQuality)

-- | Indicates whether or not the face is wearing eye glasses, and the
--   confidence level in the determination.
faceDetail_eyeglasses :: Lens' FaceDetail (Maybe Eyeglasses)

-- | Indicates whether or not the face has a beard, and the confidence
--   level in the determination.
faceDetail_beard :: Lens' FaceDetail (Maybe Beard)

-- | Indicates whether or not the face has a mustache, and the confidence
--   level in the determination.
faceDetail_mustache :: Lens' FaceDetail (Maybe Mustache)

-- | Indicates whether or not the face is smiling, and the confidence level
--   in the determination.
faceDetail_smile :: Lens' FaceDetail (Maybe Smile)

-- | Indicates the location of landmarks on the face. Default attribute.
faceDetail_landmarks :: Lens' FaceDetail (Maybe [Landmark])
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.FaceDetail.FaceDetail
instance GHC.Show.Show Network.AWS.Rekognition.Types.FaceDetail.FaceDetail
instance GHC.Read.Read Network.AWS.Rekognition.Types.FaceDetail.FaceDetail
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.FaceDetail.FaceDetail
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.FaceDetail.FaceDetail
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.FaceDetail.FaceDetail
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.FaceDetail.FaceDetail


module Network.AWS.Rekognition.Types.PersonDetail

-- | Details about a person detected in a video analysis request.
--   
--   <i>See:</i> <a>newPersonDetail</a> smart constructor.
data PersonDetail
PersonDetail' :: Maybe BoundingBox -> Maybe Integer -> Maybe FaceDetail -> PersonDetail

-- | Bounding box around the detected person.
[$sel:boundingBox:PersonDetail'] :: PersonDetail -> Maybe BoundingBox

-- | Identifier for the person detected person within a video. Use to keep
--   track of the person throughout the video. The identifier is not stored
--   by Amazon Rekognition.
[$sel:index:PersonDetail'] :: PersonDetail -> Maybe Integer

-- | Face details for the detected person.
[$sel:face:PersonDetail'] :: PersonDetail -> Maybe FaceDetail

-- | Create a value of <a>PersonDetail</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:boundingBox:PersonDetail'</a>, <a>personDetail_boundingBox</a>
--   - Bounding box around the detected person.
--   
--   <a>$sel:index:PersonDetail'</a>, <a>personDetail_index</a> -
--   Identifier for the person detected person within a video. Use to keep
--   track of the person throughout the video. The identifier is not stored
--   by Amazon Rekognition.
--   
--   <a>$sel:face:PersonDetail'</a>, <a>personDetail_face</a> - Face
--   details for the detected person.
newPersonDetail :: PersonDetail

-- | Bounding box around the detected person.
personDetail_boundingBox :: Lens' PersonDetail (Maybe BoundingBox)

-- | Identifier for the person detected person within a video. Use to keep
--   track of the person throughout the video. The identifier is not stored
--   by Amazon Rekognition.
personDetail_index :: Lens' PersonDetail (Maybe Integer)

-- | Face details for the detected person.
personDetail_face :: Lens' PersonDetail (Maybe FaceDetail)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.PersonDetail.PersonDetail
instance GHC.Show.Show Network.AWS.Rekognition.Types.PersonDetail.PersonDetail
instance GHC.Read.Read Network.AWS.Rekognition.Types.PersonDetail.PersonDetail
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.PersonDetail.PersonDetail
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.PersonDetail.PersonDetail
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.PersonDetail.PersonDetail
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.PersonDetail.PersonDetail


module Network.AWS.Rekognition.Types.PersonMatch

-- | Information about a person whose face matches a face(s) in an Amazon
--   Rekognition collection. Includes information about the faces in the
--   Amazon Rekognition collection (FaceMatch), information about the
--   person (PersonDetail), and the time stamp for when the person was
--   detected in a video. An array of <tt>PersonMatch</tt> objects is
--   returned by GetFaceSearch.
--   
--   <i>See:</i> <a>newPersonMatch</a> smart constructor.
data PersonMatch
PersonMatch' :: Maybe [FaceMatch] -> Maybe PersonDetail -> Maybe Integer -> PersonMatch

-- | Information about the faces in the input collection that match the
--   face of a person in the video.
[$sel:faceMatches:PersonMatch'] :: PersonMatch -> Maybe [FaceMatch]

-- | Information about the matched person.
[$sel:person:PersonMatch'] :: PersonMatch -> Maybe PersonDetail

-- | The time, in milliseconds from the beginning of the video, that the
--   person was matched in the video.
[$sel:timestamp:PersonMatch'] :: PersonMatch -> Maybe Integer

-- | Create a value of <a>PersonMatch</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:faceMatches:PersonMatch'</a>, <a>personMatch_faceMatches</a> -
--   Information about the faces in the input collection that match the
--   face of a person in the video.
--   
--   <a>$sel:person:PersonMatch'</a>, <a>personMatch_person</a> -
--   Information about the matched person.
--   
--   <a>$sel:timestamp:PersonMatch'</a>, <a>personMatch_timestamp</a> - The
--   time, in milliseconds from the beginning of the video, that the person
--   was matched in the video.
newPersonMatch :: PersonMatch

-- | Information about the faces in the input collection that match the
--   face of a person in the video.
personMatch_faceMatches :: Lens' PersonMatch (Maybe [FaceMatch])

-- | Information about the matched person.
personMatch_person :: Lens' PersonMatch (Maybe PersonDetail)

-- | The time, in milliseconds from the beginning of the video, that the
--   person was matched in the video.
personMatch_timestamp :: Lens' PersonMatch (Maybe Integer)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.PersonMatch.PersonMatch
instance GHC.Show.Show Network.AWS.Rekognition.Types.PersonMatch.PersonMatch
instance GHC.Read.Read Network.AWS.Rekognition.Types.PersonMatch.PersonMatch
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.PersonMatch.PersonMatch
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.PersonMatch.PersonMatch
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.PersonMatch.PersonMatch
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.PersonMatch.PersonMatch


module Network.AWS.Rekognition.Types.PersonDetection

-- | Details and path tracking information for a single time a person's
--   path is tracked in a video. Amazon Rekognition operations that track
--   people's paths return an array of <tt>PersonDetection</tt> objects
--   with elements for each time a person's path is tracked in a video.
--   
--   For more information, see GetPersonTracking in the Amazon Rekognition
--   Developer Guide.
--   
--   <i>See:</i> <a>newPersonDetection</a> smart constructor.
data PersonDetection
PersonDetection' :: Maybe PersonDetail -> Maybe Integer -> PersonDetection

-- | Details about a person whose path was tracked in a video.
[$sel:person:PersonDetection'] :: PersonDetection -> Maybe PersonDetail

-- | The time, in milliseconds from the start of the video, that the
--   person's path was tracked.
[$sel:timestamp:PersonDetection'] :: PersonDetection -> Maybe Integer

-- | Create a value of <a>PersonDetection</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:person:PersonDetection'</a>, <a>personDetection_person</a> -
--   Details about a person whose path was tracked in a video.
--   
--   <a>$sel:timestamp:PersonDetection'</a>,
--   <a>personDetection_timestamp</a> - The time, in milliseconds from the
--   start of the video, that the person's path was tracked.
newPersonDetection :: PersonDetection

-- | Details about a person whose path was tracked in a video.
personDetection_person :: Lens' PersonDetection (Maybe PersonDetail)

-- | The time, in milliseconds from the start of the video, that the
--   person's path was tracked.
personDetection_timestamp :: Lens' PersonDetection (Maybe Integer)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.PersonDetection.PersonDetection
instance GHC.Show.Show Network.AWS.Rekognition.Types.PersonDetection.PersonDetection
instance GHC.Read.Read Network.AWS.Rekognition.Types.PersonDetection.PersonDetection
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.PersonDetection.PersonDetection
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.PersonDetection.PersonDetection
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.PersonDetection.PersonDetection
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.PersonDetection.PersonDetection


module Network.AWS.Rekognition.Types.FaceRecord

-- | Object containing both the face metadata (stored in the backend
--   database), and facial attributes that are detected but aren't stored
--   in the database.
--   
--   <i>See:</i> <a>newFaceRecord</a> smart constructor.
data FaceRecord
FaceRecord' :: Maybe FaceDetail -> Maybe Face -> FaceRecord

-- | Structure containing attributes of the face that the algorithm
--   detected.
[$sel:faceDetail:FaceRecord'] :: FaceRecord -> Maybe FaceDetail

-- | Describes the face properties such as the bounding box, face ID, image
--   ID of the input image, and external image ID that you assigned.
[$sel:face:FaceRecord'] :: FaceRecord -> Maybe Face

-- | Create a value of <a>FaceRecord</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:faceDetail:FaceRecord'</a>, <a>faceRecord_faceDetail</a> -
--   Structure containing attributes of the face that the algorithm
--   detected.
--   
--   <a>$sel:face:FaceRecord'</a>, <a>faceRecord_face</a> - Describes the
--   face properties such as the bounding box, face ID, image ID of the
--   input image, and external image ID that you assigned.
newFaceRecord :: FaceRecord

-- | Structure containing attributes of the face that the algorithm
--   detected.
faceRecord_faceDetail :: Lens' FaceRecord (Maybe FaceDetail)

-- | Describes the face properties such as the bounding box, face ID, image
--   ID of the input image, and external image ID that you assigned.
faceRecord_face :: Lens' FaceRecord (Maybe Face)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.FaceRecord.FaceRecord
instance GHC.Show.Show Network.AWS.Rekognition.Types.FaceRecord.FaceRecord
instance GHC.Read.Read Network.AWS.Rekognition.Types.FaceRecord.FaceRecord
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.FaceRecord.FaceRecord
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.FaceRecord.FaceRecord
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.FaceRecord.FaceRecord
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.FaceRecord.FaceRecord


module Network.AWS.Rekognition.Types.FaceDetection

-- | Information about a face detected in a video analysis request and the
--   time the face was detected in the video.
--   
--   <i>See:</i> <a>newFaceDetection</a> smart constructor.
data FaceDetection
FaceDetection' :: Maybe Integer -> Maybe FaceDetail -> FaceDetection

-- | Time, in milliseconds from the start of the video, that the face was
--   detected.
[$sel:timestamp:FaceDetection'] :: FaceDetection -> Maybe Integer

-- | The face properties for the detected face.
[$sel:face:FaceDetection'] :: FaceDetection -> Maybe FaceDetail

-- | Create a value of <a>FaceDetection</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:timestamp:FaceDetection'</a>, <a>faceDetection_timestamp</a> -
--   Time, in milliseconds from the start of the video, that the face was
--   detected.
--   
--   <a>$sel:face:FaceDetection'</a>, <a>faceDetection_face</a> - The face
--   properties for the detected face.
newFaceDetection :: FaceDetection

-- | Time, in milliseconds from the start of the video, that the face was
--   detected.
faceDetection_timestamp :: Lens' FaceDetection (Maybe Integer)

-- | The face properties for the detected face.
faceDetection_face :: Lens' FaceDetection (Maybe FaceDetail)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.FaceDetection.FaceDetection
instance GHC.Show.Show Network.AWS.Rekognition.Types.FaceDetection.FaceDetection
instance GHC.Read.Read Network.AWS.Rekognition.Types.FaceDetection.FaceDetection
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.FaceDetection.FaceDetection
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.FaceDetection.FaceDetection
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.FaceDetection.FaceDetection
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.FaceDetection.FaceDetection


module Network.AWS.Rekognition.Types.CelebrityDetail

-- | Information about a recognized celebrity.
--   
--   <i>See:</i> <a>newCelebrityDetail</a> smart constructor.
data CelebrityDetail
CelebrityDetail' :: Maybe BoundingBox -> Maybe [Text] -> Maybe Double -> Maybe Text -> Maybe Text -> Maybe FaceDetail -> CelebrityDetail

-- | Bounding box around the body of a celebrity.
[$sel:boundingBox:CelebrityDetail'] :: CelebrityDetail -> Maybe BoundingBox

-- | An array of URLs pointing to additional celebrity information.
[$sel:urls:CelebrityDetail'] :: CelebrityDetail -> Maybe [Text]

-- | The confidence, in percentage, that Amazon Rekognition has that the
--   recognized face is the celebrity.
[$sel:confidence:CelebrityDetail'] :: CelebrityDetail -> Maybe Double

-- | The name of the celebrity.
[$sel:name:CelebrityDetail'] :: CelebrityDetail -> Maybe Text

-- | The unique identifier for the celebrity.
[$sel:id:CelebrityDetail'] :: CelebrityDetail -> Maybe Text

-- | Face details for the recognized celebrity.
[$sel:face:CelebrityDetail'] :: CelebrityDetail -> Maybe FaceDetail

-- | Create a value of <a>CelebrityDetail</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:boundingBox:CelebrityDetail'</a>,
--   <a>celebrityDetail_boundingBox</a> - Bounding box around the body of a
--   celebrity.
--   
--   <a>$sel:urls:CelebrityDetail'</a>, <a>celebrityDetail_urls</a> - An
--   array of URLs pointing to additional celebrity information.
--   
--   <a>$sel:confidence:CelebrityDetail'</a>,
--   <a>celebrityDetail_confidence</a> - The confidence, in percentage,
--   that Amazon Rekognition has that the recognized face is the celebrity.
--   
--   <a>$sel:name:CelebrityDetail'</a>, <a>celebrityDetail_name</a> - The
--   name of the celebrity.
--   
--   <a>$sel:id:CelebrityDetail'</a>, <a>celebrityDetail_id</a> - The
--   unique identifier for the celebrity.
--   
--   <a>$sel:face:CelebrityDetail'</a>, <a>celebrityDetail_face</a> - Face
--   details for the recognized celebrity.
newCelebrityDetail :: CelebrityDetail

-- | Bounding box around the body of a celebrity.
celebrityDetail_boundingBox :: Lens' CelebrityDetail (Maybe BoundingBox)

-- | An array of URLs pointing to additional celebrity information.
celebrityDetail_urls :: Lens' CelebrityDetail (Maybe [Text])

-- | The confidence, in percentage, that Amazon Rekognition has that the
--   recognized face is the celebrity.
celebrityDetail_confidence :: Lens' CelebrityDetail (Maybe Double)

-- | The name of the celebrity.
celebrityDetail_name :: Lens' CelebrityDetail (Maybe Text)

-- | The unique identifier for the celebrity.
celebrityDetail_id :: Lens' CelebrityDetail (Maybe Text)

-- | Face details for the recognized celebrity.
celebrityDetail_face :: Lens' CelebrityDetail (Maybe FaceDetail)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.CelebrityDetail.CelebrityDetail
instance GHC.Show.Show Network.AWS.Rekognition.Types.CelebrityDetail.CelebrityDetail
instance GHC.Read.Read Network.AWS.Rekognition.Types.CelebrityDetail.CelebrityDetail
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.CelebrityDetail.CelebrityDetail
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.CelebrityDetail.CelebrityDetail
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.CelebrityDetail.CelebrityDetail
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.CelebrityDetail.CelebrityDetail


module Network.AWS.Rekognition.Types.CelebrityRecognition

-- | Information about a detected celebrity and the time the celebrity was
--   detected in a stored video. For more information, see
--   GetCelebrityRecognition in the Amazon Rekognition Developer Guide.
--   
--   <i>See:</i> <a>newCelebrityRecognition</a> smart constructor.
data CelebrityRecognition
CelebrityRecognition' :: Maybe CelebrityDetail -> Maybe Integer -> CelebrityRecognition

-- | Information about a recognized celebrity.
[$sel:celebrity:CelebrityRecognition'] :: CelebrityRecognition -> Maybe CelebrityDetail

-- | The time, in milliseconds from the start of the video, that the
--   celebrity was recognized.
[$sel:timestamp:CelebrityRecognition'] :: CelebrityRecognition -> Maybe Integer

-- | Create a value of <a>CelebrityRecognition</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:celebrity:CelebrityRecognition'</a>,
--   <a>celebrityRecognition_celebrity</a> - Information about a recognized
--   celebrity.
--   
--   <a>$sel:timestamp:CelebrityRecognition'</a>,
--   <a>celebrityRecognition_timestamp</a> - The time, in milliseconds from
--   the start of the video, that the celebrity was recognized.
newCelebrityRecognition :: CelebrityRecognition

-- | Information about a recognized celebrity.
celebrityRecognition_celebrity :: Lens' CelebrityRecognition (Maybe CelebrityDetail)

-- | The time, in milliseconds from the start of the video, that the
--   celebrity was recognized.
celebrityRecognition_timestamp :: Lens' CelebrityRecognition (Maybe Integer)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.CelebrityRecognition.CelebrityRecognition
instance GHC.Show.Show Network.AWS.Rekognition.Types.CelebrityRecognition.CelebrityRecognition
instance GHC.Read.Read Network.AWS.Rekognition.Types.CelebrityRecognition.CelebrityRecognition
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.CelebrityRecognition.CelebrityRecognition
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.CelebrityRecognition.CelebrityRecognition
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.CelebrityRecognition.CelebrityRecognition
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.CelebrityRecognition.CelebrityRecognition


module Network.AWS.Rekognition.Types.TechnicalCueType
newtype TechnicalCueType
TechnicalCueType' :: Text -> TechnicalCueType
[fromTechnicalCueType] :: TechnicalCueType -> Text
pattern TechnicalCueType_BlackFrames :: TechnicalCueType
pattern TechnicalCueType_ColorBars :: TechnicalCueType
pattern TechnicalCueType_Content :: TechnicalCueType
pattern TechnicalCueType_EndCredits :: TechnicalCueType
pattern TechnicalCueType_OpeningCredits :: TechnicalCueType
pattern TechnicalCueType_Slate :: TechnicalCueType
pattern TechnicalCueType_StudioLogo :: TechnicalCueType
instance Network.AWS.Data.XML.ToXML Network.AWS.Rekognition.Types.TechnicalCueType.TechnicalCueType
instance Network.AWS.Data.XML.FromXML Network.AWS.Rekognition.Types.TechnicalCueType.TechnicalCueType
instance Data.Aeson.Types.ToJSON.ToJSONKey Network.AWS.Rekognition.Types.TechnicalCueType.TechnicalCueType
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.Types.TechnicalCueType.TechnicalCueType
instance Data.Aeson.Types.FromJSON.FromJSONKey Network.AWS.Rekognition.Types.TechnicalCueType.TechnicalCueType
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.TechnicalCueType.TechnicalCueType
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.Types.TechnicalCueType.TechnicalCueType
instance Network.AWS.Data.Headers.ToHeader Network.AWS.Rekognition.Types.TechnicalCueType.TechnicalCueType
instance Network.AWS.Data.Log.ToLog Network.AWS.Rekognition.Types.TechnicalCueType.TechnicalCueType
instance Network.AWS.Data.ByteString.ToByteString Network.AWS.Rekognition.Types.TechnicalCueType.TechnicalCueType
instance Network.AWS.Data.Text.ToText Network.AWS.Rekognition.Types.TechnicalCueType.TechnicalCueType
instance Network.AWS.Data.Text.FromText Network.AWS.Rekognition.Types.TechnicalCueType.TechnicalCueType
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.TechnicalCueType.TechnicalCueType
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.TechnicalCueType.TechnicalCueType
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.TechnicalCueType.TechnicalCueType
instance GHC.Classes.Ord Network.AWS.Rekognition.Types.TechnicalCueType.TechnicalCueType
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.TechnicalCueType.TechnicalCueType
instance GHC.Read.Read Network.AWS.Rekognition.Types.TechnicalCueType.TechnicalCueType
instance GHC.Show.Show Network.AWS.Rekognition.Types.TechnicalCueType.TechnicalCueType


module Network.AWS.Rekognition.Types.TechnicalCueSegment

-- | Information about a technical cue segment. For more information, see
--   SegmentDetection.
--   
--   <i>See:</i> <a>newTechnicalCueSegment</a> smart constructor.
data TechnicalCueSegment
TechnicalCueSegment' :: Maybe Double -> Maybe TechnicalCueType -> TechnicalCueSegment

-- | The confidence that Amazon Rekognition Video has in the accuracy of
--   the detected segment.
[$sel:confidence:TechnicalCueSegment'] :: TechnicalCueSegment -> Maybe Double

-- | The type of the technical cue.
[$sel:type':TechnicalCueSegment'] :: TechnicalCueSegment -> Maybe TechnicalCueType

-- | Create a value of <a>TechnicalCueSegment</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:confidence:TechnicalCueSegment'</a>,
--   <a>technicalCueSegment_confidence</a> - The confidence that Amazon
--   Rekognition Video has in the accuracy of the detected segment.
--   
--   <a>$sel:type':TechnicalCueSegment'</a>,
--   <a>technicalCueSegment_type</a> - The type of the technical cue.
newTechnicalCueSegment :: TechnicalCueSegment

-- | The confidence that Amazon Rekognition Video has in the accuracy of
--   the detected segment.
technicalCueSegment_confidence :: Lens' TechnicalCueSegment (Maybe Double)

-- | The type of the technical cue.
technicalCueSegment_type :: Lens' TechnicalCueSegment (Maybe TechnicalCueType)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.TechnicalCueSegment.TechnicalCueSegment
instance GHC.Show.Show Network.AWS.Rekognition.Types.TechnicalCueSegment.TechnicalCueSegment
instance GHC.Read.Read Network.AWS.Rekognition.Types.TechnicalCueSegment.TechnicalCueSegment
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.TechnicalCueSegment.TechnicalCueSegment
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.TechnicalCueSegment.TechnicalCueSegment
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.TechnicalCueSegment.TechnicalCueSegment
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.TechnicalCueSegment.TechnicalCueSegment


module Network.AWS.Rekognition.Types.SegmentDetection

-- | A technical cue or shot detection segment detected in a video. An
--   array of <tt>SegmentDetection</tt> objects containing all segments
--   detected in a stored video is returned by GetSegmentDetection.
--   
--   <i>See:</i> <a>newSegmentDetection</a> smart constructor.
data SegmentDetection
SegmentDetection' :: Maybe TechnicalCueSegment -> Maybe Natural -> Maybe Text -> Maybe Integer -> Maybe Text -> Maybe Text -> Maybe Natural -> Maybe Natural -> Maybe Integer -> Maybe SegmentType -> Maybe ShotSegment -> Maybe Natural -> SegmentDetection

-- | If the segment is a technical cue, contains information about the
--   technical cue.
[$sel:technicalCueSegment:SegmentDetection'] :: SegmentDetection -> Maybe TechnicalCueSegment

-- | The frame number at the end of a video segment, using a frame index
--   that starts with 0.
[$sel:endFrameNumber:SegmentDetection'] :: SegmentDetection -> Maybe Natural

-- | The duration of the timecode for the detected segment in SMPTE format.
[$sel:durationSMPTE:SegmentDetection'] :: SegmentDetection -> Maybe Text

-- | The end time of the detected segment, in milliseconds, from the start
--   of the video. This value is rounded down.
[$sel:endTimestampMillis:SegmentDetection'] :: SegmentDetection -> Maybe Integer

-- | The frame-accurate SMPTE timecode, from the start of a video, for the
--   start of a detected segment. <tt>StartTimecode</tt> is in
--   <i>HH:MM:SS:fr</i> format (and <i>;fr</i> for drop frame-rates).
[$sel:startTimecodeSMPTE:SegmentDetection'] :: SegmentDetection -> Maybe Text

-- | The frame-accurate SMPTE timecode, from the start of a video, for the
--   end of a detected segment. <tt>EndTimecode</tt> is in
--   <i>HH:MM:SS:fr</i> format (and <i>;fr</i> for drop frame-rates).
[$sel:endTimecodeSMPTE:SegmentDetection'] :: SegmentDetection -> Maybe Text

-- | The duration of the detected segment in milliseconds.
[$sel:durationMillis:SegmentDetection'] :: SegmentDetection -> Maybe Natural

-- | The duration of a video segment, expressed in frames.
[$sel:durationFrames:SegmentDetection'] :: SegmentDetection -> Maybe Natural

-- | The start time of the detected segment in milliseconds from the start
--   of the video. This value is rounded down. For example, if the actual
--   timestamp is 100.6667 milliseconds, Amazon Rekognition Video returns a
--   value of 100 millis.
[$sel:startTimestampMillis:SegmentDetection'] :: SegmentDetection -> Maybe Integer

-- | The type of the segment. Valid values are <tt>TECHNICAL_CUE</tt> and
--   <tt>SHOT</tt>.
[$sel:type':SegmentDetection'] :: SegmentDetection -> Maybe SegmentType

-- | If the segment is a shot detection, contains information about the
--   shot detection.
[$sel:shotSegment:SegmentDetection'] :: SegmentDetection -> Maybe ShotSegment

-- | The frame number of the start of a video segment, using a frame index
--   that starts with 0.
[$sel:startFrameNumber:SegmentDetection'] :: SegmentDetection -> Maybe Natural

-- | Create a value of <a>SegmentDetection</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:technicalCueSegment:SegmentDetection'</a>,
--   <a>segmentDetection_technicalCueSegment</a> - If the segment is a
--   technical cue, contains information about the technical cue.
--   
--   <a>$sel:endFrameNumber:SegmentDetection'</a>,
--   <a>segmentDetection_endFrameNumber</a> - The frame number at the end
--   of a video segment, using a frame index that starts with 0.
--   
--   <a>$sel:durationSMPTE:SegmentDetection'</a>,
--   <a>segmentDetection_durationSMPTE</a> - The duration of the timecode
--   for the detected segment in SMPTE format.
--   
--   <a>$sel:endTimestampMillis:SegmentDetection'</a>,
--   <a>segmentDetection_endTimestampMillis</a> - The end time of the
--   detected segment, in milliseconds, from the start of the video. This
--   value is rounded down.
--   
--   <a>$sel:startTimecodeSMPTE:SegmentDetection'</a>,
--   <a>segmentDetection_startTimecodeSMPTE</a> - The frame-accurate SMPTE
--   timecode, from the start of a video, for the start of a detected
--   segment. <tt>StartTimecode</tt> is in <i>HH:MM:SS:fr</i> format (and
--   <i>;fr</i> for drop frame-rates).
--   
--   <a>$sel:endTimecodeSMPTE:SegmentDetection'</a>,
--   <a>segmentDetection_endTimecodeSMPTE</a> - The frame-accurate SMPTE
--   timecode, from the start of a video, for the end of a detected
--   segment. <tt>EndTimecode</tt> is in <i>HH:MM:SS:fr</i> format (and
--   <i>;fr</i> for drop frame-rates).
--   
--   <a>$sel:durationMillis:SegmentDetection'</a>,
--   <a>segmentDetection_durationMillis</a> - The duration of the detected
--   segment in milliseconds.
--   
--   <a>$sel:durationFrames:SegmentDetection'</a>,
--   <a>segmentDetection_durationFrames</a> - The duration of a video
--   segment, expressed in frames.
--   
--   <a>$sel:startTimestampMillis:SegmentDetection'</a>,
--   <a>segmentDetection_startTimestampMillis</a> - The start time of the
--   detected segment in milliseconds from the start of the video. This
--   value is rounded down. For example, if the actual timestamp is
--   100.6667 milliseconds, Amazon Rekognition Video returns a value of 100
--   millis.
--   
--   <a>$sel:type':SegmentDetection'</a>, <a>segmentDetection_type</a> -
--   The type of the segment. Valid values are <tt>TECHNICAL_CUE</tt> and
--   <tt>SHOT</tt>.
--   
--   <a>$sel:shotSegment:SegmentDetection'</a>,
--   <a>segmentDetection_shotSegment</a> - If the segment is a shot
--   detection, contains information about the shot detection.
--   
--   <a>$sel:startFrameNumber:SegmentDetection'</a>,
--   <a>segmentDetection_startFrameNumber</a> - The frame number of the
--   start of a video segment, using a frame index that starts with 0.
newSegmentDetection :: SegmentDetection

-- | If the segment is a technical cue, contains information about the
--   technical cue.
segmentDetection_technicalCueSegment :: Lens' SegmentDetection (Maybe TechnicalCueSegment)

-- | The frame number at the end of a video segment, using a frame index
--   that starts with 0.
segmentDetection_endFrameNumber :: Lens' SegmentDetection (Maybe Natural)

-- | The duration of the timecode for the detected segment in SMPTE format.
segmentDetection_durationSMPTE :: Lens' SegmentDetection (Maybe Text)

-- | The end time of the detected segment, in milliseconds, from the start
--   of the video. This value is rounded down.
segmentDetection_endTimestampMillis :: Lens' SegmentDetection (Maybe Integer)

-- | The frame-accurate SMPTE timecode, from the start of a video, for the
--   start of a detected segment. <tt>StartTimecode</tt> is in
--   <i>HH:MM:SS:fr</i> format (and <i>;fr</i> for drop frame-rates).
segmentDetection_startTimecodeSMPTE :: Lens' SegmentDetection (Maybe Text)

-- | The frame-accurate SMPTE timecode, from the start of a video, for the
--   end of a detected segment. <tt>EndTimecode</tt> is in
--   <i>HH:MM:SS:fr</i> format (and <i>;fr</i> for drop frame-rates).
segmentDetection_endTimecodeSMPTE :: Lens' SegmentDetection (Maybe Text)

-- | The duration of the detected segment in milliseconds.
segmentDetection_durationMillis :: Lens' SegmentDetection (Maybe Natural)

-- | The duration of a video segment, expressed in frames.
segmentDetection_durationFrames :: Lens' SegmentDetection (Maybe Natural)

-- | The start time of the detected segment in milliseconds from the start
--   of the video. This value is rounded down. For example, if the actual
--   timestamp is 100.6667 milliseconds, Amazon Rekognition Video returns a
--   value of 100 millis.
segmentDetection_startTimestampMillis :: Lens' SegmentDetection (Maybe Integer)

-- | The type of the segment. Valid values are <tt>TECHNICAL_CUE</tt> and
--   <tt>SHOT</tt>.
segmentDetection_type :: Lens' SegmentDetection (Maybe SegmentType)

-- | If the segment is a shot detection, contains information about the
--   shot detection.
segmentDetection_shotSegment :: Lens' SegmentDetection (Maybe ShotSegment)

-- | The frame number of the start of a video segment, using a frame index
--   that starts with 0.
segmentDetection_startFrameNumber :: Lens' SegmentDetection (Maybe Natural)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.SegmentDetection.SegmentDetection
instance GHC.Show.Show Network.AWS.Rekognition.Types.SegmentDetection.SegmentDetection
instance GHC.Read.Read Network.AWS.Rekognition.Types.SegmentDetection.SegmentDetection
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.SegmentDetection.SegmentDetection
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.SegmentDetection.SegmentDetection
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.SegmentDetection.SegmentDetection
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.SegmentDetection.SegmentDetection


module Network.AWS.Rekognition.Types.TestingData

-- | The dataset used for testing. Optionally, if <tt>AutoCreate</tt> is
--   set, Amazon Rekognition Custom Labels creates a testing dataset using
--   an 80/20 split of the training dataset.
--   
--   <i>See:</i> <a>newTestingData</a> smart constructor.
data TestingData
TestingData' :: Maybe [Asset] -> Maybe Bool -> TestingData

-- | The assets used for testing.
[$sel:assets:TestingData'] :: TestingData -> Maybe [Asset]

-- | If specified, Amazon Rekognition Custom Labels creates a testing
--   dataset with an 80/20 split of the training dataset.
[$sel:autoCreate:TestingData'] :: TestingData -> Maybe Bool

-- | Create a value of <a>TestingData</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:assets:TestingData'</a>, <a>testingData_assets</a> - The
--   assets used for testing.
--   
--   <a>$sel:autoCreate:TestingData'</a>, <a>testingData_autoCreate</a> -
--   If specified, Amazon Rekognition Custom Labels creates a testing
--   dataset with an 80/20 split of the training dataset.
newTestingData :: TestingData

-- | The assets used for testing.
testingData_assets :: Lens' TestingData (Maybe [Asset])

-- | If specified, Amazon Rekognition Custom Labels creates a testing
--   dataset with an 80/20 split of the training dataset.
testingData_autoCreate :: Lens' TestingData (Maybe Bool)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.TestingData.TestingData
instance GHC.Show.Show Network.AWS.Rekognition.Types.TestingData.TestingData
instance GHC.Read.Read Network.AWS.Rekognition.Types.TestingData.TestingData
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.TestingData.TestingData
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.TestingData.TestingData
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.TestingData.TestingData
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.TestingData.TestingData
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.Types.TestingData.TestingData


module Network.AWS.Rekognition.Types.TextTypes
newtype TextTypes
TextTypes' :: Text -> TextTypes
[fromTextTypes] :: TextTypes -> Text
pattern TextTypes_LINE :: TextTypes
pattern TextTypes_WORD :: TextTypes
instance Network.AWS.Data.XML.ToXML Network.AWS.Rekognition.Types.TextTypes.TextTypes
instance Network.AWS.Data.XML.FromXML Network.AWS.Rekognition.Types.TextTypes.TextTypes
instance Data.Aeson.Types.ToJSON.ToJSONKey Network.AWS.Rekognition.Types.TextTypes.TextTypes
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.Types.TextTypes.TextTypes
instance Data.Aeson.Types.FromJSON.FromJSONKey Network.AWS.Rekognition.Types.TextTypes.TextTypes
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.TextTypes.TextTypes
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.Types.TextTypes.TextTypes
instance Network.AWS.Data.Headers.ToHeader Network.AWS.Rekognition.Types.TextTypes.TextTypes
instance Network.AWS.Data.Log.ToLog Network.AWS.Rekognition.Types.TextTypes.TextTypes
instance Network.AWS.Data.ByteString.ToByteString Network.AWS.Rekognition.Types.TextTypes.TextTypes
instance Network.AWS.Data.Text.ToText Network.AWS.Rekognition.Types.TextTypes.TextTypes
instance Network.AWS.Data.Text.FromText Network.AWS.Rekognition.Types.TextTypes.TextTypes
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.TextTypes.TextTypes
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.TextTypes.TextTypes
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.TextTypes.TextTypes
instance GHC.Classes.Ord Network.AWS.Rekognition.Types.TextTypes.TextTypes
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.TextTypes.TextTypes
instance GHC.Read.Read Network.AWS.Rekognition.Types.TextTypes.TextTypes
instance GHC.Show.Show Network.AWS.Rekognition.Types.TextTypes.TextTypes


module Network.AWS.Rekognition.Types.TextDetection

-- | Information about a word or line of text detected by DetectText.
--   
--   The <tt>DetectedText</tt> field contains the text that Amazon
--   Rekognition detected in the image.
--   
--   Every word and line has an identifier (<tt>Id</tt>). Each word belongs
--   to a line and has a parent identifier (<tt>ParentId</tt>) that
--   identifies the line of text in which the word appears. The word
--   <tt>Id</tt> is also an index for the word within a line of words.
--   
--   For more information, see Detecting Text in the Amazon Rekognition
--   Developer Guide.
--   
--   <i>See:</i> <a>newTextDetection</a> smart constructor.
data TextDetection
TextDetection' :: Maybe Text -> Maybe Double -> Maybe Geometry -> Maybe Natural -> Maybe TextTypes -> Maybe Natural -> TextDetection

-- | The word or line of text recognized by Amazon Rekognition.
[$sel:detectedText:TextDetection'] :: TextDetection -> Maybe Text

-- | The confidence that Amazon Rekognition has in the accuracy of the
--   detected text and the accuracy of the geometry points around the
--   detected text.
[$sel:confidence:TextDetection'] :: TextDetection -> Maybe Double

-- | The location of the detected text on the image. Includes an axis
--   aligned coarse bounding box surrounding the text and a finer grain
--   polygon for more accurate spatial information.
[$sel:geometry:TextDetection'] :: TextDetection -> Maybe Geometry

-- | The identifier for the detected text. The identifier is only unique
--   for a single call to <tt>DetectText</tt>.
[$sel:id:TextDetection'] :: TextDetection -> Maybe Natural

-- | The type of text that was detected.
[$sel:type':TextDetection'] :: TextDetection -> Maybe TextTypes

-- | The Parent identifier for the detected text identified by the value of
--   <tt>ID</tt>. If the type of detected text is <tt>LINE</tt>, the value
--   of <tt>ParentId</tt> is <tt>Null</tt>.
[$sel:parentId:TextDetection'] :: TextDetection -> Maybe Natural

-- | Create a value of <a>TextDetection</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:detectedText:TextDetection'</a>,
--   <a>textDetection_detectedText</a> - The word or line of text
--   recognized by Amazon Rekognition.
--   
--   <a>$sel:confidence:TextDetection'</a>, <a>textDetection_confidence</a>
--   - The confidence that Amazon Rekognition has in the accuracy of the
--   detected text and the accuracy of the geometry points around the
--   detected text.
--   
--   <a>$sel:geometry:TextDetection'</a>, <a>textDetection_geometry</a> -
--   The location of the detected text on the image. Includes an axis
--   aligned coarse bounding box surrounding the text and a finer grain
--   polygon for more accurate spatial information.
--   
--   <a>$sel:id:TextDetection'</a>, <a>textDetection_id</a> - The
--   identifier for the detected text. The identifier is only unique for a
--   single call to <tt>DetectText</tt>.
--   
--   <a>$sel:type':TextDetection'</a>, <a>textDetection_type</a> - The type
--   of text that was detected.
--   
--   <a>$sel:parentId:TextDetection'</a>, <a>textDetection_parentId</a> -
--   The Parent identifier for the detected text identified by the value of
--   <tt>ID</tt>. If the type of detected text is <tt>LINE</tt>, the value
--   of <tt>ParentId</tt> is <tt>Null</tt>.
newTextDetection :: TextDetection

-- | The word or line of text recognized by Amazon Rekognition.
textDetection_detectedText :: Lens' TextDetection (Maybe Text)

-- | The confidence that Amazon Rekognition has in the accuracy of the
--   detected text and the accuracy of the geometry points around the
--   detected text.
textDetection_confidence :: Lens' TextDetection (Maybe Double)

-- | The location of the detected text on the image. Includes an axis
--   aligned coarse bounding box surrounding the text and a finer grain
--   polygon for more accurate spatial information.
textDetection_geometry :: Lens' TextDetection (Maybe Geometry)

-- | The identifier for the detected text. The identifier is only unique
--   for a single call to <tt>DetectText</tt>.
textDetection_id :: Lens' TextDetection (Maybe Natural)

-- | The type of text that was detected.
textDetection_type :: Lens' TextDetection (Maybe TextTypes)

-- | The Parent identifier for the detected text identified by the value of
--   <tt>ID</tt>. If the type of detected text is <tt>LINE</tt>, the value
--   of <tt>ParentId</tt> is <tt>Null</tt>.
textDetection_parentId :: Lens' TextDetection (Maybe Natural)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.TextDetection.TextDetection
instance GHC.Show.Show Network.AWS.Rekognition.Types.TextDetection.TextDetection
instance GHC.Read.Read Network.AWS.Rekognition.Types.TextDetection.TextDetection
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.TextDetection.TextDetection
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.TextDetection.TextDetection
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.TextDetection.TextDetection
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.TextDetection.TextDetection


module Network.AWS.Rekognition.Types.TextDetectionResult

-- | Information about text detected in a video. Incudes the detected text,
--   the time in milliseconds from the start of the video that the text was
--   detected, and where it was detected on the screen.
--   
--   <i>See:</i> <a>newTextDetectionResult</a> smart constructor.
data TextDetectionResult
TextDetectionResult' :: Maybe TextDetection -> Maybe Integer -> TextDetectionResult

-- | Details about text detected in a video.
[$sel:textDetection:TextDetectionResult'] :: TextDetectionResult -> Maybe TextDetection

-- | The time, in milliseconds from the start of the video, that the text
--   was detected.
[$sel:timestamp:TextDetectionResult'] :: TextDetectionResult -> Maybe Integer

-- | Create a value of <a>TextDetectionResult</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:textDetection:TextDetectionResult'</a>,
--   <a>textDetectionResult_textDetection</a> - Details about text detected
--   in a video.
--   
--   <a>$sel:timestamp:TextDetectionResult'</a>,
--   <a>textDetectionResult_timestamp</a> - The time, in milliseconds from
--   the start of the video, that the text was detected.
newTextDetectionResult :: TextDetectionResult

-- | Details about text detected in a video.
textDetectionResult_textDetection :: Lens' TextDetectionResult (Maybe TextDetection)

-- | The time, in milliseconds from the start of the video, that the text
--   was detected.
textDetectionResult_timestamp :: Lens' TextDetectionResult (Maybe Integer)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.TextDetectionResult.TextDetectionResult
instance GHC.Show.Show Network.AWS.Rekognition.Types.TextDetectionResult.TextDetectionResult
instance GHC.Read.Read Network.AWS.Rekognition.Types.TextDetectionResult.TextDetectionResult
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.TextDetectionResult.TextDetectionResult
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.TextDetectionResult.TextDetectionResult
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.TextDetectionResult.TextDetectionResult
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.TextDetectionResult.TextDetectionResult


module Network.AWS.Rekognition.Types.TrainingData

-- | The dataset used for training.
--   
--   <i>See:</i> <a>newTrainingData</a> smart constructor.
data TrainingData
TrainingData' :: Maybe [Asset] -> TrainingData

-- | A Sagemaker GroundTruth manifest file that contains the training
--   images (assets).
[$sel:assets:TrainingData'] :: TrainingData -> Maybe [Asset]

-- | Create a value of <a>TrainingData</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:assets:TrainingData'</a>, <a>trainingData_assets</a> - A
--   Sagemaker GroundTruth manifest file that contains the training images
--   (assets).
newTrainingData :: TrainingData

-- | A Sagemaker GroundTruth manifest file that contains the training
--   images (assets).
trainingData_assets :: Lens' TrainingData (Maybe [Asset])
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.TrainingData.TrainingData
instance GHC.Show.Show Network.AWS.Rekognition.Types.TrainingData.TrainingData
instance GHC.Read.Read Network.AWS.Rekognition.Types.TrainingData.TrainingData
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.TrainingData.TrainingData
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.TrainingData.TrainingData
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.TrainingData.TrainingData
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.TrainingData.TrainingData
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.Types.TrainingData.TrainingData


module Network.AWS.Rekognition.Types.UnindexedFace

-- | A face that IndexFaces detected, but didn't index. Use the
--   <tt>Reasons</tt> response attribute to determine why a face wasn't
--   indexed.
--   
--   <i>See:</i> <a>newUnindexedFace</a> smart constructor.
data UnindexedFace
UnindexedFace' :: Maybe [Reason] -> Maybe FaceDetail -> UnindexedFace

-- | An array of reasons that specify why a face wasn't indexed.
--   
--   <ul>
--   <li>EXTREME_POSE - The face is at a pose that can't be detected. For
--   example, the head is turned too far away from the camera.</li>
--   <li>EXCEEDS_MAX_FACES - The number of faces detected is already higher
--   than that specified by the <tt>MaxFaces</tt> input parameter for
--   <tt>IndexFaces</tt>.</li>
--   <li>LOW_BRIGHTNESS - The image is too dark.</li>
--   <li>LOW_SHARPNESS - The image is too blurry.</li>
--   <li>LOW_CONFIDENCE - The face was detected with a low confidence.</li>
--   <li>SMALL_BOUNDING_BOX - The bounding box around the face is too
--   small.</li>
--   </ul>
[$sel:reasons:UnindexedFace'] :: UnindexedFace -> Maybe [Reason]

-- | The structure that contains attributes of a face that
--   <tt>IndexFaces</tt>detected, but didn't index.
[$sel:faceDetail:UnindexedFace'] :: UnindexedFace -> Maybe FaceDetail

-- | Create a value of <a>UnindexedFace</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:reasons:UnindexedFace'</a>, <a>unindexedFace_reasons</a> - An
--   array of reasons that specify why a face wasn't indexed.
--   
--   <ul>
--   <li>EXTREME_POSE - The face is at a pose that can't be detected. For
--   example, the head is turned too far away from the camera.</li>
--   <li>EXCEEDS_MAX_FACES - The number of faces detected is already higher
--   than that specified by the <tt>MaxFaces</tt> input parameter for
--   <tt>IndexFaces</tt>.</li>
--   <li>LOW_BRIGHTNESS - The image is too dark.</li>
--   <li>LOW_SHARPNESS - The image is too blurry.</li>
--   <li>LOW_CONFIDENCE - The face was detected with a low confidence.</li>
--   <li>SMALL_BOUNDING_BOX - The bounding box around the face is too
--   small.</li>
--   </ul>
--   
--   <a>$sel:faceDetail:UnindexedFace'</a>, <a>unindexedFace_faceDetail</a>
--   - The structure that contains attributes of a face that
--   <tt>IndexFaces</tt>detected, but didn't index.
newUnindexedFace :: UnindexedFace

-- | An array of reasons that specify why a face wasn't indexed.
--   
--   <ul>
--   <li>EXTREME_POSE - The face is at a pose that can't be detected. For
--   example, the head is turned too far away from the camera.</li>
--   <li>EXCEEDS_MAX_FACES - The number of faces detected is already higher
--   than that specified by the <tt>MaxFaces</tt> input parameter for
--   <tt>IndexFaces</tt>.</li>
--   <li>LOW_BRIGHTNESS - The image is too dark.</li>
--   <li>LOW_SHARPNESS - The image is too blurry.</li>
--   <li>LOW_CONFIDENCE - The face was detected with a low confidence.</li>
--   <li>SMALL_BOUNDING_BOX - The bounding box around the face is too
--   small.</li>
--   </ul>
unindexedFace_reasons :: Lens' UnindexedFace (Maybe [Reason])

-- | The structure that contains attributes of a face that
--   <tt>IndexFaces</tt>detected, but didn't index.
unindexedFace_faceDetail :: Lens' UnindexedFace (Maybe FaceDetail)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.UnindexedFace.UnindexedFace
instance GHC.Show.Show Network.AWS.Rekognition.Types.UnindexedFace.UnindexedFace
instance GHC.Read.Read Network.AWS.Rekognition.Types.UnindexedFace.UnindexedFace
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.UnindexedFace.UnindexedFace
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.UnindexedFace.UnindexedFace
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.UnindexedFace.UnindexedFace
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.UnindexedFace.UnindexedFace


module Network.AWS.Rekognition.Types.ValidationData

-- | Contains the Amazon S3 bucket location of the validation data for a
--   model training job.
--   
--   The validation data includes error information for individual JSON
--   lines in the dataset. For more information, see Debugging a Failed
--   Model Training in the Amazon Rekognition Custom Labels Developer
--   Guide.
--   
--   You get the <tt>ValidationData</tt> object for the training dataset
--   (TrainingDataResult) and the test dataset (TestingDataResult) by
--   calling DescribeProjectVersions.
--   
--   The assets array contains a single Asset object. The
--   GroundTruthManifest field of the Asset object contains the S3 bucket
--   location of the validation data.
--   
--   <i>See:</i> <a>newValidationData</a> smart constructor.
data ValidationData
ValidationData' :: Maybe [Asset] -> ValidationData

-- | The assets that comprise the validation data.
[$sel:assets:ValidationData'] :: ValidationData -> Maybe [Asset]

-- | Create a value of <a>ValidationData</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:assets:ValidationData'</a>, <a>validationData_assets</a> - The
--   assets that comprise the validation data.
newValidationData :: ValidationData

-- | The assets that comprise the validation data.
validationData_assets :: Lens' ValidationData (Maybe [Asset])
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.ValidationData.ValidationData
instance GHC.Show.Show Network.AWS.Rekognition.Types.ValidationData.ValidationData
instance GHC.Read.Read Network.AWS.Rekognition.Types.ValidationData.ValidationData
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.ValidationData.ValidationData
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.ValidationData.ValidationData
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.ValidationData.ValidationData
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.ValidationData.ValidationData


module Network.AWS.Rekognition.Types.TrainingDataResult

-- | Sagemaker Groundtruth format manifest files for the input, output and
--   validation datasets that are used and created during testing.
--   
--   <i>See:</i> <a>newTrainingDataResult</a> smart constructor.
data TrainingDataResult
TrainingDataResult' :: Maybe TrainingData -> Maybe TrainingData -> Maybe ValidationData -> TrainingDataResult

-- | The training assets that you supplied for training.
[$sel:input:TrainingDataResult'] :: TrainingDataResult -> Maybe TrainingData

-- | The images (assets) that were actually trained by Amazon Rekognition
--   Custom Labels.
[$sel:output:TrainingDataResult'] :: TrainingDataResult -> Maybe TrainingData

-- | The location of the data validation manifest. The data validation
--   manifest is created for the training dataset during model training.
[$sel:validation:TrainingDataResult'] :: TrainingDataResult -> Maybe ValidationData

-- | Create a value of <a>TrainingDataResult</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:input:TrainingDataResult'</a>, <a>trainingDataResult_input</a>
--   - The training assets that you supplied for training.
--   
--   <a>$sel:output:TrainingDataResult'</a>,
--   <a>trainingDataResult_output</a> - The images (assets) that were
--   actually trained by Amazon Rekognition Custom Labels.
--   
--   <a>$sel:validation:TrainingDataResult'</a>,
--   <a>trainingDataResult_validation</a> - The location of the data
--   validation manifest. The data validation manifest is created for the
--   training dataset during model training.
newTrainingDataResult :: TrainingDataResult

-- | The training assets that you supplied for training.
trainingDataResult_input :: Lens' TrainingDataResult (Maybe TrainingData)

-- | The images (assets) that were actually trained by Amazon Rekognition
--   Custom Labels.
trainingDataResult_output :: Lens' TrainingDataResult (Maybe TrainingData)

-- | The location of the data validation manifest. The data validation
--   manifest is created for the training dataset during model training.
trainingDataResult_validation :: Lens' TrainingDataResult (Maybe ValidationData)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.TrainingDataResult.TrainingDataResult
instance GHC.Show.Show Network.AWS.Rekognition.Types.TrainingDataResult.TrainingDataResult
instance GHC.Read.Read Network.AWS.Rekognition.Types.TrainingDataResult.TrainingDataResult
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.TrainingDataResult.TrainingDataResult
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.TrainingDataResult.TrainingDataResult
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.TrainingDataResult.TrainingDataResult
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.TrainingDataResult.TrainingDataResult


module Network.AWS.Rekognition.Types.TestingDataResult

-- | Sagemaker Groundtruth format manifest files for the input, output and
--   validation datasets that are used and created during testing.
--   
--   <i>See:</i> <a>newTestingDataResult</a> smart constructor.
data TestingDataResult
TestingDataResult' :: Maybe TestingData -> Maybe TestingData -> Maybe ValidationData -> TestingDataResult

-- | The testing dataset that was supplied for training.
[$sel:input:TestingDataResult'] :: TestingDataResult -> Maybe TestingData

-- | The subset of the dataset that was actually tested. Some images
--   (assets) might not be tested due to file formatting and other issues.
[$sel:output:TestingDataResult'] :: TestingDataResult -> Maybe TestingData

-- | The location of the data validation manifest. The data validation
--   manifest is created for the test dataset during model training.
[$sel:validation:TestingDataResult'] :: TestingDataResult -> Maybe ValidationData

-- | Create a value of <a>TestingDataResult</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:input:TestingDataResult'</a>, <a>testingDataResult_input</a> -
--   The testing dataset that was supplied for training.
--   
--   <a>$sel:output:TestingDataResult'</a>, <a>testingDataResult_output</a>
--   - The subset of the dataset that was actually tested. Some images
--   (assets) might not be tested due to file formatting and other issues.
--   
--   <a>$sel:validation:TestingDataResult'</a>,
--   <a>testingDataResult_validation</a> - The location of the data
--   validation manifest. The data validation manifest is created for the
--   test dataset during model training.
newTestingDataResult :: TestingDataResult

-- | The testing dataset that was supplied for training.
testingDataResult_input :: Lens' TestingDataResult (Maybe TestingData)

-- | The subset of the dataset that was actually tested. Some images
--   (assets) might not be tested due to file formatting and other issues.
testingDataResult_output :: Lens' TestingDataResult (Maybe TestingData)

-- | The location of the data validation manifest. The data validation
--   manifest is created for the test dataset during model training.
testingDataResult_validation :: Lens' TestingDataResult (Maybe ValidationData)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.TestingDataResult.TestingDataResult
instance GHC.Show.Show Network.AWS.Rekognition.Types.TestingDataResult.TestingDataResult
instance GHC.Read.Read Network.AWS.Rekognition.Types.TestingDataResult.TestingDataResult
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.TestingDataResult.TestingDataResult
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.TestingDataResult.TestingDataResult
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.TestingDataResult.TestingDataResult
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.TestingDataResult.TestingDataResult


module Network.AWS.Rekognition.Types.ProjectVersionDescription

-- | The description of a version of a model.
--   
--   <i>See:</i> <a>newProjectVersionDescription</a> smart constructor.
data ProjectVersionDescription
ProjectVersionDescription' :: Maybe Natural -> Maybe ProjectVersionStatus -> Maybe EvaluationResult -> Maybe GroundTruthManifest -> Maybe Text -> Maybe TestingDataResult -> Maybe Text -> Maybe POSIX -> Maybe Text -> Maybe OutputConfig -> Maybe Natural -> Maybe POSIX -> Maybe TrainingDataResult -> ProjectVersionDescription

-- | The minimum number of inference units used by the model. For more
--   information, see StartProjectVersion.
[$sel:minInferenceUnits:ProjectVersionDescription'] :: ProjectVersionDescription -> Maybe Natural

-- | The current status of the model version.
[$sel:status:ProjectVersionDescription'] :: ProjectVersionDescription -> Maybe ProjectVersionStatus

-- | The training results. <tt>EvaluationResult</tt> is only returned if
--   training is successful.
[$sel:evaluationResult:ProjectVersionDescription'] :: ProjectVersionDescription -> Maybe EvaluationResult

-- | The location of the summary manifest. The summary manifest provides
--   aggregate data validation results for the training and test datasets.
[$sel:manifestSummary:ProjectVersionDescription'] :: ProjectVersionDescription -> Maybe GroundTruthManifest

-- | The identifer for the AWS Key Management Service (AWS KMS) customer
--   master key that was used to encrypt the model during training.
[$sel:kmsKeyId:ProjectVersionDescription'] :: ProjectVersionDescription -> Maybe Text

-- | Contains information about the testing results.
[$sel:testingDataResult:ProjectVersionDescription'] :: ProjectVersionDescription -> Maybe TestingDataResult

-- | A descriptive message for an error or warning that occurred.
[$sel:statusMessage:ProjectVersionDescription'] :: ProjectVersionDescription -> Maybe Text

-- | The Unix datetime for the date and time that training started.
[$sel:creationTimestamp:ProjectVersionDescription'] :: ProjectVersionDescription -> Maybe POSIX

-- | The Amazon Resource Name (ARN) of the model version.
[$sel:projectVersionArn:ProjectVersionDescription'] :: ProjectVersionDescription -> Maybe Text

-- | The location where training results are saved.
[$sel:outputConfig:ProjectVersionDescription'] :: ProjectVersionDescription -> Maybe OutputConfig

-- | The duration, in seconds, that the model version has been billed for
--   training. This value is only returned if the model version has been
--   successfully trained.
[$sel:billableTrainingTimeInSeconds:ProjectVersionDescription'] :: ProjectVersionDescription -> Maybe Natural

-- | The Unix date and time that training of the model ended.
[$sel:trainingEndTimestamp:ProjectVersionDescription'] :: ProjectVersionDescription -> Maybe POSIX

-- | Contains information about the training results.
[$sel:trainingDataResult:ProjectVersionDescription'] :: ProjectVersionDescription -> Maybe TrainingDataResult

-- | Create a value of <a>ProjectVersionDescription</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:minInferenceUnits:ProjectVersionDescription'</a>,
--   <a>projectVersionDescription_minInferenceUnits</a> - The minimum
--   number of inference units used by the model. For more information, see
--   StartProjectVersion.
--   
--   <a>$sel:status:ProjectVersionDescription'</a>,
--   <a>projectVersionDescription_status</a> - The current status of the
--   model version.
--   
--   <a>$sel:evaluationResult:ProjectVersionDescription'</a>,
--   <a>projectVersionDescription_evaluationResult</a> - The training
--   results. <tt>EvaluationResult</tt> is only returned if training is
--   successful.
--   
--   <a>$sel:manifestSummary:ProjectVersionDescription'</a>,
--   <a>projectVersionDescription_manifestSummary</a> - The location of the
--   summary manifest. The summary manifest provides aggregate data
--   validation results for the training and test datasets.
--   
--   <a>$sel:kmsKeyId:ProjectVersionDescription'</a>,
--   <a>projectVersionDescription_kmsKeyId</a> - The identifer for the AWS
--   Key Management Service (AWS KMS) customer master key that was used to
--   encrypt the model during training.
--   
--   <a>$sel:testingDataResult:ProjectVersionDescription'</a>,
--   <a>projectVersionDescription_testingDataResult</a> - Contains
--   information about the testing results.
--   
--   <a>$sel:statusMessage:ProjectVersionDescription'</a>,
--   <a>projectVersionDescription_statusMessage</a> - A descriptive message
--   for an error or warning that occurred.
--   
--   <a>$sel:creationTimestamp:ProjectVersionDescription'</a>,
--   <a>projectVersionDescription_creationTimestamp</a> - The Unix datetime
--   for the date and time that training started.
--   
--   <a>$sel:projectVersionArn:ProjectVersionDescription'</a>,
--   <a>projectVersionDescription_projectVersionArn</a> - The Amazon
--   Resource Name (ARN) of the model version.
--   
--   <a>$sel:outputConfig:ProjectVersionDescription'</a>,
--   <a>projectVersionDescription_outputConfig</a> - The location where
--   training results are saved.
--   
--   <a>$sel:billableTrainingTimeInSeconds:ProjectVersionDescription'</a>,
--   <a>projectVersionDescription_billableTrainingTimeInSeconds</a> - The
--   duration, in seconds, that the model version has been billed for
--   training. This value is only returned if the model version has been
--   successfully trained.
--   
--   <a>$sel:trainingEndTimestamp:ProjectVersionDescription'</a>,
--   <a>projectVersionDescription_trainingEndTimestamp</a> - The Unix date
--   and time that training of the model ended.
--   
--   <a>$sel:trainingDataResult:ProjectVersionDescription'</a>,
--   <a>projectVersionDescription_trainingDataResult</a> - Contains
--   information about the training results.
newProjectVersionDescription :: ProjectVersionDescription

-- | The minimum number of inference units used by the model. For more
--   information, see StartProjectVersion.
projectVersionDescription_minInferenceUnits :: Lens' ProjectVersionDescription (Maybe Natural)

-- | The current status of the model version.
projectVersionDescription_status :: Lens' ProjectVersionDescription (Maybe ProjectVersionStatus)

-- | The training results. <tt>EvaluationResult</tt> is only returned if
--   training is successful.
projectVersionDescription_evaluationResult :: Lens' ProjectVersionDescription (Maybe EvaluationResult)

-- | The location of the summary manifest. The summary manifest provides
--   aggregate data validation results for the training and test datasets.
projectVersionDescription_manifestSummary :: Lens' ProjectVersionDescription (Maybe GroundTruthManifest)

-- | The identifer for the AWS Key Management Service (AWS KMS) customer
--   master key that was used to encrypt the model during training.
projectVersionDescription_kmsKeyId :: Lens' ProjectVersionDescription (Maybe Text)

-- | Contains information about the testing results.
projectVersionDescription_testingDataResult :: Lens' ProjectVersionDescription (Maybe TestingDataResult)

-- | A descriptive message for an error or warning that occurred.
projectVersionDescription_statusMessage :: Lens' ProjectVersionDescription (Maybe Text)

-- | The Unix datetime for the date and time that training started.
projectVersionDescription_creationTimestamp :: Lens' ProjectVersionDescription (Maybe UTCTime)

-- | The Amazon Resource Name (ARN) of the model version.
projectVersionDescription_projectVersionArn :: Lens' ProjectVersionDescription (Maybe Text)

-- | The location where training results are saved.
projectVersionDescription_outputConfig :: Lens' ProjectVersionDescription (Maybe OutputConfig)

-- | The duration, in seconds, that the model version has been billed for
--   training. This value is only returned if the model version has been
--   successfully trained.
projectVersionDescription_billableTrainingTimeInSeconds :: Lens' ProjectVersionDescription (Maybe Natural)

-- | The Unix date and time that training of the model ended.
projectVersionDescription_trainingEndTimestamp :: Lens' ProjectVersionDescription (Maybe UTCTime)

-- | Contains information about the training results.
projectVersionDescription_trainingDataResult :: Lens' ProjectVersionDescription (Maybe TrainingDataResult)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.ProjectVersionDescription.ProjectVersionDescription
instance GHC.Show.Show Network.AWS.Rekognition.Types.ProjectVersionDescription.ProjectVersionDescription
instance GHC.Read.Read Network.AWS.Rekognition.Types.ProjectVersionDescription.ProjectVersionDescription
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.ProjectVersionDescription.ProjectVersionDescription
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.ProjectVersionDescription.ProjectVersionDescription
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.ProjectVersionDescription.ProjectVersionDescription
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.ProjectVersionDescription.ProjectVersionDescription


module Network.AWS.Rekognition.Types.Video

-- | Video file stored in an Amazon S3 bucket. Amazon Rekognition video
--   start operations such as StartLabelDetection use <tt>Video</tt> to
--   specify a video for analysis. The supported file formats are .mp4,
--   .mov and .avi.
--   
--   <i>See:</i> <a>newVideo</a> smart constructor.
data Video
Video' :: Maybe S3Object -> Video

-- | The Amazon S3 bucket name and file name for the video.
[$sel:s3Object:Video'] :: Video -> Maybe S3Object

-- | Create a value of <a>Video</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:s3Object:Video'</a>, <a>video_s3Object</a> - The Amazon S3
--   bucket name and file name for the video.
newVideo :: Video

-- | The Amazon S3 bucket name and file name for the video.
video_s3Object :: Lens' Video (Maybe S3Object)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.Video.Video
instance GHC.Show.Show Network.AWS.Rekognition.Types.Video.Video
instance GHC.Read.Read Network.AWS.Rekognition.Types.Video.Video
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.Video.Video
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.Video.Video
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.Video.Video
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.Types.Video.Video


module Network.AWS.Rekognition.Types.VideoColorRange
newtype VideoColorRange
VideoColorRange' :: Text -> VideoColorRange
[fromVideoColorRange] :: VideoColorRange -> Text
pattern VideoColorRange_FULL :: VideoColorRange
pattern VideoColorRange_LIMITED :: VideoColorRange
instance Network.AWS.Data.XML.ToXML Network.AWS.Rekognition.Types.VideoColorRange.VideoColorRange
instance Network.AWS.Data.XML.FromXML Network.AWS.Rekognition.Types.VideoColorRange.VideoColorRange
instance Data.Aeson.Types.ToJSON.ToJSONKey Network.AWS.Rekognition.Types.VideoColorRange.VideoColorRange
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.Types.VideoColorRange.VideoColorRange
instance Data.Aeson.Types.FromJSON.FromJSONKey Network.AWS.Rekognition.Types.VideoColorRange.VideoColorRange
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.VideoColorRange.VideoColorRange
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.Types.VideoColorRange.VideoColorRange
instance Network.AWS.Data.Headers.ToHeader Network.AWS.Rekognition.Types.VideoColorRange.VideoColorRange
instance Network.AWS.Data.Log.ToLog Network.AWS.Rekognition.Types.VideoColorRange.VideoColorRange
instance Network.AWS.Data.ByteString.ToByteString Network.AWS.Rekognition.Types.VideoColorRange.VideoColorRange
instance Network.AWS.Data.Text.ToText Network.AWS.Rekognition.Types.VideoColorRange.VideoColorRange
instance Network.AWS.Data.Text.FromText Network.AWS.Rekognition.Types.VideoColorRange.VideoColorRange
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.VideoColorRange.VideoColorRange
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.VideoColorRange.VideoColorRange
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.VideoColorRange.VideoColorRange
instance GHC.Classes.Ord Network.AWS.Rekognition.Types.VideoColorRange.VideoColorRange
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.VideoColorRange.VideoColorRange
instance GHC.Read.Read Network.AWS.Rekognition.Types.VideoColorRange.VideoColorRange
instance GHC.Show.Show Network.AWS.Rekognition.Types.VideoColorRange.VideoColorRange


module Network.AWS.Rekognition.Types.VideoJobStatus
newtype VideoJobStatus
VideoJobStatus' :: Text -> VideoJobStatus
[fromVideoJobStatus] :: VideoJobStatus -> Text
pattern VideoJobStatus_FAILED :: VideoJobStatus
pattern VideoJobStatus_IN_PROGRESS :: VideoJobStatus
pattern VideoJobStatus_SUCCEEDED :: VideoJobStatus
instance Network.AWS.Data.XML.ToXML Network.AWS.Rekognition.Types.VideoJobStatus.VideoJobStatus
instance Network.AWS.Data.XML.FromXML Network.AWS.Rekognition.Types.VideoJobStatus.VideoJobStatus
instance Data.Aeson.Types.ToJSON.ToJSONKey Network.AWS.Rekognition.Types.VideoJobStatus.VideoJobStatus
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.Types.VideoJobStatus.VideoJobStatus
instance Data.Aeson.Types.FromJSON.FromJSONKey Network.AWS.Rekognition.Types.VideoJobStatus.VideoJobStatus
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.VideoJobStatus.VideoJobStatus
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.Types.VideoJobStatus.VideoJobStatus
instance Network.AWS.Data.Headers.ToHeader Network.AWS.Rekognition.Types.VideoJobStatus.VideoJobStatus
instance Network.AWS.Data.Log.ToLog Network.AWS.Rekognition.Types.VideoJobStatus.VideoJobStatus
instance Network.AWS.Data.ByteString.ToByteString Network.AWS.Rekognition.Types.VideoJobStatus.VideoJobStatus
instance Network.AWS.Data.Text.ToText Network.AWS.Rekognition.Types.VideoJobStatus.VideoJobStatus
instance Network.AWS.Data.Text.FromText Network.AWS.Rekognition.Types.VideoJobStatus.VideoJobStatus
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.VideoJobStatus.VideoJobStatus
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.VideoJobStatus.VideoJobStatus
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.VideoJobStatus.VideoJobStatus
instance GHC.Classes.Ord Network.AWS.Rekognition.Types.VideoJobStatus.VideoJobStatus
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.VideoJobStatus.VideoJobStatus
instance GHC.Read.Read Network.AWS.Rekognition.Types.VideoJobStatus.VideoJobStatus
instance GHC.Show.Show Network.AWS.Rekognition.Types.VideoJobStatus.VideoJobStatus


module Network.AWS.Rekognition.Types.VideoMetadata

-- | Information about a video that Amazon Rekognition analyzed.
--   <tt>Videometadata</tt> is returned in every page of paginated
--   responses from a Amazon Rekognition video operation.
--   
--   <i>See:</i> <a>newVideoMetadata</a> smart constructor.
data VideoMetadata
VideoMetadata' :: Maybe Double -> Maybe VideoColorRange -> Maybe Text -> Maybe Text -> Maybe Natural -> Maybe Natural -> Maybe Natural -> VideoMetadata

-- | Number of frames per second in the video.
[$sel:frameRate:VideoMetadata'] :: VideoMetadata -> Maybe Double

-- | A description of the range of luminance values in a video, either
--   LIMITED (16 to 235) or FULL (0 to 255).
[$sel:colorRange:VideoMetadata'] :: VideoMetadata -> Maybe VideoColorRange

-- | Format of the analyzed video. Possible values are MP4, MOV and AVI.
[$sel:format:VideoMetadata'] :: VideoMetadata -> Maybe Text

-- | Type of compression used in the analyzed video.
[$sel:codec:VideoMetadata'] :: VideoMetadata -> Maybe Text

-- | Vertical pixel dimension of the video.
[$sel:frameHeight:VideoMetadata'] :: VideoMetadata -> Maybe Natural

-- | Length of the video in milliseconds.
[$sel:durationMillis:VideoMetadata'] :: VideoMetadata -> Maybe Natural

-- | Horizontal pixel dimension of the video.
[$sel:frameWidth:VideoMetadata'] :: VideoMetadata -> Maybe Natural

-- | Create a value of <a>VideoMetadata</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:frameRate:VideoMetadata'</a>, <a>videoMetadata_frameRate</a> -
--   Number of frames per second in the video.
--   
--   <a>$sel:colorRange:VideoMetadata'</a>, <a>videoMetadata_colorRange</a>
--   - A description of the range of luminance values in a video, either
--   LIMITED (16 to 235) or FULL (0 to 255).
--   
--   <a>$sel:format:VideoMetadata'</a>, <a>videoMetadata_format</a> -
--   Format of the analyzed video. Possible values are MP4, MOV and AVI.
--   
--   <a>$sel:codec:VideoMetadata'</a>, <a>videoMetadata_codec</a> - Type of
--   compression used in the analyzed video.
--   
--   <a>$sel:frameHeight:VideoMetadata'</a>,
--   <a>videoMetadata_frameHeight</a> - Vertical pixel dimension of the
--   video.
--   
--   <a>$sel:durationMillis:VideoMetadata'</a>,
--   <a>videoMetadata_durationMillis</a> - Length of the video in
--   milliseconds.
--   
--   <a>$sel:frameWidth:VideoMetadata'</a>, <a>videoMetadata_frameWidth</a>
--   - Horizontal pixel dimension of the video.
newVideoMetadata :: VideoMetadata

-- | Number of frames per second in the video.
videoMetadata_frameRate :: Lens' VideoMetadata (Maybe Double)

-- | A description of the range of luminance values in a video, either
--   LIMITED (16 to 235) or FULL (0 to 255).
videoMetadata_colorRange :: Lens' VideoMetadata (Maybe VideoColorRange)

-- | Format of the analyzed video. Possible values are MP4, MOV and AVI.
videoMetadata_format :: Lens' VideoMetadata (Maybe Text)

-- | Type of compression used in the analyzed video.
videoMetadata_codec :: Lens' VideoMetadata (Maybe Text)

-- | Vertical pixel dimension of the video.
videoMetadata_frameHeight :: Lens' VideoMetadata (Maybe Natural)

-- | Length of the video in milliseconds.
videoMetadata_durationMillis :: Lens' VideoMetadata (Maybe Natural)

-- | Horizontal pixel dimension of the video.
videoMetadata_frameWidth :: Lens' VideoMetadata (Maybe Natural)
instance GHC.Generics.Generic Network.AWS.Rekognition.Types.VideoMetadata.VideoMetadata
instance GHC.Show.Show Network.AWS.Rekognition.Types.VideoMetadata.VideoMetadata
instance GHC.Read.Read Network.AWS.Rekognition.Types.VideoMetadata.VideoMetadata
instance GHC.Classes.Eq Network.AWS.Rekognition.Types.VideoMetadata.VideoMetadata
instance Data.Aeson.Types.FromJSON.FromJSON Network.AWS.Rekognition.Types.VideoMetadata.VideoMetadata
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.Types.VideoMetadata.VideoMetadata
instance Control.DeepSeq.NFData Network.AWS.Rekognition.Types.VideoMetadata.VideoMetadata


module Network.AWS.Rekognition.Types

-- | API version <tt>2016-06-27</tt> of the Amazon Rekognition SDK
--   configuration.
defaultService :: Service

-- | You are not authorized to perform the action.
_AccessDeniedException :: AsError a => Getting (First ServiceError) a ServiceError

-- | The file size or duration of the supplied media is too large. The
--   maximum file size is 10GB. The maximum duration is 6 hours.
_VideoTooLargeException :: AsError a => Getting (First ServiceError) a ServiceError

-- | Input parameter violated a constraint. Validate your parameter before
--   calling the API operation again.
_InvalidParameterException :: AsError a => Getting (First ServiceError) a ServiceError

-- | The provided image format is not supported.
_InvalidImageFormatException :: AsError a => Getting (First ServiceError) a ServiceError

-- | A resource with the specified ID already exists.
_ResourceAlreadyExistsException :: AsError a => Getting (First ServiceError) a ServiceError

-- | Amazon Rekognition is unable to access the S3 object specified in the
--   request.
_InvalidS3ObjectException :: AsError a => Getting (First ServiceError) a ServiceError

-- | The number of requests exceeded your throughput limit. If you want to
--   increase this limit, contact Amazon Rekognition.
_ProvisionedThroughputExceededException :: AsError a => Getting (First ServiceError) a ServiceError

-- | The input image size exceeds the allowed limit. If you are calling
--   DetectProtectiveEquipment, the image size or resolution exceeds the
--   allowed limit. For more information, see Limits in Amazon Rekognition
--   in the Amazon Rekognition Developer Guide.
_ImageTooLargeException :: AsError a => Getting (First ServiceError) a ServiceError

-- | The size of the collection exceeds the allowed limit. For more
--   information, see Limits in Amazon Rekognition in the Amazon
--   Rekognition Developer Guide.
_ServiceQuotaExceededException :: AsError a => Getting (First ServiceError) a ServiceError

-- | Amazon Rekognition is temporarily unable to process the request. Try
--   your call again.
_ThrottlingException :: AsError a => Getting (First ServiceError) a ServiceError

-- | Amazon Rekognition experienced a service issue. Try your call again.
_InternalServerError :: AsError a => Getting (First ServiceError) a ServiceError

-- | A <tt>ClientRequestToken</tt> input parameter was reused with an
--   operation, but at least one of the other input parameters is different
--   from the previous call to the operation.
_IdempotentParameterMismatchException :: AsError a => Getting (First ServiceError) a ServiceError

-- | The requested resource isn't ready. For example, this exception occurs
--   when you call <tt>DetectCustomLabels</tt> with a model version that
--   isn't deployed.
_ResourceNotReadyException :: AsError a => Getting (First ServiceError) a ServiceError

-- | The resource specified in the request cannot be found.
_ResourceNotFoundException :: AsError a => Getting (First ServiceError) a ServiceError

-- | The number of in-progress human reviews you have has exceeded the
--   number allowed.
_HumanLoopQuotaExceededException :: AsError a => Getting (First ServiceError) a ServiceError

-- | Pagination token in the request is not valid.
_InvalidPaginationTokenException :: AsError a => Getting (First ServiceError) a ServiceError

-- | An Amazon Rekognition service limit was exceeded. For example, if you
--   start too many Amazon Rekognition Video jobs concurrently, calls to
--   start operations (<tt>StartLabelDetection</tt>, for example) will
--   raise a <tt>LimitExceededException</tt> exception (HTTP status code:
--   400) until the number of concurrently running jobs is below the Amazon
--   Rekognition service limit.
_LimitExceededException :: AsError a => Getting (First ServiceError) a ServiceError

-- | The specified resource is already being used.
_ResourceInUseException :: AsError a => Getting (First ServiceError) a ServiceError
newtype Attribute
Attribute' :: Text -> Attribute
[fromAttribute] :: Attribute -> Text
pattern Attribute_ALL :: Attribute
pattern Attribute_DEFAULT :: Attribute
newtype BodyPart
BodyPart' :: Text -> BodyPart
[fromBodyPart] :: BodyPart -> Text
pattern BodyPart_FACE :: BodyPart
pattern BodyPart_HEAD :: BodyPart
pattern BodyPart_LEFT_HAND :: BodyPart
pattern BodyPart_RIGHT_HAND :: BodyPart
newtype CelebrityRecognitionSortBy
CelebrityRecognitionSortBy' :: Text -> CelebrityRecognitionSortBy
[fromCelebrityRecognitionSortBy] :: CelebrityRecognitionSortBy -> Text
pattern CelebrityRecognitionSortBy_ID :: CelebrityRecognitionSortBy
pattern CelebrityRecognitionSortBy_TIMESTAMP :: CelebrityRecognitionSortBy
newtype ContentClassifier
ContentClassifier' :: Text -> ContentClassifier
[fromContentClassifier] :: ContentClassifier -> Text
pattern ContentClassifier_FreeOfAdultContent :: ContentClassifier
pattern ContentClassifier_FreeOfPersonallyIdentifiableInformation :: ContentClassifier
newtype ContentModerationSortBy
ContentModerationSortBy' :: Text -> ContentModerationSortBy
[fromContentModerationSortBy] :: ContentModerationSortBy -> Text
pattern ContentModerationSortBy_NAME :: ContentModerationSortBy
pattern ContentModerationSortBy_TIMESTAMP :: ContentModerationSortBy
newtype EmotionName
EmotionName' :: Text -> EmotionName
[fromEmotionName] :: EmotionName -> Text
pattern EmotionName_ANGRY :: EmotionName
pattern EmotionName_CALM :: EmotionName
pattern EmotionName_CONFUSED :: EmotionName
pattern EmotionName_DISGUSTED :: EmotionName
pattern EmotionName_FEAR :: EmotionName
pattern EmotionName_HAPPY :: EmotionName
pattern EmotionName_SAD :: EmotionName
pattern EmotionName_SURPRISED :: EmotionName
pattern EmotionName_UNKNOWN :: EmotionName
newtype FaceAttributes
FaceAttributes' :: Text -> FaceAttributes
[fromFaceAttributes] :: FaceAttributes -> Text
pattern FaceAttributes_ALL :: FaceAttributes
pattern FaceAttributes_DEFAULT :: FaceAttributes
newtype FaceSearchSortBy
FaceSearchSortBy' :: Text -> FaceSearchSortBy
[fromFaceSearchSortBy] :: FaceSearchSortBy -> Text
pattern FaceSearchSortBy_INDEX :: FaceSearchSortBy
pattern FaceSearchSortBy_TIMESTAMP :: FaceSearchSortBy
newtype GenderType
GenderType' :: Text -> GenderType
[fromGenderType] :: GenderType -> Text
pattern GenderType_Female :: GenderType
pattern GenderType_Male :: GenderType

-- | A list of enum string of possible gender values that Celebrity
--   returns.
newtype KnownGenderType
KnownGenderType' :: Text -> KnownGenderType
[fromKnownGenderType] :: KnownGenderType -> Text
pattern KnownGenderType_Female :: KnownGenderType
pattern KnownGenderType_Male :: KnownGenderType
newtype LabelDetectionSortBy
LabelDetectionSortBy' :: Text -> LabelDetectionSortBy
[fromLabelDetectionSortBy] :: LabelDetectionSortBy -> Text
pattern LabelDetectionSortBy_NAME :: LabelDetectionSortBy
pattern LabelDetectionSortBy_TIMESTAMP :: LabelDetectionSortBy
newtype LandmarkType
LandmarkType' :: Text -> LandmarkType
[fromLandmarkType] :: LandmarkType -> Text
pattern LandmarkType_ChinBottom :: LandmarkType
pattern LandmarkType_EyeLeft :: LandmarkType
pattern LandmarkType_EyeRight :: LandmarkType
pattern LandmarkType_LeftEyeBrowLeft :: LandmarkType
pattern LandmarkType_LeftEyeBrowRight :: LandmarkType
pattern LandmarkType_LeftEyeBrowUp :: LandmarkType
pattern LandmarkType_LeftEyeDown :: LandmarkType
pattern LandmarkType_LeftEyeLeft :: LandmarkType
pattern LandmarkType_LeftEyeRight :: LandmarkType
pattern LandmarkType_LeftEyeUp :: LandmarkType
pattern LandmarkType_LeftPupil :: LandmarkType
pattern LandmarkType_MidJawlineLeft :: LandmarkType
pattern LandmarkType_MidJawlineRight :: LandmarkType
pattern LandmarkType_MouthDown :: LandmarkType
pattern LandmarkType_MouthLeft :: LandmarkType
pattern LandmarkType_MouthRight :: LandmarkType
pattern LandmarkType_MouthUp :: LandmarkType
pattern LandmarkType_Nose :: LandmarkType
pattern LandmarkType_NoseLeft :: LandmarkType
pattern LandmarkType_NoseRight :: LandmarkType
pattern LandmarkType_RightEyeBrowLeft :: LandmarkType
pattern LandmarkType_RightEyeBrowRight :: LandmarkType
pattern LandmarkType_RightEyeBrowUp :: LandmarkType
pattern LandmarkType_RightEyeDown :: LandmarkType
pattern LandmarkType_RightEyeLeft :: LandmarkType
pattern LandmarkType_RightEyeRight :: LandmarkType
pattern LandmarkType_RightEyeUp :: LandmarkType
pattern LandmarkType_RightPupil :: LandmarkType
pattern LandmarkType_UpperJawlineLeft :: LandmarkType
pattern LandmarkType_UpperJawlineRight :: LandmarkType
newtype OrientationCorrection
OrientationCorrection' :: Text -> OrientationCorrection
[fromOrientationCorrection] :: OrientationCorrection -> Text
pattern OrientationCorrection_ROTATE_0 :: OrientationCorrection
pattern OrientationCorrection_ROTATE_180 :: OrientationCorrection
pattern OrientationCorrection_ROTATE_270 :: OrientationCorrection
pattern OrientationCorrection_ROTATE_90 :: OrientationCorrection
newtype PersonTrackingSortBy
PersonTrackingSortBy' :: Text -> PersonTrackingSortBy
[fromPersonTrackingSortBy] :: PersonTrackingSortBy -> Text
pattern PersonTrackingSortBy_INDEX :: PersonTrackingSortBy
pattern PersonTrackingSortBy_TIMESTAMP :: PersonTrackingSortBy
newtype ProjectStatus
ProjectStatus' :: Text -> ProjectStatus
[fromProjectStatus] :: ProjectStatus -> Text
pattern ProjectStatus_CREATED :: ProjectStatus
pattern ProjectStatus_CREATING :: ProjectStatus
pattern ProjectStatus_DELETING :: ProjectStatus
newtype ProjectVersionStatus
ProjectVersionStatus' :: Text -> ProjectVersionStatus
[fromProjectVersionStatus] :: ProjectVersionStatus -> Text
pattern ProjectVersionStatus_DELETING :: ProjectVersionStatus
pattern ProjectVersionStatus_FAILED :: ProjectVersionStatus
pattern ProjectVersionStatus_RUNNING :: ProjectVersionStatus
pattern ProjectVersionStatus_STARTING :: ProjectVersionStatus
pattern ProjectVersionStatus_STOPPED :: ProjectVersionStatus
pattern ProjectVersionStatus_STOPPING :: ProjectVersionStatus
pattern ProjectVersionStatus_TRAINING_COMPLETED :: ProjectVersionStatus
pattern ProjectVersionStatus_TRAINING_FAILED :: ProjectVersionStatus
pattern ProjectVersionStatus_TRAINING_IN_PROGRESS :: ProjectVersionStatus
newtype ProtectiveEquipmentType
ProtectiveEquipmentType' :: Text -> ProtectiveEquipmentType
[fromProtectiveEquipmentType] :: ProtectiveEquipmentType -> Text
pattern ProtectiveEquipmentType_FACE_COVER :: ProtectiveEquipmentType
pattern ProtectiveEquipmentType_HAND_COVER :: ProtectiveEquipmentType
pattern ProtectiveEquipmentType_HEAD_COVER :: ProtectiveEquipmentType
newtype QualityFilter
QualityFilter' :: Text -> QualityFilter
[fromQualityFilter] :: QualityFilter -> Text
pattern QualityFilter_AUTO :: QualityFilter
pattern QualityFilter_HIGH :: QualityFilter
pattern QualityFilter_LOW :: QualityFilter
pattern QualityFilter_MEDIUM :: QualityFilter
pattern QualityFilter_NONE :: QualityFilter
newtype Reason
Reason' :: Text -> Reason
[fromReason] :: Reason -> Text
pattern Reason_EXCEEDS_MAX_FACES :: Reason
pattern Reason_EXTREME_POSE :: Reason
pattern Reason_LOW_BRIGHTNESS :: Reason
pattern Reason_LOW_CONFIDENCE :: Reason
pattern Reason_LOW_FACE_QUALITY :: Reason
pattern Reason_LOW_SHARPNESS :: Reason
pattern Reason_SMALL_BOUNDING_BOX :: Reason
newtype SegmentType
SegmentType' :: Text -> SegmentType
[fromSegmentType] :: SegmentType -> Text
pattern SegmentType_SHOT :: SegmentType
pattern SegmentType_TECHNICAL_CUE :: SegmentType
newtype StreamProcessorStatus
StreamProcessorStatus' :: Text -> StreamProcessorStatus
[fromStreamProcessorStatus] :: StreamProcessorStatus -> Text
pattern StreamProcessorStatus_FAILED :: StreamProcessorStatus
pattern StreamProcessorStatus_RUNNING :: StreamProcessorStatus
pattern StreamProcessorStatus_STARTING :: StreamProcessorStatus
pattern StreamProcessorStatus_STOPPED :: StreamProcessorStatus
pattern StreamProcessorStatus_STOPPING :: StreamProcessorStatus
newtype TechnicalCueType
TechnicalCueType' :: Text -> TechnicalCueType
[fromTechnicalCueType] :: TechnicalCueType -> Text
pattern TechnicalCueType_BlackFrames :: TechnicalCueType
pattern TechnicalCueType_ColorBars :: TechnicalCueType
pattern TechnicalCueType_Content :: TechnicalCueType
pattern TechnicalCueType_EndCredits :: TechnicalCueType
pattern TechnicalCueType_OpeningCredits :: TechnicalCueType
pattern TechnicalCueType_Slate :: TechnicalCueType
pattern TechnicalCueType_StudioLogo :: TechnicalCueType
newtype TextTypes
TextTypes' :: Text -> TextTypes
[fromTextTypes] :: TextTypes -> Text
pattern TextTypes_LINE :: TextTypes
pattern TextTypes_WORD :: TextTypes
newtype VideoColorRange
VideoColorRange' :: Text -> VideoColorRange
[fromVideoColorRange] :: VideoColorRange -> Text
pattern VideoColorRange_FULL :: VideoColorRange
pattern VideoColorRange_LIMITED :: VideoColorRange
newtype VideoJobStatus
VideoJobStatus' :: Text -> VideoJobStatus
[fromVideoJobStatus] :: VideoJobStatus -> Text
pattern VideoJobStatus_FAILED :: VideoJobStatus
pattern VideoJobStatus_IN_PROGRESS :: VideoJobStatus
pattern VideoJobStatus_SUCCEEDED :: VideoJobStatus

-- | Structure containing the estimated age range, in years, for a face.
--   
--   Amazon Rekognition estimates an age range for faces detected in the
--   input image. Estimated age ranges can overlap. A face of a 5-year-old
--   might have an estimated range of 4-6, while the face of a 6-year-old
--   might have an estimated range of 4-8.
--   
--   <i>See:</i> <a>newAgeRange</a> smart constructor.
data AgeRange
AgeRange' :: Maybe Natural -> Maybe Natural -> AgeRange

-- | The lowest estimated age.
[$sel:low:AgeRange'] :: AgeRange -> Maybe Natural

-- | The highest estimated age.
[$sel:high:AgeRange'] :: AgeRange -> Maybe Natural

-- | Create a value of <a>AgeRange</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:low:AgeRange'</a>, <a>ageRange_low</a> - The lowest estimated
--   age.
--   
--   <a>$sel:high:AgeRange'</a>, <a>ageRange_high</a> - The highest
--   estimated age.
newAgeRange :: AgeRange

-- | The lowest estimated age.
ageRange_low :: Lens' AgeRange (Maybe Natural)

-- | The highest estimated age.
ageRange_high :: Lens' AgeRange (Maybe Natural)

-- | Assets are the images that you use to train and evaluate a model
--   version. Assets can also contain validation information that you use
--   to debug a failed model training.
--   
--   <i>See:</i> <a>newAsset</a> smart constructor.
data Asset
Asset' :: Maybe GroundTruthManifest -> Asset
[$sel:groundTruthManifest:Asset'] :: Asset -> Maybe GroundTruthManifest

-- | Create a value of <a>Asset</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:groundTruthManifest:Asset'</a>,
--   <a>asset_groundTruthManifest</a> - Undocumented member.
newAsset :: Asset

-- | Undocumented member.
asset_groundTruthManifest :: Lens' Asset (Maybe GroundTruthManifest)

-- | Metadata information about an audio stream. An array of
--   <tt>AudioMetadata</tt> objects for the audio streams found in a stored
--   video is returned by GetSegmentDetection.
--   
--   <i>See:</i> <a>newAudioMetadata</a> smart constructor.
data AudioMetadata
AudioMetadata' :: Maybe Text -> Maybe Natural -> Maybe Natural -> Maybe Natural -> AudioMetadata

-- | The audio codec used to encode or decode the audio stream.
[$sel:codec:AudioMetadata'] :: AudioMetadata -> Maybe Text

-- | The sample rate for the audio stream.
[$sel:sampleRate:AudioMetadata'] :: AudioMetadata -> Maybe Natural

-- | The number of audio channels in the segment.
[$sel:numberOfChannels:AudioMetadata'] :: AudioMetadata -> Maybe Natural

-- | The duration of the audio stream in milliseconds.
[$sel:durationMillis:AudioMetadata'] :: AudioMetadata -> Maybe Natural

-- | Create a value of <a>AudioMetadata</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:codec:AudioMetadata'</a>, <a>audioMetadata_codec</a> - The
--   audio codec used to encode or decode the audio stream.
--   
--   <a>$sel:sampleRate:AudioMetadata'</a>, <a>audioMetadata_sampleRate</a>
--   - The sample rate for the audio stream.
--   
--   <a>$sel:numberOfChannels:AudioMetadata'</a>,
--   <a>audioMetadata_numberOfChannels</a> - The number of audio channels
--   in the segment.
--   
--   <a>$sel:durationMillis:AudioMetadata'</a>,
--   <a>audioMetadata_durationMillis</a> - The duration of the audio stream
--   in milliseconds.
newAudioMetadata :: AudioMetadata

-- | The audio codec used to encode or decode the audio stream.
audioMetadata_codec :: Lens' AudioMetadata (Maybe Text)

-- | The sample rate for the audio stream.
audioMetadata_sampleRate :: Lens' AudioMetadata (Maybe Natural)

-- | The number of audio channels in the segment.
audioMetadata_numberOfChannels :: Lens' AudioMetadata (Maybe Natural)

-- | The duration of the audio stream in milliseconds.
audioMetadata_durationMillis :: Lens' AudioMetadata (Maybe Natural)

-- | Indicates whether or not the face has a beard, and the confidence
--   level in the determination.
--   
--   <i>See:</i> <a>newBeard</a> smart constructor.
data Beard
Beard' :: Maybe Bool -> Maybe Double -> Beard

-- | Boolean value that indicates whether the face has beard or not.
[$sel:value:Beard'] :: Beard -> Maybe Bool

-- | Level of confidence in the determination.
[$sel:confidence:Beard'] :: Beard -> Maybe Double

-- | Create a value of <a>Beard</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:value:Beard'</a>, <a>beard_value</a> - Boolean value that
--   indicates whether the face has beard or not.
--   
--   <a>$sel:confidence:Beard'</a>, <a>beard_confidence</a> - Level of
--   confidence in the determination.
newBeard :: Beard

-- | Boolean value that indicates whether the face has beard or not.
beard_value :: Lens' Beard (Maybe Bool)

-- | Level of confidence in the determination.
beard_confidence :: Lens' Beard (Maybe Double)

-- | A filter that allows you to control the black frame detection by
--   specifying the black levels and pixel coverage of black pixels in a
--   frame. As videos can come from multiple sources, formats, and time
--   periods, they may contain different standards and varying noise levels
--   for black frames that need to be accounted for. For more information,
--   see StartSegmentDetection.
--   
--   <i>See:</i> <a>newBlackFrame</a> smart constructor.
data BlackFrame
BlackFrame' :: Maybe Double -> Maybe Double -> BlackFrame

-- | A threshold used to determine the maximum luminance value for a pixel
--   to be considered black. In a full color range video, luminance values
--   range from 0-255. A pixel value of 0 is pure black, and the most
--   strict filter. The maximum black pixel value is computed as follows:
--   max_black_pixel_value = minimum_luminance + MaxPixelThreshold
--   *luminance_range.
--   
--   For example, for a full range video with BlackPixelThreshold = 0.1,
--   max_black_pixel_value is 0 + 0.1 * (255-0) = 25.5.
--   
--   The default value of MaxPixelThreshold is 0.2, which maps to a
--   max_black_pixel_value of 51 for a full range video. You can lower this
--   threshold to be more strict on black levels.
[$sel:maxPixelThreshold:BlackFrame'] :: BlackFrame -> Maybe Double

-- | The minimum percentage of pixels in a frame that need to have a
--   luminance below the max_black_pixel_value for a frame to be considered
--   a black frame. Luminance is calculated using the BT.709 matrix.
--   
--   The default value is 99, which means at least 99% of all pixels in the
--   frame are black pixels as per the <tt>MaxPixelThreshold</tt> set. You
--   can reduce this value to allow more noise on the black frame.
[$sel:minCoveragePercentage:BlackFrame'] :: BlackFrame -> Maybe Double

-- | Create a value of <a>BlackFrame</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:maxPixelThreshold:BlackFrame'</a>,
--   <a>blackFrame_maxPixelThreshold</a> - A threshold used to determine
--   the maximum luminance value for a pixel to be considered black. In a
--   full color range video, luminance values range from 0-255. A pixel
--   value of 0 is pure black, and the most strict filter. The maximum
--   black pixel value is computed as follows: max_black_pixel_value =
--   minimum_luminance + MaxPixelThreshold *luminance_range.
--   
--   For example, for a full range video with BlackPixelThreshold = 0.1,
--   max_black_pixel_value is 0 + 0.1 * (255-0) = 25.5.
--   
--   The default value of MaxPixelThreshold is 0.2, which maps to a
--   max_black_pixel_value of 51 for a full range video. You can lower this
--   threshold to be more strict on black levels.
--   
--   <a>$sel:minCoveragePercentage:BlackFrame'</a>,
--   <a>blackFrame_minCoveragePercentage</a> - The minimum percentage of
--   pixels in a frame that need to have a luminance below the
--   max_black_pixel_value for a frame to be considered a black frame.
--   Luminance is calculated using the BT.709 matrix.
--   
--   The default value is 99, which means at least 99% of all pixels in the
--   frame are black pixels as per the <tt>MaxPixelThreshold</tt> set. You
--   can reduce this value to allow more noise on the black frame.
newBlackFrame :: BlackFrame

-- | A threshold used to determine the maximum luminance value for a pixel
--   to be considered black. In a full color range video, luminance values
--   range from 0-255. A pixel value of 0 is pure black, and the most
--   strict filter. The maximum black pixel value is computed as follows:
--   max_black_pixel_value = minimum_luminance + MaxPixelThreshold
--   *luminance_range.
--   
--   For example, for a full range video with BlackPixelThreshold = 0.1,
--   max_black_pixel_value is 0 + 0.1 * (255-0) = 25.5.
--   
--   The default value of MaxPixelThreshold is 0.2, which maps to a
--   max_black_pixel_value of 51 for a full range video. You can lower this
--   threshold to be more strict on black levels.
blackFrame_maxPixelThreshold :: Lens' BlackFrame (Maybe Double)

-- | The minimum percentage of pixels in a frame that need to have a
--   luminance below the max_black_pixel_value for a frame to be considered
--   a black frame. Luminance is calculated using the BT.709 matrix.
--   
--   The default value is 99, which means at least 99% of all pixels in the
--   frame are black pixels as per the <tt>MaxPixelThreshold</tt> set. You
--   can reduce this value to allow more noise on the black frame.
blackFrame_minCoveragePercentage :: Lens' BlackFrame (Maybe Double)

-- | Identifies the bounding box around the label, face, text or personal
--   protective equipment. The <tt>left</tt> (x-coordinate) and
--   <tt>top</tt> (y-coordinate) are coordinates representing the top and
--   left sides of the bounding box. Note that the upper-left corner of the
--   image is the origin (0,0).
--   
--   The <tt>top</tt> and <tt>left</tt> values returned are ratios of the
--   overall image size. For example, if the input image is 700x200 pixels,
--   and the top-left coordinate of the bounding box is 350x50 pixels, the
--   API returns a <tt>left</tt> value of 0.5 (350/700) and a <tt>top</tt>
--   value of 0.25 (50/200).
--   
--   The <tt>width</tt> and <tt>height</tt> values represent the dimensions
--   of the bounding box as a ratio of the overall image dimension. For
--   example, if the input image is 700x200 pixels, and the bounding box
--   width is 70 pixels, the width returned is 0.1.
--   
--   The bounding box coordinates can have negative values. For example, if
--   Amazon Rekognition is able to detect a face that is at the image edge
--   and is only partially visible, the service can return coordinates that
--   are outside the image bounds and, depending on the image edge, you
--   might get negative values or values greater than 1 for the
--   <tt>left</tt> or <tt>top</tt> values.
--   
--   <i>See:</i> <a>newBoundingBox</a> smart constructor.
data BoundingBox
BoundingBox' :: Maybe Double -> Maybe Double -> Maybe Double -> Maybe Double -> BoundingBox

-- | Height of the bounding box as a ratio of the overall image height.
[$sel:height:BoundingBox'] :: BoundingBox -> Maybe Double

-- | Left coordinate of the bounding box as a ratio of overall image width.
[$sel:left:BoundingBox'] :: BoundingBox -> Maybe Double

-- | Width of the bounding box as a ratio of the overall image width.
[$sel:width:BoundingBox'] :: BoundingBox -> Maybe Double

-- | Top coordinate of the bounding box as a ratio of overall image height.
[$sel:top:BoundingBox'] :: BoundingBox -> Maybe Double

-- | Create a value of <a>BoundingBox</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:height:BoundingBox'</a>, <a>boundingBox_height</a> - Height of
--   the bounding box as a ratio of the overall image height.
--   
--   <a>$sel:left:BoundingBox'</a>, <a>boundingBox_left</a> - Left
--   coordinate of the bounding box as a ratio of overall image width.
--   
--   <a>$sel:width:BoundingBox'</a>, <a>boundingBox_width</a> - Width of
--   the bounding box as a ratio of the overall image width.
--   
--   <a>$sel:top:BoundingBox'</a>, <a>boundingBox_top</a> - Top coordinate
--   of the bounding box as a ratio of overall image height.
newBoundingBox :: BoundingBox

-- | Height of the bounding box as a ratio of the overall image height.
boundingBox_height :: Lens' BoundingBox (Maybe Double)

-- | Left coordinate of the bounding box as a ratio of overall image width.
boundingBox_left :: Lens' BoundingBox (Maybe Double)

-- | Width of the bounding box as a ratio of the overall image width.
boundingBox_width :: Lens' BoundingBox (Maybe Double)

-- | Top coordinate of the bounding box as a ratio of overall image height.
boundingBox_top :: Lens' BoundingBox (Maybe Double)

-- | Provides information about a celebrity recognized by the
--   RecognizeCelebrities operation.
--   
--   <i>See:</i> <a>newCelebrity</a> smart constructor.
data Celebrity
Celebrity' :: Maybe Double -> Maybe [Text] -> Maybe KnownGender -> Maybe Text -> Maybe Text -> Maybe ComparedFace -> Celebrity

-- | The confidence, in percentage, that Amazon Rekognition has that the
--   recognized face is the celebrity.
[$sel:matchConfidence:Celebrity'] :: Celebrity -> Maybe Double

-- | An array of URLs pointing to additional information about the
--   celebrity. If there is no additional information about the celebrity,
--   this list is empty.
[$sel:urls:Celebrity'] :: Celebrity -> Maybe [Text]
[$sel:knownGender:Celebrity'] :: Celebrity -> Maybe KnownGender

-- | The name of the celebrity.
[$sel:name:Celebrity'] :: Celebrity -> Maybe Text

-- | A unique identifier for the celebrity.
[$sel:id:Celebrity'] :: Celebrity -> Maybe Text

-- | Provides information about the celebrity's face, such as its location
--   on the image.
[$sel:face:Celebrity'] :: Celebrity -> Maybe ComparedFace

-- | Create a value of <a>Celebrity</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:matchConfidence:Celebrity'</a>,
--   <a>celebrity_matchConfidence</a> - The confidence, in percentage, that
--   Amazon Rekognition has that the recognized face is the celebrity.
--   
--   <a>$sel:urls:Celebrity'</a>, <a>celebrity_urls</a> - An array of URLs
--   pointing to additional information about the celebrity. If there is no
--   additional information about the celebrity, this list is empty.
--   
--   <a>$sel:knownGender:Celebrity'</a>, <a>celebrity_knownGender</a> -
--   Undocumented member.
--   
--   <a>$sel:name:Celebrity'</a>, <a>celebrity_name</a> - The name of the
--   celebrity.
--   
--   <a>$sel:id:Celebrity'</a>, <a>celebrity_id</a> - A unique identifier
--   for the celebrity.
--   
--   <a>$sel:face:Celebrity'</a>, <a>celebrity_face</a> - Provides
--   information about the celebrity's face, such as its location on the
--   image.
newCelebrity :: Celebrity

-- | The confidence, in percentage, that Amazon Rekognition has that the
--   recognized face is the celebrity.
celebrity_matchConfidence :: Lens' Celebrity (Maybe Double)

-- | An array of URLs pointing to additional information about the
--   celebrity. If there is no additional information about the celebrity,
--   this list is empty.
celebrity_urls :: Lens' Celebrity (Maybe [Text])

-- | Undocumented member.
celebrity_knownGender :: Lens' Celebrity (Maybe KnownGender)

-- | The name of the celebrity.
celebrity_name :: Lens' Celebrity (Maybe Text)

-- | A unique identifier for the celebrity.
celebrity_id :: Lens' Celebrity (Maybe Text)

-- | Provides information about the celebrity's face, such as its location
--   on the image.
celebrity_face :: Lens' Celebrity (Maybe ComparedFace)

-- | Information about a recognized celebrity.
--   
--   <i>See:</i> <a>newCelebrityDetail</a> smart constructor.
data CelebrityDetail
CelebrityDetail' :: Maybe BoundingBox -> Maybe [Text] -> Maybe Double -> Maybe Text -> Maybe Text -> Maybe FaceDetail -> CelebrityDetail

-- | Bounding box around the body of a celebrity.
[$sel:boundingBox:CelebrityDetail'] :: CelebrityDetail -> Maybe BoundingBox

-- | An array of URLs pointing to additional celebrity information.
[$sel:urls:CelebrityDetail'] :: CelebrityDetail -> Maybe [Text]

-- | The confidence, in percentage, that Amazon Rekognition has that the
--   recognized face is the celebrity.
[$sel:confidence:CelebrityDetail'] :: CelebrityDetail -> Maybe Double

-- | The name of the celebrity.
[$sel:name:CelebrityDetail'] :: CelebrityDetail -> Maybe Text

-- | The unique identifier for the celebrity.
[$sel:id:CelebrityDetail'] :: CelebrityDetail -> Maybe Text

-- | Face details for the recognized celebrity.
[$sel:face:CelebrityDetail'] :: CelebrityDetail -> Maybe FaceDetail

-- | Create a value of <a>CelebrityDetail</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:boundingBox:CelebrityDetail'</a>,
--   <a>celebrityDetail_boundingBox</a> - Bounding box around the body of a
--   celebrity.
--   
--   <a>$sel:urls:CelebrityDetail'</a>, <a>celebrityDetail_urls</a> - An
--   array of URLs pointing to additional celebrity information.
--   
--   <a>$sel:confidence:CelebrityDetail'</a>,
--   <a>celebrityDetail_confidence</a> - The confidence, in percentage,
--   that Amazon Rekognition has that the recognized face is the celebrity.
--   
--   <a>$sel:name:CelebrityDetail'</a>, <a>celebrityDetail_name</a> - The
--   name of the celebrity.
--   
--   <a>$sel:id:CelebrityDetail'</a>, <a>celebrityDetail_id</a> - The
--   unique identifier for the celebrity.
--   
--   <a>$sel:face:CelebrityDetail'</a>, <a>celebrityDetail_face</a> - Face
--   details for the recognized celebrity.
newCelebrityDetail :: CelebrityDetail

-- | Bounding box around the body of a celebrity.
celebrityDetail_boundingBox :: Lens' CelebrityDetail (Maybe BoundingBox)

-- | An array of URLs pointing to additional celebrity information.
celebrityDetail_urls :: Lens' CelebrityDetail (Maybe [Text])

-- | The confidence, in percentage, that Amazon Rekognition has that the
--   recognized face is the celebrity.
celebrityDetail_confidence :: Lens' CelebrityDetail (Maybe Double)

-- | The name of the celebrity.
celebrityDetail_name :: Lens' CelebrityDetail (Maybe Text)

-- | The unique identifier for the celebrity.
celebrityDetail_id :: Lens' CelebrityDetail (Maybe Text)

-- | Face details for the recognized celebrity.
celebrityDetail_face :: Lens' CelebrityDetail (Maybe FaceDetail)

-- | Information about a detected celebrity and the time the celebrity was
--   detected in a stored video. For more information, see
--   GetCelebrityRecognition in the Amazon Rekognition Developer Guide.
--   
--   <i>See:</i> <a>newCelebrityRecognition</a> smart constructor.
data CelebrityRecognition
CelebrityRecognition' :: Maybe CelebrityDetail -> Maybe Integer -> CelebrityRecognition

-- | Information about a recognized celebrity.
[$sel:celebrity:CelebrityRecognition'] :: CelebrityRecognition -> Maybe CelebrityDetail

-- | The time, in milliseconds from the start of the video, that the
--   celebrity was recognized.
[$sel:timestamp:CelebrityRecognition'] :: CelebrityRecognition -> Maybe Integer

-- | Create a value of <a>CelebrityRecognition</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:celebrity:CelebrityRecognition'</a>,
--   <a>celebrityRecognition_celebrity</a> - Information about a recognized
--   celebrity.
--   
--   <a>$sel:timestamp:CelebrityRecognition'</a>,
--   <a>celebrityRecognition_timestamp</a> - The time, in milliseconds from
--   the start of the video, that the celebrity was recognized.
newCelebrityRecognition :: CelebrityRecognition

-- | Information about a recognized celebrity.
celebrityRecognition_celebrity :: Lens' CelebrityRecognition (Maybe CelebrityDetail)

-- | The time, in milliseconds from the start of the video, that the
--   celebrity was recognized.
celebrityRecognition_timestamp :: Lens' CelebrityRecognition (Maybe Integer)

-- | Provides information about a face in a target image that matches the
--   source image face analyzed by <tt>CompareFaces</tt>. The <tt>Face</tt>
--   property contains the bounding box of the face in the target image.
--   The <tt>Similarity</tt> property is the confidence that the source
--   image face matches the face in the bounding box.
--   
--   <i>See:</i> <a>newCompareFacesMatch</a> smart constructor.
data CompareFacesMatch
CompareFacesMatch' :: Maybe Double -> Maybe ComparedFace -> CompareFacesMatch

-- | Level of confidence that the faces match.
[$sel:similarity:CompareFacesMatch'] :: CompareFacesMatch -> Maybe Double

-- | Provides face metadata (bounding box and confidence that the bounding
--   box actually contains a face).
[$sel:face:CompareFacesMatch'] :: CompareFacesMatch -> Maybe ComparedFace

-- | Create a value of <a>CompareFacesMatch</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:similarity:CompareFacesMatch'</a>,
--   <a>compareFacesMatch_similarity</a> - Level of confidence that the
--   faces match.
--   
--   <a>$sel:face:CompareFacesMatch'</a>, <a>compareFacesMatch_face</a> -
--   Provides face metadata (bounding box and confidence that the bounding
--   box actually contains a face).
newCompareFacesMatch :: CompareFacesMatch

-- | Level of confidence that the faces match.
compareFacesMatch_similarity :: Lens' CompareFacesMatch (Maybe Double)

-- | Provides face metadata (bounding box and confidence that the bounding
--   box actually contains a face).
compareFacesMatch_face :: Lens' CompareFacesMatch (Maybe ComparedFace)

-- | Provides face metadata for target image faces that are analyzed by
--   <tt>CompareFaces</tt> and <tt>RecognizeCelebrities</tt>.
--   
--   <i>See:</i> <a>newComparedFace</a> smart constructor.
data ComparedFace
ComparedFace' :: Maybe BoundingBox -> Maybe [Emotion] -> Maybe Pose -> Maybe Double -> Maybe ImageQuality -> Maybe Smile -> Maybe [Landmark] -> ComparedFace

-- | Bounding box of the face.
[$sel:boundingBox:ComparedFace'] :: ComparedFace -> Maybe BoundingBox

-- | The emotions that appear to be expressed on the face, and the
--   confidence level in the determination. Valid values include "Happy",
--   "Sad", "Angry", "Confused", "Disgusted", "Surprised", "Calm",
--   "Unknown", and "Fear".
[$sel:emotions:ComparedFace'] :: ComparedFace -> Maybe [Emotion]

-- | Indicates the pose of the face as determined by its pitch, roll, and
--   yaw.
[$sel:pose:ComparedFace'] :: ComparedFace -> Maybe Pose

-- | Level of confidence that what the bounding box contains is a face.
[$sel:confidence:ComparedFace'] :: ComparedFace -> Maybe Double

-- | Identifies face image brightness and sharpness.
[$sel:quality:ComparedFace'] :: ComparedFace -> Maybe ImageQuality

-- | Indicates whether or not the face is smiling, and the confidence level
--   in the determination.
[$sel:smile:ComparedFace'] :: ComparedFace -> Maybe Smile

-- | An array of facial landmarks.
[$sel:landmarks:ComparedFace'] :: ComparedFace -> Maybe [Landmark]

-- | Create a value of <a>ComparedFace</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:boundingBox:ComparedFace'</a>, <a>comparedFace_boundingBox</a>
--   - Bounding box of the face.
--   
--   <a>$sel:emotions:ComparedFace'</a>, <a>comparedFace_emotions</a> - The
--   emotions that appear to be expressed on the face, and the confidence
--   level in the determination. Valid values include "Happy", "Sad",
--   "Angry", "Confused", "Disgusted", "Surprised", "Calm", "Unknown", and
--   "Fear".
--   
--   <a>$sel:pose:ComparedFace'</a>, <a>comparedFace_pose</a> - Indicates
--   the pose of the face as determined by its pitch, roll, and yaw.
--   
--   <a>$sel:confidence:ComparedFace'</a>, <a>comparedFace_confidence</a> -
--   Level of confidence that what the bounding box contains is a face.
--   
--   <a>$sel:quality:ComparedFace'</a>, <a>comparedFace_quality</a> -
--   Identifies face image brightness and sharpness.
--   
--   <a>$sel:smile:ComparedFace'</a>, <a>comparedFace_smile</a> - Indicates
--   whether or not the face is smiling, and the confidence level in the
--   determination.
--   
--   <a>$sel:landmarks:ComparedFace'</a>, <a>comparedFace_landmarks</a> -
--   An array of facial landmarks.
newComparedFace :: ComparedFace

-- | Bounding box of the face.
comparedFace_boundingBox :: Lens' ComparedFace (Maybe BoundingBox)

-- | The emotions that appear to be expressed on the face, and the
--   confidence level in the determination. Valid values include "Happy",
--   "Sad", "Angry", "Confused", "Disgusted", "Surprised", "Calm",
--   "Unknown", and "Fear".
comparedFace_emotions :: Lens' ComparedFace (Maybe [Emotion])

-- | Indicates the pose of the face as determined by its pitch, roll, and
--   yaw.
comparedFace_pose :: Lens' ComparedFace (Maybe Pose)

-- | Level of confidence that what the bounding box contains is a face.
comparedFace_confidence :: Lens' ComparedFace (Maybe Double)

-- | Identifies face image brightness and sharpness.
comparedFace_quality :: Lens' ComparedFace (Maybe ImageQuality)

-- | Indicates whether or not the face is smiling, and the confidence level
--   in the determination.
comparedFace_smile :: Lens' ComparedFace (Maybe Smile)

-- | An array of facial landmarks.
comparedFace_landmarks :: Lens' ComparedFace (Maybe [Landmark])

-- | Type that describes the face Amazon Rekognition chose to compare with
--   the faces in the target. This contains a bounding box for the selected
--   face and confidence level that the bounding box contains a face. Note
--   that Amazon Rekognition selects the largest face in the source image
--   for this comparison.
--   
--   <i>See:</i> <a>newComparedSourceImageFace</a> smart constructor.
data ComparedSourceImageFace
ComparedSourceImageFace' :: Maybe BoundingBox -> Maybe Double -> ComparedSourceImageFace

-- | Bounding box of the face.
[$sel:boundingBox:ComparedSourceImageFace'] :: ComparedSourceImageFace -> Maybe BoundingBox

-- | Confidence level that the selected bounding box contains a face.
[$sel:confidence:ComparedSourceImageFace'] :: ComparedSourceImageFace -> Maybe Double

-- | Create a value of <a>ComparedSourceImageFace</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:boundingBox:ComparedSourceImageFace'</a>,
--   <a>comparedSourceImageFace_boundingBox</a> - Bounding box of the face.
--   
--   <a>$sel:confidence:ComparedSourceImageFace'</a>,
--   <a>comparedSourceImageFace_confidence</a> - Confidence level that the
--   selected bounding box contains a face.
newComparedSourceImageFace :: ComparedSourceImageFace

-- | Bounding box of the face.
comparedSourceImageFace_boundingBox :: Lens' ComparedSourceImageFace (Maybe BoundingBox)

-- | Confidence level that the selected bounding box contains a face.
comparedSourceImageFace_confidence :: Lens' ComparedSourceImageFace (Maybe Double)

-- | Information about an inappropriate, unwanted, or offensive content
--   label detection in a stored video.
--   
--   <i>See:</i> <a>newContentModerationDetection</a> smart constructor.
data ContentModerationDetection
ContentModerationDetection' :: Maybe ModerationLabel -> Maybe Integer -> ContentModerationDetection

-- | The content moderation label detected by in the stored video.
[$sel:moderationLabel:ContentModerationDetection'] :: ContentModerationDetection -> Maybe ModerationLabel

-- | Time, in milliseconds from the beginning of the video, that the
--   content moderation label was detected.
[$sel:timestamp:ContentModerationDetection'] :: ContentModerationDetection -> Maybe Integer

-- | Create a value of <a>ContentModerationDetection</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:moderationLabel:ContentModerationDetection'</a>,
--   <a>contentModerationDetection_moderationLabel</a> - The content
--   moderation label detected by in the stored video.
--   
--   <a>$sel:timestamp:ContentModerationDetection'</a>,
--   <a>contentModerationDetection_timestamp</a> - Time, in milliseconds
--   from the beginning of the video, that the content moderation label was
--   detected.
newContentModerationDetection :: ContentModerationDetection

-- | The content moderation label detected by in the stored video.
contentModerationDetection_moderationLabel :: Lens' ContentModerationDetection (Maybe ModerationLabel)

-- | Time, in milliseconds from the beginning of the video, that the
--   content moderation label was detected.
contentModerationDetection_timestamp :: Lens' ContentModerationDetection (Maybe Integer)

-- | Information about an item of Personal Protective Equipment covering a
--   corresponding body part. For more information, see
--   DetectProtectiveEquipment.
--   
--   <i>See:</i> <a>newCoversBodyPart</a> smart constructor.
data CoversBodyPart
CoversBodyPart' :: Maybe Bool -> Maybe Double -> CoversBodyPart

-- | True if the PPE covers the corresponding body part, otherwise false.
[$sel:value:CoversBodyPart'] :: CoversBodyPart -> Maybe Bool

-- | The confidence that Amazon Rekognition has in the value of
--   <tt>Value</tt>.
[$sel:confidence:CoversBodyPart'] :: CoversBodyPart -> Maybe Double

-- | Create a value of <a>CoversBodyPart</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:value:CoversBodyPart'</a>, <a>coversBodyPart_value</a> - True
--   if the PPE covers the corresponding body part, otherwise false.
--   
--   <a>$sel:confidence:CoversBodyPart'</a>,
--   <a>coversBodyPart_confidence</a> - The confidence that Amazon
--   Rekognition has in the value of <tt>Value</tt>.
newCoversBodyPart :: CoversBodyPart

-- | True if the PPE covers the corresponding body part, otherwise false.
coversBodyPart_value :: Lens' CoversBodyPart (Maybe Bool)

-- | The confidence that Amazon Rekognition has in the value of
--   <tt>Value</tt>.
coversBodyPart_confidence :: Lens' CoversBodyPart (Maybe Double)

-- | A custom label detected in an image by a call to DetectCustomLabels.
--   
--   <i>See:</i> <a>newCustomLabel</a> smart constructor.
data CustomLabel
CustomLabel' :: Maybe Double -> Maybe Text -> Maybe Geometry -> CustomLabel

-- | The confidence that the model has in the detection of the custom
--   label. The range is 0-100. A higher value indicates a higher
--   confidence.
[$sel:confidence:CustomLabel'] :: CustomLabel -> Maybe Double

-- | The name of the custom label.
[$sel:name:CustomLabel'] :: CustomLabel -> Maybe Text

-- | The location of the detected object on the image that corresponds to
--   the custom label. Includes an axis aligned coarse bounding box
--   surrounding the object and a finer grain polygon for more accurate
--   spatial information.
[$sel:geometry:CustomLabel'] :: CustomLabel -> Maybe Geometry

-- | Create a value of <a>CustomLabel</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:confidence:CustomLabel'</a>, <a>customLabel_confidence</a> -
--   The confidence that the model has in the detection of the custom
--   label. The range is 0-100. A higher value indicates a higher
--   confidence.
--   
--   <a>$sel:name:CustomLabel'</a>, <a>customLabel_name</a> - The name of
--   the custom label.
--   
--   <a>$sel:geometry:CustomLabel'</a>, <a>customLabel_geometry</a> - The
--   location of the detected object on the image that corresponds to the
--   custom label. Includes an axis aligned coarse bounding box surrounding
--   the object and a finer grain polygon for more accurate spatial
--   information.
newCustomLabel :: CustomLabel

-- | The confidence that the model has in the detection of the custom
--   label. The range is 0-100. A higher value indicates a higher
--   confidence.
customLabel_confidence :: Lens' CustomLabel (Maybe Double)

-- | The name of the custom label.
customLabel_name :: Lens' CustomLabel (Maybe Text)

-- | The location of the detected object on the image that corresponds to
--   the custom label. Includes an axis aligned coarse bounding box
--   surrounding the object and a finer grain polygon for more accurate
--   spatial information.
customLabel_geometry :: Lens' CustomLabel (Maybe Geometry)

-- | A set of optional parameters that you can use to set the criteria that
--   the text must meet to be included in your response.
--   <tt>WordFilter</tt> looks at a word’s height, width, and minimum
--   confidence. <tt>RegionOfInterest</tt> lets you set a specific region
--   of the image to look for text in.
--   
--   <i>See:</i> <a>newDetectTextFilters</a> smart constructor.
data DetectTextFilters
DetectTextFilters' :: Maybe [RegionOfInterest] -> Maybe DetectionFilter -> DetectTextFilters

-- | A Filter focusing on a certain area of the image. Uses a
--   <tt>BoundingBox</tt> object to set the region of the image.
[$sel:regionsOfInterest:DetectTextFilters'] :: DetectTextFilters -> Maybe [RegionOfInterest]
[$sel:wordFilter:DetectTextFilters'] :: DetectTextFilters -> Maybe DetectionFilter

-- | Create a value of <a>DetectTextFilters</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:regionsOfInterest:DetectTextFilters'</a>,
--   <a>detectTextFilters_regionsOfInterest</a> - A Filter focusing on a
--   certain area of the image. Uses a <tt>BoundingBox</tt> object to set
--   the region of the image.
--   
--   <a>$sel:wordFilter:DetectTextFilters'</a>,
--   <a>detectTextFilters_wordFilter</a> - Undocumented member.
newDetectTextFilters :: DetectTextFilters

-- | A Filter focusing on a certain area of the image. Uses a
--   <tt>BoundingBox</tt> object to set the region of the image.
detectTextFilters_regionsOfInterest :: Lens' DetectTextFilters (Maybe [RegionOfInterest])

-- | Undocumented member.
detectTextFilters_wordFilter :: Lens' DetectTextFilters (Maybe DetectionFilter)

-- | A set of parameters that allow you to filter out certain results from
--   your returned results.
--   
--   <i>See:</i> <a>newDetectionFilter</a> smart constructor.
data DetectionFilter
DetectionFilter' :: Maybe Double -> Maybe Double -> Maybe Double -> DetectionFilter

-- | Sets the minimum height of the word bounding box. Words with bounding
--   box heights lesser than this value will be excluded from the result.
--   Value is relative to the video frame height.
[$sel:minBoundingBoxHeight:DetectionFilter'] :: DetectionFilter -> Maybe Double

-- | Sets the minimum width of the word bounding box. Words with bounding
--   boxes widths lesser than this value will be excluded from the result.
--   Value is relative to the video frame width.
[$sel:minBoundingBoxWidth:DetectionFilter'] :: DetectionFilter -> Maybe Double

-- | Sets the confidence of word detection. Words with detection confidence
--   below this will be excluded from the result. Values should be between
--   50 and 100 as Text in Video will not return any result below 50.
[$sel:minConfidence:DetectionFilter'] :: DetectionFilter -> Maybe Double

-- | Create a value of <a>DetectionFilter</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:minBoundingBoxHeight:DetectionFilter'</a>,
--   <a>detectionFilter_minBoundingBoxHeight</a> - Sets the minimum height
--   of the word bounding box. Words with bounding box heights lesser than
--   this value will be excluded from the result. Value is relative to the
--   video frame height.
--   
--   <a>$sel:minBoundingBoxWidth:DetectionFilter'</a>,
--   <a>detectionFilter_minBoundingBoxWidth</a> - Sets the minimum width of
--   the word bounding box. Words with bounding boxes widths lesser than
--   this value will be excluded from the result. Value is relative to the
--   video frame width.
--   
--   <a>$sel:minConfidence:DetectionFilter'</a>,
--   <a>detectionFilter_minConfidence</a> - Sets the confidence of word
--   detection. Words with detection confidence below this will be excluded
--   from the result. Values should be between 50 and 100 as Text in Video
--   will not return any result below 50.
newDetectionFilter :: DetectionFilter

-- | Sets the minimum height of the word bounding box. Words with bounding
--   box heights lesser than this value will be excluded from the result.
--   Value is relative to the video frame height.
detectionFilter_minBoundingBoxHeight :: Lens' DetectionFilter (Maybe Double)

-- | Sets the minimum width of the word bounding box. Words with bounding
--   boxes widths lesser than this value will be excluded from the result.
--   Value is relative to the video frame width.
detectionFilter_minBoundingBoxWidth :: Lens' DetectionFilter (Maybe Double)

-- | Sets the confidence of word detection. Words with detection confidence
--   below this will be excluded from the result. Values should be between
--   50 and 100 as Text in Video will not return any result below 50.
detectionFilter_minConfidence :: Lens' DetectionFilter (Maybe Double)

-- | The emotions that appear to be expressed on the face, and the
--   confidence level in the determination. The API is only making a
--   determination of the physical appearance of a person's face. It is not
--   a determination of the person’s internal emotional state and should
--   not be used in such a way. For example, a person pretending to have a
--   sad face might not be sad emotionally.
--   
--   <i>See:</i> <a>newEmotion</a> smart constructor.
data Emotion
Emotion' :: Maybe Double -> Maybe EmotionName -> Emotion

-- | Level of confidence in the determination.
[$sel:confidence:Emotion'] :: Emotion -> Maybe Double

-- | Type of emotion detected.
[$sel:type':Emotion'] :: Emotion -> Maybe EmotionName

-- | Create a value of <a>Emotion</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:confidence:Emotion'</a>, <a>emotion_confidence</a> - Level of
--   confidence in the determination.
--   
--   <a>$sel:type':Emotion'</a>, <a>emotion_type</a> - Type of emotion
--   detected.
newEmotion :: Emotion

-- | Level of confidence in the determination.
emotion_confidence :: Lens' Emotion (Maybe Double)

-- | Type of emotion detected.
emotion_type :: Lens' Emotion (Maybe EmotionName)

-- | Information about an item of Personal Protective Equipment (PPE)
--   detected by DetectProtectiveEquipment. For more information, see
--   DetectProtectiveEquipment.
--   
--   <i>See:</i> <a>newEquipmentDetection</a> smart constructor.
data EquipmentDetection
EquipmentDetection' :: Maybe BoundingBox -> Maybe CoversBodyPart -> Maybe Double -> Maybe ProtectiveEquipmentType -> EquipmentDetection

-- | A bounding box surrounding the item of detected PPE.
[$sel:boundingBox:EquipmentDetection'] :: EquipmentDetection -> Maybe BoundingBox

-- | Information about the body part covered by the detected PPE.
[$sel:coversBodyPart:EquipmentDetection'] :: EquipmentDetection -> Maybe CoversBodyPart

-- | The confidence that Amazon Rekognition has that the bounding box
--   (<tt>BoundingBox</tt>) contains an item of PPE.
[$sel:confidence:EquipmentDetection'] :: EquipmentDetection -> Maybe Double

-- | The type of detected PPE.
[$sel:type':EquipmentDetection'] :: EquipmentDetection -> Maybe ProtectiveEquipmentType

-- | Create a value of <a>EquipmentDetection</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:boundingBox:EquipmentDetection'</a>,
--   <a>equipmentDetection_boundingBox</a> - A bounding box surrounding the
--   item of detected PPE.
--   
--   <a>$sel:coversBodyPart:EquipmentDetection'</a>,
--   <a>equipmentDetection_coversBodyPart</a> - Information about the body
--   part covered by the detected PPE.
--   
--   <a>$sel:confidence:EquipmentDetection'</a>,
--   <a>equipmentDetection_confidence</a> - The confidence that Amazon
--   Rekognition has that the bounding box (<tt>BoundingBox</tt>) contains
--   an item of PPE.
--   
--   <a>$sel:type':EquipmentDetection'</a>, <a>equipmentDetection_type</a>
--   - The type of detected PPE.
newEquipmentDetection :: EquipmentDetection

-- | A bounding box surrounding the item of detected PPE.
equipmentDetection_boundingBox :: Lens' EquipmentDetection (Maybe BoundingBox)

-- | Information about the body part covered by the detected PPE.
equipmentDetection_coversBodyPart :: Lens' EquipmentDetection (Maybe CoversBodyPart)

-- | The confidence that Amazon Rekognition has that the bounding box
--   (<tt>BoundingBox</tt>) contains an item of PPE.
equipmentDetection_confidence :: Lens' EquipmentDetection (Maybe Double)

-- | The type of detected PPE.
equipmentDetection_type :: Lens' EquipmentDetection (Maybe ProtectiveEquipmentType)

-- | The evaluation results for the training of a model.
--   
--   <i>See:</i> <a>newEvaluationResult</a> smart constructor.
data EvaluationResult
EvaluationResult' :: Maybe Summary -> Maybe Double -> EvaluationResult

-- | The S3 bucket that contains the training summary.
[$sel:summary:EvaluationResult'] :: EvaluationResult -> Maybe Summary

-- | The F1 score for the evaluation of all labels. The F1 score metric
--   evaluates the overall precision and recall performance of the model as
--   a single value. A higher value indicates better precision and recall
--   performance. A lower score indicates that precision, recall, or both
--   are performing poorly.
[$sel:f1Score:EvaluationResult'] :: EvaluationResult -> Maybe Double

-- | Create a value of <a>EvaluationResult</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:summary:EvaluationResult'</a>, <a>evaluationResult_summary</a>
--   - The S3 bucket that contains the training summary.
--   
--   <a>$sel:f1Score:EvaluationResult'</a>, <a>evaluationResult_f1Score</a>
--   - The F1 score for the evaluation of all labels. The F1 score metric
--   evaluates the overall precision and recall performance of the model as
--   a single value. A higher value indicates better precision and recall
--   performance. A lower score indicates that precision, recall, or both
--   are performing poorly.
newEvaluationResult :: EvaluationResult

-- | The S3 bucket that contains the training summary.
evaluationResult_summary :: Lens' EvaluationResult (Maybe Summary)

-- | The F1 score for the evaluation of all labels. The F1 score metric
--   evaluates the overall precision and recall performance of the model as
--   a single value. A higher value indicates better precision and recall
--   performance. A lower score indicates that precision, recall, or both
--   are performing poorly.
evaluationResult_f1Score :: Lens' EvaluationResult (Maybe Double)

-- | Indicates whether or not the eyes on the face are open, and the
--   confidence level in the determination.
--   
--   <i>See:</i> <a>newEyeOpen</a> smart constructor.
data EyeOpen
EyeOpen' :: Maybe Bool -> Maybe Double -> EyeOpen

-- | Boolean value that indicates whether the eyes on the face are open.
[$sel:value:EyeOpen'] :: EyeOpen -> Maybe Bool

-- | Level of confidence in the determination.
[$sel:confidence:EyeOpen'] :: EyeOpen -> Maybe Double

-- | Create a value of <a>EyeOpen</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:value:EyeOpen'</a>, <a>eyeOpen_value</a> - Boolean value that
--   indicates whether the eyes on the face are open.
--   
--   <a>$sel:confidence:EyeOpen'</a>, <a>eyeOpen_confidence</a> - Level of
--   confidence in the determination.
newEyeOpen :: EyeOpen

-- | Boolean value that indicates whether the eyes on the face are open.
eyeOpen_value :: Lens' EyeOpen (Maybe Bool)

-- | Level of confidence in the determination.
eyeOpen_confidence :: Lens' EyeOpen (Maybe Double)

-- | Indicates whether or not the face is wearing eye glasses, and the
--   confidence level in the determination.
--   
--   <i>See:</i> <a>newEyeglasses</a> smart constructor.
data Eyeglasses
Eyeglasses' :: Maybe Bool -> Maybe Double -> Eyeglasses

-- | Boolean value that indicates whether the face is wearing eye glasses
--   or not.
[$sel:value:Eyeglasses'] :: Eyeglasses -> Maybe Bool

-- | Level of confidence in the determination.
[$sel:confidence:Eyeglasses'] :: Eyeglasses -> Maybe Double

-- | Create a value of <a>Eyeglasses</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:value:Eyeglasses'</a>, <a>eyeglasses_value</a> - Boolean value
--   that indicates whether the face is wearing eye glasses or not.
--   
--   <a>$sel:confidence:Eyeglasses'</a>, <a>eyeglasses_confidence</a> -
--   Level of confidence in the determination.
newEyeglasses :: Eyeglasses

-- | Boolean value that indicates whether the face is wearing eye glasses
--   or not.
eyeglasses_value :: Lens' Eyeglasses (Maybe Bool)

-- | Level of confidence in the determination.
eyeglasses_confidence :: Lens' Eyeglasses (Maybe Double)

-- | Describes the face properties such as the bounding box, face ID, image
--   ID of the input image, and external image ID that you assigned.
--   
--   <i>See:</i> <a>newFace</a> smart constructor.
data Face
Face' :: Maybe Text -> Maybe BoundingBox -> Maybe Text -> Maybe Double -> Maybe Text -> Face

-- | Unique identifier that Amazon Rekognition assigns to the face.
[$sel:faceId:Face'] :: Face -> Maybe Text

-- | Bounding box of the face.
[$sel:boundingBox:Face'] :: Face -> Maybe BoundingBox

-- | Identifier that you assign to all the faces in the input image.
[$sel:externalImageId:Face'] :: Face -> Maybe Text

-- | Confidence level that the bounding box contains a face (and not a
--   different object such as a tree).
[$sel:confidence:Face'] :: Face -> Maybe Double

-- | Unique identifier that Amazon Rekognition assigns to the input image.
[$sel:imageId:Face'] :: Face -> Maybe Text

-- | Create a value of <a>Face</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:faceId:Face'</a>, <a>face_faceId</a> - Unique identifier that
--   Amazon Rekognition assigns to the face.
--   
--   <a>$sel:boundingBox:Face'</a>, <a>face_boundingBox</a> - Bounding box
--   of the face.
--   
--   <a>$sel:externalImageId:Face'</a>, <a>face_externalImageId</a> -
--   Identifier that you assign to all the faces in the input image.
--   
--   <a>$sel:confidence:Face'</a>, <a>face_confidence</a> - Confidence
--   level that the bounding box contains a face (and not a different
--   object such as a tree).
--   
--   <a>$sel:imageId:Face'</a>, <a>face_imageId</a> - Unique identifier
--   that Amazon Rekognition assigns to the input image.
newFace :: Face

-- | Unique identifier that Amazon Rekognition assigns to the face.
face_faceId :: Lens' Face (Maybe Text)

-- | Bounding box of the face.
face_boundingBox :: Lens' Face (Maybe BoundingBox)

-- | Identifier that you assign to all the faces in the input image.
face_externalImageId :: Lens' Face (Maybe Text)

-- | Confidence level that the bounding box contains a face (and not a
--   different object such as a tree).
face_confidence :: Lens' Face (Maybe Double)

-- | Unique identifier that Amazon Rekognition assigns to the input image.
face_imageId :: Lens' Face (Maybe Text)

-- | Structure containing attributes of the face that the algorithm
--   detected.
--   
--   A <tt>FaceDetail</tt> object contains either the default facial
--   attributes or all facial attributes. The default attributes are
--   <tt>BoundingBox</tt>, <tt>Confidence</tt>, <tt>Landmarks</tt>,
--   <tt>Pose</tt>, and <tt>Quality</tt>.
--   
--   GetFaceDetection is the only Amazon Rekognition Video stored video
--   operation that can return a <tt>FaceDetail</tt> object with all
--   attributes. To specify which attributes to return, use the
--   <tt>FaceAttributes</tt> input parameter for StartFaceDetection. The
--   following Amazon Rekognition Video operations return only the default
--   attributes. The corresponding Start operations don't have a
--   <tt>FaceAttributes</tt> input parameter.
--   
--   <ul>
--   <li>GetCelebrityRecognition</li>
--   <li>GetPersonTracking</li>
--   <li>GetFaceSearch</li>
--   </ul>
--   
--   The Amazon Rekognition Image DetectFaces and IndexFaces operations can
--   return all facial attributes. To specify which attributes to return,
--   use the <tt>Attributes</tt> input parameter for <tt>DetectFaces</tt>.
--   For <tt>IndexFaces</tt>, use the <tt>DetectAttributes</tt> input
--   parameter.
--   
--   <i>See:</i> <a>newFaceDetail</a> smart constructor.
data FaceDetail
FaceDetail' :: Maybe AgeRange -> Maybe Sunglasses -> Maybe MouthOpen -> Maybe BoundingBox -> Maybe [Emotion] -> Maybe EyeOpen -> Maybe Pose -> Maybe Double -> Maybe Gender -> Maybe ImageQuality -> Maybe Eyeglasses -> Maybe Beard -> Maybe Mustache -> Maybe Smile -> Maybe [Landmark] -> FaceDetail

-- | The estimated age range, in years, for the face. Low represents the
--   lowest estimated age and High represents the highest estimated age.
[$sel:ageRange:FaceDetail'] :: FaceDetail -> Maybe AgeRange

-- | Indicates whether or not the face is wearing sunglasses, and the
--   confidence level in the determination.
[$sel:sunglasses:FaceDetail'] :: FaceDetail -> Maybe Sunglasses

-- | Indicates whether or not the mouth on the face is open, and the
--   confidence level in the determination.
[$sel:mouthOpen:FaceDetail'] :: FaceDetail -> Maybe MouthOpen

-- | Bounding box of the face. Default attribute.
[$sel:boundingBox:FaceDetail'] :: FaceDetail -> Maybe BoundingBox

-- | The emotions that appear to be expressed on the face, and the
--   confidence level in the determination. The API is only making a
--   determination of the physical appearance of a person's face. It is not
--   a determination of the person’s internal emotional state and should
--   not be used in such a way. For example, a person pretending to have a
--   sad face might not be sad emotionally.
[$sel:emotions:FaceDetail'] :: FaceDetail -> Maybe [Emotion]

-- | Indicates whether or not the eyes on the face are open, and the
--   confidence level in the determination.
[$sel:eyesOpen:FaceDetail'] :: FaceDetail -> Maybe EyeOpen

-- | Indicates the pose of the face as determined by its pitch, roll, and
--   yaw. Default attribute.
[$sel:pose:FaceDetail'] :: FaceDetail -> Maybe Pose

-- | Confidence level that the bounding box contains a face (and not a
--   different object such as a tree). Default attribute.
[$sel:confidence:FaceDetail'] :: FaceDetail -> Maybe Double

-- | The predicted gender of a detected face.
[$sel:gender:FaceDetail'] :: FaceDetail -> Maybe Gender

-- | Identifies image brightness and sharpness. Default attribute.
[$sel:quality:FaceDetail'] :: FaceDetail -> Maybe ImageQuality

-- | Indicates whether or not the face is wearing eye glasses, and the
--   confidence level in the determination.
[$sel:eyeglasses:FaceDetail'] :: FaceDetail -> Maybe Eyeglasses

-- | Indicates whether or not the face has a beard, and the confidence
--   level in the determination.
[$sel:beard:FaceDetail'] :: FaceDetail -> Maybe Beard

-- | Indicates whether or not the face has a mustache, and the confidence
--   level in the determination.
[$sel:mustache:FaceDetail'] :: FaceDetail -> Maybe Mustache

-- | Indicates whether or not the face is smiling, and the confidence level
--   in the determination.
[$sel:smile:FaceDetail'] :: FaceDetail -> Maybe Smile

-- | Indicates the location of landmarks on the face. Default attribute.
[$sel:landmarks:FaceDetail'] :: FaceDetail -> Maybe [Landmark]

-- | Create a value of <a>FaceDetail</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:ageRange:FaceDetail'</a>, <a>faceDetail_ageRange</a> - The
--   estimated age range, in years, for the face. Low represents the lowest
--   estimated age and High represents the highest estimated age.
--   
--   <a>$sel:sunglasses:FaceDetail'</a>, <a>faceDetail_sunglasses</a> -
--   Indicates whether or not the face is wearing sunglasses, and the
--   confidence level in the determination.
--   
--   <a>$sel:mouthOpen:FaceDetail'</a>, <a>faceDetail_mouthOpen</a> -
--   Indicates whether or not the mouth on the face is open, and the
--   confidence level in the determination.
--   
--   <a>$sel:boundingBox:FaceDetail'</a>, <a>faceDetail_boundingBox</a> -
--   Bounding box of the face. Default attribute.
--   
--   <a>$sel:emotions:FaceDetail'</a>, <a>faceDetail_emotions</a> - The
--   emotions that appear to be expressed on the face, and the confidence
--   level in the determination. The API is only making a determination of
--   the physical appearance of a person's face. It is not a determination
--   of the person’s internal emotional state and should not be used in
--   such a way. For example, a person pretending to have a sad face might
--   not be sad emotionally.
--   
--   <a>$sel:eyesOpen:FaceDetail'</a>, <a>faceDetail_eyesOpen</a> -
--   Indicates whether or not the eyes on the face are open, and the
--   confidence level in the determination.
--   
--   <a>$sel:pose:FaceDetail'</a>, <a>faceDetail_pose</a> - Indicates the
--   pose of the face as determined by its pitch, roll, and yaw. Default
--   attribute.
--   
--   <a>$sel:confidence:FaceDetail'</a>, <a>faceDetail_confidence</a> -
--   Confidence level that the bounding box contains a face (and not a
--   different object such as a tree). Default attribute.
--   
--   <a>$sel:gender:FaceDetail'</a>, <a>faceDetail_gender</a> - The
--   predicted gender of a detected face.
--   
--   <a>$sel:quality:FaceDetail'</a>, <a>faceDetail_quality</a> -
--   Identifies image brightness and sharpness. Default attribute.
--   
--   <a>$sel:eyeglasses:FaceDetail'</a>, <a>faceDetail_eyeglasses</a> -
--   Indicates whether or not the face is wearing eye glasses, and the
--   confidence level in the determination.
--   
--   <a>$sel:beard:FaceDetail'</a>, <a>faceDetail_beard</a> - Indicates
--   whether or not the face has a beard, and the confidence level in the
--   determination.
--   
--   <a>$sel:mustache:FaceDetail'</a>, <a>faceDetail_mustache</a> -
--   Indicates whether or not the face has a mustache, and the confidence
--   level in the determination.
--   
--   <a>$sel:smile:FaceDetail'</a>, <a>faceDetail_smile</a> - Indicates
--   whether or not the face is smiling, and the confidence level in the
--   determination.
--   
--   <a>$sel:landmarks:FaceDetail'</a>, <a>faceDetail_landmarks</a> -
--   Indicates the location of landmarks on the face. Default attribute.
newFaceDetail :: FaceDetail

-- | The estimated age range, in years, for the face. Low represents the
--   lowest estimated age and High represents the highest estimated age.
faceDetail_ageRange :: Lens' FaceDetail (Maybe AgeRange)

-- | Indicates whether or not the face is wearing sunglasses, and the
--   confidence level in the determination.
faceDetail_sunglasses :: Lens' FaceDetail (Maybe Sunglasses)

-- | Indicates whether or not the mouth on the face is open, and the
--   confidence level in the determination.
faceDetail_mouthOpen :: Lens' FaceDetail (Maybe MouthOpen)

-- | Bounding box of the face. Default attribute.
faceDetail_boundingBox :: Lens' FaceDetail (Maybe BoundingBox)

-- | The emotions that appear to be expressed on the face, and the
--   confidence level in the determination. The API is only making a
--   determination of the physical appearance of a person's face. It is not
--   a determination of the person’s internal emotional state and should
--   not be used in such a way. For example, a person pretending to have a
--   sad face might not be sad emotionally.
faceDetail_emotions :: Lens' FaceDetail (Maybe [Emotion])

-- | Indicates whether or not the eyes on the face are open, and the
--   confidence level in the determination.
faceDetail_eyesOpen :: Lens' FaceDetail (Maybe EyeOpen)

-- | Indicates the pose of the face as determined by its pitch, roll, and
--   yaw. Default attribute.
faceDetail_pose :: Lens' FaceDetail (Maybe Pose)

-- | Confidence level that the bounding box contains a face (and not a
--   different object such as a tree). Default attribute.
faceDetail_confidence :: Lens' FaceDetail (Maybe Double)

-- | The predicted gender of a detected face.
faceDetail_gender :: Lens' FaceDetail (Maybe Gender)

-- | Identifies image brightness and sharpness. Default attribute.
faceDetail_quality :: Lens' FaceDetail (Maybe ImageQuality)

-- | Indicates whether or not the face is wearing eye glasses, and the
--   confidence level in the determination.
faceDetail_eyeglasses :: Lens' FaceDetail (Maybe Eyeglasses)

-- | Indicates whether or not the face has a beard, and the confidence
--   level in the determination.
faceDetail_beard :: Lens' FaceDetail (Maybe Beard)

-- | Indicates whether or not the face has a mustache, and the confidence
--   level in the determination.
faceDetail_mustache :: Lens' FaceDetail (Maybe Mustache)

-- | Indicates whether or not the face is smiling, and the confidence level
--   in the determination.
faceDetail_smile :: Lens' FaceDetail (Maybe Smile)

-- | Indicates the location of landmarks on the face. Default attribute.
faceDetail_landmarks :: Lens' FaceDetail (Maybe [Landmark])

-- | Information about a face detected in a video analysis request and the
--   time the face was detected in the video.
--   
--   <i>See:</i> <a>newFaceDetection</a> smart constructor.
data FaceDetection
FaceDetection' :: Maybe Integer -> Maybe FaceDetail -> FaceDetection

-- | Time, in milliseconds from the start of the video, that the face was
--   detected.
[$sel:timestamp:FaceDetection'] :: FaceDetection -> Maybe Integer

-- | The face properties for the detected face.
[$sel:face:FaceDetection'] :: FaceDetection -> Maybe FaceDetail

-- | Create a value of <a>FaceDetection</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:timestamp:FaceDetection'</a>, <a>faceDetection_timestamp</a> -
--   Time, in milliseconds from the start of the video, that the face was
--   detected.
--   
--   <a>$sel:face:FaceDetection'</a>, <a>faceDetection_face</a> - The face
--   properties for the detected face.
newFaceDetection :: FaceDetection

-- | Time, in milliseconds from the start of the video, that the face was
--   detected.
faceDetection_timestamp :: Lens' FaceDetection (Maybe Integer)

-- | The face properties for the detected face.
faceDetection_face :: Lens' FaceDetection (Maybe FaceDetail)

-- | Provides face metadata. In addition, it also provides the confidence
--   in the match of this face with the input face.
--   
--   <i>See:</i> <a>newFaceMatch</a> smart constructor.
data FaceMatch
FaceMatch' :: Maybe Double -> Maybe Face -> FaceMatch

-- | Confidence in the match of this face with the input face.
[$sel:similarity:FaceMatch'] :: FaceMatch -> Maybe Double

-- | Describes the face properties such as the bounding box, face ID, image
--   ID of the source image, and external image ID that you assigned.
[$sel:face:FaceMatch'] :: FaceMatch -> Maybe Face

-- | Create a value of <a>FaceMatch</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:similarity:FaceMatch'</a>, <a>faceMatch_similarity</a> -
--   Confidence in the match of this face with the input face.
--   
--   <a>$sel:face:FaceMatch'</a>, <a>faceMatch_face</a> - Describes the
--   face properties such as the bounding box, face ID, image ID of the
--   source image, and external image ID that you assigned.
newFaceMatch :: FaceMatch

-- | Confidence in the match of this face with the input face.
faceMatch_similarity :: Lens' FaceMatch (Maybe Double)

-- | Describes the face properties such as the bounding box, face ID, image
--   ID of the source image, and external image ID that you assigned.
faceMatch_face :: Lens' FaceMatch (Maybe Face)

-- | Object containing both the face metadata (stored in the backend
--   database), and facial attributes that are detected but aren't stored
--   in the database.
--   
--   <i>See:</i> <a>newFaceRecord</a> smart constructor.
data FaceRecord
FaceRecord' :: Maybe FaceDetail -> Maybe Face -> FaceRecord

-- | Structure containing attributes of the face that the algorithm
--   detected.
[$sel:faceDetail:FaceRecord'] :: FaceRecord -> Maybe FaceDetail

-- | Describes the face properties such as the bounding box, face ID, image
--   ID of the input image, and external image ID that you assigned.
[$sel:face:FaceRecord'] :: FaceRecord -> Maybe Face

-- | Create a value of <a>FaceRecord</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:faceDetail:FaceRecord'</a>, <a>faceRecord_faceDetail</a> -
--   Structure containing attributes of the face that the algorithm
--   detected.
--   
--   <a>$sel:face:FaceRecord'</a>, <a>faceRecord_face</a> - Describes the
--   face properties such as the bounding box, face ID, image ID of the
--   input image, and external image ID that you assigned.
newFaceRecord :: FaceRecord

-- | Structure containing attributes of the face that the algorithm
--   detected.
faceRecord_faceDetail :: Lens' FaceRecord (Maybe FaceDetail)

-- | Describes the face properties such as the bounding box, face ID, image
--   ID of the input image, and external image ID that you assigned.
faceRecord_face :: Lens' FaceRecord (Maybe Face)

-- | Input face recognition parameters for an Amazon Rekognition stream
--   processor. <tt>FaceRecognitionSettings</tt> is a request parameter for
--   CreateStreamProcessor.
--   
--   <i>See:</i> <a>newFaceSearchSettings</a> smart constructor.
data FaceSearchSettings
FaceSearchSettings' :: Maybe Double -> Maybe Text -> FaceSearchSettings

-- | Minimum face match confidence score that must be met to return a
--   result for a recognized face. Default is 80. 0 is the lowest
--   confidence. 100 is the highest confidence.
[$sel:faceMatchThreshold:FaceSearchSettings'] :: FaceSearchSettings -> Maybe Double

-- | The ID of a collection that contains faces that you want to search
--   for.
[$sel:collectionId:FaceSearchSettings'] :: FaceSearchSettings -> Maybe Text

-- | Create a value of <a>FaceSearchSettings</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:faceMatchThreshold:FaceSearchSettings'</a>,
--   <a>faceSearchSettings_faceMatchThreshold</a> - Minimum face match
--   confidence score that must be met to return a result for a recognized
--   face. Default is 80. 0 is the lowest confidence. 100 is the highest
--   confidence.
--   
--   <a>$sel:collectionId:FaceSearchSettings'</a>,
--   <a>faceSearchSettings_collectionId</a> - The ID of a collection that
--   contains faces that you want to search for.
newFaceSearchSettings :: FaceSearchSettings

-- | Minimum face match confidence score that must be met to return a
--   result for a recognized face. Default is 80. 0 is the lowest
--   confidence. 100 is the highest confidence.
faceSearchSettings_faceMatchThreshold :: Lens' FaceSearchSettings (Maybe Double)

-- | The ID of a collection that contains faces that you want to search
--   for.
faceSearchSettings_collectionId :: Lens' FaceSearchSettings (Maybe Text)

-- | The predicted gender of a detected face.
--   
--   Amazon Rekognition makes gender binary (male/female) predictions based
--   on the physical appearance of a face in a particular image. This kind
--   of prediction is not designed to categorize a person’s gender
--   identity, and you shouldn't use Amazon Rekognition to make such a
--   determination. For example, a male actor wearing a long-haired wig and
--   earrings for a role might be predicted as female.
--   
--   Using Amazon Rekognition to make gender binary predictions is best
--   suited for use cases where aggregate gender distribution statistics
--   need to be analyzed without identifying specific users. For example,
--   the percentage of female users compared to male users on a social
--   media platform.
--   
--   We don't recommend using gender binary predictions to make decisions
--   that impact  an individual's rights, privacy, or access to services.
--   
--   <i>See:</i> <a>newGender</a> smart constructor.
data Gender
Gender' :: Maybe GenderType -> Maybe Double -> Gender

-- | The predicted gender of the face.
[$sel:value:Gender'] :: Gender -> Maybe GenderType

-- | Level of confidence in the prediction.
[$sel:confidence:Gender'] :: Gender -> Maybe Double

-- | Create a value of <a>Gender</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:value:Gender'</a>, <a>gender_value</a> - The predicted gender
--   of the face.
--   
--   <a>$sel:confidence:Gender'</a>, <a>gender_confidence</a> - Level of
--   confidence in the prediction.
newGender :: Gender

-- | The predicted gender of the face.
gender_value :: Lens' Gender (Maybe GenderType)

-- | Level of confidence in the prediction.
gender_confidence :: Lens' Gender (Maybe Double)

-- | Information about where an object (DetectCustomLabels) or text
--   (DetectText) is located on an image.
--   
--   <i>See:</i> <a>newGeometry</a> smart constructor.
data Geometry
Geometry' :: Maybe BoundingBox -> Maybe [Point] -> Geometry

-- | An axis-aligned coarse representation of the detected item's location
--   on the image.
[$sel:boundingBox:Geometry'] :: Geometry -> Maybe BoundingBox

-- | Within the bounding box, a fine-grained polygon around the detected
--   item.
[$sel:polygon:Geometry'] :: Geometry -> Maybe [Point]

-- | Create a value of <a>Geometry</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:boundingBox:Geometry'</a>, <a>geometry_boundingBox</a> - An
--   axis-aligned coarse representation of the detected item's location on
--   the image.
--   
--   <a>$sel:polygon:Geometry'</a>, <a>geometry_polygon</a> - Within the
--   bounding box, a fine-grained polygon around the detected item.
newGeometry :: Geometry

-- | An axis-aligned coarse representation of the detected item's location
--   on the image.
geometry_boundingBox :: Lens' Geometry (Maybe BoundingBox)

-- | Within the bounding box, a fine-grained polygon around the detected
--   item.
geometry_polygon :: Lens' Geometry (Maybe [Point])

-- | The S3 bucket that contains an Amazon Sagemaker Ground Truth format
--   manifest file.
--   
--   <i>See:</i> <a>newGroundTruthManifest</a> smart constructor.
data GroundTruthManifest
GroundTruthManifest' :: Maybe S3Object -> GroundTruthManifest
[$sel:s3Object:GroundTruthManifest'] :: GroundTruthManifest -> Maybe S3Object

-- | Create a value of <a>GroundTruthManifest</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:s3Object:GroundTruthManifest'</a>,
--   <a>groundTruthManifest_s3Object</a> - Undocumented member.
newGroundTruthManifest :: GroundTruthManifest

-- | Undocumented member.
groundTruthManifest_s3Object :: Lens' GroundTruthManifest (Maybe S3Object)

-- | Shows the results of the human in the loop evaluation. If there is no
--   HumanLoopArn, the input did not trigger human review.
--   
--   <i>See:</i> <a>newHumanLoopActivationOutput</a> smart constructor.
data HumanLoopActivationOutput
HumanLoopActivationOutput' :: Maybe (NonEmpty Text) -> Maybe Text -> Maybe Text -> HumanLoopActivationOutput

-- | Shows if and why human review was needed.
[$sel:humanLoopActivationReasons:HumanLoopActivationOutput'] :: HumanLoopActivationOutput -> Maybe (NonEmpty Text)

-- | The Amazon Resource Name (ARN) of the HumanLoop created.
[$sel:humanLoopArn:HumanLoopActivationOutput'] :: HumanLoopActivationOutput -> Maybe Text

-- | Shows the result of condition evaluations, including those conditions
--   which activated a human review.
[$sel:humanLoopActivationConditionsEvaluationResults:HumanLoopActivationOutput'] :: HumanLoopActivationOutput -> Maybe Text

-- | Create a value of <a>HumanLoopActivationOutput</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:humanLoopActivationReasons:HumanLoopActivationOutput'</a>,
--   <a>humanLoopActivationOutput_humanLoopActivationReasons</a> - Shows if
--   and why human review was needed.
--   
--   <a>$sel:humanLoopArn:HumanLoopActivationOutput'</a>,
--   <a>humanLoopActivationOutput_humanLoopArn</a> - The Amazon Resource
--   Name (ARN) of the HumanLoop created.
--   
--   
--   <a>$sel:humanLoopActivationConditionsEvaluationResults:HumanLoopActivationOutput'</a>,
--   <a>humanLoopActivationOutput_humanLoopActivationConditionsEvaluationResults</a>
--   - Shows the result of condition evaluations, including those
--   conditions which activated a human review.
newHumanLoopActivationOutput :: HumanLoopActivationOutput

-- | Shows if and why human review was needed.
humanLoopActivationOutput_humanLoopActivationReasons :: Lens' HumanLoopActivationOutput (Maybe (NonEmpty Text))

-- | The Amazon Resource Name (ARN) of the HumanLoop created.
humanLoopActivationOutput_humanLoopArn :: Lens' HumanLoopActivationOutput (Maybe Text)

-- | Shows the result of condition evaluations, including those conditions
--   which activated a human review.
humanLoopActivationOutput_humanLoopActivationConditionsEvaluationResults :: Lens' HumanLoopActivationOutput (Maybe Text)

-- | Sets up the flow definition the image will be sent to if one of the
--   conditions is met. You can also set certain attributes of the image
--   before review.
--   
--   <i>See:</i> <a>newHumanLoopConfig</a> smart constructor.
data HumanLoopConfig
HumanLoopConfig' :: Maybe HumanLoopDataAttributes -> Text -> Text -> HumanLoopConfig

-- | Sets attributes of the input data.
[$sel:dataAttributes:HumanLoopConfig'] :: HumanLoopConfig -> Maybe HumanLoopDataAttributes

-- | The name of the human review used for this image. This should be kept
--   unique within a region.
[$sel:humanLoopName:HumanLoopConfig'] :: HumanLoopConfig -> Text

-- | The Amazon Resource Name (ARN) of the flow definition. You can create
--   a flow definition by using the Amazon Sagemaker
--   <a>CreateFlowDefinition</a> Operation.
[$sel:flowDefinitionArn:HumanLoopConfig'] :: HumanLoopConfig -> Text

-- | Create a value of <a>HumanLoopConfig</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:dataAttributes:HumanLoopConfig'</a>,
--   <a>humanLoopConfig_dataAttributes</a> - Sets attributes of the input
--   data.
--   
--   <a>$sel:humanLoopName:HumanLoopConfig'</a>,
--   <a>humanLoopConfig_humanLoopName</a> - The name of the human review
--   used for this image. This should be kept unique within a region.
--   
--   <a>$sel:flowDefinitionArn:HumanLoopConfig'</a>,
--   <a>humanLoopConfig_flowDefinitionArn</a> - The Amazon Resource Name
--   (ARN) of the flow definition. You can create a flow definition by
--   using the Amazon Sagemaker <a>CreateFlowDefinition</a> Operation.
newHumanLoopConfig :: Text -> Text -> HumanLoopConfig

-- | Sets attributes of the input data.
humanLoopConfig_dataAttributes :: Lens' HumanLoopConfig (Maybe HumanLoopDataAttributes)

-- | The name of the human review used for this image. This should be kept
--   unique within a region.
humanLoopConfig_humanLoopName :: Lens' HumanLoopConfig Text

-- | The Amazon Resource Name (ARN) of the flow definition. You can create
--   a flow definition by using the Amazon Sagemaker
--   <a>CreateFlowDefinition</a> Operation.
humanLoopConfig_flowDefinitionArn :: Lens' HumanLoopConfig Text

-- | Allows you to set attributes of the image. Currently, you can declare
--   an image as free of personally identifiable information.
--   
--   <i>See:</i> <a>newHumanLoopDataAttributes</a> smart constructor.
data HumanLoopDataAttributes
HumanLoopDataAttributes' :: Maybe [ContentClassifier] -> HumanLoopDataAttributes

-- | Sets whether the input image is free of personally identifiable
--   information.
[$sel:contentClassifiers:HumanLoopDataAttributes'] :: HumanLoopDataAttributes -> Maybe [ContentClassifier]

-- | Create a value of <a>HumanLoopDataAttributes</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:contentClassifiers:HumanLoopDataAttributes'</a>,
--   <a>humanLoopDataAttributes_contentClassifiers</a> - Sets whether the
--   input image is free of personally identifiable information.
newHumanLoopDataAttributes :: HumanLoopDataAttributes

-- | Sets whether the input image is free of personally identifiable
--   information.
humanLoopDataAttributes_contentClassifiers :: Lens' HumanLoopDataAttributes (Maybe [ContentClassifier])

-- | Provides the input image either as bytes or an S3 object.
--   
--   You pass image bytes to an Amazon Rekognition API operation by using
--   the <tt>Bytes</tt> property. For example, you would use the
--   <tt>Bytes</tt> property to pass an image loaded from a local file
--   system. Image bytes passed by using the <tt>Bytes</tt> property must
--   be base64-encoded. Your code may not need to encode image bytes if you
--   are using an AWS SDK to call Amazon Rekognition API operations.
--   
--   For more information, see Analyzing an Image Loaded from a Local File
--   System in the Amazon Rekognition Developer Guide.
--   
--   You pass images stored in an S3 bucket to an Amazon Rekognition API
--   operation by using the <tt>S3Object</tt> property. Images stored in an
--   S3 bucket do not need to be base64-encoded.
--   
--   The region for the S3 bucket containing the S3 object must match the
--   region you use for Amazon Rekognition operations.
--   
--   If you use the AWS CLI to call Amazon Rekognition operations, passing
--   image bytes using the Bytes property is not supported. You must first
--   upload the image to an Amazon S3 bucket and then call the operation
--   using the S3Object property.
--   
--   For Amazon Rekognition to process an S3 object, the user must have
--   permission to access the S3 object. For more information, see Resource
--   Based Policies in the Amazon Rekognition Developer Guide.
--   
--   <i>See:</i> <a>newImage</a> smart constructor.
data Image
Image' :: Maybe S3Object -> Maybe Base64 -> Image

-- | Identifies an S3 object as the image source.
[$sel:s3Object:Image'] :: Image -> Maybe S3Object

-- | Blob of image bytes up to 5 MBs.
[$sel:bytes:Image'] :: Image -> Maybe Base64

-- | Create a value of <a>Image</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:s3Object:Image'</a>, <a>image_s3Object</a> - Identifies an S3
--   object as the image source.
--   
--   <a>$sel:bytes:Image'</a>, <a>image_bytes</a> - Blob of image bytes up
--   to 5 MBs.-- -- <i>Note:</i> This <tt>Lens</tt> automatically encodes
--   and decodes Base64 data. -- The underlying isomorphism will encode to
--   Base64 representation during -- serialisation, and decode from Base64
--   representation during deserialisation. -- This <tt>Lens</tt> accepts
--   and returns only raw unencoded data.
newImage :: Image

-- | Identifies an S3 object as the image source.
image_s3Object :: Lens' Image (Maybe S3Object)

-- | Blob of image bytes up to 5 MBs.-- -- <i>Note:</i> This <tt>Lens</tt>
--   automatically encodes and decodes Base64 data. -- The underlying
--   isomorphism will encode to Base64 representation during --
--   serialisation, and decode from Base64 representation during
--   deserialisation. -- This <tt>Lens</tt> accepts and returns only raw
--   unencoded data.
image_bytes :: Lens' Image (Maybe ByteString)

-- | Identifies face image brightness and sharpness.
--   
--   <i>See:</i> <a>newImageQuality</a> smart constructor.
data ImageQuality
ImageQuality' :: Maybe Double -> Maybe Double -> ImageQuality

-- | Value representing sharpness of the face. The service returns a value
--   between 0 and 100 (inclusive). A higher value indicates a sharper face
--   image.
[$sel:sharpness:ImageQuality'] :: ImageQuality -> Maybe Double

-- | Value representing brightness of the face. The service returns a value
--   between 0 and 100 (inclusive). A higher value indicates a brighter
--   face image.
[$sel:brightness:ImageQuality'] :: ImageQuality -> Maybe Double

-- | Create a value of <a>ImageQuality</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:sharpness:ImageQuality'</a>, <a>imageQuality_sharpness</a> -
--   Value representing sharpness of the face. The service returns a value
--   between 0 and 100 (inclusive). A higher value indicates a sharper face
--   image.
--   
--   <a>$sel:brightness:ImageQuality'</a>, <a>imageQuality_brightness</a> -
--   Value representing brightness of the face. The service returns a value
--   between 0 and 100 (inclusive). A higher value indicates a brighter
--   face image.
newImageQuality :: ImageQuality

-- | Value representing sharpness of the face. The service returns a value
--   between 0 and 100 (inclusive). A higher value indicates a sharper face
--   image.
imageQuality_sharpness :: Lens' ImageQuality (Maybe Double)

-- | Value representing brightness of the face. The service returns a value
--   between 0 and 100 (inclusive). A higher value indicates a brighter
--   face image.
imageQuality_brightness :: Lens' ImageQuality (Maybe Double)

-- | An instance of a label returned by Amazon Rekognition Image
--   (DetectLabels) or by Amazon Rekognition Video (GetLabelDetection).
--   
--   <i>See:</i> <a>newInstance</a> smart constructor.
data Instance
Instance' :: Maybe BoundingBox -> Maybe Double -> Instance

-- | The position of the label instance on the image.
[$sel:boundingBox:Instance'] :: Instance -> Maybe BoundingBox

-- | The confidence that Amazon Rekognition has in the accuracy of the
--   bounding box.
[$sel:confidence:Instance'] :: Instance -> Maybe Double

-- | Create a value of <a>Instance</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:boundingBox:Instance'</a>, <a>instance_boundingBox</a> - The
--   position of the label instance on the image.
--   
--   <a>$sel:confidence:Instance'</a>, <a>instance_confidence</a> - The
--   confidence that Amazon Rekognition has in the accuracy of the bounding
--   box.
newInstance :: Instance

-- | The position of the label instance on the image.
instance_boundingBox :: Lens' Instance (Maybe BoundingBox)

-- | The confidence that Amazon Rekognition has in the accuracy of the
--   bounding box.
instance_confidence :: Lens' Instance (Maybe Double)

-- | The Kinesis data stream Amazon Rekognition to which the analysis
--   results of a Amazon Rekognition stream processor are streamed. For
--   more information, see CreateStreamProcessor in the Amazon Rekognition
--   Developer Guide.
--   
--   <i>See:</i> <a>newKinesisDataStream</a> smart constructor.
data KinesisDataStream
KinesisDataStream' :: Maybe Text -> KinesisDataStream

-- | ARN of the output Amazon Kinesis Data Streams stream.
[$sel:arn:KinesisDataStream'] :: KinesisDataStream -> Maybe Text

-- | Create a value of <a>KinesisDataStream</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:arn:KinesisDataStream'</a>, <a>kinesisDataStream_arn</a> - ARN
--   of the output Amazon Kinesis Data Streams stream.
newKinesisDataStream :: KinesisDataStream

-- | ARN of the output Amazon Kinesis Data Streams stream.
kinesisDataStream_arn :: Lens' KinesisDataStream (Maybe Text)

-- | Kinesis video stream stream that provides the source streaming video
--   for a Amazon Rekognition Video stream processor. For more information,
--   see CreateStreamProcessor in the Amazon Rekognition Developer Guide.
--   
--   <i>See:</i> <a>newKinesisVideoStream</a> smart constructor.
data KinesisVideoStream
KinesisVideoStream' :: Maybe Text -> KinesisVideoStream

-- | ARN of the Kinesis video stream stream that streams the source video.
[$sel:arn:KinesisVideoStream'] :: KinesisVideoStream -> Maybe Text

-- | Create a value of <a>KinesisVideoStream</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:arn:KinesisVideoStream'</a>, <a>kinesisVideoStream_arn</a> -
--   ARN of the Kinesis video stream stream that streams the source video.
newKinesisVideoStream :: KinesisVideoStream

-- | ARN of the Kinesis video stream stream that streams the source video.
kinesisVideoStream_arn :: Lens' KinesisVideoStream (Maybe Text)

-- | The known gender identity for the celebrity that matches the provided
--   ID.
--   
--   <i>See:</i> <a>newKnownGender</a> smart constructor.
data KnownGender
KnownGender' :: Maybe KnownGenderType -> KnownGender

-- | A string value of the KnownGender info about the Celebrity.
[$sel:type':KnownGender'] :: KnownGender -> Maybe KnownGenderType

-- | Create a value of <a>KnownGender</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:type':KnownGender'</a>, <a>knownGender_type</a> - A string
--   value of the KnownGender info about the Celebrity.
newKnownGender :: KnownGender

-- | A string value of the KnownGender info about the Celebrity.
knownGender_type :: Lens' KnownGender (Maybe KnownGenderType)

-- | Structure containing details about the detected label, including the
--   name, detected instances, parent labels, and level of confidence.
--   
--   <i>See:</i> <a>newLabel</a> smart constructor.
data Label
Label' :: Maybe Double -> Maybe [Parent] -> Maybe Text -> Maybe [Instance] -> Label

-- | Level of confidence.
[$sel:confidence:Label'] :: Label -> Maybe Double

-- | The parent labels for a label. The response includes all ancestor
--   labels.
[$sel:parents:Label'] :: Label -> Maybe [Parent]

-- | The name (label) of the object or scene.
[$sel:name:Label'] :: Label -> Maybe Text

-- | If <tt>Label</tt> represents an object, <tt>Instances</tt> contains
--   the bounding boxes for each instance of the detected object. Bounding
--   boxes are returned for common object labels such as people, cars,
--   furniture, apparel or pets.
[$sel:instances:Label'] :: Label -> Maybe [Instance]

-- | Create a value of <a>Label</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:confidence:Label'</a>, <a>label_confidence</a> - Level of
--   confidence.
--   
--   <a>$sel:parents:Label'</a>, <a>label_parents</a> - The parent labels
--   for a label. The response includes all ancestor labels.
--   
--   <a>$sel:name:Label'</a>, <a>label_name</a> - The name (label) of the
--   object or scene.
--   
--   <a>$sel:instances:Label'</a>, <a>label_instances</a> - If
--   <tt>Label</tt> represents an object, <tt>Instances</tt> contains the
--   bounding boxes for each instance of the detected object. Bounding
--   boxes are returned for common object labels such as people, cars,
--   furniture, apparel or pets.
newLabel :: Label

-- | Level of confidence.
label_confidence :: Lens' Label (Maybe Double)

-- | The parent labels for a label. The response includes all ancestor
--   labels.
label_parents :: Lens' Label (Maybe [Parent])

-- | The name (label) of the object or scene.
label_name :: Lens' Label (Maybe Text)

-- | If <tt>Label</tt> represents an object, <tt>Instances</tt> contains
--   the bounding boxes for each instance of the detected object. Bounding
--   boxes are returned for common object labels such as people, cars,
--   furniture, apparel or pets.
label_instances :: Lens' Label (Maybe [Instance])

-- | Information about a label detected in a video analysis request and the
--   time the label was detected in the video.
--   
--   <i>See:</i> <a>newLabelDetection</a> smart constructor.
data LabelDetection
LabelDetection' :: Maybe Label -> Maybe Integer -> LabelDetection

-- | Details about the detected label.
[$sel:label:LabelDetection'] :: LabelDetection -> Maybe Label

-- | Time, in milliseconds from the start of the video, that the label was
--   detected.
[$sel:timestamp:LabelDetection'] :: LabelDetection -> Maybe Integer

-- | Create a value of <a>LabelDetection</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:label:LabelDetection'</a>, <a>labelDetection_label</a> -
--   Details about the detected label.
--   
--   <a>$sel:timestamp:LabelDetection'</a>, <a>labelDetection_timestamp</a>
--   - Time, in milliseconds from the start of the video, that the label
--   was detected.
newLabelDetection :: LabelDetection

-- | Details about the detected label.
labelDetection_label :: Lens' LabelDetection (Maybe Label)

-- | Time, in milliseconds from the start of the video, that the label was
--   detected.
labelDetection_timestamp :: Lens' LabelDetection (Maybe Integer)

-- | Indicates the location of the landmark on the face.
--   
--   <i>See:</i> <a>newLandmark</a> smart constructor.
data Landmark
Landmark' :: Maybe LandmarkType -> Maybe Double -> Maybe Double -> Landmark

-- | Type of landmark.
[$sel:type':Landmark'] :: Landmark -> Maybe LandmarkType

-- | The x-coordinate of the landmark expressed as a ratio of the width of
--   the image. The x-coordinate is measured from the left-side of the
--   image. For example, if the image is 700 pixels wide and the
--   x-coordinate of the landmark is at 350 pixels, this value is 0.5.
[$sel:x:Landmark'] :: Landmark -> Maybe Double

-- | The y-coordinate of the landmark expressed as a ratio of the height of
--   the image. The y-coordinate is measured from the top of the image. For
--   example, if the image height is 200 pixels and the y-coordinate of the
--   landmark is at 50 pixels, this value is 0.25.
[$sel:y:Landmark'] :: Landmark -> Maybe Double

-- | Create a value of <a>Landmark</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:type':Landmark'</a>, <a>landmark_type</a> - Type of landmark.
--   
--   <a>$sel:x:Landmark'</a>, <a>landmark_x</a> - The x-coordinate of the
--   landmark expressed as a ratio of the width of the image. The
--   x-coordinate is measured from the left-side of the image. For example,
--   if the image is 700 pixels wide and the x-coordinate of the landmark
--   is at 350 pixels, this value is 0.5.
--   
--   <a>$sel:y:Landmark'</a>, <a>landmark_y</a> - The y-coordinate of the
--   landmark expressed as a ratio of the height of the image. The
--   y-coordinate is measured from the top of the image. For example, if
--   the image height is 200 pixels and the y-coordinate of the landmark is
--   at 50 pixels, this value is 0.25.
newLandmark :: Landmark

-- | Type of landmark.
landmark_type :: Lens' Landmark (Maybe LandmarkType)

-- | The x-coordinate of the landmark expressed as a ratio of the width of
--   the image. The x-coordinate is measured from the left-side of the
--   image. For example, if the image is 700 pixels wide and the
--   x-coordinate of the landmark is at 350 pixels, this value is 0.5.
landmark_x :: Lens' Landmark (Maybe Double)

-- | The y-coordinate of the landmark expressed as a ratio of the height of
--   the image. The y-coordinate is measured from the top of the image. For
--   example, if the image height is 200 pixels and the y-coordinate of the
--   landmark is at 50 pixels, this value is 0.25.
landmark_y :: Lens' Landmark (Maybe Double)

-- | Provides information about a single type of inappropriate, unwanted,
--   or offensive content found in an image or video. Each type of
--   moderated content has a label within a hierarchical taxonomy. For more
--   information, see Content moderation in the Amazon Rekognition
--   Developer Guide.
--   
--   <i>See:</i> <a>newModerationLabel</a> smart constructor.
data ModerationLabel
ModerationLabel' :: Maybe Double -> Maybe Text -> Maybe Text -> ModerationLabel

-- | Specifies the confidence that Amazon Rekognition has that the label
--   has been correctly identified.
--   
--   If you don't specify the <tt>MinConfidence</tt> parameter in the call
--   to <tt>DetectModerationLabels</tt>, the operation returns labels with
--   a confidence value greater than or equal to 50 percent.
[$sel:confidence:ModerationLabel'] :: ModerationLabel -> Maybe Double

-- | The label name for the type of unsafe content detected in the image.
[$sel:name:ModerationLabel'] :: ModerationLabel -> Maybe Text

-- | The name for the parent label. Labels at the top level of the
--   hierarchy have the parent label <tt>""</tt>.
[$sel:parentName:ModerationLabel'] :: ModerationLabel -> Maybe Text

-- | Create a value of <a>ModerationLabel</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:confidence:ModerationLabel'</a>,
--   <a>moderationLabel_confidence</a> - Specifies the confidence that
--   Amazon Rekognition has that the label has been correctly identified.
--   
--   If you don't specify the <tt>MinConfidence</tt> parameter in the call
--   to <tt>DetectModerationLabels</tt>, the operation returns labels with
--   a confidence value greater than or equal to 50 percent.
--   
--   <a>$sel:name:ModerationLabel'</a>, <a>moderationLabel_name</a> - The
--   label name for the type of unsafe content detected in the image.
--   
--   <a>$sel:parentName:ModerationLabel'</a>,
--   <a>moderationLabel_parentName</a> - The name for the parent label.
--   Labels at the top level of the hierarchy have the parent label
--   <tt>""</tt>.
newModerationLabel :: ModerationLabel

-- | Specifies the confidence that Amazon Rekognition has that the label
--   has been correctly identified.
--   
--   If you don't specify the <tt>MinConfidence</tt> parameter in the call
--   to <tt>DetectModerationLabels</tt>, the operation returns labels with
--   a confidence value greater than or equal to 50 percent.
moderationLabel_confidence :: Lens' ModerationLabel (Maybe Double)

-- | The label name for the type of unsafe content detected in the image.
moderationLabel_name :: Lens' ModerationLabel (Maybe Text)

-- | The name for the parent label. Labels at the top level of the
--   hierarchy have the parent label <tt>""</tt>.
moderationLabel_parentName :: Lens' ModerationLabel (Maybe Text)

-- | Indicates whether or not the mouth on the face is open, and the
--   confidence level in the determination.
--   
--   <i>See:</i> <a>newMouthOpen</a> smart constructor.
data MouthOpen
MouthOpen' :: Maybe Bool -> Maybe Double -> MouthOpen

-- | Boolean value that indicates whether the mouth on the face is open or
--   not.
[$sel:value:MouthOpen'] :: MouthOpen -> Maybe Bool

-- | Level of confidence in the determination.
[$sel:confidence:MouthOpen'] :: MouthOpen -> Maybe Double

-- | Create a value of <a>MouthOpen</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:value:MouthOpen'</a>, <a>mouthOpen_value</a> - Boolean value
--   that indicates whether the mouth on the face is open or not.
--   
--   <a>$sel:confidence:MouthOpen'</a>, <a>mouthOpen_confidence</a> - Level
--   of confidence in the determination.
newMouthOpen :: MouthOpen

-- | Boolean value that indicates whether the mouth on the face is open or
--   not.
mouthOpen_value :: Lens' MouthOpen (Maybe Bool)

-- | Level of confidence in the determination.
mouthOpen_confidence :: Lens' MouthOpen (Maybe Double)

-- | Indicates whether or not the face has a mustache, and the confidence
--   level in the determination.
--   
--   <i>See:</i> <a>newMustache</a> smart constructor.
data Mustache
Mustache' :: Maybe Bool -> Maybe Double -> Mustache

-- | Boolean value that indicates whether the face has mustache or not.
[$sel:value:Mustache'] :: Mustache -> Maybe Bool

-- | Level of confidence in the determination.
[$sel:confidence:Mustache'] :: Mustache -> Maybe Double

-- | Create a value of <a>Mustache</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:value:Mustache'</a>, <a>mustache_value</a> - Boolean value
--   that indicates whether the face has mustache or not.
--   
--   <a>$sel:confidence:Mustache'</a>, <a>mustache_confidence</a> - Level
--   of confidence in the determination.
newMustache :: Mustache

-- | Boolean value that indicates whether the face has mustache or not.
mustache_value :: Lens' Mustache (Maybe Bool)

-- | Level of confidence in the determination.
mustache_confidence :: Lens' Mustache (Maybe Double)

-- | The Amazon Simple Notification Service topic to which Amazon
--   Rekognition publishes the completion status of a video analysis
--   operation. For more information, see api-video. Note that the Amazon
--   SNS topic must have a topic name that begins with
--   <i>AmazonRekognition</i> if you are using the
--   AmazonRekognitionServiceRole permissions policy to access the topic.
--   For more information, see <a>Giving access to multiple Amazon SNS
--   topics</a>.
--   
--   <i>See:</i> <a>newNotificationChannel</a> smart constructor.
data NotificationChannel
NotificationChannel' :: Text -> Text -> NotificationChannel

-- | The Amazon SNS topic to which Amazon Rekognition to posts the
--   completion status.
[$sel:sNSTopicArn:NotificationChannel'] :: NotificationChannel -> Text

-- | The ARN of an IAM role that gives Amazon Rekognition publishing
--   permissions to the Amazon SNS topic.
[$sel:roleArn:NotificationChannel'] :: NotificationChannel -> Text

-- | Create a value of <a>NotificationChannel</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:sNSTopicArn:NotificationChannel'</a>,
--   <a>notificationChannel_sNSTopicArn</a> - The Amazon SNS topic to which
--   Amazon Rekognition to posts the completion status.
--   
--   <a>$sel:roleArn:NotificationChannel'</a>,
--   <a>notificationChannel_roleArn</a> - The ARN of an IAM role that gives
--   Amazon Rekognition publishing permissions to the Amazon SNS topic.
newNotificationChannel :: Text -> Text -> NotificationChannel

-- | The Amazon SNS topic to which Amazon Rekognition to posts the
--   completion status.
notificationChannel_sNSTopicArn :: Lens' NotificationChannel Text

-- | The ARN of an IAM role that gives Amazon Rekognition publishing
--   permissions to the Amazon SNS topic.
notificationChannel_roleArn :: Lens' NotificationChannel Text

-- | The S3 bucket and folder location where training output is placed.
--   
--   <i>See:</i> <a>newOutputConfig</a> smart constructor.
data OutputConfig
OutputConfig' :: Maybe Text -> Maybe Text -> OutputConfig

-- | The prefix applied to the training output files.
[$sel:s3KeyPrefix:OutputConfig'] :: OutputConfig -> Maybe Text

-- | The S3 bucket where training output is placed.
[$sel:s3Bucket:OutputConfig'] :: OutputConfig -> Maybe Text

-- | Create a value of <a>OutputConfig</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:s3KeyPrefix:OutputConfig'</a>, <a>outputConfig_s3KeyPrefix</a>
--   - The prefix applied to the training output files.
--   
--   <a>$sel:s3Bucket:OutputConfig'</a>, <a>outputConfig_s3Bucket</a> - The
--   S3 bucket where training output is placed.
newOutputConfig :: OutputConfig

-- | The prefix applied to the training output files.
outputConfig_s3KeyPrefix :: Lens' OutputConfig (Maybe Text)

-- | The S3 bucket where training output is placed.
outputConfig_s3Bucket :: Lens' OutputConfig (Maybe Text)

-- | A parent label for a label. A label can have 0, 1, or more parents.
--   
--   <i>See:</i> <a>newParent</a> smart constructor.
data Parent
Parent' :: Maybe Text -> Parent

-- | The name of the parent label.
[$sel:name:Parent'] :: Parent -> Maybe Text

-- | Create a value of <a>Parent</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:name:Parent'</a>, <a>parent_name</a> - The name of the parent
--   label.
newParent :: Parent

-- | The name of the parent label.
parent_name :: Lens' Parent (Maybe Text)

-- | Details about a person detected in a video analysis request.
--   
--   <i>See:</i> <a>newPersonDetail</a> smart constructor.
data PersonDetail
PersonDetail' :: Maybe BoundingBox -> Maybe Integer -> Maybe FaceDetail -> PersonDetail

-- | Bounding box around the detected person.
[$sel:boundingBox:PersonDetail'] :: PersonDetail -> Maybe BoundingBox

-- | Identifier for the person detected person within a video. Use to keep
--   track of the person throughout the video. The identifier is not stored
--   by Amazon Rekognition.
[$sel:index:PersonDetail'] :: PersonDetail -> Maybe Integer

-- | Face details for the detected person.
[$sel:face:PersonDetail'] :: PersonDetail -> Maybe FaceDetail

-- | Create a value of <a>PersonDetail</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:boundingBox:PersonDetail'</a>, <a>personDetail_boundingBox</a>
--   - Bounding box around the detected person.
--   
--   <a>$sel:index:PersonDetail'</a>, <a>personDetail_index</a> -
--   Identifier for the person detected person within a video. Use to keep
--   track of the person throughout the video. The identifier is not stored
--   by Amazon Rekognition.
--   
--   <a>$sel:face:PersonDetail'</a>, <a>personDetail_face</a> - Face
--   details for the detected person.
newPersonDetail :: PersonDetail

-- | Bounding box around the detected person.
personDetail_boundingBox :: Lens' PersonDetail (Maybe BoundingBox)

-- | Identifier for the person detected person within a video. Use to keep
--   track of the person throughout the video. The identifier is not stored
--   by Amazon Rekognition.
personDetail_index :: Lens' PersonDetail (Maybe Integer)

-- | Face details for the detected person.
personDetail_face :: Lens' PersonDetail (Maybe FaceDetail)

-- | Details and path tracking information for a single time a person's
--   path is tracked in a video. Amazon Rekognition operations that track
--   people's paths return an array of <tt>PersonDetection</tt> objects
--   with elements for each time a person's path is tracked in a video.
--   
--   For more information, see GetPersonTracking in the Amazon Rekognition
--   Developer Guide.
--   
--   <i>See:</i> <a>newPersonDetection</a> smart constructor.
data PersonDetection
PersonDetection' :: Maybe PersonDetail -> Maybe Integer -> PersonDetection

-- | Details about a person whose path was tracked in a video.
[$sel:person:PersonDetection'] :: PersonDetection -> Maybe PersonDetail

-- | The time, in milliseconds from the start of the video, that the
--   person's path was tracked.
[$sel:timestamp:PersonDetection'] :: PersonDetection -> Maybe Integer

-- | Create a value of <a>PersonDetection</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:person:PersonDetection'</a>, <a>personDetection_person</a> -
--   Details about a person whose path was tracked in a video.
--   
--   <a>$sel:timestamp:PersonDetection'</a>,
--   <a>personDetection_timestamp</a> - The time, in milliseconds from the
--   start of the video, that the person's path was tracked.
newPersonDetection :: PersonDetection

-- | Details about a person whose path was tracked in a video.
personDetection_person :: Lens' PersonDetection (Maybe PersonDetail)

-- | The time, in milliseconds from the start of the video, that the
--   person's path was tracked.
personDetection_timestamp :: Lens' PersonDetection (Maybe Integer)

-- | Information about a person whose face matches a face(s) in an Amazon
--   Rekognition collection. Includes information about the faces in the
--   Amazon Rekognition collection (FaceMatch), information about the
--   person (PersonDetail), and the time stamp for when the person was
--   detected in a video. An array of <tt>PersonMatch</tt> objects is
--   returned by GetFaceSearch.
--   
--   <i>See:</i> <a>newPersonMatch</a> smart constructor.
data PersonMatch
PersonMatch' :: Maybe [FaceMatch] -> Maybe PersonDetail -> Maybe Integer -> PersonMatch

-- | Information about the faces in the input collection that match the
--   face of a person in the video.
[$sel:faceMatches:PersonMatch'] :: PersonMatch -> Maybe [FaceMatch]

-- | Information about the matched person.
[$sel:person:PersonMatch'] :: PersonMatch -> Maybe PersonDetail

-- | The time, in milliseconds from the beginning of the video, that the
--   person was matched in the video.
[$sel:timestamp:PersonMatch'] :: PersonMatch -> Maybe Integer

-- | Create a value of <a>PersonMatch</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:faceMatches:PersonMatch'</a>, <a>personMatch_faceMatches</a> -
--   Information about the faces in the input collection that match the
--   face of a person in the video.
--   
--   <a>$sel:person:PersonMatch'</a>, <a>personMatch_person</a> -
--   Information about the matched person.
--   
--   <a>$sel:timestamp:PersonMatch'</a>, <a>personMatch_timestamp</a> - The
--   time, in milliseconds from the beginning of the video, that the person
--   was matched in the video.
newPersonMatch :: PersonMatch

-- | Information about the faces in the input collection that match the
--   face of a person in the video.
personMatch_faceMatches :: Lens' PersonMatch (Maybe [FaceMatch])

-- | Information about the matched person.
personMatch_person :: Lens' PersonMatch (Maybe PersonDetail)

-- | The time, in milliseconds from the beginning of the video, that the
--   person was matched in the video.
personMatch_timestamp :: Lens' PersonMatch (Maybe Integer)

-- | The X and Y coordinates of a point on an image. The X and Y values
--   returned are ratios of the overall image size. For example, if the
--   input image is 700x200 and the operation returns X=0.5 and Y=0.25,
--   then the point is at the (350,50) pixel coordinate on the image.
--   
--   An array of <tt>Point</tt> objects, <tt>Polygon</tt>, is returned by
--   DetectText and by DetectCustomLabels. <tt>Polygon</tt> represents a
--   fine-grained polygon around a detected item. For more information, see
--   Geometry in the Amazon Rekognition Developer Guide.
--   
--   <i>See:</i> <a>newPoint</a> smart constructor.
data Point
Point' :: Maybe Double -> Maybe Double -> Point

-- | The value of the X coordinate for a point on a <tt>Polygon</tt>.
[$sel:x:Point'] :: Point -> Maybe Double

-- | The value of the Y coordinate for a point on a <tt>Polygon</tt>.
[$sel:y:Point'] :: Point -> Maybe Double

-- | Create a value of <a>Point</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:x:Point'</a>, <a>point_x</a> - The value of the X coordinate
--   for a point on a <tt>Polygon</tt>.
--   
--   <a>$sel:y:Point'</a>, <a>point_y</a> - The value of the Y coordinate
--   for a point on a <tt>Polygon</tt>.
newPoint :: Point

-- | The value of the X coordinate for a point on a <tt>Polygon</tt>.
point_x :: Lens' Point (Maybe Double)

-- | The value of the Y coordinate for a point on a <tt>Polygon</tt>.
point_y :: Lens' Point (Maybe Double)

-- | Indicates the pose of the face as determined by its pitch, roll, and
--   yaw.
--   
--   <i>See:</i> <a>newPose</a> smart constructor.
data Pose
Pose' :: Maybe Double -> Maybe Double -> Maybe Double -> Pose

-- | Value representing the face rotation on the yaw axis.
[$sel:yaw:Pose'] :: Pose -> Maybe Double

-- | Value representing the face rotation on the roll axis.
[$sel:roll:Pose'] :: Pose -> Maybe Double

-- | Value representing the face rotation on the pitch axis.
[$sel:pitch:Pose'] :: Pose -> Maybe Double

-- | Create a value of <a>Pose</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:yaw:Pose'</a>, <a>pose_yaw</a> - Value representing the face
--   rotation on the yaw axis.
--   
--   <a>$sel:roll:Pose'</a>, <a>pose_roll</a> - Value representing the face
--   rotation on the roll axis.
--   
--   <a>$sel:pitch:Pose'</a>, <a>pose_pitch</a> - Value representing the
--   face rotation on the pitch axis.
newPose :: Pose

-- | Value representing the face rotation on the yaw axis.
pose_yaw :: Lens' Pose (Maybe Double)

-- | Value representing the face rotation on the roll axis.
pose_roll :: Lens' Pose (Maybe Double)

-- | Value representing the face rotation on the pitch axis.
pose_pitch :: Lens' Pose (Maybe Double)

-- | A description of a Amazon Rekognition Custom Labels project.
--   
--   <i>See:</i> <a>newProjectDescription</a> smart constructor.
data ProjectDescription
ProjectDescription' :: Maybe ProjectStatus -> Maybe POSIX -> Maybe Text -> ProjectDescription

-- | The current status of the project.
[$sel:status:ProjectDescription'] :: ProjectDescription -> Maybe ProjectStatus

-- | The Unix timestamp for the date and time that the project was created.
[$sel:creationTimestamp:ProjectDescription'] :: ProjectDescription -> Maybe POSIX

-- | The Amazon Resource Name (ARN) of the project.
[$sel:projectArn:ProjectDescription'] :: ProjectDescription -> Maybe Text

-- | Create a value of <a>ProjectDescription</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:status:ProjectDescription'</a>,
--   <a>projectDescription_status</a> - The current status of the project.
--   
--   <a>$sel:creationTimestamp:ProjectDescription'</a>,
--   <a>projectDescription_creationTimestamp</a> - The Unix timestamp for
--   the date and time that the project was created.
--   
--   <a>$sel:projectArn:ProjectDescription'</a>,
--   <a>projectDescription_projectArn</a> - The Amazon Resource Name (ARN)
--   of the project.
newProjectDescription :: ProjectDescription

-- | The current status of the project.
projectDescription_status :: Lens' ProjectDescription (Maybe ProjectStatus)

-- | The Unix timestamp for the date and time that the project was created.
projectDescription_creationTimestamp :: Lens' ProjectDescription (Maybe UTCTime)

-- | The Amazon Resource Name (ARN) of the project.
projectDescription_projectArn :: Lens' ProjectDescription (Maybe Text)

-- | The description of a version of a model.
--   
--   <i>See:</i> <a>newProjectVersionDescription</a> smart constructor.
data ProjectVersionDescription
ProjectVersionDescription' :: Maybe Natural -> Maybe ProjectVersionStatus -> Maybe EvaluationResult -> Maybe GroundTruthManifest -> Maybe Text -> Maybe TestingDataResult -> Maybe Text -> Maybe POSIX -> Maybe Text -> Maybe OutputConfig -> Maybe Natural -> Maybe POSIX -> Maybe TrainingDataResult -> ProjectVersionDescription

-- | The minimum number of inference units used by the model. For more
--   information, see StartProjectVersion.
[$sel:minInferenceUnits:ProjectVersionDescription'] :: ProjectVersionDescription -> Maybe Natural

-- | The current status of the model version.
[$sel:status:ProjectVersionDescription'] :: ProjectVersionDescription -> Maybe ProjectVersionStatus

-- | The training results. <tt>EvaluationResult</tt> is only returned if
--   training is successful.
[$sel:evaluationResult:ProjectVersionDescription'] :: ProjectVersionDescription -> Maybe EvaluationResult

-- | The location of the summary manifest. The summary manifest provides
--   aggregate data validation results for the training and test datasets.
[$sel:manifestSummary:ProjectVersionDescription'] :: ProjectVersionDescription -> Maybe GroundTruthManifest

-- | The identifer for the AWS Key Management Service (AWS KMS) customer
--   master key that was used to encrypt the model during training.
[$sel:kmsKeyId:ProjectVersionDescription'] :: ProjectVersionDescription -> Maybe Text

-- | Contains information about the testing results.
[$sel:testingDataResult:ProjectVersionDescription'] :: ProjectVersionDescription -> Maybe TestingDataResult

-- | A descriptive message for an error or warning that occurred.
[$sel:statusMessage:ProjectVersionDescription'] :: ProjectVersionDescription -> Maybe Text

-- | The Unix datetime for the date and time that training started.
[$sel:creationTimestamp:ProjectVersionDescription'] :: ProjectVersionDescription -> Maybe POSIX

-- | The Amazon Resource Name (ARN) of the model version.
[$sel:projectVersionArn:ProjectVersionDescription'] :: ProjectVersionDescription -> Maybe Text

-- | The location where training results are saved.
[$sel:outputConfig:ProjectVersionDescription'] :: ProjectVersionDescription -> Maybe OutputConfig

-- | The duration, in seconds, that the model version has been billed for
--   training. This value is only returned if the model version has been
--   successfully trained.
[$sel:billableTrainingTimeInSeconds:ProjectVersionDescription'] :: ProjectVersionDescription -> Maybe Natural

-- | The Unix date and time that training of the model ended.
[$sel:trainingEndTimestamp:ProjectVersionDescription'] :: ProjectVersionDescription -> Maybe POSIX

-- | Contains information about the training results.
[$sel:trainingDataResult:ProjectVersionDescription'] :: ProjectVersionDescription -> Maybe TrainingDataResult

-- | Create a value of <a>ProjectVersionDescription</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:minInferenceUnits:ProjectVersionDescription'</a>,
--   <a>projectVersionDescription_minInferenceUnits</a> - The minimum
--   number of inference units used by the model. For more information, see
--   StartProjectVersion.
--   
--   <a>$sel:status:ProjectVersionDescription'</a>,
--   <a>projectVersionDescription_status</a> - The current status of the
--   model version.
--   
--   <a>$sel:evaluationResult:ProjectVersionDescription'</a>,
--   <a>projectVersionDescription_evaluationResult</a> - The training
--   results. <tt>EvaluationResult</tt> is only returned if training is
--   successful.
--   
--   <a>$sel:manifestSummary:ProjectVersionDescription'</a>,
--   <a>projectVersionDescription_manifestSummary</a> - The location of the
--   summary manifest. The summary manifest provides aggregate data
--   validation results for the training and test datasets.
--   
--   <a>$sel:kmsKeyId:ProjectVersionDescription'</a>,
--   <a>projectVersionDescription_kmsKeyId</a> - The identifer for the AWS
--   Key Management Service (AWS KMS) customer master key that was used to
--   encrypt the model during training.
--   
--   <a>$sel:testingDataResult:ProjectVersionDescription'</a>,
--   <a>projectVersionDescription_testingDataResult</a> - Contains
--   information about the testing results.
--   
--   <a>$sel:statusMessage:ProjectVersionDescription'</a>,
--   <a>projectVersionDescription_statusMessage</a> - A descriptive message
--   for an error or warning that occurred.
--   
--   <a>$sel:creationTimestamp:ProjectVersionDescription'</a>,
--   <a>projectVersionDescription_creationTimestamp</a> - The Unix datetime
--   for the date and time that training started.
--   
--   <a>$sel:projectVersionArn:ProjectVersionDescription'</a>,
--   <a>projectVersionDescription_projectVersionArn</a> - The Amazon
--   Resource Name (ARN) of the model version.
--   
--   <a>$sel:outputConfig:ProjectVersionDescription'</a>,
--   <a>projectVersionDescription_outputConfig</a> - The location where
--   training results are saved.
--   
--   <a>$sel:billableTrainingTimeInSeconds:ProjectVersionDescription'</a>,
--   <a>projectVersionDescription_billableTrainingTimeInSeconds</a> - The
--   duration, in seconds, that the model version has been billed for
--   training. This value is only returned if the model version has been
--   successfully trained.
--   
--   <a>$sel:trainingEndTimestamp:ProjectVersionDescription'</a>,
--   <a>projectVersionDescription_trainingEndTimestamp</a> - The Unix date
--   and time that training of the model ended.
--   
--   <a>$sel:trainingDataResult:ProjectVersionDescription'</a>,
--   <a>projectVersionDescription_trainingDataResult</a> - Contains
--   information about the training results.
newProjectVersionDescription :: ProjectVersionDescription

-- | The minimum number of inference units used by the model. For more
--   information, see StartProjectVersion.
projectVersionDescription_minInferenceUnits :: Lens' ProjectVersionDescription (Maybe Natural)

-- | The current status of the model version.
projectVersionDescription_status :: Lens' ProjectVersionDescription (Maybe ProjectVersionStatus)

-- | The training results. <tt>EvaluationResult</tt> is only returned if
--   training is successful.
projectVersionDescription_evaluationResult :: Lens' ProjectVersionDescription (Maybe EvaluationResult)

-- | The location of the summary manifest. The summary manifest provides
--   aggregate data validation results for the training and test datasets.
projectVersionDescription_manifestSummary :: Lens' ProjectVersionDescription (Maybe GroundTruthManifest)

-- | The identifer for the AWS Key Management Service (AWS KMS) customer
--   master key that was used to encrypt the model during training.
projectVersionDescription_kmsKeyId :: Lens' ProjectVersionDescription (Maybe Text)

-- | Contains information about the testing results.
projectVersionDescription_testingDataResult :: Lens' ProjectVersionDescription (Maybe TestingDataResult)

-- | A descriptive message for an error or warning that occurred.
projectVersionDescription_statusMessage :: Lens' ProjectVersionDescription (Maybe Text)

-- | The Unix datetime for the date and time that training started.
projectVersionDescription_creationTimestamp :: Lens' ProjectVersionDescription (Maybe UTCTime)

-- | The Amazon Resource Name (ARN) of the model version.
projectVersionDescription_projectVersionArn :: Lens' ProjectVersionDescription (Maybe Text)

-- | The location where training results are saved.
projectVersionDescription_outputConfig :: Lens' ProjectVersionDescription (Maybe OutputConfig)

-- | The duration, in seconds, that the model version has been billed for
--   training. This value is only returned if the model version has been
--   successfully trained.
projectVersionDescription_billableTrainingTimeInSeconds :: Lens' ProjectVersionDescription (Maybe Natural)

-- | The Unix date and time that training of the model ended.
projectVersionDescription_trainingEndTimestamp :: Lens' ProjectVersionDescription (Maybe UTCTime)

-- | Contains information about the training results.
projectVersionDescription_trainingDataResult :: Lens' ProjectVersionDescription (Maybe TrainingDataResult)

-- | Information about a body part detected by DetectProtectiveEquipment
--   that contains PPE. An array of <tt>ProtectiveEquipmentBodyPart</tt>
--   objects is returned for each person detected by
--   <tt>DetectProtectiveEquipment</tt>.
--   
--   <i>See:</i> <a>newProtectiveEquipmentBodyPart</a> smart constructor.
data ProtectiveEquipmentBodyPart
ProtectiveEquipmentBodyPart' :: Maybe [EquipmentDetection] -> Maybe Double -> Maybe BodyPart -> ProtectiveEquipmentBodyPart

-- | An array of Personal Protective Equipment items detected around a body
--   part.
[$sel:equipmentDetections:ProtectiveEquipmentBodyPart'] :: ProtectiveEquipmentBodyPart -> Maybe [EquipmentDetection]

-- | The confidence that Amazon Rekognition has in the detection accuracy
--   of the detected body part.
[$sel:confidence:ProtectiveEquipmentBodyPart'] :: ProtectiveEquipmentBodyPart -> Maybe Double

-- | The detected body part.
[$sel:name:ProtectiveEquipmentBodyPart'] :: ProtectiveEquipmentBodyPart -> Maybe BodyPart

-- | Create a value of <a>ProtectiveEquipmentBodyPart</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:equipmentDetections:ProtectiveEquipmentBodyPart'</a>,
--   <a>protectiveEquipmentBodyPart_equipmentDetections</a> - An array of
--   Personal Protective Equipment items detected around a body part.
--   
--   <a>$sel:confidence:ProtectiveEquipmentBodyPart'</a>,
--   <a>protectiveEquipmentBodyPart_confidence</a> - The confidence that
--   Amazon Rekognition has in the detection accuracy of the detected body
--   part.
--   
--   <a>$sel:name:ProtectiveEquipmentBodyPart'</a>,
--   <a>protectiveEquipmentBodyPart_name</a> - The detected body part.
newProtectiveEquipmentBodyPart :: ProtectiveEquipmentBodyPart

-- | An array of Personal Protective Equipment items detected around a body
--   part.
protectiveEquipmentBodyPart_equipmentDetections :: Lens' ProtectiveEquipmentBodyPart (Maybe [EquipmentDetection])

-- | The confidence that Amazon Rekognition has in the detection accuracy
--   of the detected body part.
protectiveEquipmentBodyPart_confidence :: Lens' ProtectiveEquipmentBodyPart (Maybe Double)

-- | The detected body part.
protectiveEquipmentBodyPart_name :: Lens' ProtectiveEquipmentBodyPart (Maybe BodyPart)

-- | A person detected by a call to DetectProtectiveEquipment. The API
--   returns all persons detected in the input image in an array of
--   <tt>ProtectiveEquipmentPerson</tt> objects.
--   
--   <i>See:</i> <a>newProtectiveEquipmentPerson</a> smart constructor.
data ProtectiveEquipmentPerson
ProtectiveEquipmentPerson' :: Maybe [ProtectiveEquipmentBodyPart] -> Maybe BoundingBox -> Maybe Double -> Maybe Natural -> ProtectiveEquipmentPerson

-- | An array of body parts detected on a person's body (including body
--   parts without PPE).
[$sel:bodyParts:ProtectiveEquipmentPerson'] :: ProtectiveEquipmentPerson -> Maybe [ProtectiveEquipmentBodyPart]

-- | A bounding box around the detected person.
[$sel:boundingBox:ProtectiveEquipmentPerson'] :: ProtectiveEquipmentPerson -> Maybe BoundingBox

-- | The confidence that Amazon Rekognition has that the bounding box
--   contains a person.
[$sel:confidence:ProtectiveEquipmentPerson'] :: ProtectiveEquipmentPerson -> Maybe Double

-- | The identifier for the detected person. The identifier is only unique
--   for a single call to <tt>DetectProtectiveEquipment</tt>.
[$sel:id:ProtectiveEquipmentPerson'] :: ProtectiveEquipmentPerson -> Maybe Natural

-- | Create a value of <a>ProtectiveEquipmentPerson</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:bodyParts:ProtectiveEquipmentPerson'</a>,
--   <a>protectiveEquipmentPerson_bodyParts</a> - An array of body parts
--   detected on a person's body (including body parts without PPE).
--   
--   <a>$sel:boundingBox:ProtectiveEquipmentPerson'</a>,
--   <a>protectiveEquipmentPerson_boundingBox</a> - A bounding box around
--   the detected person.
--   
--   <a>$sel:confidence:ProtectiveEquipmentPerson'</a>,
--   <a>protectiveEquipmentPerson_confidence</a> - The confidence that
--   Amazon Rekognition has that the bounding box contains a person.
--   
--   <a>$sel:id:ProtectiveEquipmentPerson'</a>,
--   <a>protectiveEquipmentPerson_id</a> - The identifier for the detected
--   person. The identifier is only unique for a single call to
--   <tt>DetectProtectiveEquipment</tt>.
newProtectiveEquipmentPerson :: ProtectiveEquipmentPerson

-- | An array of body parts detected on a person's body (including body
--   parts without PPE).
protectiveEquipmentPerson_bodyParts :: Lens' ProtectiveEquipmentPerson (Maybe [ProtectiveEquipmentBodyPart])

-- | A bounding box around the detected person.
protectiveEquipmentPerson_boundingBox :: Lens' ProtectiveEquipmentPerson (Maybe BoundingBox)

-- | The confidence that Amazon Rekognition has that the bounding box
--   contains a person.
protectiveEquipmentPerson_confidence :: Lens' ProtectiveEquipmentPerson (Maybe Double)

-- | The identifier for the detected person. The identifier is only unique
--   for a single call to <tt>DetectProtectiveEquipment</tt>.
protectiveEquipmentPerson_id :: Lens' ProtectiveEquipmentPerson (Maybe Natural)

-- | Specifies summary attributes to return from a call to
--   DetectProtectiveEquipment. You can specify which types of PPE to
--   summarize. You can also specify a minimum confidence value for
--   detections. Summary information is returned in the <tt>Summary</tt>
--   (ProtectiveEquipmentSummary) field of the response from
--   <tt>DetectProtectiveEquipment</tt>. The summary includes which persons
--   in an image were detected wearing the requested types of person
--   protective equipment (PPE), which persons were detected as not wearing
--   PPE, and the persons in which a determination could not be made. For
--   more information, see ProtectiveEquipmentSummary.
--   
--   <i>See:</i> <a>newProtectiveEquipmentSummarizationAttributes</a> smart
--   constructor.
data ProtectiveEquipmentSummarizationAttributes
ProtectiveEquipmentSummarizationAttributes' :: Double -> [ProtectiveEquipmentType] -> ProtectiveEquipmentSummarizationAttributes

-- | The minimum confidence level for which you want summary information.
--   The confidence level applies to person detection, body part detection,
--   equipment detection, and body part coverage. Amazon Rekognition
--   doesn't return summary information with a confidence than this
--   specified value. There isn't a default value.
--   
--   Specify a <tt>MinConfidence</tt> value that is between 50-100% as
--   <tt>DetectProtectiveEquipment</tt> returns predictions only where the
--   detection confidence is between 50% - 100%. If you specify a value
--   that is less than 50%, the results are the same specifying a value of
--   50%.
[$sel:minConfidence:ProtectiveEquipmentSummarizationAttributes'] :: ProtectiveEquipmentSummarizationAttributes -> Double

-- | An array of personal protective equipment types for which you want
--   summary information. If a person is detected wearing a required
--   requipment type, the person's ID is added to the
--   <tt>PersonsWithRequiredEquipment</tt> array field returned in
--   ProtectiveEquipmentSummary by <tt>DetectProtectiveEquipment</tt>.
[$sel:requiredEquipmentTypes:ProtectiveEquipmentSummarizationAttributes'] :: ProtectiveEquipmentSummarizationAttributes -> [ProtectiveEquipmentType]

-- | Create a value of <a>ProtectiveEquipmentSummarizationAttributes</a>
--   with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:minConfidence:ProtectiveEquipmentSummarizationAttributes'</a>,
--   <a>protectiveEquipmentSummarizationAttributes_minConfidence</a> - The
--   minimum confidence level for which you want summary information. The
--   confidence level applies to person detection, body part detection,
--   equipment detection, and body part coverage. Amazon Rekognition
--   doesn't return summary information with a confidence than this
--   specified value. There isn't a default value.
--   
--   Specify a <tt>MinConfidence</tt> value that is between 50-100% as
--   <tt>DetectProtectiveEquipment</tt> returns predictions only where the
--   detection confidence is between 50% - 100%. If you specify a value
--   that is less than 50%, the results are the same specifying a value of
--   50%.
--   
--   
--   <a>$sel:requiredEquipmentTypes:ProtectiveEquipmentSummarizationAttributes'</a>,
--   <a>protectiveEquipmentSummarizationAttributes_requiredEquipmentTypes</a>
--   - An array of personal protective equipment types for which you want
--   summary information. If a person is detected wearing a required
--   requipment type, the person's ID is added to the
--   <tt>PersonsWithRequiredEquipment</tt> array field returned in
--   ProtectiveEquipmentSummary by <tt>DetectProtectiveEquipment</tt>.
newProtectiveEquipmentSummarizationAttributes :: Double -> ProtectiveEquipmentSummarizationAttributes

-- | The minimum confidence level for which you want summary information.
--   The confidence level applies to person detection, body part detection,
--   equipment detection, and body part coverage. Amazon Rekognition
--   doesn't return summary information with a confidence than this
--   specified value. There isn't a default value.
--   
--   Specify a <tt>MinConfidence</tt> value that is between 50-100% as
--   <tt>DetectProtectiveEquipment</tt> returns predictions only where the
--   detection confidence is between 50% - 100%. If you specify a value
--   that is less than 50%, the results are the same specifying a value of
--   50%.
protectiveEquipmentSummarizationAttributes_minConfidence :: Lens' ProtectiveEquipmentSummarizationAttributes Double

-- | An array of personal protective equipment types for which you want
--   summary information. If a person is detected wearing a required
--   requipment type, the person's ID is added to the
--   <tt>PersonsWithRequiredEquipment</tt> array field returned in
--   ProtectiveEquipmentSummary by <tt>DetectProtectiveEquipment</tt>.
protectiveEquipmentSummarizationAttributes_requiredEquipmentTypes :: Lens' ProtectiveEquipmentSummarizationAttributes [ProtectiveEquipmentType]

-- | Summary information for required items of personal protective
--   equipment (PPE) detected on persons by a call to
--   DetectProtectiveEquipment. You specify the required type of PPE in the
--   <tt>SummarizationAttributes</tt>
--   (ProtectiveEquipmentSummarizationAttributes) input parameter. The
--   summary includes which persons were detected wearing the required
--   personal protective equipment (<tt>PersonsWithRequiredEquipment</tt>),
--   which persons were detected as not wearing the required PPE
--   (<tt>PersonsWithoutRequiredEquipment</tt>), and the persons in which a
--   determination could not be made (<tt>PersonsIndeterminate</tt>).
--   
--   To get a total for each category, use the size of the field array. For
--   example, to find out how many people were detected as wearing the
--   specified PPE, use the size of the
--   <tt>PersonsWithRequiredEquipment</tt> array. If you want to find out
--   more about a person, such as the location (BoundingBox) of the person
--   on the image, use the person ID in each array element. Each person ID
--   matches the ID field of a ProtectiveEquipmentPerson object returned in
--   the <tt>Persons</tt> array by <tt>DetectProtectiveEquipment</tt>.
--   
--   <i>See:</i> <a>newProtectiveEquipmentSummary</a> smart constructor.
data ProtectiveEquipmentSummary
ProtectiveEquipmentSummary' :: Maybe [Natural] -> Maybe [Natural] -> Maybe [Natural] -> ProtectiveEquipmentSummary

-- | An array of IDs for persons who are wearing detected personal
--   protective equipment.
[$sel:personsWithRequiredEquipment:ProtectiveEquipmentSummary'] :: ProtectiveEquipmentSummary -> Maybe [Natural]

-- | An array of IDs for persons who are not wearing all of the types of
--   PPE specified in the <tt>RequiredEquipmentTypes</tt> field of the
--   detected personal protective equipment.
[$sel:personsWithoutRequiredEquipment:ProtectiveEquipmentSummary'] :: ProtectiveEquipmentSummary -> Maybe [Natural]

-- | An array of IDs for persons where it was not possible to determine if
--   they are wearing personal protective equipment.
[$sel:personsIndeterminate:ProtectiveEquipmentSummary'] :: ProtectiveEquipmentSummary -> Maybe [Natural]

-- | Create a value of <a>ProtectiveEquipmentSummary</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:personsWithRequiredEquipment:ProtectiveEquipmentSummary'</a>,
--   <a>protectiveEquipmentSummary_personsWithRequiredEquipment</a> - An
--   array of IDs for persons who are wearing detected personal protective
--   equipment.
--   
--   
--   <a>$sel:personsWithoutRequiredEquipment:ProtectiveEquipmentSummary'</a>,
--   <a>protectiveEquipmentSummary_personsWithoutRequiredEquipment</a> - An
--   array of IDs for persons who are not wearing all of the types of PPE
--   specified in the <tt>RequiredEquipmentTypes</tt> field of the detected
--   personal protective equipment.
--   
--   <a>$sel:personsIndeterminate:ProtectiveEquipmentSummary'</a>,
--   <a>protectiveEquipmentSummary_personsIndeterminate</a> - An array of
--   IDs for persons where it was not possible to determine if they are
--   wearing personal protective equipment.
newProtectiveEquipmentSummary :: ProtectiveEquipmentSummary

-- | An array of IDs for persons who are wearing detected personal
--   protective equipment.
protectiveEquipmentSummary_personsWithRequiredEquipment :: Lens' ProtectiveEquipmentSummary (Maybe [Natural])

-- | An array of IDs for persons who are not wearing all of the types of
--   PPE specified in the <tt>RequiredEquipmentTypes</tt> field of the
--   detected personal protective equipment.
protectiveEquipmentSummary_personsWithoutRequiredEquipment :: Lens' ProtectiveEquipmentSummary (Maybe [Natural])

-- | An array of IDs for persons where it was not possible to determine if
--   they are wearing personal protective equipment.
protectiveEquipmentSummary_personsIndeterminate :: Lens' ProtectiveEquipmentSummary (Maybe [Natural])

-- | Specifies a location within the frame that Rekognition checks for
--   text. Uses a <tt>BoundingBox</tt> object to set a region of the
--   screen.
--   
--   A word is included in the region if the word is more than half in that
--   region. If there is more than one region, the word will be compared
--   with all regions of the screen. Any word more than half in a region is
--   kept in the results.
--   
--   <i>See:</i> <a>newRegionOfInterest</a> smart constructor.
data RegionOfInterest
RegionOfInterest' :: Maybe BoundingBox -> RegionOfInterest

-- | The box representing a region of interest on screen.
[$sel:boundingBox:RegionOfInterest'] :: RegionOfInterest -> Maybe BoundingBox

-- | Create a value of <a>RegionOfInterest</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:boundingBox:RegionOfInterest'</a>,
--   <a>regionOfInterest_boundingBox</a> - The box representing a region of
--   interest on screen.
newRegionOfInterest :: RegionOfInterest

-- | The box representing a region of interest on screen.
regionOfInterest_boundingBox :: Lens' RegionOfInterest (Maybe BoundingBox)

-- | Provides the S3 bucket name and object name.
--   
--   The region for the S3 bucket containing the S3 object must match the
--   region you use for Amazon Rekognition operations.
--   
--   For Amazon Rekognition to process an S3 object, the user must have
--   permission to access the S3 object. For more information, see
--   Resource-Based Policies in the Amazon Rekognition Developer Guide.
--   
--   <i>See:</i> <a>newS3Object</a> smart constructor.
data S3Object
S3Object' :: Maybe Text -> Maybe Text -> Maybe Text -> S3Object

-- | Name of the S3 bucket.
[$sel:bucket:S3Object'] :: S3Object -> Maybe Text

-- | S3 object key name.
[$sel:name:S3Object'] :: S3Object -> Maybe Text

-- | If the bucket is versioning enabled, you can specify the object
--   version.
[$sel:version:S3Object'] :: S3Object -> Maybe Text

-- | Create a value of <a>S3Object</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:bucket:S3Object'</a>, <a>s3Object_bucket</a> - Name of the S3
--   bucket.
--   
--   <a>$sel:name:S3Object'</a>, <a>s3Object_name</a> - S3 object key name.
--   
--   <a>$sel:version:S3Object'</a>, <a>s3Object_version</a> - If the bucket
--   is versioning enabled, you can specify the object version.
newS3Object :: S3Object

-- | Name of the S3 bucket.
s3Object_bucket :: Lens' S3Object (Maybe Text)

-- | S3 object key name.
s3Object_name :: Lens' S3Object (Maybe Text)

-- | If the bucket is versioning enabled, you can specify the object
--   version.
s3Object_version :: Lens' S3Object (Maybe Text)

-- | A technical cue or shot detection segment detected in a video. An
--   array of <tt>SegmentDetection</tt> objects containing all segments
--   detected in a stored video is returned by GetSegmentDetection.
--   
--   <i>See:</i> <a>newSegmentDetection</a> smart constructor.
data SegmentDetection
SegmentDetection' :: Maybe TechnicalCueSegment -> Maybe Natural -> Maybe Text -> Maybe Integer -> Maybe Text -> Maybe Text -> Maybe Natural -> Maybe Natural -> Maybe Integer -> Maybe SegmentType -> Maybe ShotSegment -> Maybe Natural -> SegmentDetection

-- | If the segment is a technical cue, contains information about the
--   technical cue.
[$sel:technicalCueSegment:SegmentDetection'] :: SegmentDetection -> Maybe TechnicalCueSegment

-- | The frame number at the end of a video segment, using a frame index
--   that starts with 0.
[$sel:endFrameNumber:SegmentDetection'] :: SegmentDetection -> Maybe Natural

-- | The duration of the timecode for the detected segment in SMPTE format.
[$sel:durationSMPTE:SegmentDetection'] :: SegmentDetection -> Maybe Text

-- | The end time of the detected segment, in milliseconds, from the start
--   of the video. This value is rounded down.
[$sel:endTimestampMillis:SegmentDetection'] :: SegmentDetection -> Maybe Integer

-- | The frame-accurate SMPTE timecode, from the start of a video, for the
--   start of a detected segment. <tt>StartTimecode</tt> is in
--   <i>HH:MM:SS:fr</i> format (and <i>;fr</i> for drop frame-rates).
[$sel:startTimecodeSMPTE:SegmentDetection'] :: SegmentDetection -> Maybe Text

-- | The frame-accurate SMPTE timecode, from the start of a video, for the
--   end of a detected segment. <tt>EndTimecode</tt> is in
--   <i>HH:MM:SS:fr</i> format (and <i>;fr</i> for drop frame-rates).
[$sel:endTimecodeSMPTE:SegmentDetection'] :: SegmentDetection -> Maybe Text

-- | The duration of the detected segment in milliseconds.
[$sel:durationMillis:SegmentDetection'] :: SegmentDetection -> Maybe Natural

-- | The duration of a video segment, expressed in frames.
[$sel:durationFrames:SegmentDetection'] :: SegmentDetection -> Maybe Natural

-- | The start time of the detected segment in milliseconds from the start
--   of the video. This value is rounded down. For example, if the actual
--   timestamp is 100.6667 milliseconds, Amazon Rekognition Video returns a
--   value of 100 millis.
[$sel:startTimestampMillis:SegmentDetection'] :: SegmentDetection -> Maybe Integer

-- | The type of the segment. Valid values are <tt>TECHNICAL_CUE</tt> and
--   <tt>SHOT</tt>.
[$sel:type':SegmentDetection'] :: SegmentDetection -> Maybe SegmentType

-- | If the segment is a shot detection, contains information about the
--   shot detection.
[$sel:shotSegment:SegmentDetection'] :: SegmentDetection -> Maybe ShotSegment

-- | The frame number of the start of a video segment, using a frame index
--   that starts with 0.
[$sel:startFrameNumber:SegmentDetection'] :: SegmentDetection -> Maybe Natural

-- | Create a value of <a>SegmentDetection</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:technicalCueSegment:SegmentDetection'</a>,
--   <a>segmentDetection_technicalCueSegment</a> - If the segment is a
--   technical cue, contains information about the technical cue.
--   
--   <a>$sel:endFrameNumber:SegmentDetection'</a>,
--   <a>segmentDetection_endFrameNumber</a> - The frame number at the end
--   of a video segment, using a frame index that starts with 0.
--   
--   <a>$sel:durationSMPTE:SegmentDetection'</a>,
--   <a>segmentDetection_durationSMPTE</a> - The duration of the timecode
--   for the detected segment in SMPTE format.
--   
--   <a>$sel:endTimestampMillis:SegmentDetection'</a>,
--   <a>segmentDetection_endTimestampMillis</a> - The end time of the
--   detected segment, in milliseconds, from the start of the video. This
--   value is rounded down.
--   
--   <a>$sel:startTimecodeSMPTE:SegmentDetection'</a>,
--   <a>segmentDetection_startTimecodeSMPTE</a> - The frame-accurate SMPTE
--   timecode, from the start of a video, for the start of a detected
--   segment. <tt>StartTimecode</tt> is in <i>HH:MM:SS:fr</i> format (and
--   <i>;fr</i> for drop frame-rates).
--   
--   <a>$sel:endTimecodeSMPTE:SegmentDetection'</a>,
--   <a>segmentDetection_endTimecodeSMPTE</a> - The frame-accurate SMPTE
--   timecode, from the start of a video, for the end of a detected
--   segment. <tt>EndTimecode</tt> is in <i>HH:MM:SS:fr</i> format (and
--   <i>;fr</i> for drop frame-rates).
--   
--   <a>$sel:durationMillis:SegmentDetection'</a>,
--   <a>segmentDetection_durationMillis</a> - The duration of the detected
--   segment in milliseconds.
--   
--   <a>$sel:durationFrames:SegmentDetection'</a>,
--   <a>segmentDetection_durationFrames</a> - The duration of a video
--   segment, expressed in frames.
--   
--   <a>$sel:startTimestampMillis:SegmentDetection'</a>,
--   <a>segmentDetection_startTimestampMillis</a> - The start time of the
--   detected segment in milliseconds from the start of the video. This
--   value is rounded down. For example, if the actual timestamp is
--   100.6667 milliseconds, Amazon Rekognition Video returns a value of 100
--   millis.
--   
--   <a>$sel:type':SegmentDetection'</a>, <a>segmentDetection_type</a> -
--   The type of the segment. Valid values are <tt>TECHNICAL_CUE</tt> and
--   <tt>SHOT</tt>.
--   
--   <a>$sel:shotSegment:SegmentDetection'</a>,
--   <a>segmentDetection_shotSegment</a> - If the segment is a shot
--   detection, contains information about the shot detection.
--   
--   <a>$sel:startFrameNumber:SegmentDetection'</a>,
--   <a>segmentDetection_startFrameNumber</a> - The frame number of the
--   start of a video segment, using a frame index that starts with 0.
newSegmentDetection :: SegmentDetection

-- | If the segment is a technical cue, contains information about the
--   technical cue.
segmentDetection_technicalCueSegment :: Lens' SegmentDetection (Maybe TechnicalCueSegment)

-- | The frame number at the end of a video segment, using a frame index
--   that starts with 0.
segmentDetection_endFrameNumber :: Lens' SegmentDetection (Maybe Natural)

-- | The duration of the timecode for the detected segment in SMPTE format.
segmentDetection_durationSMPTE :: Lens' SegmentDetection (Maybe Text)

-- | The end time of the detected segment, in milliseconds, from the start
--   of the video. This value is rounded down.
segmentDetection_endTimestampMillis :: Lens' SegmentDetection (Maybe Integer)

-- | The frame-accurate SMPTE timecode, from the start of a video, for the
--   start of a detected segment. <tt>StartTimecode</tt> is in
--   <i>HH:MM:SS:fr</i> format (and <i>;fr</i> for drop frame-rates).
segmentDetection_startTimecodeSMPTE :: Lens' SegmentDetection (Maybe Text)

-- | The frame-accurate SMPTE timecode, from the start of a video, for the
--   end of a detected segment. <tt>EndTimecode</tt> is in
--   <i>HH:MM:SS:fr</i> format (and <i>;fr</i> for drop frame-rates).
segmentDetection_endTimecodeSMPTE :: Lens' SegmentDetection (Maybe Text)

-- | The duration of the detected segment in milliseconds.
segmentDetection_durationMillis :: Lens' SegmentDetection (Maybe Natural)

-- | The duration of a video segment, expressed in frames.
segmentDetection_durationFrames :: Lens' SegmentDetection (Maybe Natural)

-- | The start time of the detected segment in milliseconds from the start
--   of the video. This value is rounded down. For example, if the actual
--   timestamp is 100.6667 milliseconds, Amazon Rekognition Video returns a
--   value of 100 millis.
segmentDetection_startTimestampMillis :: Lens' SegmentDetection (Maybe Integer)

-- | The type of the segment. Valid values are <tt>TECHNICAL_CUE</tt> and
--   <tt>SHOT</tt>.
segmentDetection_type :: Lens' SegmentDetection (Maybe SegmentType)

-- | If the segment is a shot detection, contains information about the
--   shot detection.
segmentDetection_shotSegment :: Lens' SegmentDetection (Maybe ShotSegment)

-- | The frame number of the start of a video segment, using a frame index
--   that starts with 0.
segmentDetection_startFrameNumber :: Lens' SegmentDetection (Maybe Natural)

-- | Information about the type of a segment requested in a call to
--   StartSegmentDetection. An array of <tt>SegmentTypeInfo</tt> objects is
--   returned by the response from GetSegmentDetection.
--   
--   <i>See:</i> <a>newSegmentTypeInfo</a> smart constructor.
data SegmentTypeInfo
SegmentTypeInfo' :: Maybe Text -> Maybe SegmentType -> SegmentTypeInfo

-- | The version of the model used to detect segments.
[$sel:modelVersion:SegmentTypeInfo'] :: SegmentTypeInfo -> Maybe Text

-- | The type of a segment (technical cue or shot detection).
[$sel:type':SegmentTypeInfo'] :: SegmentTypeInfo -> Maybe SegmentType

-- | Create a value of <a>SegmentTypeInfo</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:modelVersion:SegmentTypeInfo'</a>,
--   <a>segmentTypeInfo_modelVersion</a> - The version of the model used to
--   detect segments.
--   
--   <a>$sel:type':SegmentTypeInfo'</a>, <a>segmentTypeInfo_type</a> - The
--   type of a segment (technical cue or shot detection).
newSegmentTypeInfo :: SegmentTypeInfo

-- | The version of the model used to detect segments.
segmentTypeInfo_modelVersion :: Lens' SegmentTypeInfo (Maybe Text)

-- | The type of a segment (technical cue or shot detection).
segmentTypeInfo_type :: Lens' SegmentTypeInfo (Maybe SegmentType)

-- | Information about a shot detection segment detected in a video. For
--   more information, see SegmentDetection.
--   
--   <i>See:</i> <a>newShotSegment</a> smart constructor.
data ShotSegment
ShotSegment' :: Maybe Double -> Maybe Natural -> ShotSegment

-- | The confidence that Amazon Rekognition Video has in the accuracy of
--   the detected segment.
[$sel:confidence:ShotSegment'] :: ShotSegment -> Maybe Double

-- | An Identifier for a shot detection segment detected in a video.
[$sel:index:ShotSegment'] :: ShotSegment -> Maybe Natural

-- | Create a value of <a>ShotSegment</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:confidence:ShotSegment'</a>, <a>shotSegment_confidence</a> -
--   The confidence that Amazon Rekognition Video has in the accuracy of
--   the detected segment.
--   
--   <a>$sel:index:ShotSegment'</a>, <a>shotSegment_index</a> - An
--   Identifier for a shot detection segment detected in a video.
newShotSegment :: ShotSegment

-- | The confidence that Amazon Rekognition Video has in the accuracy of
--   the detected segment.
shotSegment_confidence :: Lens' ShotSegment (Maybe Double)

-- | An Identifier for a shot detection segment detected in a video.
shotSegment_index :: Lens' ShotSegment (Maybe Natural)

-- | Indicates whether or not the face is smiling, and the confidence level
--   in the determination.
--   
--   <i>See:</i> <a>newSmile</a> smart constructor.
data Smile
Smile' :: Maybe Bool -> Maybe Double -> Smile

-- | Boolean value that indicates whether the face is smiling or not.
[$sel:value:Smile'] :: Smile -> Maybe Bool

-- | Level of confidence in the determination.
[$sel:confidence:Smile'] :: Smile -> Maybe Double

-- | Create a value of <a>Smile</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:value:Smile'</a>, <a>smile_value</a> - Boolean value that
--   indicates whether the face is smiling or not.
--   
--   <a>$sel:confidence:Smile'</a>, <a>smile_confidence</a> - Level of
--   confidence in the determination.
newSmile :: Smile

-- | Boolean value that indicates whether the face is smiling or not.
smile_value :: Lens' Smile (Maybe Bool)

-- | Level of confidence in the determination.
smile_confidence :: Lens' Smile (Maybe Double)

-- | Filters applied to the technical cue or shot detection segments. For
--   more information, see StartSegmentDetection.
--   
--   <i>See:</i> <a>newStartSegmentDetectionFilters</a> smart constructor.
data StartSegmentDetectionFilters
StartSegmentDetectionFilters' :: Maybe StartTechnicalCueDetectionFilter -> Maybe StartShotDetectionFilter -> StartSegmentDetectionFilters

-- | Filters that are specific to technical cues.
[$sel:technicalCueFilter:StartSegmentDetectionFilters'] :: StartSegmentDetectionFilters -> Maybe StartTechnicalCueDetectionFilter

-- | Filters that are specific to shot detections.
[$sel:shotFilter:StartSegmentDetectionFilters'] :: StartSegmentDetectionFilters -> Maybe StartShotDetectionFilter

-- | Create a value of <a>StartSegmentDetectionFilters</a> with all
--   optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:technicalCueFilter:StartSegmentDetectionFilters'</a>,
--   <a>startSegmentDetectionFilters_technicalCueFilter</a> - Filters that
--   are specific to technical cues.
--   
--   <a>$sel:shotFilter:StartSegmentDetectionFilters'</a>,
--   <a>startSegmentDetectionFilters_shotFilter</a> - Filters that are
--   specific to shot detections.
newStartSegmentDetectionFilters :: StartSegmentDetectionFilters

-- | Filters that are specific to technical cues.
startSegmentDetectionFilters_technicalCueFilter :: Lens' StartSegmentDetectionFilters (Maybe StartTechnicalCueDetectionFilter)

-- | Filters that are specific to shot detections.
startSegmentDetectionFilters_shotFilter :: Lens' StartSegmentDetectionFilters (Maybe StartShotDetectionFilter)

-- | Filters for the shot detection segments returned by
--   <tt>GetSegmentDetection</tt>. For more information, see
--   StartSegmentDetectionFilters.
--   
--   <i>See:</i> <a>newStartShotDetectionFilter</a> smart constructor.
data StartShotDetectionFilter
StartShotDetectionFilter' :: Maybe Double -> StartShotDetectionFilter

-- | Specifies the minimum confidence that Amazon Rekognition Video must
--   have in order to return a detected segment. Confidence represents how
--   certain Amazon Rekognition is that a segment is correctly identified.
--   0 is the lowest confidence. 100 is the highest confidence. Amazon
--   Rekognition Video doesn't return any segments with a confidence level
--   lower than this specified value.
--   
--   If you don't specify <tt>MinSegmentConfidence</tt>, the
--   <tt>GetSegmentDetection</tt> returns segments with confidence values
--   greater than or equal to 50 percent.
[$sel:minSegmentConfidence:StartShotDetectionFilter'] :: StartShotDetectionFilter -> Maybe Double

-- | Create a value of <a>StartShotDetectionFilter</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:minSegmentConfidence:StartShotDetectionFilter'</a>,
--   <a>startShotDetectionFilter_minSegmentConfidence</a> - Specifies the
--   minimum confidence that Amazon Rekognition Video must have in order to
--   return a detected segment. Confidence represents how certain Amazon
--   Rekognition is that a segment is correctly identified. 0 is the lowest
--   confidence. 100 is the highest confidence. Amazon Rekognition Video
--   doesn't return any segments with a confidence level lower than this
--   specified value.
--   
--   If you don't specify <tt>MinSegmentConfidence</tt>, the
--   <tt>GetSegmentDetection</tt> returns segments with confidence values
--   greater than or equal to 50 percent.
newStartShotDetectionFilter :: StartShotDetectionFilter

-- | Specifies the minimum confidence that Amazon Rekognition Video must
--   have in order to return a detected segment. Confidence represents how
--   certain Amazon Rekognition is that a segment is correctly identified.
--   0 is the lowest confidence. 100 is the highest confidence. Amazon
--   Rekognition Video doesn't return any segments with a confidence level
--   lower than this specified value.
--   
--   If you don't specify <tt>MinSegmentConfidence</tt>, the
--   <tt>GetSegmentDetection</tt> returns segments with confidence values
--   greater than or equal to 50 percent.
startShotDetectionFilter_minSegmentConfidence :: Lens' StartShotDetectionFilter (Maybe Double)

-- | Filters for the technical segments returned by GetSegmentDetection.
--   For more information, see StartSegmentDetectionFilters.
--   
--   <i>See:</i> <a>newStartTechnicalCueDetectionFilter</a> smart
--   constructor.
data StartTechnicalCueDetectionFilter
StartTechnicalCueDetectionFilter' :: Maybe BlackFrame -> Maybe Double -> StartTechnicalCueDetectionFilter

-- | A filter that allows you to control the black frame detection by
--   specifying the black levels and pixel coverage of black pixels in a
--   frame. Videos can come from multiple sources, formats, and time
--   periods, with different standards and varying noise levels for black
--   frames that need to be accounted for.
[$sel:blackFrame:StartTechnicalCueDetectionFilter'] :: StartTechnicalCueDetectionFilter -> Maybe BlackFrame

-- | Specifies the minimum confidence that Amazon Rekognition Video must
--   have in order to return a detected segment. Confidence represents how
--   certain Amazon Rekognition is that a segment is correctly identified.
--   0 is the lowest confidence. 100 is the highest confidence. Amazon
--   Rekognition Video doesn't return any segments with a confidence level
--   lower than this specified value.
--   
--   If you don't specify <tt>MinSegmentConfidence</tt>,
--   <tt>GetSegmentDetection</tt> returns segments with confidence values
--   greater than or equal to 50 percent.
[$sel:minSegmentConfidence:StartTechnicalCueDetectionFilter'] :: StartTechnicalCueDetectionFilter -> Maybe Double

-- | Create a value of <a>StartTechnicalCueDetectionFilter</a> with all
--   optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:blackFrame:StartTechnicalCueDetectionFilter'</a>,
--   <a>startTechnicalCueDetectionFilter_blackFrame</a> - A filter that
--   allows you to control the black frame detection by specifying the
--   black levels and pixel coverage of black pixels in a frame. Videos can
--   come from multiple sources, formats, and time periods, with different
--   standards and varying noise levels for black frames that need to be
--   accounted for.
--   
--   <a>$sel:minSegmentConfidence:StartTechnicalCueDetectionFilter'</a>,
--   <a>startTechnicalCueDetectionFilter_minSegmentConfidence</a> -
--   Specifies the minimum confidence that Amazon Rekognition Video must
--   have in order to return a detected segment. Confidence represents how
--   certain Amazon Rekognition is that a segment is correctly identified.
--   0 is the lowest confidence. 100 is the highest confidence. Amazon
--   Rekognition Video doesn't return any segments with a confidence level
--   lower than this specified value.
--   
--   If you don't specify <tt>MinSegmentConfidence</tt>,
--   <tt>GetSegmentDetection</tt> returns segments with confidence values
--   greater than or equal to 50 percent.
newStartTechnicalCueDetectionFilter :: StartTechnicalCueDetectionFilter

-- | A filter that allows you to control the black frame detection by
--   specifying the black levels and pixel coverage of black pixels in a
--   frame. Videos can come from multiple sources, formats, and time
--   periods, with different standards and varying noise levels for black
--   frames that need to be accounted for.
startTechnicalCueDetectionFilter_blackFrame :: Lens' StartTechnicalCueDetectionFilter (Maybe BlackFrame)

-- | Specifies the minimum confidence that Amazon Rekognition Video must
--   have in order to return a detected segment. Confidence represents how
--   certain Amazon Rekognition is that a segment is correctly identified.
--   0 is the lowest confidence. 100 is the highest confidence. Amazon
--   Rekognition Video doesn't return any segments with a confidence level
--   lower than this specified value.
--   
--   If you don't specify <tt>MinSegmentConfidence</tt>,
--   <tt>GetSegmentDetection</tt> returns segments with confidence values
--   greater than or equal to 50 percent.
startTechnicalCueDetectionFilter_minSegmentConfidence :: Lens' StartTechnicalCueDetectionFilter (Maybe Double)

-- | Set of optional parameters that let you set the criteria text must
--   meet to be included in your response. <tt>WordFilter</tt> looks at a
--   word's height, width and minimum confidence. <tt>RegionOfInterest</tt>
--   lets you set a specific region of the screen to look for text in.
--   
--   <i>See:</i> <a>newStartTextDetectionFilters</a> smart constructor.
data StartTextDetectionFilters
StartTextDetectionFilters' :: Maybe [RegionOfInterest] -> Maybe DetectionFilter -> StartTextDetectionFilters

-- | Filter focusing on a certain area of the frame. Uses a
--   <tt>BoundingBox</tt> object to set the region of the screen.
[$sel:regionsOfInterest:StartTextDetectionFilters'] :: StartTextDetectionFilters -> Maybe [RegionOfInterest]

-- | Filters focusing on qualities of the text, such as confidence or size.
[$sel:wordFilter:StartTextDetectionFilters'] :: StartTextDetectionFilters -> Maybe DetectionFilter

-- | Create a value of <a>StartTextDetectionFilters</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:regionsOfInterest:StartTextDetectionFilters'</a>,
--   <a>startTextDetectionFilters_regionsOfInterest</a> - Filter focusing
--   on a certain area of the frame. Uses a <tt>BoundingBox</tt> object to
--   set the region of the screen.
--   
--   <a>$sel:wordFilter:StartTextDetectionFilters'</a>,
--   <a>startTextDetectionFilters_wordFilter</a> - Filters focusing on
--   qualities of the text, such as confidence or size.
newStartTextDetectionFilters :: StartTextDetectionFilters

-- | Filter focusing on a certain area of the frame. Uses a
--   <tt>BoundingBox</tt> object to set the region of the screen.
startTextDetectionFilters_regionsOfInterest :: Lens' StartTextDetectionFilters (Maybe [RegionOfInterest])

-- | Filters focusing on qualities of the text, such as confidence or size.
startTextDetectionFilters_wordFilter :: Lens' StartTextDetectionFilters (Maybe DetectionFilter)

-- | An object that recognizes faces in a streaming video. An Amazon
--   Rekognition stream processor is created by a call to
--   CreateStreamProcessor. The request parameters for
--   <tt>CreateStreamProcessor</tt> describe the Kinesis video stream
--   source for the streaming video, face recognition parameters, and where
--   to stream the analysis resullts.
--   
--   <i>See:</i> <a>newStreamProcessor</a> smart constructor.
data StreamProcessor
StreamProcessor' :: Maybe StreamProcessorStatus -> Maybe Text -> StreamProcessor

-- | Current status of the Amazon Rekognition stream processor.
[$sel:status:StreamProcessor'] :: StreamProcessor -> Maybe StreamProcessorStatus

-- | Name of the Amazon Rekognition stream processor.
[$sel:name:StreamProcessor'] :: StreamProcessor -> Maybe Text

-- | Create a value of <a>StreamProcessor</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:status:StreamProcessor'</a>, <a>streamProcessor_status</a> -
--   Current status of the Amazon Rekognition stream processor.
--   
--   <a>$sel:name:StreamProcessor'</a>, <a>streamProcessor_name</a> - Name
--   of the Amazon Rekognition stream processor.
newStreamProcessor :: StreamProcessor

-- | Current status of the Amazon Rekognition stream processor.
streamProcessor_status :: Lens' StreamProcessor (Maybe StreamProcessorStatus)

-- | Name of the Amazon Rekognition stream processor.
streamProcessor_name :: Lens' StreamProcessor (Maybe Text)

-- | Information about the source streaming video.
--   
--   <i>See:</i> <a>newStreamProcessorInput</a> smart constructor.
data StreamProcessorInput
StreamProcessorInput' :: Maybe KinesisVideoStream -> StreamProcessorInput

-- | The Kinesis video stream input stream for the source streaming video.
[$sel:kinesisVideoStream:StreamProcessorInput'] :: StreamProcessorInput -> Maybe KinesisVideoStream

-- | Create a value of <a>StreamProcessorInput</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:kinesisVideoStream:StreamProcessorInput'</a>,
--   <a>streamProcessorInput_kinesisVideoStream</a> - The Kinesis video
--   stream input stream for the source streaming video.
newStreamProcessorInput :: StreamProcessorInput

-- | The Kinesis video stream input stream for the source streaming video.
streamProcessorInput_kinesisVideoStream :: Lens' StreamProcessorInput (Maybe KinesisVideoStream)

-- | Information about the Amazon Kinesis Data Streams stream to which a
--   Amazon Rekognition Video stream processor streams the results of a
--   video analysis. For more information, see CreateStreamProcessor in the
--   Amazon Rekognition Developer Guide.
--   
--   <i>See:</i> <a>newStreamProcessorOutput</a> smart constructor.
data StreamProcessorOutput
StreamProcessorOutput' :: Maybe KinesisDataStream -> StreamProcessorOutput

-- | The Amazon Kinesis Data Streams stream to which the Amazon Rekognition
--   stream processor streams the analysis results.
[$sel:kinesisDataStream:StreamProcessorOutput'] :: StreamProcessorOutput -> Maybe KinesisDataStream

-- | Create a value of <a>StreamProcessorOutput</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:kinesisDataStream:StreamProcessorOutput'</a>,
--   <a>streamProcessorOutput_kinesisDataStream</a> - The Amazon Kinesis
--   Data Streams stream to which the Amazon Rekognition stream processor
--   streams the analysis results.
newStreamProcessorOutput :: StreamProcessorOutput

-- | The Amazon Kinesis Data Streams stream to which the Amazon Rekognition
--   stream processor streams the analysis results.
streamProcessorOutput_kinesisDataStream :: Lens' StreamProcessorOutput (Maybe KinesisDataStream)

-- | Input parameters used to recognize faces in a streaming video analyzed
--   by a Amazon Rekognition stream processor.
--   
--   <i>See:</i> <a>newStreamProcessorSettings</a> smart constructor.
data StreamProcessorSettings
StreamProcessorSettings' :: Maybe FaceSearchSettings -> StreamProcessorSettings

-- | Face search settings to use on a streaming video.
[$sel:faceSearch:StreamProcessorSettings'] :: StreamProcessorSettings -> Maybe FaceSearchSettings

-- | Create a value of <a>StreamProcessorSettings</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:faceSearch:StreamProcessorSettings'</a>,
--   <a>streamProcessorSettings_faceSearch</a> - Face search settings to
--   use on a streaming video.
newStreamProcessorSettings :: StreamProcessorSettings

-- | Face search settings to use on a streaming video.
streamProcessorSettings_faceSearch :: Lens' StreamProcessorSettings (Maybe FaceSearchSettings)

-- | The S3 bucket that contains the training summary. The training summary
--   includes aggregated evaluation metrics for the entire testing dataset
--   and metrics for each individual label.
--   
--   You get the training summary S3 bucket location by calling
--   DescribeProjectVersions.
--   
--   <i>See:</i> <a>newSummary</a> smart constructor.
data Summary
Summary' :: Maybe S3Object -> Summary
[$sel:s3Object:Summary'] :: Summary -> Maybe S3Object

-- | Create a value of <a>Summary</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:s3Object:Summary'</a>, <a>summary_s3Object</a> - Undocumented
--   member.
newSummary :: Summary

-- | Undocumented member.
summary_s3Object :: Lens' Summary (Maybe S3Object)

-- | Indicates whether or not the face is wearing sunglasses, and the
--   confidence level in the determination.
--   
--   <i>See:</i> <a>newSunglasses</a> smart constructor.
data Sunglasses
Sunglasses' :: Maybe Bool -> Maybe Double -> Sunglasses

-- | Boolean value that indicates whether the face is wearing sunglasses or
--   not.
[$sel:value:Sunglasses'] :: Sunglasses -> Maybe Bool

-- | Level of confidence in the determination.
[$sel:confidence:Sunglasses'] :: Sunglasses -> Maybe Double

-- | Create a value of <a>Sunglasses</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:value:Sunglasses'</a>, <a>sunglasses_value</a> - Boolean value
--   that indicates whether the face is wearing sunglasses or not.
--   
--   <a>$sel:confidence:Sunglasses'</a>, <a>sunglasses_confidence</a> -
--   Level of confidence in the determination.
newSunglasses :: Sunglasses

-- | Boolean value that indicates whether the face is wearing sunglasses or
--   not.
sunglasses_value :: Lens' Sunglasses (Maybe Bool)

-- | Level of confidence in the determination.
sunglasses_confidence :: Lens' Sunglasses (Maybe Double)

-- | Information about a technical cue segment. For more information, see
--   SegmentDetection.
--   
--   <i>See:</i> <a>newTechnicalCueSegment</a> smart constructor.
data TechnicalCueSegment
TechnicalCueSegment' :: Maybe Double -> Maybe TechnicalCueType -> TechnicalCueSegment

-- | The confidence that Amazon Rekognition Video has in the accuracy of
--   the detected segment.
[$sel:confidence:TechnicalCueSegment'] :: TechnicalCueSegment -> Maybe Double

-- | The type of the technical cue.
[$sel:type':TechnicalCueSegment'] :: TechnicalCueSegment -> Maybe TechnicalCueType

-- | Create a value of <a>TechnicalCueSegment</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:confidence:TechnicalCueSegment'</a>,
--   <a>technicalCueSegment_confidence</a> - The confidence that Amazon
--   Rekognition Video has in the accuracy of the detected segment.
--   
--   <a>$sel:type':TechnicalCueSegment'</a>,
--   <a>technicalCueSegment_type</a> - The type of the technical cue.
newTechnicalCueSegment :: TechnicalCueSegment

-- | The confidence that Amazon Rekognition Video has in the accuracy of
--   the detected segment.
technicalCueSegment_confidence :: Lens' TechnicalCueSegment (Maybe Double)

-- | The type of the technical cue.
technicalCueSegment_type :: Lens' TechnicalCueSegment (Maybe TechnicalCueType)

-- | The dataset used for testing. Optionally, if <tt>AutoCreate</tt> is
--   set, Amazon Rekognition Custom Labels creates a testing dataset using
--   an 80/20 split of the training dataset.
--   
--   <i>See:</i> <a>newTestingData</a> smart constructor.
data TestingData
TestingData' :: Maybe [Asset] -> Maybe Bool -> TestingData

-- | The assets used for testing.
[$sel:assets:TestingData'] :: TestingData -> Maybe [Asset]

-- | If specified, Amazon Rekognition Custom Labels creates a testing
--   dataset with an 80/20 split of the training dataset.
[$sel:autoCreate:TestingData'] :: TestingData -> Maybe Bool

-- | Create a value of <a>TestingData</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:assets:TestingData'</a>, <a>testingData_assets</a> - The
--   assets used for testing.
--   
--   <a>$sel:autoCreate:TestingData'</a>, <a>testingData_autoCreate</a> -
--   If specified, Amazon Rekognition Custom Labels creates a testing
--   dataset with an 80/20 split of the training dataset.
newTestingData :: TestingData

-- | The assets used for testing.
testingData_assets :: Lens' TestingData (Maybe [Asset])

-- | If specified, Amazon Rekognition Custom Labels creates a testing
--   dataset with an 80/20 split of the training dataset.
testingData_autoCreate :: Lens' TestingData (Maybe Bool)

-- | Sagemaker Groundtruth format manifest files for the input, output and
--   validation datasets that are used and created during testing.
--   
--   <i>See:</i> <a>newTestingDataResult</a> smart constructor.
data TestingDataResult
TestingDataResult' :: Maybe TestingData -> Maybe TestingData -> Maybe ValidationData -> TestingDataResult

-- | The testing dataset that was supplied for training.
[$sel:input:TestingDataResult'] :: TestingDataResult -> Maybe TestingData

-- | The subset of the dataset that was actually tested. Some images
--   (assets) might not be tested due to file formatting and other issues.
[$sel:output:TestingDataResult'] :: TestingDataResult -> Maybe TestingData

-- | The location of the data validation manifest. The data validation
--   manifest is created for the test dataset during model training.
[$sel:validation:TestingDataResult'] :: TestingDataResult -> Maybe ValidationData

-- | Create a value of <a>TestingDataResult</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:input:TestingDataResult'</a>, <a>testingDataResult_input</a> -
--   The testing dataset that was supplied for training.
--   
--   <a>$sel:output:TestingDataResult'</a>, <a>testingDataResult_output</a>
--   - The subset of the dataset that was actually tested. Some images
--   (assets) might not be tested due to file formatting and other issues.
--   
--   <a>$sel:validation:TestingDataResult'</a>,
--   <a>testingDataResult_validation</a> - The location of the data
--   validation manifest. The data validation manifest is created for the
--   test dataset during model training.
newTestingDataResult :: TestingDataResult

-- | The testing dataset that was supplied for training.
testingDataResult_input :: Lens' TestingDataResult (Maybe TestingData)

-- | The subset of the dataset that was actually tested. Some images
--   (assets) might not be tested due to file formatting and other issues.
testingDataResult_output :: Lens' TestingDataResult (Maybe TestingData)

-- | The location of the data validation manifest. The data validation
--   manifest is created for the test dataset during model training.
testingDataResult_validation :: Lens' TestingDataResult (Maybe ValidationData)

-- | Information about a word or line of text detected by DetectText.
--   
--   The <tt>DetectedText</tt> field contains the text that Amazon
--   Rekognition detected in the image.
--   
--   Every word and line has an identifier (<tt>Id</tt>). Each word belongs
--   to a line and has a parent identifier (<tt>ParentId</tt>) that
--   identifies the line of text in which the word appears. The word
--   <tt>Id</tt> is also an index for the word within a line of words.
--   
--   For more information, see Detecting Text in the Amazon Rekognition
--   Developer Guide.
--   
--   <i>See:</i> <a>newTextDetection</a> smart constructor.
data TextDetection
TextDetection' :: Maybe Text -> Maybe Double -> Maybe Geometry -> Maybe Natural -> Maybe TextTypes -> Maybe Natural -> TextDetection

-- | The word or line of text recognized by Amazon Rekognition.
[$sel:detectedText:TextDetection'] :: TextDetection -> Maybe Text

-- | The confidence that Amazon Rekognition has in the accuracy of the
--   detected text and the accuracy of the geometry points around the
--   detected text.
[$sel:confidence:TextDetection'] :: TextDetection -> Maybe Double

-- | The location of the detected text on the image. Includes an axis
--   aligned coarse bounding box surrounding the text and a finer grain
--   polygon for more accurate spatial information.
[$sel:geometry:TextDetection'] :: TextDetection -> Maybe Geometry

-- | The identifier for the detected text. The identifier is only unique
--   for a single call to <tt>DetectText</tt>.
[$sel:id:TextDetection'] :: TextDetection -> Maybe Natural

-- | The type of text that was detected.
[$sel:type':TextDetection'] :: TextDetection -> Maybe TextTypes

-- | The Parent identifier for the detected text identified by the value of
--   <tt>ID</tt>. If the type of detected text is <tt>LINE</tt>, the value
--   of <tt>ParentId</tt> is <tt>Null</tt>.
[$sel:parentId:TextDetection'] :: TextDetection -> Maybe Natural

-- | Create a value of <a>TextDetection</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:detectedText:TextDetection'</a>,
--   <a>textDetection_detectedText</a> - The word or line of text
--   recognized by Amazon Rekognition.
--   
--   <a>$sel:confidence:TextDetection'</a>, <a>textDetection_confidence</a>
--   - The confidence that Amazon Rekognition has in the accuracy of the
--   detected text and the accuracy of the geometry points around the
--   detected text.
--   
--   <a>$sel:geometry:TextDetection'</a>, <a>textDetection_geometry</a> -
--   The location of the detected text on the image. Includes an axis
--   aligned coarse bounding box surrounding the text and a finer grain
--   polygon for more accurate spatial information.
--   
--   <a>$sel:id:TextDetection'</a>, <a>textDetection_id</a> - The
--   identifier for the detected text. The identifier is only unique for a
--   single call to <tt>DetectText</tt>.
--   
--   <a>$sel:type':TextDetection'</a>, <a>textDetection_type</a> - The type
--   of text that was detected.
--   
--   <a>$sel:parentId:TextDetection'</a>, <a>textDetection_parentId</a> -
--   The Parent identifier for the detected text identified by the value of
--   <tt>ID</tt>. If the type of detected text is <tt>LINE</tt>, the value
--   of <tt>ParentId</tt> is <tt>Null</tt>.
newTextDetection :: TextDetection

-- | The word or line of text recognized by Amazon Rekognition.
textDetection_detectedText :: Lens' TextDetection (Maybe Text)

-- | The confidence that Amazon Rekognition has in the accuracy of the
--   detected text and the accuracy of the geometry points around the
--   detected text.
textDetection_confidence :: Lens' TextDetection (Maybe Double)

-- | The location of the detected text on the image. Includes an axis
--   aligned coarse bounding box surrounding the text and a finer grain
--   polygon for more accurate spatial information.
textDetection_geometry :: Lens' TextDetection (Maybe Geometry)

-- | The identifier for the detected text. The identifier is only unique
--   for a single call to <tt>DetectText</tt>.
textDetection_id :: Lens' TextDetection (Maybe Natural)

-- | The type of text that was detected.
textDetection_type :: Lens' TextDetection (Maybe TextTypes)

-- | The Parent identifier for the detected text identified by the value of
--   <tt>ID</tt>. If the type of detected text is <tt>LINE</tt>, the value
--   of <tt>ParentId</tt> is <tt>Null</tt>.
textDetection_parentId :: Lens' TextDetection (Maybe Natural)

-- | Information about text detected in a video. Incudes the detected text,
--   the time in milliseconds from the start of the video that the text was
--   detected, and where it was detected on the screen.
--   
--   <i>See:</i> <a>newTextDetectionResult</a> smart constructor.
data TextDetectionResult
TextDetectionResult' :: Maybe TextDetection -> Maybe Integer -> TextDetectionResult

-- | Details about text detected in a video.
[$sel:textDetection:TextDetectionResult'] :: TextDetectionResult -> Maybe TextDetection

-- | The time, in milliseconds from the start of the video, that the text
--   was detected.
[$sel:timestamp:TextDetectionResult'] :: TextDetectionResult -> Maybe Integer

-- | Create a value of <a>TextDetectionResult</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:textDetection:TextDetectionResult'</a>,
--   <a>textDetectionResult_textDetection</a> - Details about text detected
--   in a video.
--   
--   <a>$sel:timestamp:TextDetectionResult'</a>,
--   <a>textDetectionResult_timestamp</a> - The time, in milliseconds from
--   the start of the video, that the text was detected.
newTextDetectionResult :: TextDetectionResult

-- | Details about text detected in a video.
textDetectionResult_textDetection :: Lens' TextDetectionResult (Maybe TextDetection)

-- | The time, in milliseconds from the start of the video, that the text
--   was detected.
textDetectionResult_timestamp :: Lens' TextDetectionResult (Maybe Integer)

-- | The dataset used for training.
--   
--   <i>See:</i> <a>newTrainingData</a> smart constructor.
data TrainingData
TrainingData' :: Maybe [Asset] -> TrainingData

-- | A Sagemaker GroundTruth manifest file that contains the training
--   images (assets).
[$sel:assets:TrainingData'] :: TrainingData -> Maybe [Asset]

-- | Create a value of <a>TrainingData</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:assets:TrainingData'</a>, <a>trainingData_assets</a> - A
--   Sagemaker GroundTruth manifest file that contains the training images
--   (assets).
newTrainingData :: TrainingData

-- | A Sagemaker GroundTruth manifest file that contains the training
--   images (assets).
trainingData_assets :: Lens' TrainingData (Maybe [Asset])

-- | Sagemaker Groundtruth format manifest files for the input, output and
--   validation datasets that are used and created during testing.
--   
--   <i>See:</i> <a>newTrainingDataResult</a> smart constructor.
data TrainingDataResult
TrainingDataResult' :: Maybe TrainingData -> Maybe TrainingData -> Maybe ValidationData -> TrainingDataResult

-- | The training assets that you supplied for training.
[$sel:input:TrainingDataResult'] :: TrainingDataResult -> Maybe TrainingData

-- | The images (assets) that were actually trained by Amazon Rekognition
--   Custom Labels.
[$sel:output:TrainingDataResult'] :: TrainingDataResult -> Maybe TrainingData

-- | The location of the data validation manifest. The data validation
--   manifest is created for the training dataset during model training.
[$sel:validation:TrainingDataResult'] :: TrainingDataResult -> Maybe ValidationData

-- | Create a value of <a>TrainingDataResult</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:input:TrainingDataResult'</a>, <a>trainingDataResult_input</a>
--   - The training assets that you supplied for training.
--   
--   <a>$sel:output:TrainingDataResult'</a>,
--   <a>trainingDataResult_output</a> - The images (assets) that were
--   actually trained by Amazon Rekognition Custom Labels.
--   
--   <a>$sel:validation:TrainingDataResult'</a>,
--   <a>trainingDataResult_validation</a> - The location of the data
--   validation manifest. The data validation manifest is created for the
--   training dataset during model training.
newTrainingDataResult :: TrainingDataResult

-- | The training assets that you supplied for training.
trainingDataResult_input :: Lens' TrainingDataResult (Maybe TrainingData)

-- | The images (assets) that were actually trained by Amazon Rekognition
--   Custom Labels.
trainingDataResult_output :: Lens' TrainingDataResult (Maybe TrainingData)

-- | The location of the data validation manifest. The data validation
--   manifest is created for the training dataset during model training.
trainingDataResult_validation :: Lens' TrainingDataResult (Maybe ValidationData)

-- | A face that IndexFaces detected, but didn't index. Use the
--   <tt>Reasons</tt> response attribute to determine why a face wasn't
--   indexed.
--   
--   <i>See:</i> <a>newUnindexedFace</a> smart constructor.
data UnindexedFace
UnindexedFace' :: Maybe [Reason] -> Maybe FaceDetail -> UnindexedFace

-- | An array of reasons that specify why a face wasn't indexed.
--   
--   <ul>
--   <li>EXTREME_POSE - The face is at a pose that can't be detected. For
--   example, the head is turned too far away from the camera.</li>
--   <li>EXCEEDS_MAX_FACES - The number of faces detected is already higher
--   than that specified by the <tt>MaxFaces</tt> input parameter for
--   <tt>IndexFaces</tt>.</li>
--   <li>LOW_BRIGHTNESS - The image is too dark.</li>
--   <li>LOW_SHARPNESS - The image is too blurry.</li>
--   <li>LOW_CONFIDENCE - The face was detected with a low confidence.</li>
--   <li>SMALL_BOUNDING_BOX - The bounding box around the face is too
--   small.</li>
--   </ul>
[$sel:reasons:UnindexedFace'] :: UnindexedFace -> Maybe [Reason]

-- | The structure that contains attributes of a face that
--   <tt>IndexFaces</tt>detected, but didn't index.
[$sel:faceDetail:UnindexedFace'] :: UnindexedFace -> Maybe FaceDetail

-- | Create a value of <a>UnindexedFace</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:reasons:UnindexedFace'</a>, <a>unindexedFace_reasons</a> - An
--   array of reasons that specify why a face wasn't indexed.
--   
--   <ul>
--   <li>EXTREME_POSE - The face is at a pose that can't be detected. For
--   example, the head is turned too far away from the camera.</li>
--   <li>EXCEEDS_MAX_FACES - The number of faces detected is already higher
--   than that specified by the <tt>MaxFaces</tt> input parameter for
--   <tt>IndexFaces</tt>.</li>
--   <li>LOW_BRIGHTNESS - The image is too dark.</li>
--   <li>LOW_SHARPNESS - The image is too blurry.</li>
--   <li>LOW_CONFIDENCE - The face was detected with a low confidence.</li>
--   <li>SMALL_BOUNDING_BOX - The bounding box around the face is too
--   small.</li>
--   </ul>
--   
--   <a>$sel:faceDetail:UnindexedFace'</a>, <a>unindexedFace_faceDetail</a>
--   - The structure that contains attributes of a face that
--   <tt>IndexFaces</tt>detected, but didn't index.
newUnindexedFace :: UnindexedFace

-- | An array of reasons that specify why a face wasn't indexed.
--   
--   <ul>
--   <li>EXTREME_POSE - The face is at a pose that can't be detected. For
--   example, the head is turned too far away from the camera.</li>
--   <li>EXCEEDS_MAX_FACES - The number of faces detected is already higher
--   than that specified by the <tt>MaxFaces</tt> input parameter for
--   <tt>IndexFaces</tt>.</li>
--   <li>LOW_BRIGHTNESS - The image is too dark.</li>
--   <li>LOW_SHARPNESS - The image is too blurry.</li>
--   <li>LOW_CONFIDENCE - The face was detected with a low confidence.</li>
--   <li>SMALL_BOUNDING_BOX - The bounding box around the face is too
--   small.</li>
--   </ul>
unindexedFace_reasons :: Lens' UnindexedFace (Maybe [Reason])

-- | The structure that contains attributes of a face that
--   <tt>IndexFaces</tt>detected, but didn't index.
unindexedFace_faceDetail :: Lens' UnindexedFace (Maybe FaceDetail)

-- | Contains the Amazon S3 bucket location of the validation data for a
--   model training job.
--   
--   The validation data includes error information for individual JSON
--   lines in the dataset. For more information, see Debugging a Failed
--   Model Training in the Amazon Rekognition Custom Labels Developer
--   Guide.
--   
--   You get the <tt>ValidationData</tt> object for the training dataset
--   (TrainingDataResult) and the test dataset (TestingDataResult) by
--   calling DescribeProjectVersions.
--   
--   The assets array contains a single Asset object. The
--   GroundTruthManifest field of the Asset object contains the S3 bucket
--   location of the validation data.
--   
--   <i>See:</i> <a>newValidationData</a> smart constructor.
data ValidationData
ValidationData' :: Maybe [Asset] -> ValidationData

-- | The assets that comprise the validation data.
[$sel:assets:ValidationData'] :: ValidationData -> Maybe [Asset]

-- | Create a value of <a>ValidationData</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:assets:ValidationData'</a>, <a>validationData_assets</a> - The
--   assets that comprise the validation data.
newValidationData :: ValidationData

-- | The assets that comprise the validation data.
validationData_assets :: Lens' ValidationData (Maybe [Asset])

-- | Video file stored in an Amazon S3 bucket. Amazon Rekognition video
--   start operations such as StartLabelDetection use <tt>Video</tt> to
--   specify a video for analysis. The supported file formats are .mp4,
--   .mov and .avi.
--   
--   <i>See:</i> <a>newVideo</a> smart constructor.
data Video
Video' :: Maybe S3Object -> Video

-- | The Amazon S3 bucket name and file name for the video.
[$sel:s3Object:Video'] :: Video -> Maybe S3Object

-- | Create a value of <a>Video</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:s3Object:Video'</a>, <a>video_s3Object</a> - The Amazon S3
--   bucket name and file name for the video.
newVideo :: Video

-- | The Amazon S3 bucket name and file name for the video.
video_s3Object :: Lens' Video (Maybe S3Object)

-- | Information about a video that Amazon Rekognition analyzed.
--   <tt>Videometadata</tt> is returned in every page of paginated
--   responses from a Amazon Rekognition video operation.
--   
--   <i>See:</i> <a>newVideoMetadata</a> smart constructor.
data VideoMetadata
VideoMetadata' :: Maybe Double -> Maybe VideoColorRange -> Maybe Text -> Maybe Text -> Maybe Natural -> Maybe Natural -> Maybe Natural -> VideoMetadata

-- | Number of frames per second in the video.
[$sel:frameRate:VideoMetadata'] :: VideoMetadata -> Maybe Double

-- | A description of the range of luminance values in a video, either
--   LIMITED (16 to 235) or FULL (0 to 255).
[$sel:colorRange:VideoMetadata'] :: VideoMetadata -> Maybe VideoColorRange

-- | Format of the analyzed video. Possible values are MP4, MOV and AVI.
[$sel:format:VideoMetadata'] :: VideoMetadata -> Maybe Text

-- | Type of compression used in the analyzed video.
[$sel:codec:VideoMetadata'] :: VideoMetadata -> Maybe Text

-- | Vertical pixel dimension of the video.
[$sel:frameHeight:VideoMetadata'] :: VideoMetadata -> Maybe Natural

-- | Length of the video in milliseconds.
[$sel:durationMillis:VideoMetadata'] :: VideoMetadata -> Maybe Natural

-- | Horizontal pixel dimension of the video.
[$sel:frameWidth:VideoMetadata'] :: VideoMetadata -> Maybe Natural

-- | Create a value of <a>VideoMetadata</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:frameRate:VideoMetadata'</a>, <a>videoMetadata_frameRate</a> -
--   Number of frames per second in the video.
--   
--   <a>$sel:colorRange:VideoMetadata'</a>, <a>videoMetadata_colorRange</a>
--   - A description of the range of luminance values in a video, either
--   LIMITED (16 to 235) or FULL (0 to 255).
--   
--   <a>$sel:format:VideoMetadata'</a>, <a>videoMetadata_format</a> -
--   Format of the analyzed video. Possible values are MP4, MOV and AVI.
--   
--   <a>$sel:codec:VideoMetadata'</a>, <a>videoMetadata_codec</a> - Type of
--   compression used in the analyzed video.
--   
--   <a>$sel:frameHeight:VideoMetadata'</a>,
--   <a>videoMetadata_frameHeight</a> - Vertical pixel dimension of the
--   video.
--   
--   <a>$sel:durationMillis:VideoMetadata'</a>,
--   <a>videoMetadata_durationMillis</a> - Length of the video in
--   milliseconds.
--   
--   <a>$sel:frameWidth:VideoMetadata'</a>, <a>videoMetadata_frameWidth</a>
--   - Horizontal pixel dimension of the video.
newVideoMetadata :: VideoMetadata

-- | Number of frames per second in the video.
videoMetadata_frameRate :: Lens' VideoMetadata (Maybe Double)

-- | A description of the range of luminance values in a video, either
--   LIMITED (16 to 235) or FULL (0 to 255).
videoMetadata_colorRange :: Lens' VideoMetadata (Maybe VideoColorRange)

-- | Format of the analyzed video. Possible values are MP4, MOV and AVI.
videoMetadata_format :: Lens' VideoMetadata (Maybe Text)

-- | Type of compression used in the analyzed video.
videoMetadata_codec :: Lens' VideoMetadata (Maybe Text)

-- | Vertical pixel dimension of the video.
videoMetadata_frameHeight :: Lens' VideoMetadata (Maybe Natural)

-- | Length of the video in milliseconds.
videoMetadata_durationMillis :: Lens' VideoMetadata (Maybe Natural)

-- | Horizontal pixel dimension of the video.
videoMetadata_frameWidth :: Lens' VideoMetadata (Maybe Natural)


-- | Adds one or more key-value tags to an Amazon Rekognition collection,
--   stream processor, or Custom Labels model. For more information, see
--   <a>Tagging AWS Resources</a>.
--   
--   This operation requires permissions to perform the
--   <tt>rekognition:TagResource</tt> action.
module Network.AWS.Rekognition.TagResource

-- | <i>See:</i> <a>newTagResource</a> smart constructor.
data TagResource
TagResource' :: Text -> HashMap Text Text -> TagResource

-- | Amazon Resource Name (ARN) of the model, collection, or stream
--   processor that you want to assign the tags to.
[$sel:resourceArn:TagResource'] :: TagResource -> Text

-- | The key-value tags to assign to the resource.
[$sel:tags:TagResource'] :: TagResource -> HashMap Text Text

-- | Create a value of <a>TagResource</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:resourceArn:TagResource'</a>, <a>tagResource_resourceArn</a> -
--   Amazon Resource Name (ARN) of the model, collection, or stream
--   processor that you want to assign the tags to.
--   
--   <a>$sel:tags:TagResource'</a>, <a>tagResource_tags</a> - The key-value
--   tags to assign to the resource.
newTagResource :: Text -> TagResource

-- | Amazon Resource Name (ARN) of the model, collection, or stream
--   processor that you want to assign the tags to.
tagResource_resourceArn :: Lens' TagResource Text

-- | The key-value tags to assign to the resource.
tagResource_tags :: Lens' TagResource (HashMap Text Text)

-- | <i>See:</i> <a>newTagResourceResponse</a> smart constructor.
data TagResourceResponse
TagResourceResponse' :: Int -> TagResourceResponse

-- | The response's http status code.
[$sel:httpStatus:TagResourceResponse'] :: TagResourceResponse -> Int

-- | Create a value of <a>TagResourceResponse</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:httpStatus:TagResourceResponse'</a>,
--   <a>tagResourceResponse_httpStatus</a> - The response's http status
--   code.
newTagResourceResponse :: Int -> TagResourceResponse

-- | The response's http status code.
tagResourceResponse_httpStatus :: Lens' TagResourceResponse Int
instance GHC.Generics.Generic Network.AWS.Rekognition.TagResource.TagResource
instance GHC.Show.Show Network.AWS.Rekognition.TagResource.TagResource
instance GHC.Read.Read Network.AWS.Rekognition.TagResource.TagResource
instance GHC.Classes.Eq Network.AWS.Rekognition.TagResource.TagResource
instance GHC.Generics.Generic Network.AWS.Rekognition.TagResource.TagResourceResponse
instance GHC.Show.Show Network.AWS.Rekognition.TagResource.TagResourceResponse
instance GHC.Read.Read Network.AWS.Rekognition.TagResource.TagResourceResponse
instance GHC.Classes.Eq Network.AWS.Rekognition.TagResource.TagResourceResponse
instance Network.AWS.Types.AWSRequest Network.AWS.Rekognition.TagResource.TagResource
instance Control.DeepSeq.NFData Network.AWS.Rekognition.TagResource.TagResourceResponse
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.TagResource.TagResource
instance Control.DeepSeq.NFData Network.AWS.Rekognition.TagResource.TagResource
instance Network.AWS.Data.Headers.ToHeaders Network.AWS.Rekognition.TagResource.TagResource
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.TagResource.TagResource
instance Network.AWS.Data.Path.ToPath Network.AWS.Rekognition.TagResource.TagResource
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.TagResource.TagResource


-- | Stops a running stream processor that was created by
--   CreateStreamProcessor.
module Network.AWS.Rekognition.StopStreamProcessor

-- | <i>See:</i> <a>newStopStreamProcessor</a> smart constructor.
data StopStreamProcessor
StopStreamProcessor' :: Text -> StopStreamProcessor

-- | The name of a stream processor created by CreateStreamProcessor.
[$sel:name:StopStreamProcessor'] :: StopStreamProcessor -> Text

-- | Create a value of <a>StopStreamProcessor</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:name:StopStreamProcessor'</a>, <a>stopStreamProcessor_name</a>
--   - The name of a stream processor created by CreateStreamProcessor.
newStopStreamProcessor :: Text -> StopStreamProcessor

-- | The name of a stream processor created by CreateStreamProcessor.
stopStreamProcessor_name :: Lens' StopStreamProcessor Text

-- | <i>See:</i> <a>newStopStreamProcessorResponse</a> smart constructor.
data StopStreamProcessorResponse
StopStreamProcessorResponse' :: Int -> StopStreamProcessorResponse

-- | The response's http status code.
[$sel:httpStatus:StopStreamProcessorResponse'] :: StopStreamProcessorResponse -> Int

-- | Create a value of <a>StopStreamProcessorResponse</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:httpStatus:StopStreamProcessorResponse'</a>,
--   <a>stopStreamProcessorResponse_httpStatus</a> - The response's http
--   status code.
newStopStreamProcessorResponse :: Int -> StopStreamProcessorResponse

-- | The response's http status code.
stopStreamProcessorResponse_httpStatus :: Lens' StopStreamProcessorResponse Int
instance GHC.Generics.Generic Network.AWS.Rekognition.StopStreamProcessor.StopStreamProcessor
instance GHC.Show.Show Network.AWS.Rekognition.StopStreamProcessor.StopStreamProcessor
instance GHC.Read.Read Network.AWS.Rekognition.StopStreamProcessor.StopStreamProcessor
instance GHC.Classes.Eq Network.AWS.Rekognition.StopStreamProcessor.StopStreamProcessor
instance GHC.Generics.Generic Network.AWS.Rekognition.StopStreamProcessor.StopStreamProcessorResponse
instance GHC.Show.Show Network.AWS.Rekognition.StopStreamProcessor.StopStreamProcessorResponse
instance GHC.Read.Read Network.AWS.Rekognition.StopStreamProcessor.StopStreamProcessorResponse
instance GHC.Classes.Eq Network.AWS.Rekognition.StopStreamProcessor.StopStreamProcessorResponse
instance Network.AWS.Types.AWSRequest Network.AWS.Rekognition.StopStreamProcessor.StopStreamProcessor
instance Control.DeepSeq.NFData Network.AWS.Rekognition.StopStreamProcessor.StopStreamProcessorResponse
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.StopStreamProcessor.StopStreamProcessor
instance Control.DeepSeq.NFData Network.AWS.Rekognition.StopStreamProcessor.StopStreamProcessor
instance Network.AWS.Data.Headers.ToHeaders Network.AWS.Rekognition.StopStreamProcessor.StopStreamProcessor
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.StopStreamProcessor.StopStreamProcessor
instance Network.AWS.Data.Path.ToPath Network.AWS.Rekognition.StopStreamProcessor.StopStreamProcessor
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.StopStreamProcessor.StopStreamProcessor


-- | Stops a running model. The operation might take a while to complete.
--   To check the current status, call DescribeProjectVersions.
module Network.AWS.Rekognition.StopProjectVersion

-- | <i>See:</i> <a>newStopProjectVersion</a> smart constructor.
data StopProjectVersion
StopProjectVersion' :: Text -> StopProjectVersion

-- | The Amazon Resource Name (ARN) of the model version that you want to
--   delete.
--   
--   This operation requires permissions to perform the
--   <tt>rekognition:StopProjectVersion</tt> action.
[$sel:projectVersionArn:StopProjectVersion'] :: StopProjectVersion -> Text

-- | Create a value of <a>StopProjectVersion</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:projectVersionArn:StopProjectVersion'</a>,
--   <a>stopProjectVersion_projectVersionArn</a> - The Amazon Resource Name
--   (ARN) of the model version that you want to delete.
--   
--   This operation requires permissions to perform the
--   <tt>rekognition:StopProjectVersion</tt> action.
newStopProjectVersion :: Text -> StopProjectVersion

-- | The Amazon Resource Name (ARN) of the model version that you want to
--   delete.
--   
--   This operation requires permissions to perform the
--   <tt>rekognition:StopProjectVersion</tt> action.
stopProjectVersion_projectVersionArn :: Lens' StopProjectVersion Text

-- | <i>See:</i> <a>newStopProjectVersionResponse</a> smart constructor.
data StopProjectVersionResponse
StopProjectVersionResponse' :: Maybe ProjectVersionStatus -> Int -> StopProjectVersionResponse

-- | The current status of the stop operation.
[$sel:status:StopProjectVersionResponse'] :: StopProjectVersionResponse -> Maybe ProjectVersionStatus

-- | The response's http status code.
[$sel:httpStatus:StopProjectVersionResponse'] :: StopProjectVersionResponse -> Int

-- | Create a value of <a>StopProjectVersionResponse</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:status:StopProjectVersionResponse'</a>,
--   <a>stopProjectVersionResponse_status</a> - The current status of the
--   stop operation.
--   
--   <a>$sel:httpStatus:StopProjectVersionResponse'</a>,
--   <a>stopProjectVersionResponse_httpStatus</a> - The response's http
--   status code.
newStopProjectVersionResponse :: Int -> StopProjectVersionResponse

-- | The current status of the stop operation.
stopProjectVersionResponse_status :: Lens' StopProjectVersionResponse (Maybe ProjectVersionStatus)

-- | The response's http status code.
stopProjectVersionResponse_httpStatus :: Lens' StopProjectVersionResponse Int
instance GHC.Generics.Generic Network.AWS.Rekognition.StopProjectVersion.StopProjectVersion
instance GHC.Show.Show Network.AWS.Rekognition.StopProjectVersion.StopProjectVersion
instance GHC.Read.Read Network.AWS.Rekognition.StopProjectVersion.StopProjectVersion
instance GHC.Classes.Eq Network.AWS.Rekognition.StopProjectVersion.StopProjectVersion
instance GHC.Generics.Generic Network.AWS.Rekognition.StopProjectVersion.StopProjectVersionResponse
instance GHC.Show.Show Network.AWS.Rekognition.StopProjectVersion.StopProjectVersionResponse
instance GHC.Read.Read Network.AWS.Rekognition.StopProjectVersion.StopProjectVersionResponse
instance GHC.Classes.Eq Network.AWS.Rekognition.StopProjectVersion.StopProjectVersionResponse
instance Network.AWS.Types.AWSRequest Network.AWS.Rekognition.StopProjectVersion.StopProjectVersion
instance Control.DeepSeq.NFData Network.AWS.Rekognition.StopProjectVersion.StopProjectVersionResponse
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.StopProjectVersion.StopProjectVersion
instance Control.DeepSeq.NFData Network.AWS.Rekognition.StopProjectVersion.StopProjectVersion
instance Network.AWS.Data.Headers.ToHeaders Network.AWS.Rekognition.StopProjectVersion.StopProjectVersion
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.StopProjectVersion.StopProjectVersion
instance Network.AWS.Data.Path.ToPath Network.AWS.Rekognition.StopProjectVersion.StopProjectVersion
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.StopProjectVersion.StopProjectVersion


-- | Starts asynchronous detection of text in a stored video.
--   
--   Amazon Rekognition Video can detect text in a video stored in an
--   Amazon S3 bucket. Use Video to specify the bucket name and the
--   filename of the video. <tt>StartTextDetection</tt> returns a job
--   identifier (<tt>JobId</tt>) which you use to get the results of the
--   operation. When text detection is finished, Amazon Rekognition Video
--   publishes a completion status to the Amazon Simple Notification
--   Service topic that you specify in <tt>NotificationChannel</tt>.
--   
--   To get the results of the text detection operation, first check that
--   the status value published to the Amazon SNS topic is
--   <tt>SUCCEEDED</tt>. if so, call GetTextDetection and pass the job
--   identifier (<tt>JobId</tt>) from the initial call to
--   <tt>StartTextDetection</tt>.
module Network.AWS.Rekognition.StartTextDetection

-- | <i>See:</i> <a>newStartTextDetection</a> smart constructor.
data StartTextDetection
StartTextDetection' :: Maybe Text -> Maybe StartTextDetectionFilters -> Maybe NotificationChannel -> Maybe Text -> Video -> StartTextDetection

-- | An identifier returned in the completion status published by your
--   Amazon Simple Notification Service topic. For example, you can use
--   <tt>JobTag</tt> to group related jobs and identify them in the
--   completion notification.
[$sel:jobTag:StartTextDetection'] :: StartTextDetection -> Maybe Text

-- | Optional parameters that let you set criteria the text must meet to be
--   included in your response.
[$sel:filters:StartTextDetection'] :: StartTextDetection -> Maybe StartTextDetectionFilters
[$sel:notificationChannel:StartTextDetection'] :: StartTextDetection -> Maybe NotificationChannel

-- | Idempotent token used to identify the start request. If you use the
--   same token with multiple <tt>StartTextDetection</tt> requests, the
--   same <tt>JobId</tt> is returned. Use <tt>ClientRequestToken</tt> to
--   prevent the same job from being accidentaly started more than once.
[$sel:clientRequestToken:StartTextDetection'] :: StartTextDetection -> Maybe Text
[$sel:video:StartTextDetection'] :: StartTextDetection -> Video

-- | Create a value of <a>StartTextDetection</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:jobTag:StartTextDetection'</a>,
--   <a>startTextDetection_jobTag</a> - An identifier returned in the
--   completion status published by your Amazon Simple Notification Service
--   topic. For example, you can use <tt>JobTag</tt> to group related jobs
--   and identify them in the completion notification.
--   
--   <a>$sel:filters:StartTextDetection'</a>,
--   <a>startTextDetection_filters</a> - Optional parameters that let you
--   set criteria the text must meet to be included in your response.
--   
--   <a>$sel:notificationChannel:StartTextDetection'</a>,
--   <a>startTextDetection_notificationChannel</a> - Undocumented member.
--   
--   <a>$sel:clientRequestToken:StartTextDetection'</a>,
--   <a>startTextDetection_clientRequestToken</a> - Idempotent token used
--   to identify the start request. If you use the same token with multiple
--   <tt>StartTextDetection</tt> requests, the same <tt>JobId</tt> is
--   returned. Use <tt>ClientRequestToken</tt> to prevent the same job from
--   being accidentaly started more than once.
--   
--   <a>$sel:video:StartTextDetection'</a>, <a>startTextDetection_video</a>
--   - Undocumented member.
newStartTextDetection :: Video -> StartTextDetection

-- | An identifier returned in the completion status published by your
--   Amazon Simple Notification Service topic. For example, you can use
--   <tt>JobTag</tt> to group related jobs and identify them in the
--   completion notification.
startTextDetection_jobTag :: Lens' StartTextDetection (Maybe Text)

-- | Optional parameters that let you set criteria the text must meet to be
--   included in your response.
startTextDetection_filters :: Lens' StartTextDetection (Maybe StartTextDetectionFilters)

-- | Undocumented member.
startTextDetection_notificationChannel :: Lens' StartTextDetection (Maybe NotificationChannel)

-- | Idempotent token used to identify the start request. If you use the
--   same token with multiple <tt>StartTextDetection</tt> requests, the
--   same <tt>JobId</tt> is returned. Use <tt>ClientRequestToken</tt> to
--   prevent the same job from being accidentaly started more than once.
startTextDetection_clientRequestToken :: Lens' StartTextDetection (Maybe Text)

-- | Undocumented member.
startTextDetection_video :: Lens' StartTextDetection Video

-- | <i>See:</i> <a>newStartTextDetectionResponse</a> smart constructor.
data StartTextDetectionResponse
StartTextDetectionResponse' :: Maybe Text -> Int -> StartTextDetectionResponse

-- | Identifier for the text detection job. Use <tt>JobId</tt> to identify
--   the job in a subsequent call to <tt>GetTextDetection</tt>.
[$sel:jobId:StartTextDetectionResponse'] :: StartTextDetectionResponse -> Maybe Text

-- | The response's http status code.
[$sel:httpStatus:StartTextDetectionResponse'] :: StartTextDetectionResponse -> Int

-- | Create a value of <a>StartTextDetectionResponse</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:jobId:StartTextDetectionResponse'</a>,
--   <a>startTextDetectionResponse_jobId</a> - Identifier for the text
--   detection job. Use <tt>JobId</tt> to identify the job in a subsequent
--   call to <tt>GetTextDetection</tt>.
--   
--   <a>$sel:httpStatus:StartTextDetectionResponse'</a>,
--   <a>startTextDetectionResponse_httpStatus</a> - The response's http
--   status code.
newStartTextDetectionResponse :: Int -> StartTextDetectionResponse

-- | Identifier for the text detection job. Use <tt>JobId</tt> to identify
--   the job in a subsequent call to <tt>GetTextDetection</tt>.
startTextDetectionResponse_jobId :: Lens' StartTextDetectionResponse (Maybe Text)

-- | The response's http status code.
startTextDetectionResponse_httpStatus :: Lens' StartTextDetectionResponse Int
instance GHC.Generics.Generic Network.AWS.Rekognition.StartTextDetection.StartTextDetection
instance GHC.Show.Show Network.AWS.Rekognition.StartTextDetection.StartTextDetection
instance GHC.Read.Read Network.AWS.Rekognition.StartTextDetection.StartTextDetection
instance GHC.Classes.Eq Network.AWS.Rekognition.StartTextDetection.StartTextDetection
instance GHC.Generics.Generic Network.AWS.Rekognition.StartTextDetection.StartTextDetectionResponse
instance GHC.Show.Show Network.AWS.Rekognition.StartTextDetection.StartTextDetectionResponse
instance GHC.Read.Read Network.AWS.Rekognition.StartTextDetection.StartTextDetectionResponse
instance GHC.Classes.Eq Network.AWS.Rekognition.StartTextDetection.StartTextDetectionResponse
instance Network.AWS.Types.AWSRequest Network.AWS.Rekognition.StartTextDetection.StartTextDetection
instance Control.DeepSeq.NFData Network.AWS.Rekognition.StartTextDetection.StartTextDetectionResponse
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.StartTextDetection.StartTextDetection
instance Control.DeepSeq.NFData Network.AWS.Rekognition.StartTextDetection.StartTextDetection
instance Network.AWS.Data.Headers.ToHeaders Network.AWS.Rekognition.StartTextDetection.StartTextDetection
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.StartTextDetection.StartTextDetection
instance Network.AWS.Data.Path.ToPath Network.AWS.Rekognition.StartTextDetection.StartTextDetection
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.StartTextDetection.StartTextDetection


-- | Starts processing a stream processor. You create a stream processor by
--   calling CreateStreamProcessor. To tell <tt>StartStreamProcessor</tt>
--   which stream processor to start, use the value of the <tt>Name</tt>
--   field specified in the call to <tt>CreateStreamProcessor</tt>.
module Network.AWS.Rekognition.StartStreamProcessor

-- | <i>See:</i> <a>newStartStreamProcessor</a> smart constructor.
data StartStreamProcessor
StartStreamProcessor' :: Text -> StartStreamProcessor

-- | The name of the stream processor to start processing.
[$sel:name:StartStreamProcessor'] :: StartStreamProcessor -> Text

-- | Create a value of <a>StartStreamProcessor</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:name:StartStreamProcessor'</a>,
--   <a>startStreamProcessor_name</a> - The name of the stream processor to
--   start processing.
newStartStreamProcessor :: Text -> StartStreamProcessor

-- | The name of the stream processor to start processing.
startStreamProcessor_name :: Lens' StartStreamProcessor Text

-- | <i>See:</i> <a>newStartStreamProcessorResponse</a> smart constructor.
data StartStreamProcessorResponse
StartStreamProcessorResponse' :: Int -> StartStreamProcessorResponse

-- | The response's http status code.
[$sel:httpStatus:StartStreamProcessorResponse'] :: StartStreamProcessorResponse -> Int

-- | Create a value of <a>StartStreamProcessorResponse</a> with all
--   optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:httpStatus:StartStreamProcessorResponse'</a>,
--   <a>startStreamProcessorResponse_httpStatus</a> - The response's http
--   status code.
newStartStreamProcessorResponse :: Int -> StartStreamProcessorResponse

-- | The response's http status code.
startStreamProcessorResponse_httpStatus :: Lens' StartStreamProcessorResponse Int
instance GHC.Generics.Generic Network.AWS.Rekognition.StartStreamProcessor.StartStreamProcessor
instance GHC.Show.Show Network.AWS.Rekognition.StartStreamProcessor.StartStreamProcessor
instance GHC.Read.Read Network.AWS.Rekognition.StartStreamProcessor.StartStreamProcessor
instance GHC.Classes.Eq Network.AWS.Rekognition.StartStreamProcessor.StartStreamProcessor
instance GHC.Generics.Generic Network.AWS.Rekognition.StartStreamProcessor.StartStreamProcessorResponse
instance GHC.Show.Show Network.AWS.Rekognition.StartStreamProcessor.StartStreamProcessorResponse
instance GHC.Read.Read Network.AWS.Rekognition.StartStreamProcessor.StartStreamProcessorResponse
instance GHC.Classes.Eq Network.AWS.Rekognition.StartStreamProcessor.StartStreamProcessorResponse
instance Network.AWS.Types.AWSRequest Network.AWS.Rekognition.StartStreamProcessor.StartStreamProcessor
instance Control.DeepSeq.NFData Network.AWS.Rekognition.StartStreamProcessor.StartStreamProcessorResponse
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.StartStreamProcessor.StartStreamProcessor
instance Control.DeepSeq.NFData Network.AWS.Rekognition.StartStreamProcessor.StartStreamProcessor
instance Network.AWS.Data.Headers.ToHeaders Network.AWS.Rekognition.StartStreamProcessor.StartStreamProcessor
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.StartStreamProcessor.StartStreamProcessor
instance Network.AWS.Data.Path.ToPath Network.AWS.Rekognition.StartStreamProcessor.StartStreamProcessor
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.StartStreamProcessor.StartStreamProcessor


-- | Starts asynchronous detection of segment detection in a stored video.
--   
--   Amazon Rekognition Video can detect segments in a video stored in an
--   Amazon S3 bucket. Use Video to specify the bucket name and the
--   filename of the video. <tt>StartSegmentDetection</tt> returns a job
--   identifier (<tt>JobId</tt>) which you use to get the results of the
--   operation. When segment detection is finished, Amazon Rekognition
--   Video publishes a completion status to the Amazon Simple Notification
--   Service topic that you specify in <tt>NotificationChannel</tt>.
--   
--   You can use the <tt>Filters</tt> (StartSegmentDetectionFilters) input
--   parameter to specify the minimum detection confidence returned in the
--   response. Within <tt>Filters</tt>, use <tt>ShotFilter</tt>
--   (StartShotDetectionFilter) to filter detected shots. Use
--   <tt>TechnicalCueFilter</tt> (StartTechnicalCueDetectionFilter) to
--   filter technical cues.
--   
--   To get the results of the segment detection operation, first check
--   that the status value published to the Amazon SNS topic is
--   <tt>SUCCEEDED</tt>. if so, call GetSegmentDetection and pass the job
--   identifier (<tt>JobId</tt>) from the initial call to
--   <tt>StartSegmentDetection</tt>.
--   
--   For more information, see Detecting Video Segments in Stored Video in
--   the Amazon Rekognition Developer Guide.
module Network.AWS.Rekognition.StartSegmentDetection

-- | <i>See:</i> <a>newStartSegmentDetection</a> smart constructor.
data StartSegmentDetection
StartSegmentDetection' :: Maybe Text -> Maybe StartSegmentDetectionFilters -> Maybe NotificationChannel -> Maybe Text -> Video -> NonEmpty SegmentType -> StartSegmentDetection

-- | An identifier you specify that's returned in the completion
--   notification that's published to your Amazon Simple Notification
--   Service topic. For example, you can use <tt>JobTag</tt> to group
--   related jobs and identify them in the completion notification.
[$sel:jobTag:StartSegmentDetection'] :: StartSegmentDetection -> Maybe Text

-- | Filters for technical cue or shot detection.
[$sel:filters:StartSegmentDetection'] :: StartSegmentDetection -> Maybe StartSegmentDetectionFilters

-- | The ARN of the Amazon SNS topic to which you want Amazon Rekognition
--   Video to publish the completion status of the segment detection
--   operation. Note that the Amazon SNS topic must have a topic name that
--   begins with <i>AmazonRekognition</i> if you are using the
--   AmazonRekognitionServiceRole permissions policy to access the topic.
[$sel:notificationChannel:StartSegmentDetection'] :: StartSegmentDetection -> Maybe NotificationChannel

-- | Idempotent token used to identify the start request. If you use the
--   same token with multiple <tt>StartSegmentDetection</tt> requests, the
--   same <tt>JobId</tt> is returned. Use <tt>ClientRequestToken</tt> to
--   prevent the same job from being accidently started more than once.
[$sel:clientRequestToken:StartSegmentDetection'] :: StartSegmentDetection -> Maybe Text
[$sel:video:StartSegmentDetection'] :: StartSegmentDetection -> Video

-- | An array of segment types to detect in the video. Valid values are
--   TECHNICAL_CUE and SHOT.
[$sel:segmentTypes:StartSegmentDetection'] :: StartSegmentDetection -> NonEmpty SegmentType

-- | Create a value of <a>StartSegmentDetection</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:jobTag:StartSegmentDetection'</a>,
--   <a>startSegmentDetection_jobTag</a> - An identifier you specify that's
--   returned in the completion notification that's published to your
--   Amazon Simple Notification Service topic. For example, you can use
--   <tt>JobTag</tt> to group related jobs and identify them in the
--   completion notification.
--   
--   <a>$sel:filters:StartSegmentDetection'</a>,
--   <a>startSegmentDetection_filters</a> - Filters for technical cue or
--   shot detection.
--   
--   <a>$sel:notificationChannel:StartSegmentDetection'</a>,
--   <a>startSegmentDetection_notificationChannel</a> - The ARN of the
--   Amazon SNS topic to which you want Amazon Rekognition Video to publish
--   the completion status of the segment detection operation. Note that
--   the Amazon SNS topic must have a topic name that begins with
--   <i>AmazonRekognition</i> if you are using the
--   AmazonRekognitionServiceRole permissions policy to access the topic.
--   
--   <a>$sel:clientRequestToken:StartSegmentDetection'</a>,
--   <a>startSegmentDetection_clientRequestToken</a> - Idempotent token
--   used to identify the start request. If you use the same token with
--   multiple <tt>StartSegmentDetection</tt> requests, the same
--   <tt>JobId</tt> is returned. Use <tt>ClientRequestToken</tt> to prevent
--   the same job from being accidently started more than once.
--   
--   <a>$sel:video:StartSegmentDetection'</a>,
--   <a>startSegmentDetection_video</a> - Undocumented member.
--   
--   <a>$sel:segmentTypes:StartSegmentDetection'</a>,
--   <a>startSegmentDetection_segmentTypes</a> - An array of segment types
--   to detect in the video. Valid values are TECHNICAL_CUE and SHOT.
newStartSegmentDetection :: Video -> NonEmpty SegmentType -> StartSegmentDetection

-- | An identifier you specify that's returned in the completion
--   notification that's published to your Amazon Simple Notification
--   Service topic. For example, you can use <tt>JobTag</tt> to group
--   related jobs and identify them in the completion notification.
startSegmentDetection_jobTag :: Lens' StartSegmentDetection (Maybe Text)

-- | Filters for technical cue or shot detection.
startSegmentDetection_filters :: Lens' StartSegmentDetection (Maybe StartSegmentDetectionFilters)

-- | The ARN of the Amazon SNS topic to which you want Amazon Rekognition
--   Video to publish the completion status of the segment detection
--   operation. Note that the Amazon SNS topic must have a topic name that
--   begins with <i>AmazonRekognition</i> if you are using the
--   AmazonRekognitionServiceRole permissions policy to access the topic.
startSegmentDetection_notificationChannel :: Lens' StartSegmentDetection (Maybe NotificationChannel)

-- | Idempotent token used to identify the start request. If you use the
--   same token with multiple <tt>StartSegmentDetection</tt> requests, the
--   same <tt>JobId</tt> is returned. Use <tt>ClientRequestToken</tt> to
--   prevent the same job from being accidently started more than once.
startSegmentDetection_clientRequestToken :: Lens' StartSegmentDetection (Maybe Text)

-- | Undocumented member.
startSegmentDetection_video :: Lens' StartSegmentDetection Video

-- | An array of segment types to detect in the video. Valid values are
--   TECHNICAL_CUE and SHOT.
startSegmentDetection_segmentTypes :: Lens' StartSegmentDetection (NonEmpty SegmentType)

-- | <i>See:</i> <a>newStartSegmentDetectionResponse</a> smart constructor.
data StartSegmentDetectionResponse
StartSegmentDetectionResponse' :: Maybe Text -> Int -> StartSegmentDetectionResponse

-- | Unique identifier for the segment detection job. The <tt>JobId</tt> is
--   returned from <tt>StartSegmentDetection</tt>.
[$sel:jobId:StartSegmentDetectionResponse'] :: StartSegmentDetectionResponse -> Maybe Text

-- | The response's http status code.
[$sel:httpStatus:StartSegmentDetectionResponse'] :: StartSegmentDetectionResponse -> Int

-- | Create a value of <a>StartSegmentDetectionResponse</a> with all
--   optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:jobId:StartSegmentDetectionResponse'</a>,
--   <a>startSegmentDetectionResponse_jobId</a> - Unique identifier for the
--   segment detection job. The <tt>JobId</tt> is returned from
--   <tt>StartSegmentDetection</tt>.
--   
--   <a>$sel:httpStatus:StartSegmentDetectionResponse'</a>,
--   <a>startSegmentDetectionResponse_httpStatus</a> - The response's http
--   status code.
newStartSegmentDetectionResponse :: Int -> StartSegmentDetectionResponse

-- | Unique identifier for the segment detection job. The <tt>JobId</tt> is
--   returned from <tt>StartSegmentDetection</tt>.
startSegmentDetectionResponse_jobId :: Lens' StartSegmentDetectionResponse (Maybe Text)

-- | The response's http status code.
startSegmentDetectionResponse_httpStatus :: Lens' StartSegmentDetectionResponse Int
instance GHC.Generics.Generic Network.AWS.Rekognition.StartSegmentDetection.StartSegmentDetection
instance GHC.Show.Show Network.AWS.Rekognition.StartSegmentDetection.StartSegmentDetection
instance GHC.Read.Read Network.AWS.Rekognition.StartSegmentDetection.StartSegmentDetection
instance GHC.Classes.Eq Network.AWS.Rekognition.StartSegmentDetection.StartSegmentDetection
instance GHC.Generics.Generic Network.AWS.Rekognition.StartSegmentDetection.StartSegmentDetectionResponse
instance GHC.Show.Show Network.AWS.Rekognition.StartSegmentDetection.StartSegmentDetectionResponse
instance GHC.Read.Read Network.AWS.Rekognition.StartSegmentDetection.StartSegmentDetectionResponse
instance GHC.Classes.Eq Network.AWS.Rekognition.StartSegmentDetection.StartSegmentDetectionResponse
instance Network.AWS.Types.AWSRequest Network.AWS.Rekognition.StartSegmentDetection.StartSegmentDetection
instance Control.DeepSeq.NFData Network.AWS.Rekognition.StartSegmentDetection.StartSegmentDetectionResponse
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.StartSegmentDetection.StartSegmentDetection
instance Control.DeepSeq.NFData Network.AWS.Rekognition.StartSegmentDetection.StartSegmentDetection
instance Network.AWS.Data.Headers.ToHeaders Network.AWS.Rekognition.StartSegmentDetection.StartSegmentDetection
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.StartSegmentDetection.StartSegmentDetection
instance Network.AWS.Data.Path.ToPath Network.AWS.Rekognition.StartSegmentDetection.StartSegmentDetection
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.StartSegmentDetection.StartSegmentDetection


-- | Starts the running of the version of a model. Starting a model takes a
--   while to complete. To check the current state of the model, use
--   DescribeProjectVersions.
--   
--   Once the model is running, you can detect custom labels in new images
--   by calling DetectCustomLabels.
--   
--   You are charged for the amount of time that the model is running. To
--   stop a running model, call StopProjectVersion.
--   
--   This operation requires permissions to perform the
--   <tt>rekognition:StartProjectVersion</tt> action.
module Network.AWS.Rekognition.StartProjectVersion

-- | <i>See:</i> <a>newStartProjectVersion</a> smart constructor.
data StartProjectVersion
StartProjectVersion' :: Text -> Natural -> StartProjectVersion

-- | The Amazon Resource Name(ARN) of the model version that you want to
--   start.
[$sel:projectVersionArn:StartProjectVersion'] :: StartProjectVersion -> Text

-- | The minimum number of inference units to use. A single inference unit
--   represents 1 hour of processing and can support up to 5 Transaction
--   Pers Second (TPS). Use a higher number to increase the TPS throughput
--   of your model. You are charged for the number of inference units that
--   you use.
[$sel:minInferenceUnits:StartProjectVersion'] :: StartProjectVersion -> Natural

-- | Create a value of <a>StartProjectVersion</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:projectVersionArn:StartProjectVersion'</a>,
--   <a>startProjectVersion_projectVersionArn</a> - The Amazon Resource
--   Name(ARN) of the model version that you want to start.
--   
--   <a>$sel:minInferenceUnits:StartProjectVersion'</a>,
--   <a>startProjectVersion_minInferenceUnits</a> - The minimum number of
--   inference units to use. A single inference unit represents 1 hour of
--   processing and can support up to 5 Transaction Pers Second (TPS). Use
--   a higher number to increase the TPS throughput of your model. You are
--   charged for the number of inference units that you use.
newStartProjectVersion :: Text -> Natural -> StartProjectVersion

-- | The Amazon Resource Name(ARN) of the model version that you want to
--   start.
startProjectVersion_projectVersionArn :: Lens' StartProjectVersion Text

-- | The minimum number of inference units to use. A single inference unit
--   represents 1 hour of processing and can support up to 5 Transaction
--   Pers Second (TPS). Use a higher number to increase the TPS throughput
--   of your model. You are charged for the number of inference units that
--   you use.
startProjectVersion_minInferenceUnits :: Lens' StartProjectVersion Natural

-- | <i>See:</i> <a>newStartProjectVersionResponse</a> smart constructor.
data StartProjectVersionResponse
StartProjectVersionResponse' :: Maybe ProjectVersionStatus -> Int -> StartProjectVersionResponse

-- | The current running status of the model.
[$sel:status:StartProjectVersionResponse'] :: StartProjectVersionResponse -> Maybe ProjectVersionStatus

-- | The response's http status code.
[$sel:httpStatus:StartProjectVersionResponse'] :: StartProjectVersionResponse -> Int

-- | Create a value of <a>StartProjectVersionResponse</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:status:StartProjectVersionResponse'</a>,
--   <a>startProjectVersionResponse_status</a> - The current running status
--   of the model.
--   
--   <a>$sel:httpStatus:StartProjectVersionResponse'</a>,
--   <a>startProjectVersionResponse_httpStatus</a> - The response's http
--   status code.
newStartProjectVersionResponse :: Int -> StartProjectVersionResponse

-- | The current running status of the model.
startProjectVersionResponse_status :: Lens' StartProjectVersionResponse (Maybe ProjectVersionStatus)

-- | The response's http status code.
startProjectVersionResponse_httpStatus :: Lens' StartProjectVersionResponse Int
instance GHC.Generics.Generic Network.AWS.Rekognition.StartProjectVersion.StartProjectVersion
instance GHC.Show.Show Network.AWS.Rekognition.StartProjectVersion.StartProjectVersion
instance GHC.Read.Read Network.AWS.Rekognition.StartProjectVersion.StartProjectVersion
instance GHC.Classes.Eq Network.AWS.Rekognition.StartProjectVersion.StartProjectVersion
instance GHC.Generics.Generic Network.AWS.Rekognition.StartProjectVersion.StartProjectVersionResponse
instance GHC.Show.Show Network.AWS.Rekognition.StartProjectVersion.StartProjectVersionResponse
instance GHC.Read.Read Network.AWS.Rekognition.StartProjectVersion.StartProjectVersionResponse
instance GHC.Classes.Eq Network.AWS.Rekognition.StartProjectVersion.StartProjectVersionResponse
instance Network.AWS.Types.AWSRequest Network.AWS.Rekognition.StartProjectVersion.StartProjectVersion
instance Control.DeepSeq.NFData Network.AWS.Rekognition.StartProjectVersion.StartProjectVersionResponse
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.StartProjectVersion.StartProjectVersion
instance Control.DeepSeq.NFData Network.AWS.Rekognition.StartProjectVersion.StartProjectVersion
instance Network.AWS.Data.Headers.ToHeaders Network.AWS.Rekognition.StartProjectVersion.StartProjectVersion
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.StartProjectVersion.StartProjectVersion
instance Network.AWS.Data.Path.ToPath Network.AWS.Rekognition.StartProjectVersion.StartProjectVersion
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.StartProjectVersion.StartProjectVersion


-- | Starts the asynchronous tracking of a person's path in a stored video.
--   
--   Amazon Rekognition Video can track the path of people in a video
--   stored in an Amazon S3 bucket. Use Video to specify the bucket name
--   and the filename of the video. <tt>StartPersonTracking</tt> returns a
--   job identifier (<tt>JobId</tt>) which you use to get the results of
--   the operation. When label detection is finished, Amazon Rekognition
--   publishes a completion status to the Amazon Simple Notification
--   Service topic that you specify in <tt>NotificationChannel</tt>.
--   
--   To get the results of the person detection operation, first check that
--   the status value published to the Amazon SNS topic is
--   <tt>SUCCEEDED</tt>. If so, call GetPersonTracking and pass the job
--   identifier (<tt>JobId</tt>) from the initial call to
--   <tt>StartPersonTracking</tt>.
module Network.AWS.Rekognition.StartPersonTracking

-- | <i>See:</i> <a>newStartPersonTracking</a> smart constructor.
data StartPersonTracking
StartPersonTracking' :: Maybe Text -> Maybe NotificationChannel -> Maybe Text -> Video -> StartPersonTracking

-- | An identifier you specify that's returned in the completion
--   notification that's published to your Amazon Simple Notification
--   Service topic. For example, you can use <tt>JobTag</tt> to group
--   related jobs and identify them in the completion notification.
[$sel:jobTag:StartPersonTracking'] :: StartPersonTracking -> Maybe Text

-- | The Amazon SNS topic ARN you want Amazon Rekognition Video to publish
--   the completion status of the people detection operation to. The Amazon
--   SNS topic must have a topic name that begins with
--   <i>AmazonRekognition</i> if you are using the
--   AmazonRekognitionServiceRole permissions policy.
[$sel:notificationChannel:StartPersonTracking'] :: StartPersonTracking -> Maybe NotificationChannel

-- | Idempotent token used to identify the start request. If you use the
--   same token with multiple <tt>StartPersonTracking</tt> requests, the
--   same <tt>JobId</tt> is returned. Use <tt>ClientRequestToken</tt> to
--   prevent the same job from being accidently started more than once.
[$sel:clientRequestToken:StartPersonTracking'] :: StartPersonTracking -> Maybe Text

-- | The video in which you want to detect people. The video must be stored
--   in an Amazon S3 bucket.
[$sel:video:StartPersonTracking'] :: StartPersonTracking -> Video

-- | Create a value of <a>StartPersonTracking</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:jobTag:StartPersonTracking'</a>,
--   <a>startPersonTracking_jobTag</a> - An identifier you specify that's
--   returned in the completion notification that's published to your
--   Amazon Simple Notification Service topic. For example, you can use
--   <tt>JobTag</tt> to group related jobs and identify them in the
--   completion notification.
--   
--   <a>$sel:notificationChannel:StartPersonTracking'</a>,
--   <a>startPersonTracking_notificationChannel</a> - The Amazon SNS topic
--   ARN you want Amazon Rekognition Video to publish the completion status
--   of the people detection operation to. The Amazon SNS topic must have a
--   topic name that begins with <i>AmazonRekognition</i> if you are using
--   the AmazonRekognitionServiceRole permissions policy.
--   
--   <a>$sel:clientRequestToken:StartPersonTracking'</a>,
--   <a>startPersonTracking_clientRequestToken</a> - Idempotent token used
--   to identify the start request. If you use the same token with multiple
--   <tt>StartPersonTracking</tt> requests, the same <tt>JobId</tt> is
--   returned. Use <tt>ClientRequestToken</tt> to prevent the same job from
--   being accidently started more than once.
--   
--   <a>$sel:video:StartPersonTracking'</a>,
--   <a>startPersonTracking_video</a> - The video in which you want to
--   detect people. The video must be stored in an Amazon S3 bucket.
newStartPersonTracking :: Video -> StartPersonTracking

-- | An identifier you specify that's returned in the completion
--   notification that's published to your Amazon Simple Notification
--   Service topic. For example, you can use <tt>JobTag</tt> to group
--   related jobs and identify them in the completion notification.
startPersonTracking_jobTag :: Lens' StartPersonTracking (Maybe Text)

-- | The Amazon SNS topic ARN you want Amazon Rekognition Video to publish
--   the completion status of the people detection operation to. The Amazon
--   SNS topic must have a topic name that begins with
--   <i>AmazonRekognition</i> if you are using the
--   AmazonRekognitionServiceRole permissions policy.
startPersonTracking_notificationChannel :: Lens' StartPersonTracking (Maybe NotificationChannel)

-- | Idempotent token used to identify the start request. If you use the
--   same token with multiple <tt>StartPersonTracking</tt> requests, the
--   same <tt>JobId</tt> is returned. Use <tt>ClientRequestToken</tt> to
--   prevent the same job from being accidently started more than once.
startPersonTracking_clientRequestToken :: Lens' StartPersonTracking (Maybe Text)

-- | The video in which you want to detect people. The video must be stored
--   in an Amazon S3 bucket.
startPersonTracking_video :: Lens' StartPersonTracking Video

-- | <i>See:</i> <a>newStartPersonTrackingResponse</a> smart constructor.
data StartPersonTrackingResponse
StartPersonTrackingResponse' :: Maybe Text -> Int -> StartPersonTrackingResponse

-- | The identifier for the person detection job. Use <tt>JobId</tt> to
--   identify the job in a subsequent call to <tt>GetPersonTracking</tt>.
[$sel:jobId:StartPersonTrackingResponse'] :: StartPersonTrackingResponse -> Maybe Text

-- | The response's http status code.
[$sel:httpStatus:StartPersonTrackingResponse'] :: StartPersonTrackingResponse -> Int

-- | Create a value of <a>StartPersonTrackingResponse</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:jobId:StartPersonTrackingResponse'</a>,
--   <a>startPersonTrackingResponse_jobId</a> - The identifier for the
--   person detection job. Use <tt>JobId</tt> to identify the job in a
--   subsequent call to <tt>GetPersonTracking</tt>.
--   
--   <a>$sel:httpStatus:StartPersonTrackingResponse'</a>,
--   <a>startPersonTrackingResponse_httpStatus</a> - The response's http
--   status code.
newStartPersonTrackingResponse :: Int -> StartPersonTrackingResponse

-- | The identifier for the person detection job. Use <tt>JobId</tt> to
--   identify the job in a subsequent call to <tt>GetPersonTracking</tt>.
startPersonTrackingResponse_jobId :: Lens' StartPersonTrackingResponse (Maybe Text)

-- | The response's http status code.
startPersonTrackingResponse_httpStatus :: Lens' StartPersonTrackingResponse Int
instance GHC.Generics.Generic Network.AWS.Rekognition.StartPersonTracking.StartPersonTracking
instance GHC.Show.Show Network.AWS.Rekognition.StartPersonTracking.StartPersonTracking
instance GHC.Read.Read Network.AWS.Rekognition.StartPersonTracking.StartPersonTracking
instance GHC.Classes.Eq Network.AWS.Rekognition.StartPersonTracking.StartPersonTracking
instance GHC.Generics.Generic Network.AWS.Rekognition.StartPersonTracking.StartPersonTrackingResponse
instance GHC.Show.Show Network.AWS.Rekognition.StartPersonTracking.StartPersonTrackingResponse
instance GHC.Read.Read Network.AWS.Rekognition.StartPersonTracking.StartPersonTrackingResponse
instance GHC.Classes.Eq Network.AWS.Rekognition.StartPersonTracking.StartPersonTrackingResponse
instance Network.AWS.Types.AWSRequest Network.AWS.Rekognition.StartPersonTracking.StartPersonTracking
instance Control.DeepSeq.NFData Network.AWS.Rekognition.StartPersonTracking.StartPersonTrackingResponse
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.StartPersonTracking.StartPersonTracking
instance Control.DeepSeq.NFData Network.AWS.Rekognition.StartPersonTracking.StartPersonTracking
instance Network.AWS.Data.Headers.ToHeaders Network.AWS.Rekognition.StartPersonTracking.StartPersonTracking
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.StartPersonTracking.StartPersonTracking
instance Network.AWS.Data.Path.ToPath Network.AWS.Rekognition.StartPersonTracking.StartPersonTracking
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.StartPersonTracking.StartPersonTracking


-- | Starts asynchronous detection of labels in a stored video.
--   
--   Amazon Rekognition Video can detect labels in a video. Labels are
--   instances of real-world entities. This includes objects like flower,
--   tree, and table; events like wedding, graduation, and birthday party;
--   concepts like landscape, evening, and nature; and activities like a
--   person getting out of a car or a person skiing.
--   
--   The video must be stored in an Amazon S3 bucket. Use Video to specify
--   the bucket name and the filename of the video.
--   <tt>StartLabelDetection</tt> returns a job identifier (<tt>JobId</tt>)
--   which you use to get the results of the operation. When label
--   detection is finished, Amazon Rekognition Video publishes a completion
--   status to the Amazon Simple Notification Service topic that you
--   specify in <tt>NotificationChannel</tt>.
--   
--   To get the results of the label detection operation, first check that
--   the status value published to the Amazon SNS topic is
--   <tt>SUCCEEDED</tt>. If so, call GetLabelDetection and pass the job
--   identifier (<tt>JobId</tt>) from the initial call to
--   <tt>StartLabelDetection</tt>.
module Network.AWS.Rekognition.StartLabelDetection

-- | <i>See:</i> <a>newStartLabelDetection</a> smart constructor.
data StartLabelDetection
StartLabelDetection' :: Maybe Text -> Maybe NotificationChannel -> Maybe Text -> Maybe Double -> Video -> StartLabelDetection

-- | An identifier you specify that's returned in the completion
--   notification that's published to your Amazon Simple Notification
--   Service topic. For example, you can use <tt>JobTag</tt> to group
--   related jobs and identify them in the completion notification.
[$sel:jobTag:StartLabelDetection'] :: StartLabelDetection -> Maybe Text

-- | The Amazon SNS topic ARN you want Amazon Rekognition Video to publish
--   the completion status of the label detection operation to. The Amazon
--   SNS topic must have a topic name that begins with
--   <i>AmazonRekognition</i> if you are using the
--   AmazonRekognitionServiceRole permissions policy.
[$sel:notificationChannel:StartLabelDetection'] :: StartLabelDetection -> Maybe NotificationChannel

-- | Idempotent token used to identify the start request. If you use the
--   same token with multiple <tt>StartLabelDetection</tt> requests, the
--   same <tt>JobId</tt> is returned. Use <tt>ClientRequestToken</tt> to
--   prevent the same job from being accidently started more than once.
[$sel:clientRequestToken:StartLabelDetection'] :: StartLabelDetection -> Maybe Text

-- | Specifies the minimum confidence that Amazon Rekognition Video must
--   have in order to return a detected label. Confidence represents how
--   certain Amazon Rekognition is that a label is correctly identified.0
--   is the lowest confidence. 100 is the highest confidence. Amazon
--   Rekognition Video doesn't return any labels with a confidence level
--   lower than this specified value.
--   
--   If you don't specify <tt>MinConfidence</tt>, the operation returns
--   labels with confidence values greater than or equal to 50 percent.
[$sel:minConfidence:StartLabelDetection'] :: StartLabelDetection -> Maybe Double

-- | The video in which you want to detect labels. The video must be stored
--   in an Amazon S3 bucket.
[$sel:video:StartLabelDetection'] :: StartLabelDetection -> Video

-- | Create a value of <a>StartLabelDetection</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:jobTag:StartLabelDetection'</a>,
--   <a>startLabelDetection_jobTag</a> - An identifier you specify that's
--   returned in the completion notification that's published to your
--   Amazon Simple Notification Service topic. For example, you can use
--   <tt>JobTag</tt> to group related jobs and identify them in the
--   completion notification.
--   
--   <a>$sel:notificationChannel:StartLabelDetection'</a>,
--   <a>startLabelDetection_notificationChannel</a> - The Amazon SNS topic
--   ARN you want Amazon Rekognition Video to publish the completion status
--   of the label detection operation to. The Amazon SNS topic must have a
--   topic name that begins with <i>AmazonRekognition</i> if you are using
--   the AmazonRekognitionServiceRole permissions policy.
--   
--   <a>$sel:clientRequestToken:StartLabelDetection'</a>,
--   <a>startLabelDetection_clientRequestToken</a> - Idempotent token used
--   to identify the start request. If you use the same token with multiple
--   <tt>StartLabelDetection</tt> requests, the same <tt>JobId</tt> is
--   returned. Use <tt>ClientRequestToken</tt> to prevent the same job from
--   being accidently started more than once.
--   
--   <a>$sel:minConfidence:StartLabelDetection'</a>,
--   <a>startLabelDetection_minConfidence</a> - Specifies the minimum
--   confidence that Amazon Rekognition Video must have in order to return
--   a detected label. Confidence represents how certain Amazon Rekognition
--   is that a label is correctly identified.0 is the lowest confidence.
--   100 is the highest confidence. Amazon Rekognition Video doesn't return
--   any labels with a confidence level lower than this specified value.
--   
--   If you don't specify <tt>MinConfidence</tt>, the operation returns
--   labels with confidence values greater than or equal to 50 percent.
--   
--   <a>$sel:video:StartLabelDetection'</a>,
--   <a>startLabelDetection_video</a> - The video in which you want to
--   detect labels. The video must be stored in an Amazon S3 bucket.
newStartLabelDetection :: Video -> StartLabelDetection

-- | An identifier you specify that's returned in the completion
--   notification that's published to your Amazon Simple Notification
--   Service topic. For example, you can use <tt>JobTag</tt> to group
--   related jobs and identify them in the completion notification.
startLabelDetection_jobTag :: Lens' StartLabelDetection (Maybe Text)

-- | The Amazon SNS topic ARN you want Amazon Rekognition Video to publish
--   the completion status of the label detection operation to. The Amazon
--   SNS topic must have a topic name that begins with
--   <i>AmazonRekognition</i> if you are using the
--   AmazonRekognitionServiceRole permissions policy.
startLabelDetection_notificationChannel :: Lens' StartLabelDetection (Maybe NotificationChannel)

-- | Idempotent token used to identify the start request. If you use the
--   same token with multiple <tt>StartLabelDetection</tt> requests, the
--   same <tt>JobId</tt> is returned. Use <tt>ClientRequestToken</tt> to
--   prevent the same job from being accidently started more than once.
startLabelDetection_clientRequestToken :: Lens' StartLabelDetection (Maybe Text)

-- | Specifies the minimum confidence that Amazon Rekognition Video must
--   have in order to return a detected label. Confidence represents how
--   certain Amazon Rekognition is that a label is correctly identified.0
--   is the lowest confidence. 100 is the highest confidence. Amazon
--   Rekognition Video doesn't return any labels with a confidence level
--   lower than this specified value.
--   
--   If you don't specify <tt>MinConfidence</tt>, the operation returns
--   labels with confidence values greater than or equal to 50 percent.
startLabelDetection_minConfidence :: Lens' StartLabelDetection (Maybe Double)

-- | The video in which you want to detect labels. The video must be stored
--   in an Amazon S3 bucket.
startLabelDetection_video :: Lens' StartLabelDetection Video

-- | <i>See:</i> <a>newStartLabelDetectionResponse</a> smart constructor.
data StartLabelDetectionResponse
StartLabelDetectionResponse' :: Maybe Text -> Int -> StartLabelDetectionResponse

-- | The identifier for the label detection job. Use <tt>JobId</tt> to
--   identify the job in a subsequent call to <tt>GetLabelDetection</tt>.
[$sel:jobId:StartLabelDetectionResponse'] :: StartLabelDetectionResponse -> Maybe Text

-- | The response's http status code.
[$sel:httpStatus:StartLabelDetectionResponse'] :: StartLabelDetectionResponse -> Int

-- | Create a value of <a>StartLabelDetectionResponse</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:jobId:StartLabelDetectionResponse'</a>,
--   <a>startLabelDetectionResponse_jobId</a> - The identifier for the
--   label detection job. Use <tt>JobId</tt> to identify the job in a
--   subsequent call to <tt>GetLabelDetection</tt>.
--   
--   <a>$sel:httpStatus:StartLabelDetectionResponse'</a>,
--   <a>startLabelDetectionResponse_httpStatus</a> - The response's http
--   status code.
newStartLabelDetectionResponse :: Int -> StartLabelDetectionResponse

-- | The identifier for the label detection job. Use <tt>JobId</tt> to
--   identify the job in a subsequent call to <tt>GetLabelDetection</tt>.
startLabelDetectionResponse_jobId :: Lens' StartLabelDetectionResponse (Maybe Text)

-- | The response's http status code.
startLabelDetectionResponse_httpStatus :: Lens' StartLabelDetectionResponse Int
instance GHC.Generics.Generic Network.AWS.Rekognition.StartLabelDetection.StartLabelDetection
instance GHC.Show.Show Network.AWS.Rekognition.StartLabelDetection.StartLabelDetection
instance GHC.Read.Read Network.AWS.Rekognition.StartLabelDetection.StartLabelDetection
instance GHC.Classes.Eq Network.AWS.Rekognition.StartLabelDetection.StartLabelDetection
instance GHC.Generics.Generic Network.AWS.Rekognition.StartLabelDetection.StartLabelDetectionResponse
instance GHC.Show.Show Network.AWS.Rekognition.StartLabelDetection.StartLabelDetectionResponse
instance GHC.Read.Read Network.AWS.Rekognition.StartLabelDetection.StartLabelDetectionResponse
instance GHC.Classes.Eq Network.AWS.Rekognition.StartLabelDetection.StartLabelDetectionResponse
instance Network.AWS.Types.AWSRequest Network.AWS.Rekognition.StartLabelDetection.StartLabelDetection
instance Control.DeepSeq.NFData Network.AWS.Rekognition.StartLabelDetection.StartLabelDetectionResponse
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.StartLabelDetection.StartLabelDetection
instance Control.DeepSeq.NFData Network.AWS.Rekognition.StartLabelDetection.StartLabelDetection
instance Network.AWS.Data.Headers.ToHeaders Network.AWS.Rekognition.StartLabelDetection.StartLabelDetection
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.StartLabelDetection.StartLabelDetection
instance Network.AWS.Data.Path.ToPath Network.AWS.Rekognition.StartLabelDetection.StartLabelDetection
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.StartLabelDetection.StartLabelDetection


-- | Starts the asynchronous search for faces in a collection that match
--   the faces of persons detected in a stored video.
--   
--   The video must be stored in an Amazon S3 bucket. Use Video to specify
--   the bucket name and the filename of the video.
--   <tt>StartFaceSearch</tt> returns a job identifier (<tt>JobId</tt>)
--   which you use to get the search results once the search has completed.
--   When searching is finished, Amazon Rekognition Video publishes a
--   completion status to the Amazon Simple Notification Service topic that
--   you specify in <tt>NotificationChannel</tt>. To get the search
--   results, first check that the status value published to the Amazon SNS
--   topic is <tt>SUCCEEDED</tt>. If so, call GetFaceSearch and pass the
--   job identifier (<tt>JobId</tt>) from the initial call to
--   <tt>StartFaceSearch</tt>. For more information, see
--   procedure-person-search-videos.
module Network.AWS.Rekognition.StartFaceSearch

-- | <i>See:</i> <a>newStartFaceSearch</a> smart constructor.
data StartFaceSearch
StartFaceSearch' :: Maybe Double -> Maybe Text -> Maybe NotificationChannel -> Maybe Text -> Video -> Text -> StartFaceSearch

-- | The minimum confidence in the person match to return. For example,
--   don't return any matches where confidence in matches is less than 70%.
--   The default value is 80%.
[$sel:faceMatchThreshold:StartFaceSearch'] :: StartFaceSearch -> Maybe Double

-- | An identifier you specify that's returned in the completion
--   notification that's published to your Amazon Simple Notification
--   Service topic. For example, you can use <tt>JobTag</tt> to group
--   related jobs and identify them in the completion notification.
[$sel:jobTag:StartFaceSearch'] :: StartFaceSearch -> Maybe Text

-- | The ARN of the Amazon SNS topic to which you want Amazon Rekognition
--   Video to publish the completion status of the search. The Amazon SNS
--   topic must have a topic name that begins with <i>AmazonRekognition</i>
--   if you are using the AmazonRekognitionServiceRole permissions policy
--   to access the topic.
[$sel:notificationChannel:StartFaceSearch'] :: StartFaceSearch -> Maybe NotificationChannel

-- | Idempotent token used to identify the start request. If you use the
--   same token with multiple <tt>StartFaceSearch</tt> requests, the same
--   <tt>JobId</tt> is returned. Use <tt>ClientRequestToken</tt> to prevent
--   the same job from being accidently started more than once.
[$sel:clientRequestToken:StartFaceSearch'] :: StartFaceSearch -> Maybe Text

-- | The video you want to search. The video must be stored in an Amazon S3
--   bucket.
[$sel:video:StartFaceSearch'] :: StartFaceSearch -> Video

-- | ID of the collection that contains the faces you want to search for.
[$sel:collectionId:StartFaceSearch'] :: StartFaceSearch -> Text

-- | Create a value of <a>StartFaceSearch</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:faceMatchThreshold:StartFaceSearch'</a>,
--   <a>startFaceSearch_faceMatchThreshold</a> - The minimum confidence in
--   the person match to return. For example, don't return any matches
--   where confidence in matches is less than 70%. The default value is
--   80%.
--   
--   <a>$sel:jobTag:StartFaceSearch'</a>, <a>startFaceSearch_jobTag</a> -
--   An identifier you specify that's returned in the completion
--   notification that's published to your Amazon Simple Notification
--   Service topic. For example, you can use <tt>JobTag</tt> to group
--   related jobs and identify them in the completion notification.
--   
--   <a>$sel:notificationChannel:StartFaceSearch'</a>,
--   <a>startFaceSearch_notificationChannel</a> - The ARN of the Amazon SNS
--   topic to which you want Amazon Rekognition Video to publish the
--   completion status of the search. The Amazon SNS topic must have a
--   topic name that begins with <i>AmazonRekognition</i> if you are using
--   the AmazonRekognitionServiceRole permissions policy to access the
--   topic.
--   
--   <a>$sel:clientRequestToken:StartFaceSearch'</a>,
--   <a>startFaceSearch_clientRequestToken</a> - Idempotent token used to
--   identify the start request. If you use the same token with multiple
--   <tt>StartFaceSearch</tt> requests, the same <tt>JobId</tt> is
--   returned. Use <tt>ClientRequestToken</tt> to prevent the same job from
--   being accidently started more than once.
--   
--   <a>$sel:video:StartFaceSearch'</a>, <a>startFaceSearch_video</a> - The
--   video you want to search. The video must be stored in an Amazon S3
--   bucket.
--   
--   <a>$sel:collectionId:StartFaceSearch'</a>,
--   <a>startFaceSearch_collectionId</a> - ID of the collection that
--   contains the faces you want to search for.
newStartFaceSearch :: Video -> Text -> StartFaceSearch

-- | The minimum confidence in the person match to return. For example,
--   don't return any matches where confidence in matches is less than 70%.
--   The default value is 80%.
startFaceSearch_faceMatchThreshold :: Lens' StartFaceSearch (Maybe Double)

-- | An identifier you specify that's returned in the completion
--   notification that's published to your Amazon Simple Notification
--   Service topic. For example, you can use <tt>JobTag</tt> to group
--   related jobs and identify them in the completion notification.
startFaceSearch_jobTag :: Lens' StartFaceSearch (Maybe Text)

-- | The ARN of the Amazon SNS topic to which you want Amazon Rekognition
--   Video to publish the completion status of the search. The Amazon SNS
--   topic must have a topic name that begins with <i>AmazonRekognition</i>
--   if you are using the AmazonRekognitionServiceRole permissions policy
--   to access the topic.
startFaceSearch_notificationChannel :: Lens' StartFaceSearch (Maybe NotificationChannel)

-- | Idempotent token used to identify the start request. If you use the
--   same token with multiple <tt>StartFaceSearch</tt> requests, the same
--   <tt>JobId</tt> is returned. Use <tt>ClientRequestToken</tt> to prevent
--   the same job from being accidently started more than once.
startFaceSearch_clientRequestToken :: Lens' StartFaceSearch (Maybe Text)

-- | The video you want to search. The video must be stored in an Amazon S3
--   bucket.
startFaceSearch_video :: Lens' StartFaceSearch Video

-- | ID of the collection that contains the faces you want to search for.
startFaceSearch_collectionId :: Lens' StartFaceSearch Text

-- | <i>See:</i> <a>newStartFaceSearchResponse</a> smart constructor.
data StartFaceSearchResponse
StartFaceSearchResponse' :: Maybe Text -> Int -> StartFaceSearchResponse

-- | The identifier for the search job. Use <tt>JobId</tt> to identify the
--   job in a subsequent call to <tt>GetFaceSearch</tt>.
[$sel:jobId:StartFaceSearchResponse'] :: StartFaceSearchResponse -> Maybe Text

-- | The response's http status code.
[$sel:httpStatus:StartFaceSearchResponse'] :: StartFaceSearchResponse -> Int

-- | Create a value of <a>StartFaceSearchResponse</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:jobId:StartFaceSearchResponse'</a>,
--   <a>startFaceSearchResponse_jobId</a> - The identifier for the search
--   job. Use <tt>JobId</tt> to identify the job in a subsequent call to
--   <tt>GetFaceSearch</tt>.
--   
--   <a>$sel:httpStatus:StartFaceSearchResponse'</a>,
--   <a>startFaceSearchResponse_httpStatus</a> - The response's http status
--   code.
newStartFaceSearchResponse :: Int -> StartFaceSearchResponse

-- | The identifier for the search job. Use <tt>JobId</tt> to identify the
--   job in a subsequent call to <tt>GetFaceSearch</tt>.
startFaceSearchResponse_jobId :: Lens' StartFaceSearchResponse (Maybe Text)

-- | The response's http status code.
startFaceSearchResponse_httpStatus :: Lens' StartFaceSearchResponse Int
instance GHC.Generics.Generic Network.AWS.Rekognition.StartFaceSearch.StartFaceSearch
instance GHC.Show.Show Network.AWS.Rekognition.StartFaceSearch.StartFaceSearch
instance GHC.Read.Read Network.AWS.Rekognition.StartFaceSearch.StartFaceSearch
instance GHC.Classes.Eq Network.AWS.Rekognition.StartFaceSearch.StartFaceSearch
instance GHC.Generics.Generic Network.AWS.Rekognition.StartFaceSearch.StartFaceSearchResponse
instance GHC.Show.Show Network.AWS.Rekognition.StartFaceSearch.StartFaceSearchResponse
instance GHC.Read.Read Network.AWS.Rekognition.StartFaceSearch.StartFaceSearchResponse
instance GHC.Classes.Eq Network.AWS.Rekognition.StartFaceSearch.StartFaceSearchResponse
instance Network.AWS.Types.AWSRequest Network.AWS.Rekognition.StartFaceSearch.StartFaceSearch
instance Control.DeepSeq.NFData Network.AWS.Rekognition.StartFaceSearch.StartFaceSearchResponse
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.StartFaceSearch.StartFaceSearch
instance Control.DeepSeq.NFData Network.AWS.Rekognition.StartFaceSearch.StartFaceSearch
instance Network.AWS.Data.Headers.ToHeaders Network.AWS.Rekognition.StartFaceSearch.StartFaceSearch
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.StartFaceSearch.StartFaceSearch
instance Network.AWS.Data.Path.ToPath Network.AWS.Rekognition.StartFaceSearch.StartFaceSearch
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.StartFaceSearch.StartFaceSearch


-- | Starts asynchronous detection of faces in a stored video.
--   
--   Amazon Rekognition Video can detect faces in a video stored in an
--   Amazon S3 bucket. Use Video to specify the bucket name and the
--   filename of the video. <tt>StartFaceDetection</tt> returns a job
--   identifier (<tt>JobId</tt>) that you use to get the results of the
--   operation. When face detection is finished, Amazon Rekognition Video
--   publishes a completion status to the Amazon Simple Notification
--   Service topic that you specify in <tt>NotificationChannel</tt>. To get
--   the results of the face detection operation, first check that the
--   status value published to the Amazon SNS topic is <tt>SUCCEEDED</tt>.
--   If so, call GetFaceDetection and pass the job identifier
--   (<tt>JobId</tt>) from the initial call to <tt>StartFaceDetection</tt>.
--   
--   For more information, see Detecting Faces in a Stored Video in the
--   Amazon Rekognition Developer Guide.
module Network.AWS.Rekognition.StartFaceDetection

-- | <i>See:</i> <a>newStartFaceDetection</a> smart constructor.
data StartFaceDetection
StartFaceDetection' :: Maybe Text -> Maybe NotificationChannel -> Maybe Text -> Maybe FaceAttributes -> Video -> StartFaceDetection

-- | An identifier you specify that's returned in the completion
--   notification that's published to your Amazon Simple Notification
--   Service topic. For example, you can use <tt>JobTag</tt> to group
--   related jobs and identify them in the completion notification.
[$sel:jobTag:StartFaceDetection'] :: StartFaceDetection -> Maybe Text

-- | The ARN of the Amazon SNS topic to which you want Amazon Rekognition
--   Video to publish the completion status of the face detection
--   operation. The Amazon SNS topic must have a topic name that begins
--   with <i>AmazonRekognition</i> if you are using the
--   AmazonRekognitionServiceRole permissions policy.
[$sel:notificationChannel:StartFaceDetection'] :: StartFaceDetection -> Maybe NotificationChannel

-- | Idempotent token used to identify the start request. If you use the
--   same token with multiple <tt>StartFaceDetection</tt> requests, the
--   same <tt>JobId</tt> is returned. Use <tt>ClientRequestToken</tt> to
--   prevent the same job from being accidently started more than once.
[$sel:clientRequestToken:StartFaceDetection'] :: StartFaceDetection -> Maybe Text

-- | The face attributes you want returned.
--   
--   <tt>DEFAULT</tt> - The following subset of facial attributes are
--   returned: BoundingBox, Confidence, Pose, Quality and Landmarks.
--   
--   <tt>ALL</tt> - All facial attributes are returned.
[$sel:faceAttributes:StartFaceDetection'] :: StartFaceDetection -> Maybe FaceAttributes

-- | The video in which you want to detect faces. The video must be stored
--   in an Amazon S3 bucket.
[$sel:video:StartFaceDetection'] :: StartFaceDetection -> Video

-- | Create a value of <a>StartFaceDetection</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:jobTag:StartFaceDetection'</a>,
--   <a>startFaceDetection_jobTag</a> - An identifier you specify that's
--   returned in the completion notification that's published to your
--   Amazon Simple Notification Service topic. For example, you can use
--   <tt>JobTag</tt> to group related jobs and identify them in the
--   completion notification.
--   
--   <a>$sel:notificationChannel:StartFaceDetection'</a>,
--   <a>startFaceDetection_notificationChannel</a> - The ARN of the Amazon
--   SNS topic to which you want Amazon Rekognition Video to publish the
--   completion status of the face detection operation. The Amazon SNS
--   topic must have a topic name that begins with <i>AmazonRekognition</i>
--   if you are using the AmazonRekognitionServiceRole permissions policy.
--   
--   <a>$sel:clientRequestToken:StartFaceDetection'</a>,
--   <a>startFaceDetection_clientRequestToken</a> - Idempotent token used
--   to identify the start request. If you use the same token with multiple
--   <tt>StartFaceDetection</tt> requests, the same <tt>JobId</tt> is
--   returned. Use <tt>ClientRequestToken</tt> to prevent the same job from
--   being accidently started more than once.
--   
--   <a>$sel:faceAttributes:StartFaceDetection'</a>,
--   <a>startFaceDetection_faceAttributes</a> - The face attributes you
--   want returned.
--   
--   <tt>DEFAULT</tt> - The following subset of facial attributes are
--   returned: BoundingBox, Confidence, Pose, Quality and Landmarks.
--   
--   <tt>ALL</tt> - All facial attributes are returned.
--   
--   <a>$sel:video:StartFaceDetection'</a>, <a>startFaceDetection_video</a>
--   - The video in which you want to detect faces. The video must be
--   stored in an Amazon S3 bucket.
newStartFaceDetection :: Video -> StartFaceDetection

-- | An identifier you specify that's returned in the completion
--   notification that's published to your Amazon Simple Notification
--   Service topic. For example, you can use <tt>JobTag</tt> to group
--   related jobs and identify them in the completion notification.
startFaceDetection_jobTag :: Lens' StartFaceDetection (Maybe Text)

-- | The ARN of the Amazon SNS topic to which you want Amazon Rekognition
--   Video to publish the completion status of the face detection
--   operation. The Amazon SNS topic must have a topic name that begins
--   with <i>AmazonRekognition</i> if you are using the
--   AmazonRekognitionServiceRole permissions policy.
startFaceDetection_notificationChannel :: Lens' StartFaceDetection (Maybe NotificationChannel)

-- | Idempotent token used to identify the start request. If you use the
--   same token with multiple <tt>StartFaceDetection</tt> requests, the
--   same <tt>JobId</tt> is returned. Use <tt>ClientRequestToken</tt> to
--   prevent the same job from being accidently started more than once.
startFaceDetection_clientRequestToken :: Lens' StartFaceDetection (Maybe Text)

-- | The face attributes you want returned.
--   
--   <tt>DEFAULT</tt> - The following subset of facial attributes are
--   returned: BoundingBox, Confidence, Pose, Quality and Landmarks.
--   
--   <tt>ALL</tt> - All facial attributes are returned.
startFaceDetection_faceAttributes :: Lens' StartFaceDetection (Maybe FaceAttributes)

-- | The video in which you want to detect faces. The video must be stored
--   in an Amazon S3 bucket.
startFaceDetection_video :: Lens' StartFaceDetection Video

-- | <i>See:</i> <a>newStartFaceDetectionResponse</a> smart constructor.
data StartFaceDetectionResponse
StartFaceDetectionResponse' :: Maybe Text -> Int -> StartFaceDetectionResponse

-- | The identifier for the face detection job. Use <tt>JobId</tt> to
--   identify the job in a subsequent call to <tt>GetFaceDetection</tt>.
[$sel:jobId:StartFaceDetectionResponse'] :: StartFaceDetectionResponse -> Maybe Text

-- | The response's http status code.
[$sel:httpStatus:StartFaceDetectionResponse'] :: StartFaceDetectionResponse -> Int

-- | Create a value of <a>StartFaceDetectionResponse</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:jobId:StartFaceDetectionResponse'</a>,
--   <a>startFaceDetectionResponse_jobId</a> - The identifier for the face
--   detection job. Use <tt>JobId</tt> to identify the job in a subsequent
--   call to <tt>GetFaceDetection</tt>.
--   
--   <a>$sel:httpStatus:StartFaceDetectionResponse'</a>,
--   <a>startFaceDetectionResponse_httpStatus</a> - The response's http
--   status code.
newStartFaceDetectionResponse :: Int -> StartFaceDetectionResponse

-- | The identifier for the face detection job. Use <tt>JobId</tt> to
--   identify the job in a subsequent call to <tt>GetFaceDetection</tt>.
startFaceDetectionResponse_jobId :: Lens' StartFaceDetectionResponse (Maybe Text)

-- | The response's http status code.
startFaceDetectionResponse_httpStatus :: Lens' StartFaceDetectionResponse Int
instance GHC.Generics.Generic Network.AWS.Rekognition.StartFaceDetection.StartFaceDetection
instance GHC.Show.Show Network.AWS.Rekognition.StartFaceDetection.StartFaceDetection
instance GHC.Read.Read Network.AWS.Rekognition.StartFaceDetection.StartFaceDetection
instance GHC.Classes.Eq Network.AWS.Rekognition.StartFaceDetection.StartFaceDetection
instance GHC.Generics.Generic Network.AWS.Rekognition.StartFaceDetection.StartFaceDetectionResponse
instance GHC.Show.Show Network.AWS.Rekognition.StartFaceDetection.StartFaceDetectionResponse
instance GHC.Read.Read Network.AWS.Rekognition.StartFaceDetection.StartFaceDetectionResponse
instance GHC.Classes.Eq Network.AWS.Rekognition.StartFaceDetection.StartFaceDetectionResponse
instance Network.AWS.Types.AWSRequest Network.AWS.Rekognition.StartFaceDetection.StartFaceDetection
instance Control.DeepSeq.NFData Network.AWS.Rekognition.StartFaceDetection.StartFaceDetectionResponse
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.StartFaceDetection.StartFaceDetection
instance Control.DeepSeq.NFData Network.AWS.Rekognition.StartFaceDetection.StartFaceDetection
instance Network.AWS.Data.Headers.ToHeaders Network.AWS.Rekognition.StartFaceDetection.StartFaceDetection
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.StartFaceDetection.StartFaceDetection
instance Network.AWS.Data.Path.ToPath Network.AWS.Rekognition.StartFaceDetection.StartFaceDetection
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.StartFaceDetection.StartFaceDetection


-- | Starts asynchronous detection of inappropriate, unwanted, or offensive
--   content in a stored video. For a list of moderation labels in Amazon
--   Rekognition, see <a>Using the image and video moderation APIs</a>.
--   
--   Amazon Rekognition Video can moderate content in a video stored in an
--   Amazon S3 bucket. Use Video to specify the bucket name and the
--   filename of the video. <tt>StartContentModeration</tt> returns a job
--   identifier (<tt>JobId</tt>) which you use to get the results of the
--   analysis. When content analysis is finished, Amazon Rekognition Video
--   publishes a completion status to the Amazon Simple Notification
--   Service topic that you specify in <tt>NotificationChannel</tt>.
--   
--   To get the results of the content analysis, first check that the
--   status value published to the Amazon SNS topic is <tt>SUCCEEDED</tt>.
--   If so, call GetContentModeration and pass the job identifier
--   (<tt>JobId</tt>) from the initial call to
--   <tt>StartContentModeration</tt>.
--   
--   For more information, see Content moderation in the Amazon Rekognition
--   Developer Guide.
module Network.AWS.Rekognition.StartContentModeration

-- | <i>See:</i> <a>newStartContentModeration</a> smart constructor.
data StartContentModeration
StartContentModeration' :: Maybe Text -> Maybe NotificationChannel -> Maybe Text -> Maybe Double -> Video -> StartContentModeration

-- | An identifier you specify that's returned in the completion
--   notification that's published to your Amazon Simple Notification
--   Service topic. For example, you can use <tt>JobTag</tt> to group
--   related jobs and identify them in the completion notification.
[$sel:jobTag:StartContentModeration'] :: StartContentModeration -> Maybe Text

-- | The Amazon SNS topic ARN that you want Amazon Rekognition Video to
--   publish the completion status of the content analysis to. The Amazon
--   SNS topic must have a topic name that begins with
--   <i>AmazonRekognition</i> if you are using the
--   AmazonRekognitionServiceRole permissions policy to access the topic.
[$sel:notificationChannel:StartContentModeration'] :: StartContentModeration -> Maybe NotificationChannel

-- | Idempotent token used to identify the start request. If you use the
--   same token with multiple <tt>StartContentModeration</tt> requests, the
--   same <tt>JobId</tt> is returned. Use <tt>ClientRequestToken</tt> to
--   prevent the same job from being accidently started more than once.
[$sel:clientRequestToken:StartContentModeration'] :: StartContentModeration -> Maybe Text

-- | Specifies the minimum confidence that Amazon Rekognition must have in
--   order to return a moderated content label. Confidence represents how
--   certain Amazon Rekognition is that the moderated content is correctly
--   identified. 0 is the lowest confidence. 100 is the highest confidence.
--   Amazon Rekognition doesn't return any moderated content labels with a
--   confidence level lower than this specified value. If you don't specify
--   <tt>MinConfidence</tt>, <tt>GetContentModeration</tt> returns labels
--   with confidence values greater than or equal to 50 percent.
[$sel:minConfidence:StartContentModeration'] :: StartContentModeration -> Maybe Double

-- | The video in which you want to detect inappropriate, unwanted, or
--   offensive content. The video must be stored in an Amazon S3 bucket.
[$sel:video:StartContentModeration'] :: StartContentModeration -> Video

-- | Create a value of <a>StartContentModeration</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:jobTag:StartContentModeration'</a>,
--   <a>startContentModeration_jobTag</a> - An identifier you specify
--   that's returned in the completion notification that's published to
--   your Amazon Simple Notification Service topic. For example, you can
--   use <tt>JobTag</tt> to group related jobs and identify them in the
--   completion notification.
--   
--   <a>$sel:notificationChannel:StartContentModeration'</a>,
--   <a>startContentModeration_notificationChannel</a> - The Amazon SNS
--   topic ARN that you want Amazon Rekognition Video to publish the
--   completion status of the content analysis to. The Amazon SNS topic
--   must have a topic name that begins with <i>AmazonRekognition</i> if
--   you are using the AmazonRekognitionServiceRole permissions policy to
--   access the topic.
--   
--   <a>$sel:clientRequestToken:StartContentModeration'</a>,
--   <a>startContentModeration_clientRequestToken</a> - Idempotent token
--   used to identify the start request. If you use the same token with
--   multiple <tt>StartContentModeration</tt> requests, the same
--   <tt>JobId</tt> is returned. Use <tt>ClientRequestToken</tt> to prevent
--   the same job from being accidently started more than once.
--   
--   <a>$sel:minConfidence:StartContentModeration'</a>,
--   <a>startContentModeration_minConfidence</a> - Specifies the minimum
--   confidence that Amazon Rekognition must have in order to return a
--   moderated content label. Confidence represents how certain Amazon
--   Rekognition is that the moderated content is correctly identified. 0
--   is the lowest confidence. 100 is the highest confidence. Amazon
--   Rekognition doesn't return any moderated content labels with a
--   confidence level lower than this specified value. If you don't specify
--   <tt>MinConfidence</tt>, <tt>GetContentModeration</tt> returns labels
--   with confidence values greater than or equal to 50 percent.
--   
--   <a>$sel:video:StartContentModeration'</a>,
--   <a>startContentModeration_video</a> - The video in which you want to
--   detect inappropriate, unwanted, or offensive content. The video must
--   be stored in an Amazon S3 bucket.
newStartContentModeration :: Video -> StartContentModeration

-- | An identifier you specify that's returned in the completion
--   notification that's published to your Amazon Simple Notification
--   Service topic. For example, you can use <tt>JobTag</tt> to group
--   related jobs and identify them in the completion notification.
startContentModeration_jobTag :: Lens' StartContentModeration (Maybe Text)

-- | The Amazon SNS topic ARN that you want Amazon Rekognition Video to
--   publish the completion status of the content analysis to. The Amazon
--   SNS topic must have a topic name that begins with
--   <i>AmazonRekognition</i> if you are using the
--   AmazonRekognitionServiceRole permissions policy to access the topic.
startContentModeration_notificationChannel :: Lens' StartContentModeration (Maybe NotificationChannel)

-- | Idempotent token used to identify the start request. If you use the
--   same token with multiple <tt>StartContentModeration</tt> requests, the
--   same <tt>JobId</tt> is returned. Use <tt>ClientRequestToken</tt> to
--   prevent the same job from being accidently started more than once.
startContentModeration_clientRequestToken :: Lens' StartContentModeration (Maybe Text)

-- | Specifies the minimum confidence that Amazon Rekognition must have in
--   order to return a moderated content label. Confidence represents how
--   certain Amazon Rekognition is that the moderated content is correctly
--   identified. 0 is the lowest confidence. 100 is the highest confidence.
--   Amazon Rekognition doesn't return any moderated content labels with a
--   confidence level lower than this specified value. If you don't specify
--   <tt>MinConfidence</tt>, <tt>GetContentModeration</tt> returns labels
--   with confidence values greater than or equal to 50 percent.
startContentModeration_minConfidence :: Lens' StartContentModeration (Maybe Double)

-- | The video in which you want to detect inappropriate, unwanted, or
--   offensive content. The video must be stored in an Amazon S3 bucket.
startContentModeration_video :: Lens' StartContentModeration Video

-- | <i>See:</i> <a>newStartContentModerationResponse</a> smart
--   constructor.
data StartContentModerationResponse
StartContentModerationResponse' :: Maybe Text -> Int -> StartContentModerationResponse

-- | The identifier for the content analysis job. Use <tt>JobId</tt> to
--   identify the job in a subsequent call to
--   <tt>GetContentModeration</tt>.
[$sel:jobId:StartContentModerationResponse'] :: StartContentModerationResponse -> Maybe Text

-- | The response's http status code.
[$sel:httpStatus:StartContentModerationResponse'] :: StartContentModerationResponse -> Int

-- | Create a value of <a>StartContentModerationResponse</a> with all
--   optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:jobId:StartContentModerationResponse'</a>,
--   <a>startContentModerationResponse_jobId</a> - The identifier for the
--   content analysis job. Use <tt>JobId</tt> to identify the job in a
--   subsequent call to <tt>GetContentModeration</tt>.
--   
--   <a>$sel:httpStatus:StartContentModerationResponse'</a>,
--   <a>startContentModerationResponse_httpStatus</a> - The response's http
--   status code.
newStartContentModerationResponse :: Int -> StartContentModerationResponse

-- | The identifier for the content analysis job. Use <tt>JobId</tt> to
--   identify the job in a subsequent call to
--   <tt>GetContentModeration</tt>.
startContentModerationResponse_jobId :: Lens' StartContentModerationResponse (Maybe Text)

-- | The response's http status code.
startContentModerationResponse_httpStatus :: Lens' StartContentModerationResponse Int
instance GHC.Generics.Generic Network.AWS.Rekognition.StartContentModeration.StartContentModeration
instance GHC.Show.Show Network.AWS.Rekognition.StartContentModeration.StartContentModeration
instance GHC.Read.Read Network.AWS.Rekognition.StartContentModeration.StartContentModeration
instance GHC.Classes.Eq Network.AWS.Rekognition.StartContentModeration.StartContentModeration
instance GHC.Generics.Generic Network.AWS.Rekognition.StartContentModeration.StartContentModerationResponse
instance GHC.Show.Show Network.AWS.Rekognition.StartContentModeration.StartContentModerationResponse
instance GHC.Read.Read Network.AWS.Rekognition.StartContentModeration.StartContentModerationResponse
instance GHC.Classes.Eq Network.AWS.Rekognition.StartContentModeration.StartContentModerationResponse
instance Network.AWS.Types.AWSRequest Network.AWS.Rekognition.StartContentModeration.StartContentModeration
instance Control.DeepSeq.NFData Network.AWS.Rekognition.StartContentModeration.StartContentModerationResponse
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.StartContentModeration.StartContentModeration
instance Control.DeepSeq.NFData Network.AWS.Rekognition.StartContentModeration.StartContentModeration
instance Network.AWS.Data.Headers.ToHeaders Network.AWS.Rekognition.StartContentModeration.StartContentModeration
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.StartContentModeration.StartContentModeration
instance Network.AWS.Data.Path.ToPath Network.AWS.Rekognition.StartContentModeration.StartContentModeration
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.StartContentModeration.StartContentModeration


-- | Starts asynchronous recognition of celebrities in a stored video.
--   
--   Amazon Rekognition Video can detect celebrities in a video must be
--   stored in an Amazon S3 bucket. Use Video to specify the bucket name
--   and the filename of the video. <tt>StartCelebrityRecognition</tt>
--   returns a job identifier (<tt>JobId</tt>) which you use to get the
--   results of the analysis. When celebrity recognition analysis is
--   finished, Amazon Rekognition Video publishes a completion status to
--   the Amazon Simple Notification Service topic that you specify in
--   <tt>NotificationChannel</tt>. To get the results of the celebrity
--   recognition analysis, first check that the status value published to
--   the Amazon SNS topic is <tt>SUCCEEDED</tt>. If so, call
--   GetCelebrityRecognition and pass the job identifier (<tt>JobId</tt>)
--   from the initial call to <tt>StartCelebrityRecognition</tt>.
--   
--   For more information, see Recognizing Celebrities in the Amazon
--   Rekognition Developer Guide.
module Network.AWS.Rekognition.StartCelebrityRecognition

-- | <i>See:</i> <a>newStartCelebrityRecognition</a> smart constructor.
data StartCelebrityRecognition
StartCelebrityRecognition' :: Maybe Text -> Maybe NotificationChannel -> Maybe Text -> Video -> StartCelebrityRecognition

-- | An identifier you specify that's returned in the completion
--   notification that's published to your Amazon Simple Notification
--   Service topic. For example, you can use <tt>JobTag</tt> to group
--   related jobs and identify them in the completion notification.
[$sel:jobTag:StartCelebrityRecognition'] :: StartCelebrityRecognition -> Maybe Text

-- | The Amazon SNS topic ARN that you want Amazon Rekognition Video to
--   publish the completion status of the celebrity recognition analysis
--   to. The Amazon SNS topic must have a topic name that begins with
--   <i>AmazonRekognition</i> if you are using the
--   AmazonRekognitionServiceRole permissions policy.
[$sel:notificationChannel:StartCelebrityRecognition'] :: StartCelebrityRecognition -> Maybe NotificationChannel

-- | Idempotent token used to identify the start request. If you use the
--   same token with multiple <tt>StartCelebrityRecognition</tt> requests,
--   the same <tt>JobId</tt> is returned. Use <tt>ClientRequestToken</tt>
--   to prevent the same job from being accidently started more than once.
[$sel:clientRequestToken:StartCelebrityRecognition'] :: StartCelebrityRecognition -> Maybe Text

-- | The video in which you want to recognize celebrities. The video must
--   be stored in an Amazon S3 bucket.
[$sel:video:StartCelebrityRecognition'] :: StartCelebrityRecognition -> Video

-- | Create a value of <a>StartCelebrityRecognition</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:jobTag:StartCelebrityRecognition'</a>,
--   <a>startCelebrityRecognition_jobTag</a> - An identifier you specify
--   that's returned in the completion notification that's published to
--   your Amazon Simple Notification Service topic. For example, you can
--   use <tt>JobTag</tt> to group related jobs and identify them in the
--   completion notification.
--   
--   <a>$sel:notificationChannel:StartCelebrityRecognition'</a>,
--   <a>startCelebrityRecognition_notificationChannel</a> - The Amazon SNS
--   topic ARN that you want Amazon Rekognition Video to publish the
--   completion status of the celebrity recognition analysis to. The Amazon
--   SNS topic must have a topic name that begins with
--   <i>AmazonRekognition</i> if you are using the
--   AmazonRekognitionServiceRole permissions policy.
--   
--   <a>$sel:clientRequestToken:StartCelebrityRecognition'</a>,
--   <a>startCelebrityRecognition_clientRequestToken</a> - Idempotent token
--   used to identify the start request. If you use the same token with
--   multiple <tt>StartCelebrityRecognition</tt> requests, the same
--   <tt>JobId</tt> is returned. Use <tt>ClientRequestToken</tt> to prevent
--   the same job from being accidently started more than once.
--   
--   <a>$sel:video:StartCelebrityRecognition'</a>,
--   <a>startCelebrityRecognition_video</a> - The video in which you want
--   to recognize celebrities. The video must be stored in an Amazon S3
--   bucket.
newStartCelebrityRecognition :: Video -> StartCelebrityRecognition

-- | An identifier you specify that's returned in the completion
--   notification that's published to your Amazon Simple Notification
--   Service topic. For example, you can use <tt>JobTag</tt> to group
--   related jobs and identify them in the completion notification.
startCelebrityRecognition_jobTag :: Lens' StartCelebrityRecognition (Maybe Text)

-- | The Amazon SNS topic ARN that you want Amazon Rekognition Video to
--   publish the completion status of the celebrity recognition analysis
--   to. The Amazon SNS topic must have a topic name that begins with
--   <i>AmazonRekognition</i> if you are using the
--   AmazonRekognitionServiceRole permissions policy.
startCelebrityRecognition_notificationChannel :: Lens' StartCelebrityRecognition (Maybe NotificationChannel)

-- | Idempotent token used to identify the start request. If you use the
--   same token with multiple <tt>StartCelebrityRecognition</tt> requests,
--   the same <tt>JobId</tt> is returned. Use <tt>ClientRequestToken</tt>
--   to prevent the same job from being accidently started more than once.
startCelebrityRecognition_clientRequestToken :: Lens' StartCelebrityRecognition (Maybe Text)

-- | The video in which you want to recognize celebrities. The video must
--   be stored in an Amazon S3 bucket.
startCelebrityRecognition_video :: Lens' StartCelebrityRecognition Video

-- | <i>See:</i> <a>newStartCelebrityRecognitionResponse</a> smart
--   constructor.
data StartCelebrityRecognitionResponse
StartCelebrityRecognitionResponse' :: Maybe Text -> Int -> StartCelebrityRecognitionResponse

-- | The identifier for the celebrity recognition analysis job. Use
--   <tt>JobId</tt> to identify the job in a subsequent call to
--   <tt>GetCelebrityRecognition</tt>.
[$sel:jobId:StartCelebrityRecognitionResponse'] :: StartCelebrityRecognitionResponse -> Maybe Text

-- | The response's http status code.
[$sel:httpStatus:StartCelebrityRecognitionResponse'] :: StartCelebrityRecognitionResponse -> Int

-- | Create a value of <a>StartCelebrityRecognitionResponse</a> with all
--   optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:jobId:StartCelebrityRecognitionResponse'</a>,
--   <a>startCelebrityRecognitionResponse_jobId</a> - The identifier for
--   the celebrity recognition analysis job. Use <tt>JobId</tt> to identify
--   the job in a subsequent call to <tt>GetCelebrityRecognition</tt>.
--   
--   <a>$sel:httpStatus:StartCelebrityRecognitionResponse'</a>,
--   <a>startCelebrityRecognitionResponse_httpStatus</a> - The response's
--   http status code.
newStartCelebrityRecognitionResponse :: Int -> StartCelebrityRecognitionResponse

-- | The identifier for the celebrity recognition analysis job. Use
--   <tt>JobId</tt> to identify the job in a subsequent call to
--   <tt>GetCelebrityRecognition</tt>.
startCelebrityRecognitionResponse_jobId :: Lens' StartCelebrityRecognitionResponse (Maybe Text)

-- | The response's http status code.
startCelebrityRecognitionResponse_httpStatus :: Lens' StartCelebrityRecognitionResponse Int
instance GHC.Generics.Generic Network.AWS.Rekognition.StartCelebrityRecognition.StartCelebrityRecognition
instance GHC.Show.Show Network.AWS.Rekognition.StartCelebrityRecognition.StartCelebrityRecognition
instance GHC.Read.Read Network.AWS.Rekognition.StartCelebrityRecognition.StartCelebrityRecognition
instance GHC.Classes.Eq Network.AWS.Rekognition.StartCelebrityRecognition.StartCelebrityRecognition
instance GHC.Generics.Generic Network.AWS.Rekognition.StartCelebrityRecognition.StartCelebrityRecognitionResponse
instance GHC.Show.Show Network.AWS.Rekognition.StartCelebrityRecognition.StartCelebrityRecognitionResponse
instance GHC.Read.Read Network.AWS.Rekognition.StartCelebrityRecognition.StartCelebrityRecognitionResponse
instance GHC.Classes.Eq Network.AWS.Rekognition.StartCelebrityRecognition.StartCelebrityRecognitionResponse
instance Network.AWS.Types.AWSRequest Network.AWS.Rekognition.StartCelebrityRecognition.StartCelebrityRecognition
instance Control.DeepSeq.NFData Network.AWS.Rekognition.StartCelebrityRecognition.StartCelebrityRecognitionResponse
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.StartCelebrityRecognition.StartCelebrityRecognition
instance Control.DeepSeq.NFData Network.AWS.Rekognition.StartCelebrityRecognition.StartCelebrityRecognition
instance Network.AWS.Data.Headers.ToHeaders Network.AWS.Rekognition.StartCelebrityRecognition.StartCelebrityRecognition
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.StartCelebrityRecognition.StartCelebrityRecognition
instance Network.AWS.Data.Path.ToPath Network.AWS.Rekognition.StartCelebrityRecognition.StartCelebrityRecognition
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.StartCelebrityRecognition.StartCelebrityRecognition


-- | For a given input image, first detects the largest face in the image,
--   and then searches the specified collection for matching faces. The
--   operation compares the features of the input face with faces in the
--   specified collection.
--   
--   To search for all faces in an input image, you might first call the
--   IndexFaces operation, and then use the face IDs returned in subsequent
--   calls to the SearchFaces operation.
--   
--   You can also call the <tt>DetectFaces</tt> operation and use the
--   bounding boxes in the response to make face crops, which then you can
--   pass in to the <tt>SearchFacesByImage</tt> operation.
--   
--   You pass the input image either as base64-encoded image bytes or as a
--   reference to an image in an Amazon S3 bucket. If you use the AWS CLI
--   to call Amazon Rekognition operations, passing image bytes is not
--   supported. The image must be either a PNG or JPEG formatted file.
--   
--   The response returns an array of faces that match, ordered by
--   similarity score with the highest similarity first. More specifically,
--   it is an array of metadata for each face match found. Along with the
--   metadata, the response also includes a <tt>similarity</tt> indicating
--   how similar the face is to the input face. In the response, the
--   operation also returns the bounding box (and a confidence level that
--   the bounding box contains a face) of the face that Amazon Rekognition
--   used for the input image.
--   
--   If no faces are detected in the input image,
--   <tt>SearchFacesByImage</tt> returns an
--   <tt>InvalidParameterException</tt> error.
--   
--   For an example, Searching for a Face Using an Image in the Amazon
--   Rekognition Developer Guide.
--   
--   The <tt>QualityFilter</tt> input parameter allows you to filter out
--   detected faces that don’t meet a required quality bar. The quality bar
--   is based on a variety of common use cases. Use <tt>QualityFilter</tt>
--   to set the quality bar for filtering by specifying <tt>LOW</tt>,
--   <tt>MEDIUM</tt>, or <tt>HIGH</tt>. If you do not want to filter
--   detected faces, specify <tt>NONE</tt>. The default value is
--   <tt>NONE</tt>.
--   
--   To use quality filtering, you need a collection associated with
--   version 3 of the face model or higher. To get the version of the face
--   model associated with a collection, call DescribeCollection.
--   
--   This operation requires permissions to perform the
--   <tt>rekognition:SearchFacesByImage</tt> action.
module Network.AWS.Rekognition.SearchFacesByImage

-- | <i>See:</i> <a>newSearchFacesByImage</a> smart constructor.
data SearchFacesByImage
SearchFacesByImage' :: Maybe QualityFilter -> Maybe Double -> Maybe Natural -> Text -> Image -> SearchFacesByImage

-- | A filter that specifies a quality bar for how much filtering is done
--   to identify faces. Filtered faces aren't searched for in the
--   collection. If you specify <tt>AUTO</tt>, Amazon Rekognition chooses
--   the quality bar. If you specify <tt>LOW</tt>, <tt>MEDIUM</tt>, or
--   <tt>HIGH</tt>, filtering removes all faces that don’t meet the chosen
--   quality bar. The quality bar is based on a variety of common use
--   cases. Low-quality detections can occur for a number of reasons. Some
--   examples are an object that's misidentified as a face, a face that's
--   too blurry, or a face with a pose that's too extreme to use. If you
--   specify <tt>NONE</tt>, no filtering is performed. The default value is
--   <tt>NONE</tt>.
--   
--   To use quality filtering, the collection you are using must be
--   associated with version 3 of the face model or higher.
[$sel:qualityFilter:SearchFacesByImage'] :: SearchFacesByImage -> Maybe QualityFilter

-- | (Optional) Specifies the minimum confidence in the face match to
--   return. For example, don't return any matches where confidence in
--   matches is less than 70%. The default value is 80%.
[$sel:faceMatchThreshold:SearchFacesByImage'] :: SearchFacesByImage -> Maybe Double

-- | Maximum number of faces to return. The operation returns the maximum
--   number of faces with the highest confidence in the match.
[$sel:maxFaces:SearchFacesByImage'] :: SearchFacesByImage -> Maybe Natural

-- | ID of the collection to search.
[$sel:collectionId:SearchFacesByImage'] :: SearchFacesByImage -> Text

-- | The input image as base64-encoded bytes or an S3 object. If you use
--   the AWS CLI to call Amazon Rekognition operations, passing
--   base64-encoded image bytes is not supported.
--   
--   If you are using an AWS SDK to call Amazon Rekognition, you might not
--   need to base64-encode image bytes passed using the <tt>Bytes</tt>
--   field. For more information, see Images in the Amazon Rekognition
--   developer guide.
[$sel:image:SearchFacesByImage'] :: SearchFacesByImage -> Image

-- | Create a value of <a>SearchFacesByImage</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:qualityFilter:SearchFacesByImage'</a>,
--   <a>searchFacesByImage_qualityFilter</a> - A filter that specifies a
--   quality bar for how much filtering is done to identify faces. Filtered
--   faces aren't searched for in the collection. If you specify
--   <tt>AUTO</tt>, Amazon Rekognition chooses the quality bar. If you
--   specify <tt>LOW</tt>, <tt>MEDIUM</tt>, or <tt>HIGH</tt>, filtering
--   removes all faces that don’t meet the chosen quality bar. The quality
--   bar is based on a variety of common use cases. Low-quality detections
--   can occur for a number of reasons. Some examples are an object that's
--   misidentified as a face, a face that's too blurry, or a face with a
--   pose that's too extreme to use. If you specify <tt>NONE</tt>, no
--   filtering is performed. The default value is <tt>NONE</tt>.
--   
--   To use quality filtering, the collection you are using must be
--   associated with version 3 of the face model or higher.
--   
--   <a>$sel:faceMatchThreshold:SearchFacesByImage'</a>,
--   <a>searchFacesByImage_faceMatchThreshold</a> - (Optional) Specifies
--   the minimum confidence in the face match to return. For example, don't
--   return any matches where confidence in matches is less than 70%. The
--   default value is 80%.
--   
--   <a>$sel:maxFaces:SearchFacesByImage'</a>,
--   <a>searchFacesByImage_maxFaces</a> - Maximum number of faces to
--   return. The operation returns the maximum number of faces with the
--   highest confidence in the match.
--   
--   <a>$sel:collectionId:SearchFacesByImage'</a>,
--   <a>searchFacesByImage_collectionId</a> - ID of the collection to
--   search.
--   
--   <a>$sel:image:SearchFacesByImage'</a>, <a>searchFacesByImage_image</a>
--   - The input image as base64-encoded bytes or an S3 object. If you use
--   the AWS CLI to call Amazon Rekognition operations, passing
--   base64-encoded image bytes is not supported.
--   
--   If you are using an AWS SDK to call Amazon Rekognition, you might not
--   need to base64-encode image bytes passed using the <tt>Bytes</tt>
--   field. For more information, see Images in the Amazon Rekognition
--   developer guide.
newSearchFacesByImage :: Text -> Image -> SearchFacesByImage

-- | A filter that specifies a quality bar for how much filtering is done
--   to identify faces. Filtered faces aren't searched for in the
--   collection. If you specify <tt>AUTO</tt>, Amazon Rekognition chooses
--   the quality bar. If you specify <tt>LOW</tt>, <tt>MEDIUM</tt>, or
--   <tt>HIGH</tt>, filtering removes all faces that don’t meet the chosen
--   quality bar. The quality bar is based on a variety of common use
--   cases. Low-quality detections can occur for a number of reasons. Some
--   examples are an object that's misidentified as a face, a face that's
--   too blurry, or a face with a pose that's too extreme to use. If you
--   specify <tt>NONE</tt>, no filtering is performed. The default value is
--   <tt>NONE</tt>.
--   
--   To use quality filtering, the collection you are using must be
--   associated with version 3 of the face model or higher.
searchFacesByImage_qualityFilter :: Lens' SearchFacesByImage (Maybe QualityFilter)

-- | (Optional) Specifies the minimum confidence in the face match to
--   return. For example, don't return any matches where confidence in
--   matches is less than 70%. The default value is 80%.
searchFacesByImage_faceMatchThreshold :: Lens' SearchFacesByImage (Maybe Double)

-- | Maximum number of faces to return. The operation returns the maximum
--   number of faces with the highest confidence in the match.
searchFacesByImage_maxFaces :: Lens' SearchFacesByImage (Maybe Natural)

-- | ID of the collection to search.
searchFacesByImage_collectionId :: Lens' SearchFacesByImage Text

-- | The input image as base64-encoded bytes or an S3 object. If you use
--   the AWS CLI to call Amazon Rekognition operations, passing
--   base64-encoded image bytes is not supported.
--   
--   If you are using an AWS SDK to call Amazon Rekognition, you might not
--   need to base64-encode image bytes passed using the <tt>Bytes</tt>
--   field. For more information, see Images in the Amazon Rekognition
--   developer guide.
searchFacesByImage_image :: Lens' SearchFacesByImage Image

-- | <i>See:</i> <a>newSearchFacesByImageResponse</a> smart constructor.
data SearchFacesByImageResponse
SearchFacesByImageResponse' :: Maybe [FaceMatch] -> Maybe Text -> Maybe BoundingBox -> Maybe Double -> Int -> SearchFacesByImageResponse

-- | An array of faces that match the input face, along with the confidence
--   in the match.
[$sel:faceMatches:SearchFacesByImageResponse'] :: SearchFacesByImageResponse -> Maybe [FaceMatch]

-- | Version number of the face detection model associated with the input
--   collection (<tt>CollectionId</tt>).
[$sel:faceModelVersion:SearchFacesByImageResponse'] :: SearchFacesByImageResponse -> Maybe Text

-- | The bounding box around the face in the input image that Amazon
--   Rekognition used for the search.
[$sel:searchedFaceBoundingBox:SearchFacesByImageResponse'] :: SearchFacesByImageResponse -> Maybe BoundingBox

-- | The level of confidence that the <tt>searchedFaceBoundingBox</tt>,
--   contains a face.
[$sel:searchedFaceConfidence:SearchFacesByImageResponse'] :: SearchFacesByImageResponse -> Maybe Double

-- | The response's http status code.
[$sel:httpStatus:SearchFacesByImageResponse'] :: SearchFacesByImageResponse -> Int

-- | Create a value of <a>SearchFacesByImageResponse</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:faceMatches:SearchFacesByImageResponse'</a>,
--   <a>searchFacesByImageResponse_faceMatches</a> - An array of faces that
--   match the input face, along with the confidence in the match.
--   
--   <a>$sel:faceModelVersion:SearchFacesByImageResponse'</a>,
--   <a>searchFacesByImageResponse_faceModelVersion</a> - Version number of
--   the face detection model associated with the input collection
--   (<tt>CollectionId</tt>).
--   
--   <a>$sel:searchedFaceBoundingBox:SearchFacesByImageResponse'</a>,
--   <a>searchFacesByImageResponse_searchedFaceBoundingBox</a> - The
--   bounding box around the face in the input image that Amazon
--   Rekognition used for the search.
--   
--   <a>$sel:searchedFaceConfidence:SearchFacesByImageResponse'</a>,
--   <a>searchFacesByImageResponse_searchedFaceConfidence</a> - The level
--   of confidence that the <tt>searchedFaceBoundingBox</tt>, contains a
--   face.
--   
--   <a>$sel:httpStatus:SearchFacesByImageResponse'</a>,
--   <a>searchFacesByImageResponse_httpStatus</a> - The response's http
--   status code.
newSearchFacesByImageResponse :: Int -> SearchFacesByImageResponse

-- | An array of faces that match the input face, along with the confidence
--   in the match.
searchFacesByImageResponse_faceMatches :: Lens' SearchFacesByImageResponse (Maybe [FaceMatch])

-- | Version number of the face detection model associated with the input
--   collection (<tt>CollectionId</tt>).
searchFacesByImageResponse_faceModelVersion :: Lens' SearchFacesByImageResponse (Maybe Text)

-- | The bounding box around the face in the input image that Amazon
--   Rekognition used for the search.
searchFacesByImageResponse_searchedFaceBoundingBox :: Lens' SearchFacesByImageResponse (Maybe BoundingBox)

-- | The level of confidence that the <tt>searchedFaceBoundingBox</tt>,
--   contains a face.
searchFacesByImageResponse_searchedFaceConfidence :: Lens' SearchFacesByImageResponse (Maybe Double)

-- | The response's http status code.
searchFacesByImageResponse_httpStatus :: Lens' SearchFacesByImageResponse Int
instance GHC.Generics.Generic Network.AWS.Rekognition.SearchFacesByImage.SearchFacesByImage
instance GHC.Show.Show Network.AWS.Rekognition.SearchFacesByImage.SearchFacesByImage
instance GHC.Read.Read Network.AWS.Rekognition.SearchFacesByImage.SearchFacesByImage
instance GHC.Classes.Eq Network.AWS.Rekognition.SearchFacesByImage.SearchFacesByImage
instance GHC.Generics.Generic Network.AWS.Rekognition.SearchFacesByImage.SearchFacesByImageResponse
instance GHC.Show.Show Network.AWS.Rekognition.SearchFacesByImage.SearchFacesByImageResponse
instance GHC.Read.Read Network.AWS.Rekognition.SearchFacesByImage.SearchFacesByImageResponse
instance GHC.Classes.Eq Network.AWS.Rekognition.SearchFacesByImage.SearchFacesByImageResponse
instance Network.AWS.Types.AWSRequest Network.AWS.Rekognition.SearchFacesByImage.SearchFacesByImage
instance Control.DeepSeq.NFData Network.AWS.Rekognition.SearchFacesByImage.SearchFacesByImageResponse
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.SearchFacesByImage.SearchFacesByImage
instance Control.DeepSeq.NFData Network.AWS.Rekognition.SearchFacesByImage.SearchFacesByImage
instance Network.AWS.Data.Headers.ToHeaders Network.AWS.Rekognition.SearchFacesByImage.SearchFacesByImage
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.SearchFacesByImage.SearchFacesByImage
instance Network.AWS.Data.Path.ToPath Network.AWS.Rekognition.SearchFacesByImage.SearchFacesByImage
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.SearchFacesByImage.SearchFacesByImage


-- | For a given input face ID, searches for matching faces in the
--   collection the face belongs to. You get a face ID when you add a face
--   to the collection using the IndexFaces operation. The operation
--   compares the features of the input face with faces in the specified
--   collection.
--   
--   You can also search faces without indexing faces by using the
--   <tt>SearchFacesByImage</tt> operation.
--   
--   The operation response returns an array of faces that match, ordered
--   by similarity score with the highest similarity first. More
--   specifically, it is an array of metadata for each face match that is
--   found. Along with the metadata, the response also includes a
--   <tt>confidence</tt> value for each face match, indicating the
--   confidence that the specific face matches the input face.
--   
--   For an example, see Searching for a Face Using Its Face ID in the
--   Amazon Rekognition Developer Guide.
--   
--   This operation requires permissions to perform the
--   <tt>rekognition:SearchFaces</tt> action.
module Network.AWS.Rekognition.SearchFaces

-- | <i>See:</i> <a>newSearchFaces</a> smart constructor.
data SearchFaces
SearchFaces' :: Maybe Double -> Maybe Natural -> Text -> Text -> SearchFaces

-- | Optional value specifying the minimum confidence in the face match to
--   return. For example, don't return any matches where confidence in
--   matches is less than 70%. The default value is 80%.
[$sel:faceMatchThreshold:SearchFaces'] :: SearchFaces -> Maybe Double

-- | Maximum number of faces to return. The operation returns the maximum
--   number of faces with the highest confidence in the match.
[$sel:maxFaces:SearchFaces'] :: SearchFaces -> Maybe Natural

-- | ID of the collection the face belongs to.
[$sel:collectionId:SearchFaces'] :: SearchFaces -> Text

-- | ID of a face to find matches for in the collection.
[$sel:faceId:SearchFaces'] :: SearchFaces -> Text

-- | Create a value of <a>SearchFaces</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:faceMatchThreshold:SearchFaces'</a>,
--   <a>searchFaces_faceMatchThreshold</a> - Optional value specifying the
--   minimum confidence in the face match to return. For example, don't
--   return any matches where confidence in matches is less than 70%. The
--   default value is 80%.
--   
--   <a>$sel:maxFaces:SearchFaces'</a>, <a>searchFaces_maxFaces</a> -
--   Maximum number of faces to return. The operation returns the maximum
--   number of faces with the highest confidence in the match.
--   
--   <a>$sel:collectionId:SearchFaces'</a>, <a>searchFaces_collectionId</a>
--   - ID of the collection the face belongs to.
--   
--   <a>$sel:faceId:SearchFaces'</a>, <a>searchFaces_faceId</a> - ID of a
--   face to find matches for in the collection.
newSearchFaces :: Text -> Text -> SearchFaces

-- | Optional value specifying the minimum confidence in the face match to
--   return. For example, don't return any matches where confidence in
--   matches is less than 70%. The default value is 80%.
searchFaces_faceMatchThreshold :: Lens' SearchFaces (Maybe Double)

-- | Maximum number of faces to return. The operation returns the maximum
--   number of faces with the highest confidence in the match.
searchFaces_maxFaces :: Lens' SearchFaces (Maybe Natural)

-- | ID of the collection the face belongs to.
searchFaces_collectionId :: Lens' SearchFaces Text

-- | ID of a face to find matches for in the collection.
searchFaces_faceId :: Lens' SearchFaces Text

-- | <i>See:</i> <a>newSearchFacesResponse</a> smart constructor.
data SearchFacesResponse
SearchFacesResponse' :: Maybe [FaceMatch] -> Maybe Text -> Maybe Text -> Int -> SearchFacesResponse

-- | An array of faces that matched the input face, along with the
--   confidence in the match.
[$sel:faceMatches:SearchFacesResponse'] :: SearchFacesResponse -> Maybe [FaceMatch]

-- | Version number of the face detection model associated with the input
--   collection (<tt>CollectionId</tt>).
[$sel:faceModelVersion:SearchFacesResponse'] :: SearchFacesResponse -> Maybe Text

-- | ID of the face that was searched for matches in a collection.
[$sel:searchedFaceId:SearchFacesResponse'] :: SearchFacesResponse -> Maybe Text

-- | The response's http status code.
[$sel:httpStatus:SearchFacesResponse'] :: SearchFacesResponse -> Int

-- | Create a value of <a>SearchFacesResponse</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:faceMatches:SearchFacesResponse'</a>,
--   <a>searchFacesResponse_faceMatches</a> - An array of faces that
--   matched the input face, along with the confidence in the match.
--   
--   <a>$sel:faceModelVersion:SearchFacesResponse'</a>,
--   <a>searchFacesResponse_faceModelVersion</a> - Version number of the
--   face detection model associated with the input collection
--   (<tt>CollectionId</tt>).
--   
--   <a>$sel:searchedFaceId:SearchFacesResponse'</a>,
--   <a>searchFacesResponse_searchedFaceId</a> - ID of the face that was
--   searched for matches in a collection.
--   
--   <a>$sel:httpStatus:SearchFacesResponse'</a>,
--   <a>searchFacesResponse_httpStatus</a> - The response's http status
--   code.
newSearchFacesResponse :: Int -> SearchFacesResponse

-- | An array of faces that matched the input face, along with the
--   confidence in the match.
searchFacesResponse_faceMatches :: Lens' SearchFacesResponse (Maybe [FaceMatch])

-- | Version number of the face detection model associated with the input
--   collection (<tt>CollectionId</tt>).
searchFacesResponse_faceModelVersion :: Lens' SearchFacesResponse (Maybe Text)

-- | ID of the face that was searched for matches in a collection.
searchFacesResponse_searchedFaceId :: Lens' SearchFacesResponse (Maybe Text)

-- | The response's http status code.
searchFacesResponse_httpStatus :: Lens' SearchFacesResponse Int
instance GHC.Generics.Generic Network.AWS.Rekognition.SearchFaces.SearchFaces
instance GHC.Show.Show Network.AWS.Rekognition.SearchFaces.SearchFaces
instance GHC.Read.Read Network.AWS.Rekognition.SearchFaces.SearchFaces
instance GHC.Classes.Eq Network.AWS.Rekognition.SearchFaces.SearchFaces
instance GHC.Generics.Generic Network.AWS.Rekognition.SearchFaces.SearchFacesResponse
instance GHC.Show.Show Network.AWS.Rekognition.SearchFaces.SearchFacesResponse
instance GHC.Read.Read Network.AWS.Rekognition.SearchFaces.SearchFacesResponse
instance GHC.Classes.Eq Network.AWS.Rekognition.SearchFaces.SearchFacesResponse
instance Network.AWS.Types.AWSRequest Network.AWS.Rekognition.SearchFaces.SearchFaces
instance Control.DeepSeq.NFData Network.AWS.Rekognition.SearchFaces.SearchFacesResponse
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.SearchFaces.SearchFaces
instance Control.DeepSeq.NFData Network.AWS.Rekognition.SearchFaces.SearchFaces
instance Network.AWS.Data.Headers.ToHeaders Network.AWS.Rekognition.SearchFaces.SearchFaces
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.SearchFaces.SearchFaces
instance Network.AWS.Data.Path.ToPath Network.AWS.Rekognition.SearchFaces.SearchFaces
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.SearchFaces.SearchFaces


-- | Returns an array of celebrities recognized in the input image. For
--   more information, see Recognizing Celebrities in the Amazon
--   Rekognition Developer Guide.
--   
--   <tt>RecognizeCelebrities</tt> returns the 64 largest faces in the
--   image. It lists recognized celebrities in the <tt>CelebrityFaces</tt>
--   array and unrecognized faces in the <tt>UnrecognizedFaces</tt> array.
--   <tt>RecognizeCelebrities</tt> doesn't return celebrities whose faces
--   aren't among the largest 64 faces in the image.
--   
--   For each celebrity recognized, <tt>RecognizeCelebrities</tt> returns a
--   <tt>Celebrity</tt> object. The <tt>Celebrity</tt> object contains the
--   celebrity name, ID, URL links to additional information, match
--   confidence, and a <tt>ComparedFace</tt> object that you can use to
--   locate the celebrity's face on the image.
--   
--   Amazon Rekognition doesn't retain information about which images a
--   celebrity has been recognized in. Your application must store this
--   information and use the <tt>Celebrity</tt> ID property as a unique
--   identifier for the celebrity. If you don't store the celebrity name or
--   additional information URLs returned by <tt>RecognizeCelebrities</tt>,
--   you will need the ID to identify the celebrity in a call to the
--   GetCelebrityInfo operation.
--   
--   You pass the input image either as base64-encoded image bytes or as a
--   reference to an image in an Amazon S3 bucket. If you use the AWS CLI
--   to call Amazon Rekognition operations, passing image bytes is not
--   supported. The image must be either a PNG or JPEG formatted file.
--   
--   For an example, see Recognizing Celebrities in an Image in the Amazon
--   Rekognition Developer Guide.
--   
--   This operation requires permissions to perform the
--   <tt>rekognition:RecognizeCelebrities</tt> operation.
module Network.AWS.Rekognition.RecognizeCelebrities

-- | <i>See:</i> <a>newRecognizeCelebrities</a> smart constructor.
data RecognizeCelebrities
RecognizeCelebrities' :: Image -> RecognizeCelebrities

-- | The input image as base64-encoded bytes or an S3 object. If you use
--   the AWS CLI to call Amazon Rekognition operations, passing
--   base64-encoded image bytes is not supported.
--   
--   If you are using an AWS SDK to call Amazon Rekognition, you might not
--   need to base64-encode image bytes passed using the <tt>Bytes</tt>
--   field. For more information, see Images in the Amazon Rekognition
--   developer guide.
[$sel:image:RecognizeCelebrities'] :: RecognizeCelebrities -> Image

-- | Create a value of <a>RecognizeCelebrities</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:image:RecognizeCelebrities'</a>,
--   <a>recognizeCelebrities_image</a> - The input image as base64-encoded
--   bytes or an S3 object. If you use the AWS CLI to call Amazon
--   Rekognition operations, passing base64-encoded image bytes is not
--   supported.
--   
--   If you are using an AWS SDK to call Amazon Rekognition, you might not
--   need to base64-encode image bytes passed using the <tt>Bytes</tt>
--   field. For more information, see Images in the Amazon Rekognition
--   developer guide.
newRecognizeCelebrities :: Image -> RecognizeCelebrities

-- | The input image as base64-encoded bytes or an S3 object. If you use
--   the AWS CLI to call Amazon Rekognition operations, passing
--   base64-encoded image bytes is not supported.
--   
--   If you are using an AWS SDK to call Amazon Rekognition, you might not
--   need to base64-encode image bytes passed using the <tt>Bytes</tt>
--   field. For more information, see Images in the Amazon Rekognition
--   developer guide.
recognizeCelebrities_image :: Lens' RecognizeCelebrities Image

-- | <i>See:</i> <a>newRecognizeCelebritiesResponse</a> smart constructor.
data RecognizeCelebritiesResponse
RecognizeCelebritiesResponse' :: Maybe [Celebrity] -> Maybe OrientationCorrection -> Maybe [ComparedFace] -> Int -> RecognizeCelebritiesResponse

-- | Details about each celebrity found in the image. Amazon Rekognition
--   can detect a maximum of 64 celebrities in an image. Each celebrity
--   object includes the following attributes: <tt>Face</tt>,
--   <tt>Confidence</tt>, <tt>Emotions</tt>, <tt>Landmarks</tt>,
--   <tt>Pose</tt>, <tt>Quality</tt>, <tt>Smile</tt>, <tt>Id</tt>,
--   <tt>KnownGender</tt>, <tt>MatchConfidence</tt>, <tt>Name</tt>,
--   <tt>Urls</tt>.
[$sel:celebrityFaces:RecognizeCelebritiesResponse'] :: RecognizeCelebritiesResponse -> Maybe [Celebrity]

-- | Support for estimating image orientation using the the
--   OrientationCorrection field has ceased as of August 2021. Any returned
--   values for this field included in an API response will always be NULL.
--   
--   The orientation of the input image (counterclockwise direction). If
--   your application displays the image, you can use this value to correct
--   the orientation. The bounding box coordinates returned in
--   <tt>CelebrityFaces</tt> and <tt>UnrecognizedFaces</tt> represent face
--   locations before the image orientation is corrected.
--   
--   If the input image is in .jpeg format, it might contain exchangeable
--   image (Exif) metadata that includes the image's orientation. If so,
--   and the Exif metadata for the input image populates the orientation
--   field, the value of <tt>OrientationCorrection</tt> is null. The
--   <tt>CelebrityFaces</tt> and <tt>UnrecognizedFaces</tt> bounding box
--   coordinates represent face locations after Exif metadata is used to
--   correct the image orientation. Images in .png format don't contain
--   Exif metadata.
[$sel:orientationCorrection:RecognizeCelebritiesResponse'] :: RecognizeCelebritiesResponse -> Maybe OrientationCorrection

-- | Details about each unrecognized face in the image.
[$sel:unrecognizedFaces:RecognizeCelebritiesResponse'] :: RecognizeCelebritiesResponse -> Maybe [ComparedFace]

-- | The response's http status code.
[$sel:httpStatus:RecognizeCelebritiesResponse'] :: RecognizeCelebritiesResponse -> Int

-- | Create a value of <a>RecognizeCelebritiesResponse</a> with all
--   optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:celebrityFaces:RecognizeCelebritiesResponse'</a>,
--   <a>recognizeCelebritiesResponse_celebrityFaces</a> - Details about
--   each celebrity found in the image. Amazon Rekognition can detect a
--   maximum of 64 celebrities in an image. Each celebrity object includes
--   the following attributes: <tt>Face</tt>, <tt>Confidence</tt>,
--   <tt>Emotions</tt>, <tt>Landmarks</tt>, <tt>Pose</tt>,
--   <tt>Quality</tt>, <tt>Smile</tt>, <tt>Id</tt>, <tt>KnownGender</tt>,
--   <tt>MatchConfidence</tt>, <tt>Name</tt>, <tt>Urls</tt>.
--   
--   <a>$sel:orientationCorrection:RecognizeCelebritiesResponse'</a>,
--   <a>recognizeCelebritiesResponse_orientationCorrection</a> - Support
--   for estimating image orientation using the the OrientationCorrection
--   field has ceased as of August 2021. Any returned values for this field
--   included in an API response will always be NULL.
--   
--   The orientation of the input image (counterclockwise direction). If
--   your application displays the image, you can use this value to correct
--   the orientation. The bounding box coordinates returned in
--   <tt>CelebrityFaces</tt> and <tt>UnrecognizedFaces</tt> represent face
--   locations before the image orientation is corrected.
--   
--   If the input image is in .jpeg format, it might contain exchangeable
--   image (Exif) metadata that includes the image's orientation. If so,
--   and the Exif metadata for the input image populates the orientation
--   field, the value of <tt>OrientationCorrection</tt> is null. The
--   <tt>CelebrityFaces</tt> and <tt>UnrecognizedFaces</tt> bounding box
--   coordinates represent face locations after Exif metadata is used to
--   correct the image orientation. Images in .png format don't contain
--   Exif metadata.
--   
--   <a>$sel:unrecognizedFaces:RecognizeCelebritiesResponse'</a>,
--   <a>recognizeCelebritiesResponse_unrecognizedFaces</a> - Details about
--   each unrecognized face in the image.
--   
--   <a>$sel:httpStatus:RecognizeCelebritiesResponse'</a>,
--   <a>recognizeCelebritiesResponse_httpStatus</a> - The response's http
--   status code.
newRecognizeCelebritiesResponse :: Int -> RecognizeCelebritiesResponse

-- | Details about each celebrity found in the image. Amazon Rekognition
--   can detect a maximum of 64 celebrities in an image. Each celebrity
--   object includes the following attributes: <tt>Face</tt>,
--   <tt>Confidence</tt>, <tt>Emotions</tt>, <tt>Landmarks</tt>,
--   <tt>Pose</tt>, <tt>Quality</tt>, <tt>Smile</tt>, <tt>Id</tt>,
--   <tt>KnownGender</tt>, <tt>MatchConfidence</tt>, <tt>Name</tt>,
--   <tt>Urls</tt>.
recognizeCelebritiesResponse_celebrityFaces :: Lens' RecognizeCelebritiesResponse (Maybe [Celebrity])

-- | Support for estimating image orientation using the the
--   OrientationCorrection field has ceased as of August 2021. Any returned
--   values for this field included in an API response will always be NULL.
--   
--   The orientation of the input image (counterclockwise direction). If
--   your application displays the image, you can use this value to correct
--   the orientation. The bounding box coordinates returned in
--   <tt>CelebrityFaces</tt> and <tt>UnrecognizedFaces</tt> represent face
--   locations before the image orientation is corrected.
--   
--   If the input image is in .jpeg format, it might contain exchangeable
--   image (Exif) metadata that includes the image's orientation. If so,
--   and the Exif metadata for the input image populates the orientation
--   field, the value of <tt>OrientationCorrection</tt> is null. The
--   <tt>CelebrityFaces</tt> and <tt>UnrecognizedFaces</tt> bounding box
--   coordinates represent face locations after Exif metadata is used to
--   correct the image orientation. Images in .png format don't contain
--   Exif metadata.
recognizeCelebritiesResponse_orientationCorrection :: Lens' RecognizeCelebritiesResponse (Maybe OrientationCorrection)

-- | Details about each unrecognized face in the image.
recognizeCelebritiesResponse_unrecognizedFaces :: Lens' RecognizeCelebritiesResponse (Maybe [ComparedFace])

-- | The response's http status code.
recognizeCelebritiesResponse_httpStatus :: Lens' RecognizeCelebritiesResponse Int
instance GHC.Generics.Generic Network.AWS.Rekognition.RecognizeCelebrities.RecognizeCelebrities
instance GHC.Show.Show Network.AWS.Rekognition.RecognizeCelebrities.RecognizeCelebrities
instance GHC.Read.Read Network.AWS.Rekognition.RecognizeCelebrities.RecognizeCelebrities
instance GHC.Classes.Eq Network.AWS.Rekognition.RecognizeCelebrities.RecognizeCelebrities
instance GHC.Generics.Generic Network.AWS.Rekognition.RecognizeCelebrities.RecognizeCelebritiesResponse
instance GHC.Show.Show Network.AWS.Rekognition.RecognizeCelebrities.RecognizeCelebritiesResponse
instance GHC.Read.Read Network.AWS.Rekognition.RecognizeCelebrities.RecognizeCelebritiesResponse
instance GHC.Classes.Eq Network.AWS.Rekognition.RecognizeCelebrities.RecognizeCelebritiesResponse
instance Network.AWS.Types.AWSRequest Network.AWS.Rekognition.RecognizeCelebrities.RecognizeCelebrities
instance Control.DeepSeq.NFData Network.AWS.Rekognition.RecognizeCelebrities.RecognizeCelebritiesResponse
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.RecognizeCelebrities.RecognizeCelebrities
instance Control.DeepSeq.NFData Network.AWS.Rekognition.RecognizeCelebrities.RecognizeCelebrities
instance Network.AWS.Data.Headers.ToHeaders Network.AWS.Rekognition.RecognizeCelebrities.RecognizeCelebrities
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.RecognizeCelebrities.RecognizeCelebrities
instance Network.AWS.Data.Path.ToPath Network.AWS.Rekognition.RecognizeCelebrities.RecognizeCelebrities
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.RecognizeCelebrities.RecognizeCelebrities


-- | Returns a list of tags in an Amazon Rekognition collection, stream
--   processor, or Custom Labels model.
--   
--   This operation requires permissions to perform the
--   <tt>rekognition:ListTagsForResource</tt> action.
module Network.AWS.Rekognition.ListTagsForResource

-- | <i>See:</i> <a>newListTagsForResource</a> smart constructor.
data ListTagsForResource
ListTagsForResource' :: Text -> ListTagsForResource

-- | Amazon Resource Name (ARN) of the model, collection, or stream
--   processor that contains the tags that you want a list of.
[$sel:resourceArn:ListTagsForResource'] :: ListTagsForResource -> Text

-- | Create a value of <a>ListTagsForResource</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:resourceArn:ListTagsForResource'</a>,
--   <a>listTagsForResource_resourceArn</a> - Amazon Resource Name (ARN) of
--   the model, collection, or stream processor that contains the tags that
--   you want a list of.
newListTagsForResource :: Text -> ListTagsForResource

-- | Amazon Resource Name (ARN) of the model, collection, or stream
--   processor that contains the tags that you want a list of.
listTagsForResource_resourceArn :: Lens' ListTagsForResource Text

-- | <i>See:</i> <a>newListTagsForResourceResponse</a> smart constructor.
data ListTagsForResourceResponse
ListTagsForResourceResponse' :: Maybe (HashMap Text Text) -> Int -> ListTagsForResourceResponse

-- | A list of key-value tags assigned to the resource.
[$sel:tags:ListTagsForResourceResponse'] :: ListTagsForResourceResponse -> Maybe (HashMap Text Text)

-- | The response's http status code.
[$sel:httpStatus:ListTagsForResourceResponse'] :: ListTagsForResourceResponse -> Int

-- | Create a value of <a>ListTagsForResourceResponse</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:tags:ListTagsForResourceResponse'</a>,
--   <a>listTagsForResourceResponse_tags</a> - A list of key-value tags
--   assigned to the resource.
--   
--   <a>$sel:httpStatus:ListTagsForResourceResponse'</a>,
--   <a>listTagsForResourceResponse_httpStatus</a> - The response's http
--   status code.
newListTagsForResourceResponse :: Int -> ListTagsForResourceResponse

-- | A list of key-value tags assigned to the resource.
listTagsForResourceResponse_tags :: Lens' ListTagsForResourceResponse (Maybe (HashMap Text Text))

-- | The response's http status code.
listTagsForResourceResponse_httpStatus :: Lens' ListTagsForResourceResponse Int
instance GHC.Generics.Generic Network.AWS.Rekognition.ListTagsForResource.ListTagsForResource
instance GHC.Show.Show Network.AWS.Rekognition.ListTagsForResource.ListTagsForResource
instance GHC.Read.Read Network.AWS.Rekognition.ListTagsForResource.ListTagsForResource
instance GHC.Classes.Eq Network.AWS.Rekognition.ListTagsForResource.ListTagsForResource
instance GHC.Generics.Generic Network.AWS.Rekognition.ListTagsForResource.ListTagsForResourceResponse
instance GHC.Show.Show Network.AWS.Rekognition.ListTagsForResource.ListTagsForResourceResponse
instance GHC.Read.Read Network.AWS.Rekognition.ListTagsForResource.ListTagsForResourceResponse
instance GHC.Classes.Eq Network.AWS.Rekognition.ListTagsForResource.ListTagsForResourceResponse
instance Network.AWS.Types.AWSRequest Network.AWS.Rekognition.ListTagsForResource.ListTagsForResource
instance Control.DeepSeq.NFData Network.AWS.Rekognition.ListTagsForResource.ListTagsForResourceResponse
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.ListTagsForResource.ListTagsForResource
instance Control.DeepSeq.NFData Network.AWS.Rekognition.ListTagsForResource.ListTagsForResource
instance Network.AWS.Data.Headers.ToHeaders Network.AWS.Rekognition.ListTagsForResource.ListTagsForResource
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.ListTagsForResource.ListTagsForResource
instance Network.AWS.Data.Path.ToPath Network.AWS.Rekognition.ListTagsForResource.ListTagsForResource
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.ListTagsForResource.ListTagsForResource


-- | Gets a list of stream processors that you have created with
--   CreateStreamProcessor.
--   
--   This operation returns paginated results.
module Network.AWS.Rekognition.ListStreamProcessors

-- | <i>See:</i> <a>newListStreamProcessors</a> smart constructor.
data ListStreamProcessors
ListStreamProcessors' :: Maybe Text -> Maybe Natural -> ListStreamProcessors

-- | If the previous response was incomplete (because there are more stream
--   processors to retrieve), Amazon Rekognition Video returns a pagination
--   token in the response. You can use this pagination token to retrieve
--   the next set of stream processors.
[$sel:nextToken:ListStreamProcessors'] :: ListStreamProcessors -> Maybe Text

-- | Maximum number of stream processors you want Amazon Rekognition Video
--   to return in the response. The default is 1000.
[$sel:maxResults:ListStreamProcessors'] :: ListStreamProcessors -> Maybe Natural

-- | Create a value of <a>ListStreamProcessors</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:nextToken:ListStreamProcessors'</a>,
--   <a>listStreamProcessors_nextToken</a> - If the previous response was
--   incomplete (because there are more stream processors to retrieve),
--   Amazon Rekognition Video returns a pagination token in the response.
--   You can use this pagination token to retrieve the next set of stream
--   processors.
--   
--   <a>$sel:maxResults:ListStreamProcessors'</a>,
--   <a>listStreamProcessors_maxResults</a> - Maximum number of stream
--   processors you want Amazon Rekognition Video to return in the
--   response. The default is 1000.
newListStreamProcessors :: ListStreamProcessors

-- | If the previous response was incomplete (because there are more stream
--   processors to retrieve), Amazon Rekognition Video returns a pagination
--   token in the response. You can use this pagination token to retrieve
--   the next set of stream processors.
listStreamProcessors_nextToken :: Lens' ListStreamProcessors (Maybe Text)

-- | Maximum number of stream processors you want Amazon Rekognition Video
--   to return in the response. The default is 1000.
listStreamProcessors_maxResults :: Lens' ListStreamProcessors (Maybe Natural)

-- | <i>See:</i> <a>newListStreamProcessorsResponse</a> smart constructor.
data ListStreamProcessorsResponse
ListStreamProcessorsResponse' :: Maybe [StreamProcessor] -> Maybe Text -> Int -> ListStreamProcessorsResponse

-- | List of stream processors that you have created.
[$sel:streamProcessors:ListStreamProcessorsResponse'] :: ListStreamProcessorsResponse -> Maybe [StreamProcessor]

-- | If the response is truncated, Amazon Rekognition Video returns this
--   token that you can use in the subsequent request to retrieve the next
--   set of stream processors.
[$sel:nextToken:ListStreamProcessorsResponse'] :: ListStreamProcessorsResponse -> Maybe Text

-- | The response's http status code.
[$sel:httpStatus:ListStreamProcessorsResponse'] :: ListStreamProcessorsResponse -> Int

-- | Create a value of <a>ListStreamProcessorsResponse</a> with all
--   optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:streamProcessors:ListStreamProcessorsResponse'</a>,
--   <a>listStreamProcessorsResponse_streamProcessors</a> - List of stream
--   processors that you have created.
--   
--   <a>$sel:nextToken:ListStreamProcessors'</a>,
--   <a>listStreamProcessorsResponse_nextToken</a> - If the response is
--   truncated, Amazon Rekognition Video returns this token that you can
--   use in the subsequent request to retrieve the next set of stream
--   processors.
--   
--   <a>$sel:httpStatus:ListStreamProcessorsResponse'</a>,
--   <a>listStreamProcessorsResponse_httpStatus</a> - The response's http
--   status code.
newListStreamProcessorsResponse :: Int -> ListStreamProcessorsResponse

-- | List of stream processors that you have created.
listStreamProcessorsResponse_streamProcessors :: Lens' ListStreamProcessorsResponse (Maybe [StreamProcessor])

-- | If the response is truncated, Amazon Rekognition Video returns this
--   token that you can use in the subsequent request to retrieve the next
--   set of stream processors.
listStreamProcessorsResponse_nextToken :: Lens' ListStreamProcessorsResponse (Maybe Text)

-- | The response's http status code.
listStreamProcessorsResponse_httpStatus :: Lens' ListStreamProcessorsResponse Int
instance GHC.Generics.Generic Network.AWS.Rekognition.ListStreamProcessors.ListStreamProcessors
instance GHC.Show.Show Network.AWS.Rekognition.ListStreamProcessors.ListStreamProcessors
instance GHC.Read.Read Network.AWS.Rekognition.ListStreamProcessors.ListStreamProcessors
instance GHC.Classes.Eq Network.AWS.Rekognition.ListStreamProcessors.ListStreamProcessors
instance GHC.Generics.Generic Network.AWS.Rekognition.ListStreamProcessors.ListStreamProcessorsResponse
instance GHC.Show.Show Network.AWS.Rekognition.ListStreamProcessors.ListStreamProcessorsResponse
instance GHC.Read.Read Network.AWS.Rekognition.ListStreamProcessors.ListStreamProcessorsResponse
instance GHC.Classes.Eq Network.AWS.Rekognition.ListStreamProcessors.ListStreamProcessorsResponse
instance Network.AWS.Types.AWSRequest Network.AWS.Rekognition.ListStreamProcessors.ListStreamProcessors
instance Control.DeepSeq.NFData Network.AWS.Rekognition.ListStreamProcessors.ListStreamProcessorsResponse
instance Network.AWS.Pager.AWSPager Network.AWS.Rekognition.ListStreamProcessors.ListStreamProcessors
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.ListStreamProcessors.ListStreamProcessors
instance Control.DeepSeq.NFData Network.AWS.Rekognition.ListStreamProcessors.ListStreamProcessors
instance Network.AWS.Data.Headers.ToHeaders Network.AWS.Rekognition.ListStreamProcessors.ListStreamProcessors
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.ListStreamProcessors.ListStreamProcessors
instance Network.AWS.Data.Path.ToPath Network.AWS.Rekognition.ListStreamProcessors.ListStreamProcessors
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.ListStreamProcessors.ListStreamProcessors


-- | Returns metadata for faces in the specified collection. This metadata
--   includes information such as the bounding box coordinates, the
--   confidence (that the bounding box contains a face), and face ID. For
--   an example, see Listing Faces in a Collection in the Amazon
--   Rekognition Developer Guide.
--   
--   This operation requires permissions to perform the
--   <tt>rekognition:ListFaces</tt> action.
--   
--   This operation returns paginated results.
module Network.AWS.Rekognition.ListFaces

-- | <i>See:</i> <a>newListFaces</a> smart constructor.
data ListFaces
ListFaces' :: Maybe Text -> Maybe Natural -> Text -> ListFaces

-- | If the previous response was incomplete (because there is more data to
--   retrieve), Amazon Rekognition returns a pagination token in the
--   response. You can use this pagination token to retrieve the next set
--   of faces.
[$sel:nextToken:ListFaces'] :: ListFaces -> Maybe Text

-- | Maximum number of faces to return.
[$sel:maxResults:ListFaces'] :: ListFaces -> Maybe Natural

-- | ID of the collection from which to list the faces.
[$sel:collectionId:ListFaces'] :: ListFaces -> Text

-- | Create a value of <a>ListFaces</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:nextToken:ListFaces'</a>, <a>listFaces_nextToken</a> - If the
--   previous response was incomplete (because there is more data to
--   retrieve), Amazon Rekognition returns a pagination token in the
--   response. You can use this pagination token to retrieve the next set
--   of faces.
--   
--   <a>$sel:maxResults:ListFaces'</a>, <a>listFaces_maxResults</a> -
--   Maximum number of faces to return.
--   
--   <a>$sel:collectionId:ListFaces'</a>, <a>listFaces_collectionId</a> -
--   ID of the collection from which to list the faces.
newListFaces :: Text -> ListFaces

-- | If the previous response was incomplete (because there is more data to
--   retrieve), Amazon Rekognition returns a pagination token in the
--   response. You can use this pagination token to retrieve the next set
--   of faces.
listFaces_nextToken :: Lens' ListFaces (Maybe Text)

-- | Maximum number of faces to return.
listFaces_maxResults :: Lens' ListFaces (Maybe Natural)

-- | ID of the collection from which to list the faces.
listFaces_collectionId :: Lens' ListFaces Text

-- | <i>See:</i> <a>newListFacesResponse</a> smart constructor.
data ListFacesResponse
ListFacesResponse' :: Maybe Text -> Maybe Text -> Maybe [Face] -> Int -> ListFacesResponse

-- | Version number of the face detection model associated with the input
--   collection (<tt>CollectionId</tt>).
[$sel:faceModelVersion:ListFacesResponse'] :: ListFacesResponse -> Maybe Text

-- | If the response is truncated, Amazon Rekognition returns this token
--   that you can use in the subsequent request to retrieve the next set of
--   faces.
[$sel:nextToken:ListFacesResponse'] :: ListFacesResponse -> Maybe Text

-- | An array of <tt>Face</tt> objects.
[$sel:faces:ListFacesResponse'] :: ListFacesResponse -> Maybe [Face]

-- | The response's http status code.
[$sel:httpStatus:ListFacesResponse'] :: ListFacesResponse -> Int

-- | Create a value of <a>ListFacesResponse</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:faceModelVersion:ListFacesResponse'</a>,
--   <a>listFacesResponse_faceModelVersion</a> - Version number of the face
--   detection model associated with the input collection
--   (<tt>CollectionId</tt>).
--   
--   <a>$sel:nextToken:ListFaces'</a>, <a>listFacesResponse_nextToken</a> -
--   If the response is truncated, Amazon Rekognition returns this token
--   that you can use in the subsequent request to retrieve the next set of
--   faces.
--   
--   <a>$sel:faces:ListFacesResponse'</a>, <a>listFacesResponse_faces</a> -
--   An array of <tt>Face</tt> objects.
--   
--   <a>$sel:httpStatus:ListFacesResponse'</a>,
--   <a>listFacesResponse_httpStatus</a> - The response's http status code.
newListFacesResponse :: Int -> ListFacesResponse

-- | Version number of the face detection model associated with the input
--   collection (<tt>CollectionId</tt>).
listFacesResponse_faceModelVersion :: Lens' ListFacesResponse (Maybe Text)

-- | If the response is truncated, Amazon Rekognition returns this token
--   that you can use in the subsequent request to retrieve the next set of
--   faces.
listFacesResponse_nextToken :: Lens' ListFacesResponse (Maybe Text)

-- | An array of <tt>Face</tt> objects.
listFacesResponse_faces :: Lens' ListFacesResponse (Maybe [Face])

-- | The response's http status code.
listFacesResponse_httpStatus :: Lens' ListFacesResponse Int
instance GHC.Generics.Generic Network.AWS.Rekognition.ListFaces.ListFaces
instance GHC.Show.Show Network.AWS.Rekognition.ListFaces.ListFaces
instance GHC.Read.Read Network.AWS.Rekognition.ListFaces.ListFaces
instance GHC.Classes.Eq Network.AWS.Rekognition.ListFaces.ListFaces
instance GHC.Generics.Generic Network.AWS.Rekognition.ListFaces.ListFacesResponse
instance GHC.Show.Show Network.AWS.Rekognition.ListFaces.ListFacesResponse
instance GHC.Read.Read Network.AWS.Rekognition.ListFaces.ListFacesResponse
instance GHC.Classes.Eq Network.AWS.Rekognition.ListFaces.ListFacesResponse
instance Network.AWS.Types.AWSRequest Network.AWS.Rekognition.ListFaces.ListFaces
instance Control.DeepSeq.NFData Network.AWS.Rekognition.ListFaces.ListFacesResponse
instance Network.AWS.Pager.AWSPager Network.AWS.Rekognition.ListFaces.ListFaces
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.ListFaces.ListFaces
instance Control.DeepSeq.NFData Network.AWS.Rekognition.ListFaces.ListFaces
instance Network.AWS.Data.Headers.ToHeaders Network.AWS.Rekognition.ListFaces.ListFaces
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.ListFaces.ListFaces
instance Network.AWS.Data.Path.ToPath Network.AWS.Rekognition.ListFaces.ListFaces
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.ListFaces.ListFaces


-- | Returns list of collection IDs in your account. If the result is
--   truncated, the response also provides a <tt>NextToken</tt> that you
--   can use in the subsequent request to fetch the next set of collection
--   IDs.
--   
--   For an example, see Listing Collections in the Amazon Rekognition
--   Developer Guide.
--   
--   This operation requires permissions to perform the
--   <tt>rekognition:ListCollections</tt> action.
--   
--   This operation returns paginated results.
module Network.AWS.Rekognition.ListCollections

-- | <i>See:</i> <a>newListCollections</a> smart constructor.
data ListCollections
ListCollections' :: Maybe Text -> Maybe Natural -> ListCollections

-- | Pagination token from the previous response.
[$sel:nextToken:ListCollections'] :: ListCollections -> Maybe Text

-- | Maximum number of collection IDs to return.
[$sel:maxResults:ListCollections'] :: ListCollections -> Maybe Natural

-- | Create a value of <a>ListCollections</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:nextToken:ListCollections'</a>,
--   <a>listCollections_nextToken</a> - Pagination token from the previous
--   response.
--   
--   <a>$sel:maxResults:ListCollections'</a>,
--   <a>listCollections_maxResults</a> - Maximum number of collection IDs
--   to return.
newListCollections :: ListCollections

-- | Pagination token from the previous response.
listCollections_nextToken :: Lens' ListCollections (Maybe Text)

-- | Maximum number of collection IDs to return.
listCollections_maxResults :: Lens' ListCollections (Maybe Natural)

-- | <i>See:</i> <a>newListCollectionsResponse</a> smart constructor.
data ListCollectionsResponse
ListCollectionsResponse' :: Maybe [Text] -> Maybe Text -> Maybe [Text] -> Int -> ListCollectionsResponse

-- | An array of collection IDs.
[$sel:collectionIds:ListCollectionsResponse'] :: ListCollectionsResponse -> Maybe [Text]

-- | If the result is truncated, the response provides a <tt>NextToken</tt>
--   that you can use in the subsequent request to fetch the next set of
--   collection IDs.
[$sel:nextToken:ListCollectionsResponse'] :: ListCollectionsResponse -> Maybe Text

-- | Version numbers of the face detection models associated with the
--   collections in the array <tt>CollectionIds</tt>. For example, the
--   value of <tt>FaceModelVersions[2]</tt> is the version number for the
--   face detection model used by the collection in
--   <tt>CollectionId[2]</tt>.
[$sel:faceModelVersions:ListCollectionsResponse'] :: ListCollectionsResponse -> Maybe [Text]

-- | The response's http status code.
[$sel:httpStatus:ListCollectionsResponse'] :: ListCollectionsResponse -> Int

-- | Create a value of <a>ListCollectionsResponse</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:collectionIds:ListCollectionsResponse'</a>,
--   <a>listCollectionsResponse_collectionIds</a> - An array of collection
--   IDs.
--   
--   <a>$sel:nextToken:ListCollections'</a>,
--   <a>listCollectionsResponse_nextToken</a> - If the result is truncated,
--   the response provides a <tt>NextToken</tt> that you can use in the
--   subsequent request to fetch the next set of collection IDs.
--   
--   <a>$sel:faceModelVersions:ListCollectionsResponse'</a>,
--   <a>listCollectionsResponse_faceModelVersions</a> - Version numbers of
--   the face detection models associated with the collections in the array
--   <tt>CollectionIds</tt>. For example, the value of
--   <tt>FaceModelVersions[2]</tt> is the version number for the face
--   detection model used by the collection in <tt>CollectionId[2]</tt>.
--   
--   <a>$sel:httpStatus:ListCollectionsResponse'</a>,
--   <a>listCollectionsResponse_httpStatus</a> - The response's http status
--   code.
newListCollectionsResponse :: Int -> ListCollectionsResponse

-- | An array of collection IDs.
listCollectionsResponse_collectionIds :: Lens' ListCollectionsResponse (Maybe [Text])

-- | If the result is truncated, the response provides a <tt>NextToken</tt>
--   that you can use in the subsequent request to fetch the next set of
--   collection IDs.
listCollectionsResponse_nextToken :: Lens' ListCollectionsResponse (Maybe Text)

-- | Version numbers of the face detection models associated with the
--   collections in the array <tt>CollectionIds</tt>. For example, the
--   value of <tt>FaceModelVersions[2]</tt> is the version number for the
--   face detection model used by the collection in
--   <tt>CollectionId[2]</tt>.
listCollectionsResponse_faceModelVersions :: Lens' ListCollectionsResponse (Maybe [Text])

-- | The response's http status code.
listCollectionsResponse_httpStatus :: Lens' ListCollectionsResponse Int
instance GHC.Generics.Generic Network.AWS.Rekognition.ListCollections.ListCollections
instance GHC.Show.Show Network.AWS.Rekognition.ListCollections.ListCollections
instance GHC.Read.Read Network.AWS.Rekognition.ListCollections.ListCollections
instance GHC.Classes.Eq Network.AWS.Rekognition.ListCollections.ListCollections
instance GHC.Generics.Generic Network.AWS.Rekognition.ListCollections.ListCollectionsResponse
instance GHC.Show.Show Network.AWS.Rekognition.ListCollections.ListCollectionsResponse
instance GHC.Read.Read Network.AWS.Rekognition.ListCollections.ListCollectionsResponse
instance GHC.Classes.Eq Network.AWS.Rekognition.ListCollections.ListCollectionsResponse
instance Network.AWS.Types.AWSRequest Network.AWS.Rekognition.ListCollections.ListCollections
instance Control.DeepSeq.NFData Network.AWS.Rekognition.ListCollections.ListCollectionsResponse
instance Network.AWS.Pager.AWSPager Network.AWS.Rekognition.ListCollections.ListCollections
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.ListCollections.ListCollections
instance Control.DeepSeq.NFData Network.AWS.Rekognition.ListCollections.ListCollections
instance Network.AWS.Data.Headers.ToHeaders Network.AWS.Rekognition.ListCollections.ListCollections
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.ListCollections.ListCollections
instance Network.AWS.Data.Path.ToPath Network.AWS.Rekognition.ListCollections.ListCollections
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.ListCollections.ListCollections


-- | Detects faces in the input image and adds them to the specified
--   collection.
--   
--   Amazon Rekognition doesn't save the actual faces that are detected.
--   Instead, the underlying detection algorithm first detects the faces in
--   the input image. For each face, the algorithm extracts facial features
--   into a feature vector, and stores it in the backend database. Amazon
--   Rekognition uses feature vectors when it performs face match and
--   search operations using the SearchFaces and SearchFacesByImage
--   operations.
--   
--   For more information, see Adding Faces to a Collection in the Amazon
--   Rekognition Developer Guide.
--   
--   To get the number of faces in a collection, call DescribeCollection.
--   
--   If you're using version 1.0 of the face detection model,
--   <tt>IndexFaces</tt> indexes the 15 largest faces in the input image.
--   Later versions of the face detection model index the 100 largest faces
--   in the input image.
--   
--   If you're using version 4 or later of the face model, image
--   orientation information is not returned in the
--   <tt>OrientationCorrection</tt> field.
--   
--   To determine which version of the model you're using, call
--   DescribeCollection and supply the collection ID. You can also get the
--   model version from the value of <tt>FaceModelVersion</tt> in the
--   response from <tt>IndexFaces</tt>
--   
--   For more information, see Model Versioning in the Amazon Rekognition
--   Developer Guide.
--   
--   If you provide the optional <tt>ExternalImageId</tt> for the input
--   image you provided, Amazon Rekognition associates this ID with all
--   faces that it detects. When you call the ListFaces operation, the
--   response returns the external ID. You can use this external image ID
--   to create a client-side index to associate the faces with each image.
--   You can then use the index to find all faces in an image.
--   
--   You can specify the maximum number of faces to index with the
--   <tt>MaxFaces</tt> input parameter. This is useful when you want to
--   index the largest faces in an image and don't want to index smaller
--   faces, such as those belonging to people standing in the background.
--   
--   The <tt>QualityFilter</tt> input parameter allows you to filter out
--   detected faces that don’t meet a required quality bar. The quality bar
--   is based on a variety of common use cases. By default,
--   <tt>IndexFaces</tt> chooses the quality bar that's used to filter
--   faces. You can also explicitly choose the quality bar. Use
--   <tt>QualityFilter</tt>, to set the quality bar by specifying
--   <tt>LOW</tt>, <tt>MEDIUM</tt>, or <tt>HIGH</tt>. If you do not want to
--   filter detected faces, specify <tt>NONE</tt>.
--   
--   To use quality filtering, you need a collection associated with
--   version 3 of the face model or higher. To get the version of the face
--   model associated with a collection, call DescribeCollection.
--   
--   Information about faces detected in an image, but not indexed, is
--   returned in an array of UnindexedFace objects,
--   <tt>UnindexedFaces</tt>. Faces aren't indexed for reasons such as:
--   
--   <ul>
--   <li>The number of faces detected exceeds the value of the
--   <tt>MaxFaces</tt> request parameter.</li>
--   <li>The face is too small compared to the image dimensions.</li>
--   <li>The face is too blurry.</li>
--   <li>The image is too dark.</li>
--   <li>The face has an extreme pose.</li>
--   <li>The face doesn’t have enough detail to be suitable for face
--   search.</li>
--   </ul>
--   
--   In response, the <tt>IndexFaces</tt> operation returns an array of
--   metadata for all detected faces, <tt>FaceRecords</tt>. This includes:
--   
--   <ul>
--   <li>The bounding box, <tt>BoundingBox</tt>, of the detected face.</li>
--   <li>A confidence value, <tt>Confidence</tt>, which indicates the
--   confidence that the bounding box contains a face.</li>
--   <li>A face ID, <tt>FaceId</tt>, assigned by the service for each face
--   that's detected and stored.</li>
--   <li>An image ID, <tt>ImageId</tt>, assigned by the service for the
--   input image.</li>
--   </ul>
--   
--   If you request all facial attributes (by using the
--   <tt>detectionAttributes</tt> parameter), Amazon Rekognition returns
--   detailed facial attributes, such as facial landmarks (for example,
--   location of eye and mouth) and other facial attributes. If you provide
--   the same image, specify the same collection, and use the same external
--   ID in the <tt>IndexFaces</tt> operation, Amazon Rekognition doesn't
--   save duplicate face metadata.
--   
--   The input image is passed either as base64-encoded image bytes, or as
--   a reference to an image in an Amazon S3 bucket. If you use the AWS CLI
--   to call Amazon Rekognition operations, passing image bytes isn't
--   supported. The image must be formatted as a PNG or JPEG file.
--   
--   This operation requires permissions to perform the
--   <tt>rekognition:IndexFaces</tt> action.
module Network.AWS.Rekognition.IndexFaces

-- | <i>See:</i> <a>newIndexFaces</a> smart constructor.
data IndexFaces
IndexFaces' :: Maybe Text -> Maybe QualityFilter -> Maybe Natural -> Maybe [Attribute] -> Text -> Image -> IndexFaces

-- | The ID you want to assign to all the faces detected in the image.
[$sel:externalImageId:IndexFaces'] :: IndexFaces -> Maybe Text

-- | A filter that specifies a quality bar for how much filtering is done
--   to identify faces. Filtered faces aren't indexed. If you specify
--   <tt>AUTO</tt>, Amazon Rekognition chooses the quality bar. If you
--   specify <tt>LOW</tt>, <tt>MEDIUM</tt>, or <tt>HIGH</tt>, filtering
--   removes all faces that don’t meet the chosen quality bar. The default
--   value is <tt>AUTO</tt>. The quality bar is based on a variety of
--   common use cases. Low-quality detections can occur for a number of
--   reasons. Some examples are an object that's misidentified as a face, a
--   face that's too blurry, or a face with a pose that's too extreme to
--   use. If you specify <tt>NONE</tt>, no filtering is performed.
--   
--   To use quality filtering, the collection you are using must be
--   associated with version 3 of the face model or higher.
[$sel:qualityFilter:IndexFaces'] :: IndexFaces -> Maybe QualityFilter

-- | The maximum number of faces to index. The value of <tt>MaxFaces</tt>
--   must be greater than or equal to 1. <tt>IndexFaces</tt> returns no
--   more than 100 detected faces in an image, even if you specify a larger
--   value for <tt>MaxFaces</tt>.
--   
--   If <tt>IndexFaces</tt> detects more faces than the value of
--   <tt>MaxFaces</tt>, the faces with the lowest quality are filtered out
--   first. If there are still more faces than the value of
--   <tt>MaxFaces</tt>, the faces with the smallest bounding boxes are
--   filtered out (up to the number that's needed to satisfy the value of
--   <tt>MaxFaces</tt>). Information about the unindexed faces is available
--   in the <tt>UnindexedFaces</tt> array.
--   
--   The faces that are returned by <tt>IndexFaces</tt> are sorted by the
--   largest face bounding box size to the smallest size, in descending
--   order.
--   
--   <tt>MaxFaces</tt> can be used with a collection associated with any
--   version of the face model.
[$sel:maxFaces:IndexFaces'] :: IndexFaces -> Maybe Natural

-- | An array of facial attributes that you want to be returned. This can
--   be the default list of attributes or all attributes. If you don't
--   specify a value for <tt>Attributes</tt> or if you specify
--   <tt>["DEFAULT"]</tt>, the API returns the following subset of facial
--   attributes: <tt>BoundingBox</tt>, <tt>Confidence</tt>, <tt>Pose</tt>,
--   <tt>Quality</tt>, and <tt>Landmarks</tt>. If you provide
--   <tt>["ALL"]</tt>, all facial attributes are returned, but the
--   operation takes longer to complete.
--   
--   If you provide both, <tt>["ALL", "DEFAULT"]</tt>, the service uses a
--   logical AND operator to determine which attributes to return (in this
--   case, all attributes).
[$sel:detectionAttributes:IndexFaces'] :: IndexFaces -> Maybe [Attribute]

-- | The ID of an existing collection to which you want to add the faces
--   that are detected in the input images.
[$sel:collectionId:IndexFaces'] :: IndexFaces -> Text

-- | The input image as base64-encoded bytes or an S3 object. If you use
--   the AWS CLI to call Amazon Rekognition operations, passing
--   base64-encoded image bytes isn't supported.
--   
--   If you are using an AWS SDK to call Amazon Rekognition, you might not
--   need to base64-encode image bytes passed using the <tt>Bytes</tt>
--   field. For more information, see Images in the Amazon Rekognition
--   developer guide.
[$sel:image:IndexFaces'] :: IndexFaces -> Image

-- | Create a value of <a>IndexFaces</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:externalImageId:IndexFaces'</a>,
--   <a>indexFaces_externalImageId</a> - The ID you want to assign to all
--   the faces detected in the image.
--   
--   <a>$sel:qualityFilter:IndexFaces'</a>, <a>indexFaces_qualityFilter</a>
--   - A filter that specifies a quality bar for how much filtering is done
--   to identify faces. Filtered faces aren't indexed. If you specify
--   <tt>AUTO</tt>, Amazon Rekognition chooses the quality bar. If you
--   specify <tt>LOW</tt>, <tt>MEDIUM</tt>, or <tt>HIGH</tt>, filtering
--   removes all faces that don’t meet the chosen quality bar. The default
--   value is <tt>AUTO</tt>. The quality bar is based on a variety of
--   common use cases. Low-quality detections can occur for a number of
--   reasons. Some examples are an object that's misidentified as a face, a
--   face that's too blurry, or a face with a pose that's too extreme to
--   use. If you specify <tt>NONE</tt>, no filtering is performed.
--   
--   To use quality filtering, the collection you are using must be
--   associated with version 3 of the face model or higher.
--   
--   <a>$sel:maxFaces:IndexFaces'</a>, <a>indexFaces_maxFaces</a> - The
--   maximum number of faces to index. The value of <tt>MaxFaces</tt> must
--   be greater than or equal to 1. <tt>IndexFaces</tt> returns no more
--   than 100 detected faces in an image, even if you specify a larger
--   value for <tt>MaxFaces</tt>.
--   
--   If <tt>IndexFaces</tt> detects more faces than the value of
--   <tt>MaxFaces</tt>, the faces with the lowest quality are filtered out
--   first. If there are still more faces than the value of
--   <tt>MaxFaces</tt>, the faces with the smallest bounding boxes are
--   filtered out (up to the number that's needed to satisfy the value of
--   <tt>MaxFaces</tt>). Information about the unindexed faces is available
--   in the <tt>UnindexedFaces</tt> array.
--   
--   The faces that are returned by <tt>IndexFaces</tt> are sorted by the
--   largest face bounding box size to the smallest size, in descending
--   order.
--   
--   <tt>MaxFaces</tt> can be used with a collection associated with any
--   version of the face model.
--   
--   <a>$sel:detectionAttributes:IndexFaces'</a>,
--   <a>indexFaces_detectionAttributes</a> - An array of facial attributes
--   that you want to be returned. This can be the default list of
--   attributes or all attributes. If you don't specify a value for
--   <tt>Attributes</tt> or if you specify <tt>["DEFAULT"]</tt>, the API
--   returns the following subset of facial attributes:
--   <tt>BoundingBox</tt>, <tt>Confidence</tt>, <tt>Pose</tt>,
--   <tt>Quality</tt>, and <tt>Landmarks</tt>. If you provide
--   <tt>["ALL"]</tt>, all facial attributes are returned, but the
--   operation takes longer to complete.
--   
--   If you provide both, <tt>["ALL", "DEFAULT"]</tt>, the service uses a
--   logical AND operator to determine which attributes to return (in this
--   case, all attributes).
--   
--   <a>$sel:collectionId:IndexFaces'</a>, <a>indexFaces_collectionId</a> -
--   The ID of an existing collection to which you want to add the faces
--   that are detected in the input images.
--   
--   <a>$sel:image:IndexFaces'</a>, <a>indexFaces_image</a> - The input
--   image as base64-encoded bytes or an S3 object. If you use the AWS CLI
--   to call Amazon Rekognition operations, passing base64-encoded image
--   bytes isn't supported.
--   
--   If you are using an AWS SDK to call Amazon Rekognition, you might not
--   need to base64-encode image bytes passed using the <tt>Bytes</tt>
--   field. For more information, see Images in the Amazon Rekognition
--   developer guide.
newIndexFaces :: Text -> Image -> IndexFaces

-- | The ID you want to assign to all the faces detected in the image.
indexFaces_externalImageId :: Lens' IndexFaces (Maybe Text)

-- | A filter that specifies a quality bar for how much filtering is done
--   to identify faces. Filtered faces aren't indexed. If you specify
--   <tt>AUTO</tt>, Amazon Rekognition chooses the quality bar. If you
--   specify <tt>LOW</tt>, <tt>MEDIUM</tt>, or <tt>HIGH</tt>, filtering
--   removes all faces that don’t meet the chosen quality bar. The default
--   value is <tt>AUTO</tt>. The quality bar is based on a variety of
--   common use cases. Low-quality detections can occur for a number of
--   reasons. Some examples are an object that's misidentified as a face, a
--   face that's too blurry, or a face with a pose that's too extreme to
--   use. If you specify <tt>NONE</tt>, no filtering is performed.
--   
--   To use quality filtering, the collection you are using must be
--   associated with version 3 of the face model or higher.
indexFaces_qualityFilter :: Lens' IndexFaces (Maybe QualityFilter)

-- | The maximum number of faces to index. The value of <tt>MaxFaces</tt>
--   must be greater than or equal to 1. <tt>IndexFaces</tt> returns no
--   more than 100 detected faces in an image, even if you specify a larger
--   value for <tt>MaxFaces</tt>.
--   
--   If <tt>IndexFaces</tt> detects more faces than the value of
--   <tt>MaxFaces</tt>, the faces with the lowest quality are filtered out
--   first. If there are still more faces than the value of
--   <tt>MaxFaces</tt>, the faces with the smallest bounding boxes are
--   filtered out (up to the number that's needed to satisfy the value of
--   <tt>MaxFaces</tt>). Information about the unindexed faces is available
--   in the <tt>UnindexedFaces</tt> array.
--   
--   The faces that are returned by <tt>IndexFaces</tt> are sorted by the
--   largest face bounding box size to the smallest size, in descending
--   order.
--   
--   <tt>MaxFaces</tt> can be used with a collection associated with any
--   version of the face model.
indexFaces_maxFaces :: Lens' IndexFaces (Maybe Natural)

-- | An array of facial attributes that you want to be returned. This can
--   be the default list of attributes or all attributes. If you don't
--   specify a value for <tt>Attributes</tt> or if you specify
--   <tt>["DEFAULT"]</tt>, the API returns the following subset of facial
--   attributes: <tt>BoundingBox</tt>, <tt>Confidence</tt>, <tt>Pose</tt>,
--   <tt>Quality</tt>, and <tt>Landmarks</tt>. If you provide
--   <tt>["ALL"]</tt>, all facial attributes are returned, but the
--   operation takes longer to complete.
--   
--   If you provide both, <tt>["ALL", "DEFAULT"]</tt>, the service uses a
--   logical AND operator to determine which attributes to return (in this
--   case, all attributes).
indexFaces_detectionAttributes :: Lens' IndexFaces (Maybe [Attribute])

-- | The ID of an existing collection to which you want to add the faces
--   that are detected in the input images.
indexFaces_collectionId :: Lens' IndexFaces Text

-- | The input image as base64-encoded bytes or an S3 object. If you use
--   the AWS CLI to call Amazon Rekognition operations, passing
--   base64-encoded image bytes isn't supported.
--   
--   If you are using an AWS SDK to call Amazon Rekognition, you might not
--   need to base64-encode image bytes passed using the <tt>Bytes</tt>
--   field. For more information, see Images in the Amazon Rekognition
--   developer guide.
indexFaces_image :: Lens' IndexFaces Image

-- | <i>See:</i> <a>newIndexFacesResponse</a> smart constructor.
data IndexFacesResponse
IndexFacesResponse' :: Maybe Text -> Maybe [FaceRecord] -> Maybe OrientationCorrection -> Maybe [UnindexedFace] -> Int -> IndexFacesResponse

-- | The version number of the face detection model that's associated with
--   the input collection (<tt>CollectionId</tt>).
[$sel:faceModelVersion:IndexFacesResponse'] :: IndexFacesResponse -> Maybe Text

-- | An array of faces detected and added to the collection. For more
--   information, see Searching Faces in a Collection in the Amazon
--   Rekognition Developer Guide.
[$sel:faceRecords:IndexFacesResponse'] :: IndexFacesResponse -> Maybe [FaceRecord]

-- | If your collection is associated with a face detection model that's
--   later than version 3.0, the value of <tt>OrientationCorrection</tt> is
--   always null and no orientation information is returned.
--   
--   If your collection is associated with a face detection model that's
--   version 3.0 or earlier, the following applies:
--   
--   <ul>
--   <li>If the input image is in .jpeg format, it might contain
--   exchangeable image file format (Exif) metadata that includes the
--   image's orientation. Amazon Rekognition uses this orientation
--   information to perform image correction - the bounding box coordinates
--   are translated to represent object locations after the orientation
--   information in the Exif metadata is used to correct the image
--   orientation. Images in .png format don't contain Exif metadata. The
--   value of <tt>OrientationCorrection</tt> is null.</li>
--   <li>If the image doesn't contain orientation information in its Exif
--   metadata, Amazon Rekognition returns an estimated orientation
--   (ROTATE_0, ROTATE_90, ROTATE_180, ROTATE_270). Amazon Rekognition
--   doesn’t perform image correction for images. The bounding box
--   coordinates aren't translated and represent the object locations
--   before the image is rotated.</li>
--   </ul>
--   
--   Bounding box information is returned in the <tt>FaceRecords</tt>
--   array. You can get the version of the face detection model by calling
--   DescribeCollection.
[$sel:orientationCorrection:IndexFacesResponse'] :: IndexFacesResponse -> Maybe OrientationCorrection

-- | An array of faces that were detected in the image but weren't indexed.
--   They weren't indexed because the quality filter identified them as low
--   quality, or the <tt>MaxFaces</tt> request parameter filtered them out.
--   To use the quality filter, you specify the <tt>QualityFilter</tt>
--   request parameter.
[$sel:unindexedFaces:IndexFacesResponse'] :: IndexFacesResponse -> Maybe [UnindexedFace]

-- | The response's http status code.
[$sel:httpStatus:IndexFacesResponse'] :: IndexFacesResponse -> Int

-- | Create a value of <a>IndexFacesResponse</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:faceModelVersion:IndexFacesResponse'</a>,
--   <a>indexFacesResponse_faceModelVersion</a> - The version number of the
--   face detection model that's associated with the input collection
--   (<tt>CollectionId</tt>).
--   
--   <a>$sel:faceRecords:IndexFacesResponse'</a>,
--   <a>indexFacesResponse_faceRecords</a> - An array of faces detected and
--   added to the collection. For more information, see Searching Faces in
--   a Collection in the Amazon Rekognition Developer Guide.
--   
--   <a>$sel:orientationCorrection:IndexFacesResponse'</a>,
--   <a>indexFacesResponse_orientationCorrection</a> - If your collection
--   is associated with a face detection model that's later than version
--   3.0, the value of <tt>OrientationCorrection</tt> is always null and no
--   orientation information is returned.
--   
--   If your collection is associated with a face detection model that's
--   version 3.0 or earlier, the following applies:
--   
--   <ul>
--   <li>If the input image is in .jpeg format, it might contain
--   exchangeable image file format (Exif) metadata that includes the
--   image's orientation. Amazon Rekognition uses this orientation
--   information to perform image correction - the bounding box coordinates
--   are translated to represent object locations after the orientation
--   information in the Exif metadata is used to correct the image
--   orientation. Images in .png format don't contain Exif metadata. The
--   value of <tt>OrientationCorrection</tt> is null.</li>
--   <li>If the image doesn't contain orientation information in its Exif
--   metadata, Amazon Rekognition returns an estimated orientation
--   (ROTATE_0, ROTATE_90, ROTATE_180, ROTATE_270). Amazon Rekognition
--   doesn’t perform image correction for images. The bounding box
--   coordinates aren't translated and represent the object locations
--   before the image is rotated.</li>
--   </ul>
--   
--   Bounding box information is returned in the <tt>FaceRecords</tt>
--   array. You can get the version of the face detection model by calling
--   DescribeCollection.
--   
--   <a>$sel:unindexedFaces:IndexFacesResponse'</a>,
--   <a>indexFacesResponse_unindexedFaces</a> - An array of faces that were
--   detected in the image but weren't indexed. They weren't indexed
--   because the quality filter identified them as low quality, or the
--   <tt>MaxFaces</tt> request parameter filtered them out. To use the
--   quality filter, you specify the <tt>QualityFilter</tt> request
--   parameter.
--   
--   <a>$sel:httpStatus:IndexFacesResponse'</a>,
--   <a>indexFacesResponse_httpStatus</a> - The response's http status
--   code.
newIndexFacesResponse :: Int -> IndexFacesResponse

-- | The version number of the face detection model that's associated with
--   the input collection (<tt>CollectionId</tt>).
indexFacesResponse_faceModelVersion :: Lens' IndexFacesResponse (Maybe Text)

-- | An array of faces detected and added to the collection. For more
--   information, see Searching Faces in a Collection in the Amazon
--   Rekognition Developer Guide.
indexFacesResponse_faceRecords :: Lens' IndexFacesResponse (Maybe [FaceRecord])

-- | If your collection is associated with a face detection model that's
--   later than version 3.0, the value of <tt>OrientationCorrection</tt> is
--   always null and no orientation information is returned.
--   
--   If your collection is associated with a face detection model that's
--   version 3.0 or earlier, the following applies:
--   
--   <ul>
--   <li>If the input image is in .jpeg format, it might contain
--   exchangeable image file format (Exif) metadata that includes the
--   image's orientation. Amazon Rekognition uses this orientation
--   information to perform image correction - the bounding box coordinates
--   are translated to represent object locations after the orientation
--   information in the Exif metadata is used to correct the image
--   orientation. Images in .png format don't contain Exif metadata. The
--   value of <tt>OrientationCorrection</tt> is null.</li>
--   <li>If the image doesn't contain orientation information in its Exif
--   metadata, Amazon Rekognition returns an estimated orientation
--   (ROTATE_0, ROTATE_90, ROTATE_180, ROTATE_270). Amazon Rekognition
--   doesn’t perform image correction for images. The bounding box
--   coordinates aren't translated and represent the object locations
--   before the image is rotated.</li>
--   </ul>
--   
--   Bounding box information is returned in the <tt>FaceRecords</tt>
--   array. You can get the version of the face detection model by calling
--   DescribeCollection.
indexFacesResponse_orientationCorrection :: Lens' IndexFacesResponse (Maybe OrientationCorrection)

-- | An array of faces that were detected in the image but weren't indexed.
--   They weren't indexed because the quality filter identified them as low
--   quality, or the <tt>MaxFaces</tt> request parameter filtered them out.
--   To use the quality filter, you specify the <tt>QualityFilter</tt>
--   request parameter.
indexFacesResponse_unindexedFaces :: Lens' IndexFacesResponse (Maybe [UnindexedFace])

-- | The response's http status code.
indexFacesResponse_httpStatus :: Lens' IndexFacesResponse Int
instance GHC.Generics.Generic Network.AWS.Rekognition.IndexFaces.IndexFaces
instance GHC.Show.Show Network.AWS.Rekognition.IndexFaces.IndexFaces
instance GHC.Read.Read Network.AWS.Rekognition.IndexFaces.IndexFaces
instance GHC.Classes.Eq Network.AWS.Rekognition.IndexFaces.IndexFaces
instance GHC.Generics.Generic Network.AWS.Rekognition.IndexFaces.IndexFacesResponse
instance GHC.Show.Show Network.AWS.Rekognition.IndexFaces.IndexFacesResponse
instance GHC.Read.Read Network.AWS.Rekognition.IndexFaces.IndexFacesResponse
instance GHC.Classes.Eq Network.AWS.Rekognition.IndexFaces.IndexFacesResponse
instance Network.AWS.Types.AWSRequest Network.AWS.Rekognition.IndexFaces.IndexFaces
instance Control.DeepSeq.NFData Network.AWS.Rekognition.IndexFaces.IndexFacesResponse
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.IndexFaces.IndexFaces
instance Control.DeepSeq.NFData Network.AWS.Rekognition.IndexFaces.IndexFaces
instance Network.AWS.Data.Headers.ToHeaders Network.AWS.Rekognition.IndexFaces.IndexFaces
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.IndexFaces.IndexFaces
instance Network.AWS.Data.Path.ToPath Network.AWS.Rekognition.IndexFaces.IndexFaces
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.IndexFaces.IndexFaces


-- | Gets the text detection results of a Amazon Rekognition Video analysis
--   started by StartTextDetection.
--   
--   Text detection with Amazon Rekognition Video is an asynchronous
--   operation. You start text detection by calling StartTextDetection
--   which returns a job identifier (<tt>JobId</tt>) When the text
--   detection operation finishes, Amazon Rekognition publishes a
--   completion status to the Amazon Simple Notification Service topic
--   registered in the initial call to <tt>StartTextDetection</tt>. To get
--   the results of the text detection operation, first check that the
--   status value published to the Amazon SNS topic is <tt>SUCCEEDED</tt>.
--   if so, call <tt>GetTextDetection</tt> and pass the job identifier
--   (<tt>JobId</tt>) from the initial call of
--   <tt>StartLabelDetection</tt>.
--   
--   <tt>GetTextDetection</tt> returns an array of detected text
--   (<tt>TextDetections</tt>) sorted by the time the text was detected, up
--   to 50 words per frame of video.
--   
--   Each element of the array includes the detected text, the precentage
--   confidence in the acuracy of the detected text, the time the text was
--   detected, bounding box information for where the text was located, and
--   unique identifiers for words and their lines.
--   
--   Use MaxResults parameter to limit the number of text detections
--   returned. If there are more results than specified in
--   <tt>MaxResults</tt>, the value of <tt>NextToken</tt> in the operation
--   response contains a pagination token for getting the next set of
--   results. To get the next page of results, call
--   <tt>GetTextDetection</tt> and populate the <tt>NextToken</tt> request
--   parameter with the token value returned from the previous call to
--   <tt>GetTextDetection</tt>.
module Network.AWS.Rekognition.GetTextDetection

-- | <i>See:</i> <a>newGetTextDetection</a> smart constructor.
data GetTextDetection
GetTextDetection' :: Maybe Text -> Maybe Natural -> Text -> GetTextDetection

-- | If the previous response was incomplete (because there are more labels
--   to retrieve), Amazon Rekognition Video returns a pagination token in
--   the response. You can use this pagination token to retrieve the next
--   set of text.
[$sel:nextToken:GetTextDetection'] :: GetTextDetection -> Maybe Text

-- | Maximum number of results to return per paginated call. The largest
--   value you can specify is 1000.
[$sel:maxResults:GetTextDetection'] :: GetTextDetection -> Maybe Natural

-- | Job identifier for the text detection operation for which you want
--   results returned. You get the job identifer from an initial call to
--   <tt>StartTextDetection</tt>.
[$sel:jobId:GetTextDetection'] :: GetTextDetection -> Text

-- | Create a value of <a>GetTextDetection</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:nextToken:GetTextDetection'</a>,
--   <a>getTextDetection_nextToken</a> - If the previous response was
--   incomplete (because there are more labels to retrieve), Amazon
--   Rekognition Video returns a pagination token in the response. You can
--   use this pagination token to retrieve the next set of text.
--   
--   <a>$sel:maxResults:GetTextDetection'</a>,
--   <a>getTextDetection_maxResults</a> - Maximum number of results to
--   return per paginated call. The largest value you can specify is 1000.
--   
--   <a>$sel:jobId:GetTextDetection'</a>, <a>getTextDetection_jobId</a> -
--   Job identifier for the text detection operation for which you want
--   results returned. You get the job identifer from an initial call to
--   <tt>StartTextDetection</tt>.
newGetTextDetection :: Text -> GetTextDetection

-- | If the previous response was incomplete (because there are more labels
--   to retrieve), Amazon Rekognition Video returns a pagination token in
--   the response. You can use this pagination token to retrieve the next
--   set of text.
getTextDetection_nextToken :: Lens' GetTextDetection (Maybe Text)

-- | Maximum number of results to return per paginated call. The largest
--   value you can specify is 1000.
getTextDetection_maxResults :: Lens' GetTextDetection (Maybe Natural)

-- | Job identifier for the text detection operation for which you want
--   results returned. You get the job identifer from an initial call to
--   <tt>StartTextDetection</tt>.
getTextDetection_jobId :: Lens' GetTextDetection Text

-- | <i>See:</i> <a>newGetTextDetectionResponse</a> smart constructor.
data GetTextDetectionResponse
GetTextDetectionResponse' :: Maybe [TextDetectionResult] -> Maybe Text -> Maybe VideoMetadata -> Maybe Text -> Maybe Text -> Maybe VideoJobStatus -> Int -> GetTextDetectionResponse

-- | An array of text detected in the video. Each element contains the
--   detected text, the time in milliseconds from the start of the video
--   that the text was detected, and where it was detected on the screen.
[$sel:textDetections:GetTextDetectionResponse'] :: GetTextDetectionResponse -> Maybe [TextDetectionResult]

-- | If the response is truncated, Amazon Rekognition Video returns this
--   token that you can use in the subsequent request to retrieve the next
--   set of text.
[$sel:nextToken:GetTextDetectionResponse'] :: GetTextDetectionResponse -> Maybe Text
[$sel:videoMetadata:GetTextDetectionResponse'] :: GetTextDetectionResponse -> Maybe VideoMetadata

-- | If the job fails, <tt>StatusMessage</tt> provides a descriptive error
--   message.
[$sel:statusMessage:GetTextDetectionResponse'] :: GetTextDetectionResponse -> Maybe Text

-- | Version number of the text detection model that was used to detect
--   text.
[$sel:textModelVersion:GetTextDetectionResponse'] :: GetTextDetectionResponse -> Maybe Text

-- | Current status of the text detection job.
[$sel:jobStatus:GetTextDetectionResponse'] :: GetTextDetectionResponse -> Maybe VideoJobStatus

-- | The response's http status code.
[$sel:httpStatus:GetTextDetectionResponse'] :: GetTextDetectionResponse -> Int

-- | Create a value of <a>GetTextDetectionResponse</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:textDetections:GetTextDetectionResponse'</a>,
--   <a>getTextDetectionResponse_textDetections</a> - An array of text
--   detected in the video. Each element contains the detected text, the
--   time in milliseconds from the start of the video that the text was
--   detected, and where it was detected on the screen.
--   
--   <a>$sel:nextToken:GetTextDetection'</a>,
--   <a>getTextDetectionResponse_nextToken</a> - If the response is
--   truncated, Amazon Rekognition Video returns this token that you can
--   use in the subsequent request to retrieve the next set of text.
--   
--   <a>$sel:videoMetadata:GetTextDetectionResponse'</a>,
--   <a>getTextDetectionResponse_videoMetadata</a> - Undocumented member.
--   
--   <a>$sel:statusMessage:GetTextDetectionResponse'</a>,
--   <a>getTextDetectionResponse_statusMessage</a> - If the job fails,
--   <tt>StatusMessage</tt> provides a descriptive error message.
--   
--   <a>$sel:textModelVersion:GetTextDetectionResponse'</a>,
--   <a>getTextDetectionResponse_textModelVersion</a> - Version number of
--   the text detection model that was used to detect text.
--   
--   <a>$sel:jobStatus:GetTextDetectionResponse'</a>,
--   <a>getTextDetectionResponse_jobStatus</a> - Current status of the text
--   detection job.
--   
--   <a>$sel:httpStatus:GetTextDetectionResponse'</a>,
--   <a>getTextDetectionResponse_httpStatus</a> - The response's http
--   status code.
newGetTextDetectionResponse :: Int -> GetTextDetectionResponse

-- | An array of text detected in the video. Each element contains the
--   detected text, the time in milliseconds from the start of the video
--   that the text was detected, and where it was detected on the screen.
getTextDetectionResponse_textDetections :: Lens' GetTextDetectionResponse (Maybe [TextDetectionResult])

-- | If the response is truncated, Amazon Rekognition Video returns this
--   token that you can use in the subsequent request to retrieve the next
--   set of text.
getTextDetectionResponse_nextToken :: Lens' GetTextDetectionResponse (Maybe Text)

-- | Undocumented member.
getTextDetectionResponse_videoMetadata :: Lens' GetTextDetectionResponse (Maybe VideoMetadata)

-- | If the job fails, <tt>StatusMessage</tt> provides a descriptive error
--   message.
getTextDetectionResponse_statusMessage :: Lens' GetTextDetectionResponse (Maybe Text)

-- | Version number of the text detection model that was used to detect
--   text.
getTextDetectionResponse_textModelVersion :: Lens' GetTextDetectionResponse (Maybe Text)

-- | Current status of the text detection job.
getTextDetectionResponse_jobStatus :: Lens' GetTextDetectionResponse (Maybe VideoJobStatus)

-- | The response's http status code.
getTextDetectionResponse_httpStatus :: Lens' GetTextDetectionResponse Int
instance GHC.Generics.Generic Network.AWS.Rekognition.GetTextDetection.GetTextDetection
instance GHC.Show.Show Network.AWS.Rekognition.GetTextDetection.GetTextDetection
instance GHC.Read.Read Network.AWS.Rekognition.GetTextDetection.GetTextDetection
instance GHC.Classes.Eq Network.AWS.Rekognition.GetTextDetection.GetTextDetection
instance GHC.Generics.Generic Network.AWS.Rekognition.GetTextDetection.GetTextDetectionResponse
instance GHC.Show.Show Network.AWS.Rekognition.GetTextDetection.GetTextDetectionResponse
instance GHC.Read.Read Network.AWS.Rekognition.GetTextDetection.GetTextDetectionResponse
instance GHC.Classes.Eq Network.AWS.Rekognition.GetTextDetection.GetTextDetectionResponse
instance Network.AWS.Types.AWSRequest Network.AWS.Rekognition.GetTextDetection.GetTextDetection
instance Control.DeepSeq.NFData Network.AWS.Rekognition.GetTextDetection.GetTextDetectionResponse
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.GetTextDetection.GetTextDetection
instance Control.DeepSeq.NFData Network.AWS.Rekognition.GetTextDetection.GetTextDetection
instance Network.AWS.Data.Headers.ToHeaders Network.AWS.Rekognition.GetTextDetection.GetTextDetection
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.GetTextDetection.GetTextDetection
instance Network.AWS.Data.Path.ToPath Network.AWS.Rekognition.GetTextDetection.GetTextDetection
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.GetTextDetection.GetTextDetection


-- | Gets the segment detection results of a Amazon Rekognition Video
--   analysis started by StartSegmentDetection.
--   
--   Segment detection with Amazon Rekognition Video is an asynchronous
--   operation. You start segment detection by calling
--   StartSegmentDetection which returns a job identifier (<tt>JobId</tt>).
--   When the segment detection operation finishes, Amazon Rekognition
--   publishes a completion status to the Amazon Simple Notification
--   Service topic registered in the initial call to
--   <tt>StartSegmentDetection</tt>. To get the results of the segment
--   detection operation, first check that the status value published to
--   the Amazon SNS topic is <tt>SUCCEEDED</tt>. if so, call
--   <tt>GetSegmentDetection</tt> and pass the job identifier
--   (<tt>JobId</tt>) from the initial call of
--   <tt>StartSegmentDetection</tt>.
--   
--   <tt>GetSegmentDetection</tt> returns detected segments in an array
--   (<tt>Segments</tt>) of SegmentDetection objects. <tt>Segments</tt> is
--   sorted by the segment types specified in the <tt>SegmentTypes</tt>
--   input parameter of <tt>StartSegmentDetection</tt>. Each element of the
--   array includes the detected segment, the precentage confidence in the
--   acuracy of the detected segment, the type of the segment, and the
--   frame in which the segment was detected.
--   
--   Use <tt>SelectedSegmentTypes</tt> to find out the type of segment
--   detection requested in the call to <tt>StartSegmentDetection</tt>.
--   
--   Use the <tt>MaxResults</tt> parameter to limit the number of segment
--   detections returned. If there are more results than specified in
--   <tt>MaxResults</tt>, the value of <tt>NextToken</tt> in the operation
--   response contains a pagination token for getting the next set of
--   results. To get the next page of results, call
--   <tt>GetSegmentDetection</tt> and populate the <tt>NextToken</tt>
--   request parameter with the token value returned from the previous call
--   to <tt>GetSegmentDetection</tt>.
--   
--   For more information, see Detecting Video Segments in Stored Video in
--   the Amazon Rekognition Developer Guide.
module Network.AWS.Rekognition.GetSegmentDetection

-- | <i>See:</i> <a>newGetSegmentDetection</a> smart constructor.
data GetSegmentDetection
GetSegmentDetection' :: Maybe Text -> Maybe Natural -> Text -> GetSegmentDetection

-- | If the response is truncated, Amazon Rekognition Video returns this
--   token that you can use in the subsequent request to retrieve the next
--   set of text.
[$sel:nextToken:GetSegmentDetection'] :: GetSegmentDetection -> Maybe Text

-- | Maximum number of results to return per paginated call. The largest
--   value you can specify is 1000.
[$sel:maxResults:GetSegmentDetection'] :: GetSegmentDetection -> Maybe Natural

-- | Job identifier for the text detection operation for which you want
--   results returned. You get the job identifer from an initial call to
--   <tt>StartSegmentDetection</tt>.
[$sel:jobId:GetSegmentDetection'] :: GetSegmentDetection -> Text

-- | Create a value of <a>GetSegmentDetection</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:nextToken:GetSegmentDetection'</a>,
--   <a>getSegmentDetection_nextToken</a> - If the response is truncated,
--   Amazon Rekognition Video returns this token that you can use in the
--   subsequent request to retrieve the next set of text.
--   
--   <a>$sel:maxResults:GetSegmentDetection'</a>,
--   <a>getSegmentDetection_maxResults</a> - Maximum number of results to
--   return per paginated call. The largest value you can specify is 1000.
--   
--   <a>$sel:jobId:GetSegmentDetection'</a>,
--   <a>getSegmentDetection_jobId</a> - Job identifier for the text
--   detection operation for which you want results returned. You get the
--   job identifer from an initial call to <tt>StartSegmentDetection</tt>.
newGetSegmentDetection :: Text -> GetSegmentDetection

-- | If the response is truncated, Amazon Rekognition Video returns this
--   token that you can use in the subsequent request to retrieve the next
--   set of text.
getSegmentDetection_nextToken :: Lens' GetSegmentDetection (Maybe Text)

-- | Maximum number of results to return per paginated call. The largest
--   value you can specify is 1000.
getSegmentDetection_maxResults :: Lens' GetSegmentDetection (Maybe Natural)

-- | Job identifier for the text detection operation for which you want
--   results returned. You get the job identifer from an initial call to
--   <tt>StartSegmentDetection</tt>.
getSegmentDetection_jobId :: Lens' GetSegmentDetection Text

-- | <i>See:</i> <a>newGetSegmentDetectionResponse</a> smart constructor.
data GetSegmentDetectionResponse
GetSegmentDetectionResponse' :: Maybe [SegmentTypeInfo] -> Maybe Text -> Maybe [VideoMetadata] -> Maybe Text -> Maybe [SegmentDetection] -> Maybe VideoJobStatus -> Maybe [AudioMetadata] -> Int -> GetSegmentDetectionResponse

-- | An array containing the segment types requested in the call to
--   <tt>StartSegmentDetection</tt>.
[$sel:selectedSegmentTypes:GetSegmentDetectionResponse'] :: GetSegmentDetectionResponse -> Maybe [SegmentTypeInfo]

-- | If the previous response was incomplete (because there are more labels
--   to retrieve), Amazon Rekognition Video returns a pagination token in
--   the response. You can use this pagination token to retrieve the next
--   set of text.
[$sel:nextToken:GetSegmentDetectionResponse'] :: GetSegmentDetectionResponse -> Maybe Text

-- | Currently, Amazon Rekognition Video returns a single object in the
--   <tt>VideoMetadata</tt> array. The object contains information about
--   the video stream in the input file that Amazon Rekognition Video chose
--   to analyze. The <tt>VideoMetadata</tt> object includes the video
--   codec, video format and other information. Video metadata is returned
--   in each page of information returned by <tt>GetSegmentDetection</tt>.
[$sel:videoMetadata:GetSegmentDetectionResponse'] :: GetSegmentDetectionResponse -> Maybe [VideoMetadata]

-- | If the job fails, <tt>StatusMessage</tt> provides a descriptive error
--   message.
[$sel:statusMessage:GetSegmentDetectionResponse'] :: GetSegmentDetectionResponse -> Maybe Text

-- | An array of segments detected in a video. The array is sorted by the
--   segment types (TECHNICAL_CUE or SHOT) specified in the
--   <tt>SegmentTypes</tt> input parameter of
--   <tt>StartSegmentDetection</tt>. Within each segment type the array is
--   sorted by timestamp values.
[$sel:segments:GetSegmentDetectionResponse'] :: GetSegmentDetectionResponse -> Maybe [SegmentDetection]

-- | Current status of the segment detection job.
[$sel:jobStatus:GetSegmentDetectionResponse'] :: GetSegmentDetectionResponse -> Maybe VideoJobStatus

-- | An array of objects. There can be multiple audio streams. Each
--   <tt>AudioMetadata</tt> object contains metadata for a single audio
--   stream. Audio information in an <tt>AudioMetadata</tt> objects
--   includes the audio codec, the number of audio channels, the duration
--   of the audio stream, and the sample rate. Audio metadata is returned
--   in each page of information returned by <tt>GetSegmentDetection</tt>.
[$sel:audioMetadata:GetSegmentDetectionResponse'] :: GetSegmentDetectionResponse -> Maybe [AudioMetadata]

-- | The response's http status code.
[$sel:httpStatus:GetSegmentDetectionResponse'] :: GetSegmentDetectionResponse -> Int

-- | Create a value of <a>GetSegmentDetectionResponse</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:selectedSegmentTypes:GetSegmentDetectionResponse'</a>,
--   <a>getSegmentDetectionResponse_selectedSegmentTypes</a> - An array
--   containing the segment types requested in the call to
--   <tt>StartSegmentDetection</tt>.
--   
--   <a>$sel:nextToken:GetSegmentDetection'</a>,
--   <a>getSegmentDetectionResponse_nextToken</a> - If the previous
--   response was incomplete (because there are more labels to retrieve),
--   Amazon Rekognition Video returns a pagination token in the response.
--   You can use this pagination token to retrieve the next set of text.
--   
--   <a>$sel:videoMetadata:GetSegmentDetectionResponse'</a>,
--   <a>getSegmentDetectionResponse_videoMetadata</a> - Currently, Amazon
--   Rekognition Video returns a single object in the
--   <tt>VideoMetadata</tt> array. The object contains information about
--   the video stream in the input file that Amazon Rekognition Video chose
--   to analyze. The <tt>VideoMetadata</tt> object includes the video
--   codec, video format and other information. Video metadata is returned
--   in each page of information returned by <tt>GetSegmentDetection</tt>.
--   
--   <a>$sel:statusMessage:GetSegmentDetectionResponse'</a>,
--   <a>getSegmentDetectionResponse_statusMessage</a> - If the job fails,
--   <tt>StatusMessage</tt> provides a descriptive error message.
--   
--   <a>$sel:segments:GetSegmentDetectionResponse'</a>,
--   <a>getSegmentDetectionResponse_segments</a> - An array of segments
--   detected in a video. The array is sorted by the segment types
--   (TECHNICAL_CUE or SHOT) specified in the <tt>SegmentTypes</tt> input
--   parameter of <tt>StartSegmentDetection</tt>. Within each segment type
--   the array is sorted by timestamp values.
--   
--   <a>$sel:jobStatus:GetSegmentDetectionResponse'</a>,
--   <a>getSegmentDetectionResponse_jobStatus</a> - Current status of the
--   segment detection job.
--   
--   <a>$sel:audioMetadata:GetSegmentDetectionResponse'</a>,
--   <a>getSegmentDetectionResponse_audioMetadata</a> - An array of
--   objects. There can be multiple audio streams. Each
--   <tt>AudioMetadata</tt> object contains metadata for a single audio
--   stream. Audio information in an <tt>AudioMetadata</tt> objects
--   includes the audio codec, the number of audio channels, the duration
--   of the audio stream, and the sample rate. Audio metadata is returned
--   in each page of information returned by <tt>GetSegmentDetection</tt>.
--   
--   <a>$sel:httpStatus:GetSegmentDetectionResponse'</a>,
--   <a>getSegmentDetectionResponse_httpStatus</a> - The response's http
--   status code.
newGetSegmentDetectionResponse :: Int -> GetSegmentDetectionResponse

-- | An array containing the segment types requested in the call to
--   <tt>StartSegmentDetection</tt>.
getSegmentDetectionResponse_selectedSegmentTypes :: Lens' GetSegmentDetectionResponse (Maybe [SegmentTypeInfo])

-- | If the previous response was incomplete (because there are more labels
--   to retrieve), Amazon Rekognition Video returns a pagination token in
--   the response. You can use this pagination token to retrieve the next
--   set of text.
getSegmentDetectionResponse_nextToken :: Lens' GetSegmentDetectionResponse (Maybe Text)

-- | Currently, Amazon Rekognition Video returns a single object in the
--   <tt>VideoMetadata</tt> array. The object contains information about
--   the video stream in the input file that Amazon Rekognition Video chose
--   to analyze. The <tt>VideoMetadata</tt> object includes the video
--   codec, video format and other information. Video metadata is returned
--   in each page of information returned by <tt>GetSegmentDetection</tt>.
getSegmentDetectionResponse_videoMetadata :: Lens' GetSegmentDetectionResponse (Maybe [VideoMetadata])

-- | If the job fails, <tt>StatusMessage</tt> provides a descriptive error
--   message.
getSegmentDetectionResponse_statusMessage :: Lens' GetSegmentDetectionResponse (Maybe Text)

-- | An array of segments detected in a video. The array is sorted by the
--   segment types (TECHNICAL_CUE or SHOT) specified in the
--   <tt>SegmentTypes</tt> input parameter of
--   <tt>StartSegmentDetection</tt>. Within each segment type the array is
--   sorted by timestamp values.
getSegmentDetectionResponse_segments :: Lens' GetSegmentDetectionResponse (Maybe [SegmentDetection])

-- | Current status of the segment detection job.
getSegmentDetectionResponse_jobStatus :: Lens' GetSegmentDetectionResponse (Maybe VideoJobStatus)

-- | An array of objects. There can be multiple audio streams. Each
--   <tt>AudioMetadata</tt> object contains metadata for a single audio
--   stream. Audio information in an <tt>AudioMetadata</tt> objects
--   includes the audio codec, the number of audio channels, the duration
--   of the audio stream, and the sample rate. Audio metadata is returned
--   in each page of information returned by <tt>GetSegmentDetection</tt>.
getSegmentDetectionResponse_audioMetadata :: Lens' GetSegmentDetectionResponse (Maybe [AudioMetadata])

-- | The response's http status code.
getSegmentDetectionResponse_httpStatus :: Lens' GetSegmentDetectionResponse Int
instance GHC.Generics.Generic Network.AWS.Rekognition.GetSegmentDetection.GetSegmentDetection
instance GHC.Show.Show Network.AWS.Rekognition.GetSegmentDetection.GetSegmentDetection
instance GHC.Read.Read Network.AWS.Rekognition.GetSegmentDetection.GetSegmentDetection
instance GHC.Classes.Eq Network.AWS.Rekognition.GetSegmentDetection.GetSegmentDetection
instance GHC.Generics.Generic Network.AWS.Rekognition.GetSegmentDetection.GetSegmentDetectionResponse
instance GHC.Show.Show Network.AWS.Rekognition.GetSegmentDetection.GetSegmentDetectionResponse
instance GHC.Read.Read Network.AWS.Rekognition.GetSegmentDetection.GetSegmentDetectionResponse
instance GHC.Classes.Eq Network.AWS.Rekognition.GetSegmentDetection.GetSegmentDetectionResponse
instance Network.AWS.Types.AWSRequest Network.AWS.Rekognition.GetSegmentDetection.GetSegmentDetection
instance Control.DeepSeq.NFData Network.AWS.Rekognition.GetSegmentDetection.GetSegmentDetectionResponse
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.GetSegmentDetection.GetSegmentDetection
instance Control.DeepSeq.NFData Network.AWS.Rekognition.GetSegmentDetection.GetSegmentDetection
instance Network.AWS.Data.Headers.ToHeaders Network.AWS.Rekognition.GetSegmentDetection.GetSegmentDetection
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.GetSegmentDetection.GetSegmentDetection
instance Network.AWS.Data.Path.ToPath Network.AWS.Rekognition.GetSegmentDetection.GetSegmentDetection
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.GetSegmentDetection.GetSegmentDetection


-- | Gets the path tracking results of a Amazon Rekognition Video analysis
--   started by StartPersonTracking.
--   
--   The person path tracking operation is started by a call to
--   <tt>StartPersonTracking</tt> which returns a job identifier
--   (<tt>JobId</tt>). When the operation finishes, Amazon Rekognition
--   Video publishes a completion status to the Amazon Simple Notification
--   Service topic registered in the initial call to
--   <tt>StartPersonTracking</tt>.
--   
--   To get the results of the person path tracking operation, first check
--   that the status value published to the Amazon SNS topic is
--   <tt>SUCCEEDED</tt>. If so, call GetPersonTracking and pass the job
--   identifier (<tt>JobId</tt>) from the initial call to
--   <tt>StartPersonTracking</tt>.
--   
--   <tt>GetPersonTracking</tt> returns an array, <tt>Persons</tt>, of
--   tracked persons and the time(s) their paths were tracked in the video.
--   
--   <tt>GetPersonTracking</tt> only returns the default facial attributes
--   (<tt>BoundingBox</tt>, <tt>Confidence</tt>, <tt>Landmarks</tt>,
--   <tt>Pose</tt>, and <tt>Quality</tt>). The other facial attributes
--   listed in the <tt>Face</tt> object of the following response syntax
--   are not returned.
--   
--   For more information, see FaceDetail in the Amazon Rekognition
--   Developer Guide.
--   
--   By default, the array is sorted by the time(s) a person's path is
--   tracked in the video. You can sort by tracked persons by specifying
--   <tt>INDEX</tt> for the <tt>SortBy</tt> input parameter.
--   
--   Use the <tt>MaxResults</tt> parameter to limit the number of items
--   returned. If there are more results than specified in
--   <tt>MaxResults</tt>, the value of <tt>NextToken</tt> in the operation
--   response contains a pagination token for getting the next set of
--   results. To get the next page of results, call
--   <tt>GetPersonTracking</tt> and populate the <tt>NextToken</tt> request
--   parameter with the token value returned from the previous call to
--   <tt>GetPersonTracking</tt>.
module Network.AWS.Rekognition.GetPersonTracking

-- | <i>See:</i> <a>newGetPersonTracking</a> smart constructor.
data GetPersonTracking
GetPersonTracking' :: Maybe Text -> Maybe Natural -> Maybe PersonTrackingSortBy -> Text -> GetPersonTracking

-- | If the previous response was incomplete (because there are more
--   persons to retrieve), Amazon Rekognition Video returns a pagination
--   token in the response. You can use this pagination token to retrieve
--   the next set of persons.
[$sel:nextToken:GetPersonTracking'] :: GetPersonTracking -> Maybe Text

-- | Maximum number of results to return per paginated call. The largest
--   value you can specify is 1000. If you specify a value greater than
--   1000, a maximum of 1000 results is returned. The default value is
--   1000.
[$sel:maxResults:GetPersonTracking'] :: GetPersonTracking -> Maybe Natural

-- | Sort to use for elements in the <tt>Persons</tt> array. Use
--   <tt>TIMESTAMP</tt> to sort array elements by the time persons are
--   detected. Use <tt>INDEX</tt> to sort by the tracked persons. If you
--   sort by <tt>INDEX</tt>, the array elements for each person are sorted
--   by detection confidence. The default sort is by <tt>TIMESTAMP</tt>.
[$sel:sortBy:GetPersonTracking'] :: GetPersonTracking -> Maybe PersonTrackingSortBy

-- | The identifier for a job that tracks persons in a video. You get the
--   <tt>JobId</tt> from a call to <tt>StartPersonTracking</tt>.
[$sel:jobId:GetPersonTracking'] :: GetPersonTracking -> Text

-- | Create a value of <a>GetPersonTracking</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:nextToken:GetPersonTracking'</a>,
--   <a>getPersonTracking_nextToken</a> - If the previous response was
--   incomplete (because there are more persons to retrieve), Amazon
--   Rekognition Video returns a pagination token in the response. You can
--   use this pagination token to retrieve the next set of persons.
--   
--   <a>$sel:maxResults:GetPersonTracking'</a>,
--   <a>getPersonTracking_maxResults</a> - Maximum number of results to
--   return per paginated call. The largest value you can specify is 1000.
--   If you specify a value greater than 1000, a maximum of 1000 results is
--   returned. The default value is 1000.
--   
--   <a>$sel:sortBy:GetPersonTracking'</a>, <a>getPersonTracking_sortBy</a>
--   - Sort to use for elements in the <tt>Persons</tt> array. Use
--   <tt>TIMESTAMP</tt> to sort array elements by the time persons are
--   detected. Use <tt>INDEX</tt> to sort by the tracked persons. If you
--   sort by <tt>INDEX</tt>, the array elements for each person are sorted
--   by detection confidence. The default sort is by <tt>TIMESTAMP</tt>.
--   
--   <a>$sel:jobId:GetPersonTracking'</a>, <a>getPersonTracking_jobId</a> -
--   The identifier for a job that tracks persons in a video. You get the
--   <tt>JobId</tt> from a call to <tt>StartPersonTracking</tt>.
newGetPersonTracking :: Text -> GetPersonTracking

-- | If the previous response was incomplete (because there are more
--   persons to retrieve), Amazon Rekognition Video returns a pagination
--   token in the response. You can use this pagination token to retrieve
--   the next set of persons.
getPersonTracking_nextToken :: Lens' GetPersonTracking (Maybe Text)

-- | Maximum number of results to return per paginated call. The largest
--   value you can specify is 1000. If you specify a value greater than
--   1000, a maximum of 1000 results is returned. The default value is
--   1000.
getPersonTracking_maxResults :: Lens' GetPersonTracking (Maybe Natural)

-- | Sort to use for elements in the <tt>Persons</tt> array. Use
--   <tt>TIMESTAMP</tt> to sort array elements by the time persons are
--   detected. Use <tt>INDEX</tt> to sort by the tracked persons. If you
--   sort by <tt>INDEX</tt>, the array elements for each person are sorted
--   by detection confidence. The default sort is by <tt>TIMESTAMP</tt>.
getPersonTracking_sortBy :: Lens' GetPersonTracking (Maybe PersonTrackingSortBy)

-- | The identifier for a job that tracks persons in a video. You get the
--   <tt>JobId</tt> from a call to <tt>StartPersonTracking</tt>.
getPersonTracking_jobId :: Lens' GetPersonTracking Text

-- | <i>See:</i> <a>newGetPersonTrackingResponse</a> smart constructor.
data GetPersonTrackingResponse
GetPersonTrackingResponse' :: Maybe Text -> Maybe VideoMetadata -> Maybe Text -> Maybe VideoJobStatus -> Maybe [PersonDetection] -> Int -> GetPersonTrackingResponse

-- | If the response is truncated, Amazon Rekognition Video returns this
--   token that you can use in the subsequent request to retrieve the next
--   set of persons.
[$sel:nextToken:GetPersonTrackingResponse'] :: GetPersonTrackingResponse -> Maybe Text

-- | Information about a video that Amazon Rekognition Video analyzed.
--   <tt>Videometadata</tt> is returned in every page of paginated
--   responses from a Amazon Rekognition Video operation.
[$sel:videoMetadata:GetPersonTrackingResponse'] :: GetPersonTrackingResponse -> Maybe VideoMetadata

-- | If the job fails, <tt>StatusMessage</tt> provides a descriptive error
--   message.
[$sel:statusMessage:GetPersonTrackingResponse'] :: GetPersonTrackingResponse -> Maybe Text

-- | The current status of the person tracking job.
[$sel:jobStatus:GetPersonTrackingResponse'] :: GetPersonTrackingResponse -> Maybe VideoJobStatus

-- | An array of the persons detected in the video and the time(s) their
--   path was tracked throughout the video. An array element will exist for
--   each time a person's path is tracked.
[$sel:persons:GetPersonTrackingResponse'] :: GetPersonTrackingResponse -> Maybe [PersonDetection]

-- | The response's http status code.
[$sel:httpStatus:GetPersonTrackingResponse'] :: GetPersonTrackingResponse -> Int

-- | Create a value of <a>GetPersonTrackingResponse</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:nextToken:GetPersonTracking'</a>,
--   <a>getPersonTrackingResponse_nextToken</a> - If the response is
--   truncated, Amazon Rekognition Video returns this token that you can
--   use in the subsequent request to retrieve the next set of persons.
--   
--   <a>$sel:videoMetadata:GetPersonTrackingResponse'</a>,
--   <a>getPersonTrackingResponse_videoMetadata</a> - Information about a
--   video that Amazon Rekognition Video analyzed. <tt>Videometadata</tt>
--   is returned in every page of paginated responses from a Amazon
--   Rekognition Video operation.
--   
--   <a>$sel:statusMessage:GetPersonTrackingResponse'</a>,
--   <a>getPersonTrackingResponse_statusMessage</a> - If the job fails,
--   <tt>StatusMessage</tt> provides a descriptive error message.
--   
--   <a>$sel:jobStatus:GetPersonTrackingResponse'</a>,
--   <a>getPersonTrackingResponse_jobStatus</a> - The current status of the
--   person tracking job.
--   
--   <a>$sel:persons:GetPersonTrackingResponse'</a>,
--   <a>getPersonTrackingResponse_persons</a> - An array of the persons
--   detected in the video and the time(s) their path was tracked
--   throughout the video. An array element will exist for each time a
--   person's path is tracked.
--   
--   <a>$sel:httpStatus:GetPersonTrackingResponse'</a>,
--   <a>getPersonTrackingResponse_httpStatus</a> - The response's http
--   status code.
newGetPersonTrackingResponse :: Int -> GetPersonTrackingResponse

-- | If the response is truncated, Amazon Rekognition Video returns this
--   token that you can use in the subsequent request to retrieve the next
--   set of persons.
getPersonTrackingResponse_nextToken :: Lens' GetPersonTrackingResponse (Maybe Text)

-- | Information about a video that Amazon Rekognition Video analyzed.
--   <tt>Videometadata</tt> is returned in every page of paginated
--   responses from a Amazon Rekognition Video operation.
getPersonTrackingResponse_videoMetadata :: Lens' GetPersonTrackingResponse (Maybe VideoMetadata)

-- | If the job fails, <tt>StatusMessage</tt> provides a descriptive error
--   message.
getPersonTrackingResponse_statusMessage :: Lens' GetPersonTrackingResponse (Maybe Text)

-- | The current status of the person tracking job.
getPersonTrackingResponse_jobStatus :: Lens' GetPersonTrackingResponse (Maybe VideoJobStatus)

-- | An array of the persons detected in the video and the time(s) their
--   path was tracked throughout the video. An array element will exist for
--   each time a person's path is tracked.
getPersonTrackingResponse_persons :: Lens' GetPersonTrackingResponse (Maybe [PersonDetection])

-- | The response's http status code.
getPersonTrackingResponse_httpStatus :: Lens' GetPersonTrackingResponse Int
instance GHC.Generics.Generic Network.AWS.Rekognition.GetPersonTracking.GetPersonTracking
instance GHC.Show.Show Network.AWS.Rekognition.GetPersonTracking.GetPersonTracking
instance GHC.Read.Read Network.AWS.Rekognition.GetPersonTracking.GetPersonTracking
instance GHC.Classes.Eq Network.AWS.Rekognition.GetPersonTracking.GetPersonTracking
instance GHC.Generics.Generic Network.AWS.Rekognition.GetPersonTracking.GetPersonTrackingResponse
instance GHC.Show.Show Network.AWS.Rekognition.GetPersonTracking.GetPersonTrackingResponse
instance GHC.Read.Read Network.AWS.Rekognition.GetPersonTracking.GetPersonTrackingResponse
instance GHC.Classes.Eq Network.AWS.Rekognition.GetPersonTracking.GetPersonTrackingResponse
instance Network.AWS.Types.AWSRequest Network.AWS.Rekognition.GetPersonTracking.GetPersonTracking
instance Control.DeepSeq.NFData Network.AWS.Rekognition.GetPersonTracking.GetPersonTrackingResponse
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.GetPersonTracking.GetPersonTracking
instance Control.DeepSeq.NFData Network.AWS.Rekognition.GetPersonTracking.GetPersonTracking
instance Network.AWS.Data.Headers.ToHeaders Network.AWS.Rekognition.GetPersonTracking.GetPersonTracking
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.GetPersonTracking.GetPersonTracking
instance Network.AWS.Data.Path.ToPath Network.AWS.Rekognition.GetPersonTracking.GetPersonTracking
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.GetPersonTracking.GetPersonTracking


-- | Gets the label detection results of a Amazon Rekognition Video
--   analysis started by StartLabelDetection.
--   
--   The label detection operation is started by a call to
--   StartLabelDetection which returns a job identifier (<tt>JobId</tt>).
--   When the label detection operation finishes, Amazon Rekognition
--   publishes a completion status to the Amazon Simple Notification
--   Service topic registered in the initial call to
--   <tt>StartlabelDetection</tt>. To get the results of the label
--   detection operation, first check that the status value published to
--   the Amazon SNS topic is <tt>SUCCEEDED</tt>. If so, call
--   GetLabelDetection and pass the job identifier (<tt>JobId</tt>) from
--   the initial call to <tt>StartLabelDetection</tt>.
--   
--   <tt>GetLabelDetection</tt> returns an array of detected labels
--   (<tt>Labels</tt>) sorted by the time the labels were detected. You can
--   also sort by the label name by specifying <tt>NAME</tt> for the
--   <tt>SortBy</tt> input parameter.
--   
--   The labels returned include the label name, the percentage confidence
--   in the accuracy of the detected label, and the time the label was
--   detected in the video.
--   
--   The returned labels also include bounding box information for common
--   objects, a hierarchical taxonomy of detected labels, and the version
--   of the label model used for detection.
--   
--   Use MaxResults parameter to limit the number of labels returned. If
--   there are more results than specified in <tt>MaxResults</tt>, the
--   value of <tt>NextToken</tt> in the operation response contains a
--   pagination token for getting the next set of results. To get the next
--   page of results, call <tt>GetlabelDetection</tt> and populate the
--   <tt>NextToken</tt> request parameter with the token value returned
--   from the previous call to <tt>GetLabelDetection</tt>.
module Network.AWS.Rekognition.GetLabelDetection

-- | <i>See:</i> <a>newGetLabelDetection</a> smart constructor.
data GetLabelDetection
GetLabelDetection' :: Maybe Text -> Maybe Natural -> Maybe LabelDetectionSortBy -> Text -> GetLabelDetection

-- | If the previous response was incomplete (because there are more labels
--   to retrieve), Amazon Rekognition Video returns a pagination token in
--   the response. You can use this pagination token to retrieve the next
--   set of labels.
[$sel:nextToken:GetLabelDetection'] :: GetLabelDetection -> Maybe Text

-- | Maximum number of results to return per paginated call. The largest
--   value you can specify is 1000. If you specify a value greater than
--   1000, a maximum of 1000 results is returned. The default value is
--   1000.
[$sel:maxResults:GetLabelDetection'] :: GetLabelDetection -> Maybe Natural

-- | Sort to use for elements in the <tt>Labels</tt> array. Use
--   <tt>TIMESTAMP</tt> to sort array elements by the time labels are
--   detected. Use <tt>NAME</tt> to alphabetically group elements for a
--   label together. Within each label group, the array element are sorted
--   by detection confidence. The default sort is by <tt>TIMESTAMP</tt>.
[$sel:sortBy:GetLabelDetection'] :: GetLabelDetection -> Maybe LabelDetectionSortBy

-- | Job identifier for the label detection operation for which you want
--   results returned. You get the job identifer from an initial call to
--   <tt>StartlabelDetection</tt>.
[$sel:jobId:GetLabelDetection'] :: GetLabelDetection -> Text

-- | Create a value of <a>GetLabelDetection</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:nextToken:GetLabelDetection'</a>,
--   <a>getLabelDetection_nextToken</a> - If the previous response was
--   incomplete (because there are more labels to retrieve), Amazon
--   Rekognition Video returns a pagination token in the response. You can
--   use this pagination token to retrieve the next set of labels.
--   
--   <a>$sel:maxResults:GetLabelDetection'</a>,
--   <a>getLabelDetection_maxResults</a> - Maximum number of results to
--   return per paginated call. The largest value you can specify is 1000.
--   If you specify a value greater than 1000, a maximum of 1000 results is
--   returned. The default value is 1000.
--   
--   <a>$sel:sortBy:GetLabelDetection'</a>, <a>getLabelDetection_sortBy</a>
--   - Sort to use for elements in the <tt>Labels</tt> array. Use
--   <tt>TIMESTAMP</tt> to sort array elements by the time labels are
--   detected. Use <tt>NAME</tt> to alphabetically group elements for a
--   label together. Within each label group, the array element are sorted
--   by detection confidence. The default sort is by <tt>TIMESTAMP</tt>.
--   
--   <a>$sel:jobId:GetLabelDetection'</a>, <a>getLabelDetection_jobId</a> -
--   Job identifier for the label detection operation for which you want
--   results returned. You get the job identifer from an initial call to
--   <tt>StartlabelDetection</tt>.
newGetLabelDetection :: Text -> GetLabelDetection

-- | If the previous response was incomplete (because there are more labels
--   to retrieve), Amazon Rekognition Video returns a pagination token in
--   the response. You can use this pagination token to retrieve the next
--   set of labels.
getLabelDetection_nextToken :: Lens' GetLabelDetection (Maybe Text)

-- | Maximum number of results to return per paginated call. The largest
--   value you can specify is 1000. If you specify a value greater than
--   1000, a maximum of 1000 results is returned. The default value is
--   1000.
getLabelDetection_maxResults :: Lens' GetLabelDetection (Maybe Natural)

-- | Sort to use for elements in the <tt>Labels</tt> array. Use
--   <tt>TIMESTAMP</tt> to sort array elements by the time labels are
--   detected. Use <tt>NAME</tt> to alphabetically group elements for a
--   label together. Within each label group, the array element are sorted
--   by detection confidence. The default sort is by <tt>TIMESTAMP</tt>.
getLabelDetection_sortBy :: Lens' GetLabelDetection (Maybe LabelDetectionSortBy)

-- | Job identifier for the label detection operation for which you want
--   results returned. You get the job identifer from an initial call to
--   <tt>StartlabelDetection</tt>.
getLabelDetection_jobId :: Lens' GetLabelDetection Text

-- | <i>See:</i> <a>newGetLabelDetectionResponse</a> smart constructor.
data GetLabelDetectionResponse
GetLabelDetectionResponse' :: Maybe Text -> Maybe VideoMetadata -> Maybe Text -> Maybe [LabelDetection] -> Maybe VideoJobStatus -> Maybe Text -> Int -> GetLabelDetectionResponse

-- | If the response is truncated, Amazon Rekognition Video returns this
--   token that you can use in the subsequent request to retrieve the next
--   set of labels.
[$sel:nextToken:GetLabelDetectionResponse'] :: GetLabelDetectionResponse -> Maybe Text

-- | Information about a video that Amazon Rekognition Video analyzed.
--   <tt>Videometadata</tt> is returned in every page of paginated
--   responses from a Amazon Rekognition video operation.
[$sel:videoMetadata:GetLabelDetectionResponse'] :: GetLabelDetectionResponse -> Maybe VideoMetadata

-- | If the job fails, <tt>StatusMessage</tt> provides a descriptive error
--   message.
[$sel:statusMessage:GetLabelDetectionResponse'] :: GetLabelDetectionResponse -> Maybe Text

-- | An array of labels detected in the video. Each element contains the
--   detected label and the time, in milliseconds from the start of the
--   video, that the label was detected.
[$sel:labels:GetLabelDetectionResponse'] :: GetLabelDetectionResponse -> Maybe [LabelDetection]

-- | The current status of the label detection job.
[$sel:jobStatus:GetLabelDetectionResponse'] :: GetLabelDetectionResponse -> Maybe VideoJobStatus

-- | Version number of the label detection model that was used to detect
--   labels.
[$sel:labelModelVersion:GetLabelDetectionResponse'] :: GetLabelDetectionResponse -> Maybe Text

-- | The response's http status code.
[$sel:httpStatus:GetLabelDetectionResponse'] :: GetLabelDetectionResponse -> Int

-- | Create a value of <a>GetLabelDetectionResponse</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:nextToken:GetLabelDetection'</a>,
--   <a>getLabelDetectionResponse_nextToken</a> - If the response is
--   truncated, Amazon Rekognition Video returns this token that you can
--   use in the subsequent request to retrieve the next set of labels.
--   
--   <a>$sel:videoMetadata:GetLabelDetectionResponse'</a>,
--   <a>getLabelDetectionResponse_videoMetadata</a> - Information about a
--   video that Amazon Rekognition Video analyzed. <tt>Videometadata</tt>
--   is returned in every page of paginated responses from a Amazon
--   Rekognition video operation.
--   
--   <a>$sel:statusMessage:GetLabelDetectionResponse'</a>,
--   <a>getLabelDetectionResponse_statusMessage</a> - If the job fails,
--   <tt>StatusMessage</tt> provides a descriptive error message.
--   
--   <a>$sel:labels:GetLabelDetectionResponse'</a>,
--   <a>getLabelDetectionResponse_labels</a> - An array of labels detected
--   in the video. Each element contains the detected label and the time,
--   in milliseconds from the start of the video, that the label was
--   detected.
--   
--   <a>$sel:jobStatus:GetLabelDetectionResponse'</a>,
--   <a>getLabelDetectionResponse_jobStatus</a> - The current status of the
--   label detection job.
--   
--   <a>$sel:labelModelVersion:GetLabelDetectionResponse'</a>,
--   <a>getLabelDetectionResponse_labelModelVersion</a> - Version number of
--   the label detection model that was used to detect labels.
--   
--   <a>$sel:httpStatus:GetLabelDetectionResponse'</a>,
--   <a>getLabelDetectionResponse_httpStatus</a> - The response's http
--   status code.
newGetLabelDetectionResponse :: Int -> GetLabelDetectionResponse

-- | If the response is truncated, Amazon Rekognition Video returns this
--   token that you can use in the subsequent request to retrieve the next
--   set of labels.
getLabelDetectionResponse_nextToken :: Lens' GetLabelDetectionResponse (Maybe Text)

-- | Information about a video that Amazon Rekognition Video analyzed.
--   <tt>Videometadata</tt> is returned in every page of paginated
--   responses from a Amazon Rekognition video operation.
getLabelDetectionResponse_videoMetadata :: Lens' GetLabelDetectionResponse (Maybe VideoMetadata)

-- | If the job fails, <tt>StatusMessage</tt> provides a descriptive error
--   message.
getLabelDetectionResponse_statusMessage :: Lens' GetLabelDetectionResponse (Maybe Text)

-- | An array of labels detected in the video. Each element contains the
--   detected label and the time, in milliseconds from the start of the
--   video, that the label was detected.
getLabelDetectionResponse_labels :: Lens' GetLabelDetectionResponse (Maybe [LabelDetection])

-- | The current status of the label detection job.
getLabelDetectionResponse_jobStatus :: Lens' GetLabelDetectionResponse (Maybe VideoJobStatus)

-- | Version number of the label detection model that was used to detect
--   labels.
getLabelDetectionResponse_labelModelVersion :: Lens' GetLabelDetectionResponse (Maybe Text)

-- | The response's http status code.
getLabelDetectionResponse_httpStatus :: Lens' GetLabelDetectionResponse Int
instance GHC.Generics.Generic Network.AWS.Rekognition.GetLabelDetection.GetLabelDetection
instance GHC.Show.Show Network.AWS.Rekognition.GetLabelDetection.GetLabelDetection
instance GHC.Read.Read Network.AWS.Rekognition.GetLabelDetection.GetLabelDetection
instance GHC.Classes.Eq Network.AWS.Rekognition.GetLabelDetection.GetLabelDetection
instance GHC.Generics.Generic Network.AWS.Rekognition.GetLabelDetection.GetLabelDetectionResponse
instance GHC.Show.Show Network.AWS.Rekognition.GetLabelDetection.GetLabelDetectionResponse
instance GHC.Read.Read Network.AWS.Rekognition.GetLabelDetection.GetLabelDetectionResponse
instance GHC.Classes.Eq Network.AWS.Rekognition.GetLabelDetection.GetLabelDetectionResponse
instance Network.AWS.Types.AWSRequest Network.AWS.Rekognition.GetLabelDetection.GetLabelDetection
instance Control.DeepSeq.NFData Network.AWS.Rekognition.GetLabelDetection.GetLabelDetectionResponse
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.GetLabelDetection.GetLabelDetection
instance Control.DeepSeq.NFData Network.AWS.Rekognition.GetLabelDetection.GetLabelDetection
instance Network.AWS.Data.Headers.ToHeaders Network.AWS.Rekognition.GetLabelDetection.GetLabelDetection
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.GetLabelDetection.GetLabelDetection
instance Network.AWS.Data.Path.ToPath Network.AWS.Rekognition.GetLabelDetection.GetLabelDetection
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.GetLabelDetection.GetLabelDetection


-- | Gets the face search results for Amazon Rekognition Video face search
--   started by StartFaceSearch. The search returns faces in a collection
--   that match the faces of persons detected in a video. It also includes
--   the time(s) that faces are matched in the video.
--   
--   Face search in a video is an asynchronous operation. You start face
--   search by calling to StartFaceSearch which returns a job identifier
--   (<tt>JobId</tt>). When the search operation finishes, Amazon
--   Rekognition Video publishes a completion status to the Amazon Simple
--   Notification Service topic registered in the initial call to
--   <tt>StartFaceSearch</tt>. To get the search results, first check that
--   the status value published to the Amazon SNS topic is
--   <tt>SUCCEEDED</tt>. If so, call <tt>GetFaceSearch</tt> and pass the
--   job identifier (<tt>JobId</tt>) from the initial call to
--   <tt>StartFaceSearch</tt>.
--   
--   For more information, see Searching Faces in a Collection in the
--   Amazon Rekognition Developer Guide.
--   
--   The search results are retured in an array, <tt>Persons</tt>, of
--   PersonMatch objects. Each<tt>PersonMatch</tt> element contains details
--   about the matching faces in the input collection, person information
--   (facial attributes, bounding boxes, and person identifer) for the
--   matched person, and the time the person was matched in the video.
--   
--   <tt>GetFaceSearch</tt> only returns the default facial attributes
--   (<tt>BoundingBox</tt>, <tt>Confidence</tt>, <tt>Landmarks</tt>,
--   <tt>Pose</tt>, and <tt>Quality</tt>). The other facial attributes
--   listed in the <tt>Face</tt> object of the following response syntax
--   are not returned. For more information, see FaceDetail in the Amazon
--   Rekognition Developer Guide.
--   
--   By default, the <tt>Persons</tt> array is sorted by the time, in
--   milliseconds from the start of the video, persons are matched. You can
--   also sort by persons by specifying <tt>INDEX</tt> for the
--   <tt>SORTBY</tt> input parameter.
module Network.AWS.Rekognition.GetFaceSearch

-- | <i>See:</i> <a>newGetFaceSearch</a> smart constructor.
data GetFaceSearch
GetFaceSearch' :: Maybe Text -> Maybe Natural -> Maybe FaceSearchSortBy -> Text -> GetFaceSearch

-- | If the previous response was incomplete (because there is more search
--   results to retrieve), Amazon Rekognition Video returns a pagination
--   token in the response. You can use this pagination token to retrieve
--   the next set of search results.
[$sel:nextToken:GetFaceSearch'] :: GetFaceSearch -> Maybe Text

-- | Maximum number of results to return per paginated call. The largest
--   value you can specify is 1000. If you specify a value greater than
--   1000, a maximum of 1000 results is returned. The default value is
--   1000.
[$sel:maxResults:GetFaceSearch'] :: GetFaceSearch -> Maybe Natural

-- | Sort to use for grouping faces in the response. Use <tt>TIMESTAMP</tt>
--   to group faces by the time that they are recognized. Use
--   <tt>INDEX</tt> to sort by recognized faces.
[$sel:sortBy:GetFaceSearch'] :: GetFaceSearch -> Maybe FaceSearchSortBy

-- | The job identifer for the search request. You get the job identifier
--   from an initial call to <tt>StartFaceSearch</tt>.
[$sel:jobId:GetFaceSearch'] :: GetFaceSearch -> Text

-- | Create a value of <a>GetFaceSearch</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:nextToken:GetFaceSearch'</a>, <a>getFaceSearch_nextToken</a> -
--   If the previous response was incomplete (because there is more search
--   results to retrieve), Amazon Rekognition Video returns a pagination
--   token in the response. You can use this pagination token to retrieve
--   the next set of search results.
--   
--   <a>$sel:maxResults:GetFaceSearch'</a>, <a>getFaceSearch_maxResults</a>
--   - Maximum number of results to return per paginated call. The largest
--   value you can specify is 1000. If you specify a value greater than
--   1000, a maximum of 1000 results is returned. The default value is
--   1000.
--   
--   <a>$sel:sortBy:GetFaceSearch'</a>, <a>getFaceSearch_sortBy</a> - Sort
--   to use for grouping faces in the response. Use <tt>TIMESTAMP</tt> to
--   group faces by the time that they are recognized. Use <tt>INDEX</tt>
--   to sort by recognized faces.
--   
--   <a>$sel:jobId:GetFaceSearch'</a>, <a>getFaceSearch_jobId</a> - The job
--   identifer for the search request. You get the job identifier from an
--   initial call to <tt>StartFaceSearch</tt>.
newGetFaceSearch :: Text -> GetFaceSearch

-- | If the previous response was incomplete (because there is more search
--   results to retrieve), Amazon Rekognition Video returns a pagination
--   token in the response. You can use this pagination token to retrieve
--   the next set of search results.
getFaceSearch_nextToken :: Lens' GetFaceSearch (Maybe Text)

-- | Maximum number of results to return per paginated call. The largest
--   value you can specify is 1000. If you specify a value greater than
--   1000, a maximum of 1000 results is returned. The default value is
--   1000.
getFaceSearch_maxResults :: Lens' GetFaceSearch (Maybe Natural)

-- | Sort to use for grouping faces in the response. Use <tt>TIMESTAMP</tt>
--   to group faces by the time that they are recognized. Use
--   <tt>INDEX</tt> to sort by recognized faces.
getFaceSearch_sortBy :: Lens' GetFaceSearch (Maybe FaceSearchSortBy)

-- | The job identifer for the search request. You get the job identifier
--   from an initial call to <tt>StartFaceSearch</tt>.
getFaceSearch_jobId :: Lens' GetFaceSearch Text

-- | <i>See:</i> <a>newGetFaceSearchResponse</a> smart constructor.
data GetFaceSearchResponse
GetFaceSearchResponse' :: Maybe Text -> Maybe VideoMetadata -> Maybe Text -> Maybe VideoJobStatus -> Maybe [PersonMatch] -> Int -> GetFaceSearchResponse

-- | If the response is truncated, Amazon Rekognition Video returns this
--   token that you can use in the subsequent request to retrieve the next
--   set of search results.
[$sel:nextToken:GetFaceSearchResponse'] :: GetFaceSearchResponse -> Maybe Text

-- | Information about a video that Amazon Rekognition analyzed.
--   <tt>Videometadata</tt> is returned in every page of paginated
--   responses from a Amazon Rekognition Video operation.
[$sel:videoMetadata:GetFaceSearchResponse'] :: GetFaceSearchResponse -> Maybe VideoMetadata

-- | If the job fails, <tt>StatusMessage</tt> provides a descriptive error
--   message.
[$sel:statusMessage:GetFaceSearchResponse'] :: GetFaceSearchResponse -> Maybe Text

-- | The current status of the face search job.
[$sel:jobStatus:GetFaceSearchResponse'] :: GetFaceSearchResponse -> Maybe VideoJobStatus

-- | An array of persons, PersonMatch, in the video whose face(s) match the
--   face(s) in an Amazon Rekognition collection. It also includes time
--   information for when persons are matched in the video. You specify the
--   input collection in an initial call to <tt>StartFaceSearch</tt>. Each
--   <tt>Persons</tt> element includes a time the person was matched, face
--   match details (<tt>FaceMatches</tt>) for matching faces in the
--   collection, and person information (<tt>Person</tt>) for the matched
--   person.
[$sel:persons:GetFaceSearchResponse'] :: GetFaceSearchResponse -> Maybe [PersonMatch]

-- | The response's http status code.
[$sel:httpStatus:GetFaceSearchResponse'] :: GetFaceSearchResponse -> Int

-- | Create a value of <a>GetFaceSearchResponse</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:nextToken:GetFaceSearch'</a>,
--   <a>getFaceSearchResponse_nextToken</a> - If the response is truncated,
--   Amazon Rekognition Video returns this token that you can use in the
--   subsequent request to retrieve the next set of search results.
--   
--   <a>$sel:videoMetadata:GetFaceSearchResponse'</a>,
--   <a>getFaceSearchResponse_videoMetadata</a> - Information about a video
--   that Amazon Rekognition analyzed. <tt>Videometadata</tt> is returned
--   in every page of paginated responses from a Amazon Rekognition Video
--   operation.
--   
--   <a>$sel:statusMessage:GetFaceSearchResponse'</a>,
--   <a>getFaceSearchResponse_statusMessage</a> - If the job fails,
--   <tt>StatusMessage</tt> provides a descriptive error message.
--   
--   <a>$sel:jobStatus:GetFaceSearchResponse'</a>,
--   <a>getFaceSearchResponse_jobStatus</a> - The current status of the
--   face search job.
--   
--   <a>$sel:persons:GetFaceSearchResponse'</a>,
--   <a>getFaceSearchResponse_persons</a> - An array of persons,
--   PersonMatch, in the video whose face(s) match the face(s) in an Amazon
--   Rekognition collection. It also includes time information for when
--   persons are matched in the video. You specify the input collection in
--   an initial call to <tt>StartFaceSearch</tt>. Each <tt>Persons</tt>
--   element includes a time the person was matched, face match details
--   (<tt>FaceMatches</tt>) for matching faces in the collection, and
--   person information (<tt>Person</tt>) for the matched person.
--   
--   <a>$sel:httpStatus:GetFaceSearchResponse'</a>,
--   <a>getFaceSearchResponse_httpStatus</a> - The response's http status
--   code.
newGetFaceSearchResponse :: Int -> GetFaceSearchResponse

-- | If the response is truncated, Amazon Rekognition Video returns this
--   token that you can use in the subsequent request to retrieve the next
--   set of search results.
getFaceSearchResponse_nextToken :: Lens' GetFaceSearchResponse (Maybe Text)

-- | Information about a video that Amazon Rekognition analyzed.
--   <tt>Videometadata</tt> is returned in every page of paginated
--   responses from a Amazon Rekognition Video operation.
getFaceSearchResponse_videoMetadata :: Lens' GetFaceSearchResponse (Maybe VideoMetadata)

-- | If the job fails, <tt>StatusMessage</tt> provides a descriptive error
--   message.
getFaceSearchResponse_statusMessage :: Lens' GetFaceSearchResponse (Maybe Text)

-- | The current status of the face search job.
getFaceSearchResponse_jobStatus :: Lens' GetFaceSearchResponse (Maybe VideoJobStatus)

-- | An array of persons, PersonMatch, in the video whose face(s) match the
--   face(s) in an Amazon Rekognition collection. It also includes time
--   information for when persons are matched in the video. You specify the
--   input collection in an initial call to <tt>StartFaceSearch</tt>. Each
--   <tt>Persons</tt> element includes a time the person was matched, face
--   match details (<tt>FaceMatches</tt>) for matching faces in the
--   collection, and person information (<tt>Person</tt>) for the matched
--   person.
getFaceSearchResponse_persons :: Lens' GetFaceSearchResponse (Maybe [PersonMatch])

-- | The response's http status code.
getFaceSearchResponse_httpStatus :: Lens' GetFaceSearchResponse Int
instance GHC.Generics.Generic Network.AWS.Rekognition.GetFaceSearch.GetFaceSearch
instance GHC.Show.Show Network.AWS.Rekognition.GetFaceSearch.GetFaceSearch
instance GHC.Read.Read Network.AWS.Rekognition.GetFaceSearch.GetFaceSearch
instance GHC.Classes.Eq Network.AWS.Rekognition.GetFaceSearch.GetFaceSearch
instance GHC.Generics.Generic Network.AWS.Rekognition.GetFaceSearch.GetFaceSearchResponse
instance GHC.Show.Show Network.AWS.Rekognition.GetFaceSearch.GetFaceSearchResponse
instance GHC.Read.Read Network.AWS.Rekognition.GetFaceSearch.GetFaceSearchResponse
instance GHC.Classes.Eq Network.AWS.Rekognition.GetFaceSearch.GetFaceSearchResponse
instance Network.AWS.Types.AWSRequest Network.AWS.Rekognition.GetFaceSearch.GetFaceSearch
instance Control.DeepSeq.NFData Network.AWS.Rekognition.GetFaceSearch.GetFaceSearchResponse
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.GetFaceSearch.GetFaceSearch
instance Control.DeepSeq.NFData Network.AWS.Rekognition.GetFaceSearch.GetFaceSearch
instance Network.AWS.Data.Headers.ToHeaders Network.AWS.Rekognition.GetFaceSearch.GetFaceSearch
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.GetFaceSearch.GetFaceSearch
instance Network.AWS.Data.Path.ToPath Network.AWS.Rekognition.GetFaceSearch.GetFaceSearch
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.GetFaceSearch.GetFaceSearch


-- | Gets face detection results for a Amazon Rekognition Video analysis
--   started by StartFaceDetection.
--   
--   Face detection with Amazon Rekognition Video is an asynchronous
--   operation. You start face detection by calling StartFaceDetection
--   which returns a job identifier (<tt>JobId</tt>). When the face
--   detection operation finishes, Amazon Rekognition Video publishes a
--   completion status to the Amazon Simple Notification Service topic
--   registered in the initial call to <tt>StartFaceDetection</tt>. To get
--   the results of the face detection operation, first check that the
--   status value published to the Amazon SNS topic is <tt>SUCCEEDED</tt>.
--   If so, call GetFaceDetection and pass the job identifier
--   (<tt>JobId</tt>) from the initial call to <tt>StartFaceDetection</tt>.
--   
--   <tt>GetFaceDetection</tt> returns an array of detected faces
--   (<tt>Faces</tt>) sorted by the time the faces were detected.
--   
--   Use MaxResults parameter to limit the number of labels returned. If
--   there are more results than specified in <tt>MaxResults</tt>, the
--   value of <tt>NextToken</tt> in the operation response contains a
--   pagination token for getting the next set of results. To get the next
--   page of results, call <tt>GetFaceDetection</tt> and populate the
--   <tt>NextToken</tt> request parameter with the token value returned
--   from the previous call to <tt>GetFaceDetection</tt>.
module Network.AWS.Rekognition.GetFaceDetection

-- | <i>See:</i> <a>newGetFaceDetection</a> smart constructor.
data GetFaceDetection
GetFaceDetection' :: Maybe Text -> Maybe Natural -> Text -> GetFaceDetection

-- | If the previous response was incomplete (because there are more faces
--   to retrieve), Amazon Rekognition Video returns a pagination token in
--   the response. You can use this pagination token to retrieve the next
--   set of faces.
[$sel:nextToken:GetFaceDetection'] :: GetFaceDetection -> Maybe Text

-- | Maximum number of results to return per paginated call. The largest
--   value you can specify is 1000. If you specify a value greater than
--   1000, a maximum of 1000 results is returned. The default value is
--   1000.
[$sel:maxResults:GetFaceDetection'] :: GetFaceDetection -> Maybe Natural

-- | Unique identifier for the face detection job. The <tt>JobId</tt> is
--   returned from <tt>StartFaceDetection</tt>.
[$sel:jobId:GetFaceDetection'] :: GetFaceDetection -> Text

-- | Create a value of <a>GetFaceDetection</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:nextToken:GetFaceDetection'</a>,
--   <a>getFaceDetection_nextToken</a> - If the previous response was
--   incomplete (because there are more faces to retrieve), Amazon
--   Rekognition Video returns a pagination token in the response. You can
--   use this pagination token to retrieve the next set of faces.
--   
--   <a>$sel:maxResults:GetFaceDetection'</a>,
--   <a>getFaceDetection_maxResults</a> - Maximum number of results to
--   return per paginated call. The largest value you can specify is 1000.
--   If you specify a value greater than 1000, a maximum of 1000 results is
--   returned. The default value is 1000.
--   
--   <a>$sel:jobId:GetFaceDetection'</a>, <a>getFaceDetection_jobId</a> -
--   Unique identifier for the face detection job. The <tt>JobId</tt> is
--   returned from <tt>StartFaceDetection</tt>.
newGetFaceDetection :: Text -> GetFaceDetection

-- | If the previous response was incomplete (because there are more faces
--   to retrieve), Amazon Rekognition Video returns a pagination token in
--   the response. You can use this pagination token to retrieve the next
--   set of faces.
getFaceDetection_nextToken :: Lens' GetFaceDetection (Maybe Text)

-- | Maximum number of results to return per paginated call. The largest
--   value you can specify is 1000. If you specify a value greater than
--   1000, a maximum of 1000 results is returned. The default value is
--   1000.
getFaceDetection_maxResults :: Lens' GetFaceDetection (Maybe Natural)

-- | Unique identifier for the face detection job. The <tt>JobId</tt> is
--   returned from <tt>StartFaceDetection</tt>.
getFaceDetection_jobId :: Lens' GetFaceDetection Text

-- | <i>See:</i> <a>newGetFaceDetectionResponse</a> smart constructor.
data GetFaceDetectionResponse
GetFaceDetectionResponse' :: Maybe Text -> Maybe VideoMetadata -> Maybe Text -> Maybe [FaceDetection] -> Maybe VideoJobStatus -> Int -> GetFaceDetectionResponse

-- | If the response is truncated, Amazon Rekognition returns this token
--   that you can use in the subsequent request to retrieve the next set of
--   faces.
[$sel:nextToken:GetFaceDetectionResponse'] :: GetFaceDetectionResponse -> Maybe Text

-- | Information about a video that Amazon Rekognition Video analyzed.
--   <tt>Videometadata</tt> is returned in every page of paginated
--   responses from a Amazon Rekognition video operation.
[$sel:videoMetadata:GetFaceDetectionResponse'] :: GetFaceDetectionResponse -> Maybe VideoMetadata

-- | If the job fails, <tt>StatusMessage</tt> provides a descriptive error
--   message.
[$sel:statusMessage:GetFaceDetectionResponse'] :: GetFaceDetectionResponse -> Maybe Text

-- | An array of faces detected in the video. Each element contains a
--   detected face's details and the time, in milliseconds from the start
--   of the video, the face was detected.
[$sel:faces:GetFaceDetectionResponse'] :: GetFaceDetectionResponse -> Maybe [FaceDetection]

-- | The current status of the face detection job.
[$sel:jobStatus:GetFaceDetectionResponse'] :: GetFaceDetectionResponse -> Maybe VideoJobStatus

-- | The response's http status code.
[$sel:httpStatus:GetFaceDetectionResponse'] :: GetFaceDetectionResponse -> Int

-- | Create a value of <a>GetFaceDetectionResponse</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:nextToken:GetFaceDetection'</a>,
--   <a>getFaceDetectionResponse_nextToken</a> - If the response is
--   truncated, Amazon Rekognition returns this token that you can use in
--   the subsequent request to retrieve the next set of faces.
--   
--   <a>$sel:videoMetadata:GetFaceDetectionResponse'</a>,
--   <a>getFaceDetectionResponse_videoMetadata</a> - Information about a
--   video that Amazon Rekognition Video analyzed. <tt>Videometadata</tt>
--   is returned in every page of paginated responses from a Amazon
--   Rekognition video operation.
--   
--   <a>$sel:statusMessage:GetFaceDetectionResponse'</a>,
--   <a>getFaceDetectionResponse_statusMessage</a> - If the job fails,
--   <tt>StatusMessage</tt> provides a descriptive error message.
--   
--   <a>$sel:faces:GetFaceDetectionResponse'</a>,
--   <a>getFaceDetectionResponse_faces</a> - An array of faces detected in
--   the video. Each element contains a detected face's details and the
--   time, in milliseconds from the start of the video, the face was
--   detected.
--   
--   <a>$sel:jobStatus:GetFaceDetectionResponse'</a>,
--   <a>getFaceDetectionResponse_jobStatus</a> - The current status of the
--   face detection job.
--   
--   <a>$sel:httpStatus:GetFaceDetectionResponse'</a>,
--   <a>getFaceDetectionResponse_httpStatus</a> - The response's http
--   status code.
newGetFaceDetectionResponse :: Int -> GetFaceDetectionResponse

-- | If the response is truncated, Amazon Rekognition returns this token
--   that you can use in the subsequent request to retrieve the next set of
--   faces.
getFaceDetectionResponse_nextToken :: Lens' GetFaceDetectionResponse (Maybe Text)

-- | Information about a video that Amazon Rekognition Video analyzed.
--   <tt>Videometadata</tt> is returned in every page of paginated
--   responses from a Amazon Rekognition video operation.
getFaceDetectionResponse_videoMetadata :: Lens' GetFaceDetectionResponse (Maybe VideoMetadata)

-- | If the job fails, <tt>StatusMessage</tt> provides a descriptive error
--   message.
getFaceDetectionResponse_statusMessage :: Lens' GetFaceDetectionResponse (Maybe Text)

-- | An array of faces detected in the video. Each element contains a
--   detected face's details and the time, in milliseconds from the start
--   of the video, the face was detected.
getFaceDetectionResponse_faces :: Lens' GetFaceDetectionResponse (Maybe [FaceDetection])

-- | The current status of the face detection job.
getFaceDetectionResponse_jobStatus :: Lens' GetFaceDetectionResponse (Maybe VideoJobStatus)

-- | The response's http status code.
getFaceDetectionResponse_httpStatus :: Lens' GetFaceDetectionResponse Int
instance GHC.Generics.Generic Network.AWS.Rekognition.GetFaceDetection.GetFaceDetection
instance GHC.Show.Show Network.AWS.Rekognition.GetFaceDetection.GetFaceDetection
instance GHC.Read.Read Network.AWS.Rekognition.GetFaceDetection.GetFaceDetection
instance GHC.Classes.Eq Network.AWS.Rekognition.GetFaceDetection.GetFaceDetection
instance GHC.Generics.Generic Network.AWS.Rekognition.GetFaceDetection.GetFaceDetectionResponse
instance GHC.Show.Show Network.AWS.Rekognition.GetFaceDetection.GetFaceDetectionResponse
instance GHC.Read.Read Network.AWS.Rekognition.GetFaceDetection.GetFaceDetectionResponse
instance GHC.Classes.Eq Network.AWS.Rekognition.GetFaceDetection.GetFaceDetectionResponse
instance Network.AWS.Types.AWSRequest Network.AWS.Rekognition.GetFaceDetection.GetFaceDetection
instance Control.DeepSeq.NFData Network.AWS.Rekognition.GetFaceDetection.GetFaceDetectionResponse
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.GetFaceDetection.GetFaceDetection
instance Control.DeepSeq.NFData Network.AWS.Rekognition.GetFaceDetection.GetFaceDetection
instance Network.AWS.Data.Headers.ToHeaders Network.AWS.Rekognition.GetFaceDetection.GetFaceDetection
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.GetFaceDetection.GetFaceDetection
instance Network.AWS.Data.Path.ToPath Network.AWS.Rekognition.GetFaceDetection.GetFaceDetection
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.GetFaceDetection.GetFaceDetection


-- | Gets the inappropriate, unwanted, or offensive content analysis
--   results for a Amazon Rekognition Video analysis started by
--   StartContentModeration. For a list of moderation labels in Amazon
--   Rekognition, see <a>Using the image and video moderation APIs</a>.
--   
--   Amazon Rekognition Video inappropriate or offensive content detection
--   in a stored video is an asynchronous operation. You start analysis by
--   calling StartContentModeration which returns a job identifier
--   (<tt>JobId</tt>). When analysis finishes, Amazon Rekognition Video
--   publishes a completion status to the Amazon Simple Notification
--   Service topic registered in the initial call to
--   <tt>StartContentModeration</tt>. To get the results of the content
--   analysis, first check that the status value published to the Amazon
--   SNS topic is <tt>SUCCEEDED</tt>. If so, call
--   <tt>GetContentModeration</tt> and pass the job identifier
--   (<tt>JobId</tt>) from the initial call to
--   <tt>StartContentModeration</tt>.
--   
--   For more information, see Working with Stored Videos in the Amazon
--   Rekognition Devlopers Guide.
--   
--   <tt>GetContentModeration</tt> returns detected inappropriate,
--   unwanted, or offensive content moderation labels, and the time they
--   are detected, in an array, <tt>ModerationLabels</tt>, of
--   ContentModerationDetection objects.
--   
--   By default, the moderated labels are returned sorted by time, in
--   milliseconds from the start of the video. You can also sort them by
--   moderated label by specifying <tt>NAME</tt> for the <tt>SortBy</tt>
--   input parameter.
--   
--   Since video analysis can return a large number of results, use the
--   <tt>MaxResults</tt> parameter to limit the number of labels returned
--   in a single call to <tt>GetContentModeration</tt>. If there are more
--   results than specified in <tt>MaxResults</tt>, the value of
--   <tt>NextToken</tt> in the operation response contains a pagination
--   token for getting the next set of results. To get the next page of
--   results, call <tt>GetContentModeration</tt> and populate the
--   <tt>NextToken</tt> request parameter with the value of
--   <tt>NextToken</tt> returned from the previous call to
--   <tt>GetContentModeration</tt>.
--   
--   For more information, see Content moderation in the Amazon Rekognition
--   Developer Guide.
module Network.AWS.Rekognition.GetContentModeration

-- | <i>See:</i> <a>newGetContentModeration</a> smart constructor.
data GetContentModeration
GetContentModeration' :: Maybe Text -> Maybe Natural -> Maybe ContentModerationSortBy -> Text -> GetContentModeration

-- | If the previous response was incomplete (because there is more data to
--   retrieve), Amazon Rekognition returns a pagination token in the
--   response. You can use this pagination token to retrieve the next set
--   of content moderation labels.
[$sel:nextToken:GetContentModeration'] :: GetContentModeration -> Maybe Text

-- | Maximum number of results to return per paginated call. The largest
--   value you can specify is 1000. If you specify a value greater than
--   1000, a maximum of 1000 results is returned. The default value is
--   1000.
[$sel:maxResults:GetContentModeration'] :: GetContentModeration -> Maybe Natural

-- | Sort to use for elements in the <tt>ModerationLabelDetections</tt>
--   array. Use <tt>TIMESTAMP</tt> to sort array elements by the time
--   labels are detected. Use <tt>NAME</tt> to alphabetically group
--   elements for a label together. Within each label group, the array
--   element are sorted by detection confidence. The default sort is by
--   <tt>TIMESTAMP</tt>.
[$sel:sortBy:GetContentModeration'] :: GetContentModeration -> Maybe ContentModerationSortBy

-- | The identifier for the inappropriate, unwanted, or offensive content
--   moderation job. Use <tt>JobId</tt> to identify the job in a subsequent
--   call to <tt>GetContentModeration</tt>.
[$sel:jobId:GetContentModeration'] :: GetContentModeration -> Text

-- | Create a value of <a>GetContentModeration</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:nextToken:GetContentModeration'</a>,
--   <a>getContentModeration_nextToken</a> - If the previous response was
--   incomplete (because there is more data to retrieve), Amazon
--   Rekognition returns a pagination token in the response. You can use
--   this pagination token to retrieve the next set of content moderation
--   labels.
--   
--   <a>$sel:maxResults:GetContentModeration'</a>,
--   <a>getContentModeration_maxResults</a> - Maximum number of results to
--   return per paginated call. The largest value you can specify is 1000.
--   If you specify a value greater than 1000, a maximum of 1000 results is
--   returned. The default value is 1000.
--   
--   <a>$sel:sortBy:GetContentModeration'</a>,
--   <a>getContentModeration_sortBy</a> - Sort to use for elements in the
--   <tt>ModerationLabelDetections</tt> array. Use <tt>TIMESTAMP</tt> to
--   sort array elements by the time labels are detected. Use <tt>NAME</tt>
--   to alphabetically group elements for a label together. Within each
--   label group, the array element are sorted by detection confidence. The
--   default sort is by <tt>TIMESTAMP</tt>.
--   
--   <a>$sel:jobId:GetContentModeration'</a>,
--   <a>getContentModeration_jobId</a> - The identifier for the
--   inappropriate, unwanted, or offensive content moderation job. Use
--   <tt>JobId</tt> to identify the job in a subsequent call to
--   <tt>GetContentModeration</tt>.
newGetContentModeration :: Text -> GetContentModeration

-- | If the previous response was incomplete (because there is more data to
--   retrieve), Amazon Rekognition returns a pagination token in the
--   response. You can use this pagination token to retrieve the next set
--   of content moderation labels.
getContentModeration_nextToken :: Lens' GetContentModeration (Maybe Text)

-- | Maximum number of results to return per paginated call. The largest
--   value you can specify is 1000. If you specify a value greater than
--   1000, a maximum of 1000 results is returned. The default value is
--   1000.
getContentModeration_maxResults :: Lens' GetContentModeration (Maybe Natural)

-- | Sort to use for elements in the <tt>ModerationLabelDetections</tt>
--   array. Use <tt>TIMESTAMP</tt> to sort array elements by the time
--   labels are detected. Use <tt>NAME</tt> to alphabetically group
--   elements for a label together. Within each label group, the array
--   element are sorted by detection confidence. The default sort is by
--   <tt>TIMESTAMP</tt>.
getContentModeration_sortBy :: Lens' GetContentModeration (Maybe ContentModerationSortBy)

-- | The identifier for the inappropriate, unwanted, or offensive content
--   moderation job. Use <tt>JobId</tt> to identify the job in a subsequent
--   call to <tt>GetContentModeration</tt>.
getContentModeration_jobId :: Lens' GetContentModeration Text

-- | <i>See:</i> <a>newGetContentModerationResponse</a> smart constructor.
data GetContentModerationResponse
GetContentModerationResponse' :: Maybe Text -> Maybe VideoMetadata -> Maybe Text -> Maybe VideoJobStatus -> Maybe Text -> Maybe [ContentModerationDetection] -> Int -> GetContentModerationResponse

-- | If the response is truncated, Amazon Rekognition Video returns this
--   token that you can use in the subsequent request to retrieve the next
--   set of content moderation labels.
[$sel:nextToken:GetContentModerationResponse'] :: GetContentModerationResponse -> Maybe Text

-- | Information about a video that Amazon Rekognition analyzed.
--   <tt>Videometadata</tt> is returned in every page of paginated
--   responses from <tt>GetContentModeration</tt>.
[$sel:videoMetadata:GetContentModerationResponse'] :: GetContentModerationResponse -> Maybe VideoMetadata

-- | If the job fails, <tt>StatusMessage</tt> provides a descriptive error
--   message.
[$sel:statusMessage:GetContentModerationResponse'] :: GetContentModerationResponse -> Maybe Text

-- | The current status of the content moderation analysis job.
[$sel:jobStatus:GetContentModerationResponse'] :: GetContentModerationResponse -> Maybe VideoJobStatus

-- | Version number of the moderation detection model that was used to
--   detect inappropriate, unwanted, or offensive content.
[$sel:moderationModelVersion:GetContentModerationResponse'] :: GetContentModerationResponse -> Maybe Text

-- | The detected inappropriate, unwanted, or offensive content moderation
--   labels and the time(s) they were detected.
[$sel:moderationLabels:GetContentModerationResponse'] :: GetContentModerationResponse -> Maybe [ContentModerationDetection]

-- | The response's http status code.
[$sel:httpStatus:GetContentModerationResponse'] :: GetContentModerationResponse -> Int

-- | Create a value of <a>GetContentModerationResponse</a> with all
--   optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:nextToken:GetContentModeration'</a>,
--   <a>getContentModerationResponse_nextToken</a> - If the response is
--   truncated, Amazon Rekognition Video returns this token that you can
--   use in the subsequent request to retrieve the next set of content
--   moderation labels.
--   
--   <a>$sel:videoMetadata:GetContentModerationResponse'</a>,
--   <a>getContentModerationResponse_videoMetadata</a> - Information about
--   a video that Amazon Rekognition analyzed. <tt>Videometadata</tt> is
--   returned in every page of paginated responses from
--   <tt>GetContentModeration</tt>.
--   
--   <a>$sel:statusMessage:GetContentModerationResponse'</a>,
--   <a>getContentModerationResponse_statusMessage</a> - If the job fails,
--   <tt>StatusMessage</tt> provides a descriptive error message.
--   
--   <a>$sel:jobStatus:GetContentModerationResponse'</a>,
--   <a>getContentModerationResponse_jobStatus</a> - The current status of
--   the content moderation analysis job.
--   
--   <a>$sel:moderationModelVersion:GetContentModerationResponse'</a>,
--   <a>getContentModerationResponse_moderationModelVersion</a> - Version
--   number of the moderation detection model that was used to detect
--   inappropriate, unwanted, or offensive content.
--   
--   <a>$sel:moderationLabels:GetContentModerationResponse'</a>,
--   <a>getContentModerationResponse_moderationLabels</a> - The detected
--   inappropriate, unwanted, or offensive content moderation labels and
--   the time(s) they were detected.
--   
--   <a>$sel:httpStatus:GetContentModerationResponse'</a>,
--   <a>getContentModerationResponse_httpStatus</a> - The response's http
--   status code.
newGetContentModerationResponse :: Int -> GetContentModerationResponse

-- | If the response is truncated, Amazon Rekognition Video returns this
--   token that you can use in the subsequent request to retrieve the next
--   set of content moderation labels.
getContentModerationResponse_nextToken :: Lens' GetContentModerationResponse (Maybe Text)

-- | Information about a video that Amazon Rekognition analyzed.
--   <tt>Videometadata</tt> is returned in every page of paginated
--   responses from <tt>GetContentModeration</tt>.
getContentModerationResponse_videoMetadata :: Lens' GetContentModerationResponse (Maybe VideoMetadata)

-- | If the job fails, <tt>StatusMessage</tt> provides a descriptive error
--   message.
getContentModerationResponse_statusMessage :: Lens' GetContentModerationResponse (Maybe Text)

-- | The current status of the content moderation analysis job.
getContentModerationResponse_jobStatus :: Lens' GetContentModerationResponse (Maybe VideoJobStatus)

-- | Version number of the moderation detection model that was used to
--   detect inappropriate, unwanted, or offensive content.
getContentModerationResponse_moderationModelVersion :: Lens' GetContentModerationResponse (Maybe Text)

-- | The detected inappropriate, unwanted, or offensive content moderation
--   labels and the time(s) they were detected.
getContentModerationResponse_moderationLabels :: Lens' GetContentModerationResponse (Maybe [ContentModerationDetection])

-- | The response's http status code.
getContentModerationResponse_httpStatus :: Lens' GetContentModerationResponse Int
instance GHC.Generics.Generic Network.AWS.Rekognition.GetContentModeration.GetContentModeration
instance GHC.Show.Show Network.AWS.Rekognition.GetContentModeration.GetContentModeration
instance GHC.Read.Read Network.AWS.Rekognition.GetContentModeration.GetContentModeration
instance GHC.Classes.Eq Network.AWS.Rekognition.GetContentModeration.GetContentModeration
instance GHC.Generics.Generic Network.AWS.Rekognition.GetContentModeration.GetContentModerationResponse
instance GHC.Show.Show Network.AWS.Rekognition.GetContentModeration.GetContentModerationResponse
instance GHC.Read.Read Network.AWS.Rekognition.GetContentModeration.GetContentModerationResponse
instance GHC.Classes.Eq Network.AWS.Rekognition.GetContentModeration.GetContentModerationResponse
instance Network.AWS.Types.AWSRequest Network.AWS.Rekognition.GetContentModeration.GetContentModeration
instance Control.DeepSeq.NFData Network.AWS.Rekognition.GetContentModeration.GetContentModerationResponse
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.GetContentModeration.GetContentModeration
instance Control.DeepSeq.NFData Network.AWS.Rekognition.GetContentModeration.GetContentModeration
instance Network.AWS.Data.Headers.ToHeaders Network.AWS.Rekognition.GetContentModeration.GetContentModeration
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.GetContentModeration.GetContentModeration
instance Network.AWS.Data.Path.ToPath Network.AWS.Rekognition.GetContentModeration.GetContentModeration
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.GetContentModeration.GetContentModeration


-- | Gets the celebrity recognition results for a Amazon Rekognition Video
--   analysis started by StartCelebrityRecognition.
--   
--   Celebrity recognition in a video is an asynchronous operation.
--   Analysis is started by a call to StartCelebrityRecognition which
--   returns a job identifier (<tt>JobId</tt>). When the celebrity
--   recognition operation finishes, Amazon Rekognition Video publishes a
--   completion status to the Amazon Simple Notification Service topic
--   registered in the initial call to <tt>StartCelebrityRecognition</tt>.
--   To get the results of the celebrity recognition analysis, first check
--   that the status value published to the Amazon SNS topic is
--   <tt>SUCCEEDED</tt>. If so, call <tt>GetCelebrityDetection</tt> and
--   pass the job identifier (<tt>JobId</tt>) from the initial call to
--   <tt>StartCelebrityDetection</tt>.
--   
--   For more information, see Working With Stored Videos in the Amazon
--   Rekognition Developer Guide.
--   
--   <tt>GetCelebrityRecognition</tt> returns detected celebrities and the
--   time(s) they are detected in an array (<tt>Celebrities</tt>) of
--   CelebrityRecognition objects. Each <tt>CelebrityRecognition</tt>
--   contains information about the celebrity in a CelebrityDetail object
--   and the time, <tt>Timestamp</tt>, the celebrity was detected.
--   
--   <tt>GetCelebrityRecognition</tt> only returns the default facial
--   attributes (<tt>BoundingBox</tt>, <tt>Confidence</tt>,
--   <tt>Landmarks</tt>, <tt>Pose</tt>, and <tt>Quality</tt>). The other
--   facial attributes listed in the <tt>Face</tt> object of the following
--   response syntax are not returned. For more information, see FaceDetail
--   in the Amazon Rekognition Developer Guide.
--   
--   By default, the <tt>Celebrities</tt> array is sorted by time
--   (milliseconds from the start of the video). You can also sort the
--   array by celebrity by specifying the value <tt>ID</tt> in the
--   <tt>SortBy</tt> input parameter.
--   
--   The <tt>CelebrityDetail</tt> object includes the celebrity identifer
--   and additional information urls. If you don't store the additional
--   information urls, you can get them later by calling GetCelebrityInfo
--   with the celebrity identifer.
--   
--   No information is returned for faces not recognized as celebrities.
--   
--   Use MaxResults parameter to limit the number of labels returned. If
--   there are more results than specified in <tt>MaxResults</tt>, the
--   value of <tt>NextToken</tt> in the operation response contains a
--   pagination token for getting the next set of results. To get the next
--   page of results, call <tt>GetCelebrityDetection</tt> and populate the
--   <tt>NextToken</tt> request parameter with the token value returned
--   from the previous call to <tt>GetCelebrityRecognition</tt>.
module Network.AWS.Rekognition.GetCelebrityRecognition

-- | <i>See:</i> <a>newGetCelebrityRecognition</a> smart constructor.
data GetCelebrityRecognition
GetCelebrityRecognition' :: Maybe Text -> Maybe Natural -> Maybe CelebrityRecognitionSortBy -> Text -> GetCelebrityRecognition

-- | If the previous response was incomplete (because there is more
--   recognized celebrities to retrieve), Amazon Rekognition Video returns
--   a pagination token in the response. You can use this pagination token
--   to retrieve the next set of celebrities.
[$sel:nextToken:GetCelebrityRecognition'] :: GetCelebrityRecognition -> Maybe Text

-- | Maximum number of results to return per paginated call. The largest
--   value you can specify is 1000. If you specify a value greater than
--   1000, a maximum of 1000 results is returned. The default value is
--   1000.
[$sel:maxResults:GetCelebrityRecognition'] :: GetCelebrityRecognition -> Maybe Natural

-- | Sort to use for celebrities returned in <tt>Celebrities</tt> field.
--   Specify <tt>ID</tt> to sort by the celebrity identifier, specify
--   <tt>TIMESTAMP</tt> to sort by the time the celebrity was recognized.
[$sel:sortBy:GetCelebrityRecognition'] :: GetCelebrityRecognition -> Maybe CelebrityRecognitionSortBy

-- | Job identifier for the required celebrity recognition analysis. You
--   can get the job identifer from a call to
--   <tt>StartCelebrityRecognition</tt>.
[$sel:jobId:GetCelebrityRecognition'] :: GetCelebrityRecognition -> Text

-- | Create a value of <a>GetCelebrityRecognition</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:nextToken:GetCelebrityRecognition'</a>,
--   <a>getCelebrityRecognition_nextToken</a> - If the previous response
--   was incomplete (because there is more recognized celebrities to
--   retrieve), Amazon Rekognition Video returns a pagination token in the
--   response. You can use this pagination token to retrieve the next set
--   of celebrities.
--   
--   <a>$sel:maxResults:GetCelebrityRecognition'</a>,
--   <a>getCelebrityRecognition_maxResults</a> - Maximum number of results
--   to return per paginated call. The largest value you can specify is
--   1000. If you specify a value greater than 1000, a maximum of 1000
--   results is returned. The default value is 1000.
--   
--   <a>$sel:sortBy:GetCelebrityRecognition'</a>,
--   <a>getCelebrityRecognition_sortBy</a> - Sort to use for celebrities
--   returned in <tt>Celebrities</tt> field. Specify <tt>ID</tt> to sort by
--   the celebrity identifier, specify <tt>TIMESTAMP</tt> to sort by the
--   time the celebrity was recognized.
--   
--   <a>$sel:jobId:GetCelebrityRecognition'</a>,
--   <a>getCelebrityRecognition_jobId</a> - Job identifier for the required
--   celebrity recognition analysis. You can get the job identifer from a
--   call to <tt>StartCelebrityRecognition</tt>.
newGetCelebrityRecognition :: Text -> GetCelebrityRecognition

-- | If the previous response was incomplete (because there is more
--   recognized celebrities to retrieve), Amazon Rekognition Video returns
--   a pagination token in the response. You can use this pagination token
--   to retrieve the next set of celebrities.
getCelebrityRecognition_nextToken :: Lens' GetCelebrityRecognition (Maybe Text)

-- | Maximum number of results to return per paginated call. The largest
--   value you can specify is 1000. If you specify a value greater than
--   1000, a maximum of 1000 results is returned. The default value is
--   1000.
getCelebrityRecognition_maxResults :: Lens' GetCelebrityRecognition (Maybe Natural)

-- | Sort to use for celebrities returned in <tt>Celebrities</tt> field.
--   Specify <tt>ID</tt> to sort by the celebrity identifier, specify
--   <tt>TIMESTAMP</tt> to sort by the time the celebrity was recognized.
getCelebrityRecognition_sortBy :: Lens' GetCelebrityRecognition (Maybe CelebrityRecognitionSortBy)

-- | Job identifier for the required celebrity recognition analysis. You
--   can get the job identifer from a call to
--   <tt>StartCelebrityRecognition</tt>.
getCelebrityRecognition_jobId :: Lens' GetCelebrityRecognition Text

-- | <i>See:</i> <a>newGetCelebrityRecognitionResponse</a> smart
--   constructor.
data GetCelebrityRecognitionResponse
GetCelebrityRecognitionResponse' :: Maybe Text -> Maybe VideoMetadata -> Maybe Text -> Maybe [CelebrityRecognition] -> Maybe VideoJobStatus -> Int -> GetCelebrityRecognitionResponse

-- | If the response is truncated, Amazon Rekognition Video returns this
--   token that you can use in the subsequent request to retrieve the next
--   set of celebrities.
[$sel:nextToken:GetCelebrityRecognitionResponse'] :: GetCelebrityRecognitionResponse -> Maybe Text

-- | Information about a video that Amazon Rekognition Video analyzed.
--   <tt>Videometadata</tt> is returned in every page of paginated
--   responses from a Amazon Rekognition Video operation.
[$sel:videoMetadata:GetCelebrityRecognitionResponse'] :: GetCelebrityRecognitionResponse -> Maybe VideoMetadata

-- | If the job fails, <tt>StatusMessage</tt> provides a descriptive error
--   message.
[$sel:statusMessage:GetCelebrityRecognitionResponse'] :: GetCelebrityRecognitionResponse -> Maybe Text

-- | Array of celebrities recognized in the video.
[$sel:celebrities:GetCelebrityRecognitionResponse'] :: GetCelebrityRecognitionResponse -> Maybe [CelebrityRecognition]

-- | The current status of the celebrity recognition job.
[$sel:jobStatus:GetCelebrityRecognitionResponse'] :: GetCelebrityRecognitionResponse -> Maybe VideoJobStatus

-- | The response's http status code.
[$sel:httpStatus:GetCelebrityRecognitionResponse'] :: GetCelebrityRecognitionResponse -> Int

-- | Create a value of <a>GetCelebrityRecognitionResponse</a> with all
--   optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:nextToken:GetCelebrityRecognition'</a>,
--   <a>getCelebrityRecognitionResponse_nextToken</a> - If the response is
--   truncated, Amazon Rekognition Video returns this token that you can
--   use in the subsequent request to retrieve the next set of celebrities.
--   
--   <a>$sel:videoMetadata:GetCelebrityRecognitionResponse'</a>,
--   <a>getCelebrityRecognitionResponse_videoMetadata</a> - Information
--   about a video that Amazon Rekognition Video analyzed.
--   <tt>Videometadata</tt> is returned in every page of paginated
--   responses from a Amazon Rekognition Video operation.
--   
--   <a>$sel:statusMessage:GetCelebrityRecognitionResponse'</a>,
--   <a>getCelebrityRecognitionResponse_statusMessage</a> - If the job
--   fails, <tt>StatusMessage</tt> provides a descriptive error message.
--   
--   <a>$sel:celebrities:GetCelebrityRecognitionResponse'</a>,
--   <a>getCelebrityRecognitionResponse_celebrities</a> - Array of
--   celebrities recognized in the video.
--   
--   <a>$sel:jobStatus:GetCelebrityRecognitionResponse'</a>,
--   <a>getCelebrityRecognitionResponse_jobStatus</a> - The current status
--   of the celebrity recognition job.
--   
--   <a>$sel:httpStatus:GetCelebrityRecognitionResponse'</a>,
--   <a>getCelebrityRecognitionResponse_httpStatus</a> - The response's
--   http status code.
newGetCelebrityRecognitionResponse :: Int -> GetCelebrityRecognitionResponse

-- | If the response is truncated, Amazon Rekognition Video returns this
--   token that you can use in the subsequent request to retrieve the next
--   set of celebrities.
getCelebrityRecognitionResponse_nextToken :: Lens' GetCelebrityRecognitionResponse (Maybe Text)

-- | Information about a video that Amazon Rekognition Video analyzed.
--   <tt>Videometadata</tt> is returned in every page of paginated
--   responses from a Amazon Rekognition Video operation.
getCelebrityRecognitionResponse_videoMetadata :: Lens' GetCelebrityRecognitionResponse (Maybe VideoMetadata)

-- | If the job fails, <tt>StatusMessage</tt> provides a descriptive error
--   message.
getCelebrityRecognitionResponse_statusMessage :: Lens' GetCelebrityRecognitionResponse (Maybe Text)

-- | Array of celebrities recognized in the video.
getCelebrityRecognitionResponse_celebrities :: Lens' GetCelebrityRecognitionResponse (Maybe [CelebrityRecognition])

-- | The current status of the celebrity recognition job.
getCelebrityRecognitionResponse_jobStatus :: Lens' GetCelebrityRecognitionResponse (Maybe VideoJobStatus)

-- | The response's http status code.
getCelebrityRecognitionResponse_httpStatus :: Lens' GetCelebrityRecognitionResponse Int
instance GHC.Generics.Generic Network.AWS.Rekognition.GetCelebrityRecognition.GetCelebrityRecognition
instance GHC.Show.Show Network.AWS.Rekognition.GetCelebrityRecognition.GetCelebrityRecognition
instance GHC.Read.Read Network.AWS.Rekognition.GetCelebrityRecognition.GetCelebrityRecognition
instance GHC.Classes.Eq Network.AWS.Rekognition.GetCelebrityRecognition.GetCelebrityRecognition
instance GHC.Generics.Generic Network.AWS.Rekognition.GetCelebrityRecognition.GetCelebrityRecognitionResponse
instance GHC.Show.Show Network.AWS.Rekognition.GetCelebrityRecognition.GetCelebrityRecognitionResponse
instance GHC.Read.Read Network.AWS.Rekognition.GetCelebrityRecognition.GetCelebrityRecognitionResponse
instance GHC.Classes.Eq Network.AWS.Rekognition.GetCelebrityRecognition.GetCelebrityRecognitionResponse
instance Network.AWS.Types.AWSRequest Network.AWS.Rekognition.GetCelebrityRecognition.GetCelebrityRecognition
instance Control.DeepSeq.NFData Network.AWS.Rekognition.GetCelebrityRecognition.GetCelebrityRecognitionResponse
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.GetCelebrityRecognition.GetCelebrityRecognition
instance Control.DeepSeq.NFData Network.AWS.Rekognition.GetCelebrityRecognition.GetCelebrityRecognition
instance Network.AWS.Data.Headers.ToHeaders Network.AWS.Rekognition.GetCelebrityRecognition.GetCelebrityRecognition
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.GetCelebrityRecognition.GetCelebrityRecognition
instance Network.AWS.Data.Path.ToPath Network.AWS.Rekognition.GetCelebrityRecognition.GetCelebrityRecognition
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.GetCelebrityRecognition.GetCelebrityRecognition


-- | Gets the name and additional information about a celebrity based on
--   their Amazon Rekognition ID. The additional information is returned as
--   an array of URLs. If there is no additional information about the
--   celebrity, this list is empty.
--   
--   For more information, see Recognizing Celebrities in an Image in the
--   Amazon Rekognition Developer Guide.
--   
--   This operation requires permissions to perform the
--   <tt>rekognition:GetCelebrityInfo</tt> action.
module Network.AWS.Rekognition.GetCelebrityInfo

-- | <i>See:</i> <a>newGetCelebrityInfo</a> smart constructor.
data GetCelebrityInfo
GetCelebrityInfo' :: Text -> GetCelebrityInfo

-- | The ID for the celebrity. You get the celebrity ID from a call to the
--   RecognizeCelebrities operation, which recognizes celebrities in an
--   image.
[$sel:id:GetCelebrityInfo'] :: GetCelebrityInfo -> Text

-- | Create a value of <a>GetCelebrityInfo</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:id:GetCelebrityInfo'</a>, <a>getCelebrityInfo_id</a> - The ID
--   for the celebrity. You get the celebrity ID from a call to the
--   RecognizeCelebrities operation, which recognizes celebrities in an
--   image.
newGetCelebrityInfo :: Text -> GetCelebrityInfo

-- | The ID for the celebrity. You get the celebrity ID from a call to the
--   RecognizeCelebrities operation, which recognizes celebrities in an
--   image.
getCelebrityInfo_id :: Lens' GetCelebrityInfo Text

-- | <i>See:</i> <a>newGetCelebrityInfoResponse</a> smart constructor.
data GetCelebrityInfoResponse
GetCelebrityInfoResponse' :: Maybe [Text] -> Maybe KnownGender -> Maybe Text -> Int -> GetCelebrityInfoResponse

-- | An array of URLs pointing to additional celebrity information.
[$sel:urls:GetCelebrityInfoResponse'] :: GetCelebrityInfoResponse -> Maybe [Text]

-- | Retrieves the known gender for the celebrity.
[$sel:knownGender:GetCelebrityInfoResponse'] :: GetCelebrityInfoResponse -> Maybe KnownGender

-- | The name of the celebrity.
[$sel:name:GetCelebrityInfoResponse'] :: GetCelebrityInfoResponse -> Maybe Text

-- | The response's http status code.
[$sel:httpStatus:GetCelebrityInfoResponse'] :: GetCelebrityInfoResponse -> Int

-- | Create a value of <a>GetCelebrityInfoResponse</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:urls:GetCelebrityInfoResponse'</a>,
--   <a>getCelebrityInfoResponse_urls</a> - An array of URLs pointing to
--   additional celebrity information.
--   
--   <a>$sel:knownGender:GetCelebrityInfoResponse'</a>,
--   <a>getCelebrityInfoResponse_knownGender</a> - Retrieves the known
--   gender for the celebrity.
--   
--   <a>$sel:name:GetCelebrityInfoResponse'</a>,
--   <a>getCelebrityInfoResponse_name</a> - The name of the celebrity.
--   
--   <a>$sel:httpStatus:GetCelebrityInfoResponse'</a>,
--   <a>getCelebrityInfoResponse_httpStatus</a> - The response's http
--   status code.
newGetCelebrityInfoResponse :: Int -> GetCelebrityInfoResponse

-- | An array of URLs pointing to additional celebrity information.
getCelebrityInfoResponse_urls :: Lens' GetCelebrityInfoResponse (Maybe [Text])

-- | Retrieves the known gender for the celebrity.
getCelebrityInfoResponse_knownGender :: Lens' GetCelebrityInfoResponse (Maybe KnownGender)

-- | The name of the celebrity.
getCelebrityInfoResponse_name :: Lens' GetCelebrityInfoResponse (Maybe Text)

-- | The response's http status code.
getCelebrityInfoResponse_httpStatus :: Lens' GetCelebrityInfoResponse Int
instance GHC.Generics.Generic Network.AWS.Rekognition.GetCelebrityInfo.GetCelebrityInfo
instance GHC.Show.Show Network.AWS.Rekognition.GetCelebrityInfo.GetCelebrityInfo
instance GHC.Read.Read Network.AWS.Rekognition.GetCelebrityInfo.GetCelebrityInfo
instance GHC.Classes.Eq Network.AWS.Rekognition.GetCelebrityInfo.GetCelebrityInfo
instance GHC.Generics.Generic Network.AWS.Rekognition.GetCelebrityInfo.GetCelebrityInfoResponse
instance GHC.Show.Show Network.AWS.Rekognition.GetCelebrityInfo.GetCelebrityInfoResponse
instance GHC.Read.Read Network.AWS.Rekognition.GetCelebrityInfo.GetCelebrityInfoResponse
instance GHC.Classes.Eq Network.AWS.Rekognition.GetCelebrityInfo.GetCelebrityInfoResponse
instance Network.AWS.Types.AWSRequest Network.AWS.Rekognition.GetCelebrityInfo.GetCelebrityInfo
instance Control.DeepSeq.NFData Network.AWS.Rekognition.GetCelebrityInfo.GetCelebrityInfoResponse
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.GetCelebrityInfo.GetCelebrityInfo
instance Control.DeepSeq.NFData Network.AWS.Rekognition.GetCelebrityInfo.GetCelebrityInfo
instance Network.AWS.Data.Headers.ToHeaders Network.AWS.Rekognition.GetCelebrityInfo.GetCelebrityInfo
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.GetCelebrityInfo.GetCelebrityInfo
instance Network.AWS.Data.Path.ToPath Network.AWS.Rekognition.GetCelebrityInfo.GetCelebrityInfo
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.GetCelebrityInfo.GetCelebrityInfo


-- | Detects text in the input image and converts it into machine-readable
--   text.
--   
--   Pass the input image as base64-encoded image bytes or as a reference
--   to an image in an Amazon S3 bucket. If you use the AWS CLI to call
--   Amazon Rekognition operations, you must pass it as a reference to an
--   image in an Amazon S3 bucket. For the AWS CLI, passing image bytes is
--   not supported. The image must be either a .png or .jpeg formatted
--   file.
--   
--   The <tt>DetectText</tt> operation returns text in an array of
--   TextDetection elements, <tt>TextDetections</tt>. Each
--   <tt>TextDetection</tt> element provides information about a single
--   word or line of text that was detected in the image.
--   
--   A word is one or more ISO basic latin script characters that are not
--   separated by spaces. <tt>DetectText</tt> can detect up to 100 words in
--   an image.
--   
--   A line is a string of equally spaced words. A line isn't necessarily a
--   complete sentence. For example, a driver's license number is detected
--   as a line. A line ends when there is no aligned text after it. Also, a
--   line ends when there is a large gap between words, relative to the
--   length of the words. This means, depending on the gap between words,
--   Amazon Rekognition may detect multiple lines in text aligned in the
--   same direction. Periods don't represent the end of a line. If a
--   sentence spans multiple lines, the <tt>DetectText</tt> operation
--   returns multiple lines.
--   
--   To determine whether a <tt>TextDetection</tt> element is a line of
--   text or a word, use the <tt>TextDetection</tt> object <tt>Type</tt>
--   field.
--   
--   To be detected, text must be within +/- 90 degrees orientation of the
--   horizontal axis.
--   
--   For more information, see DetectText in the Amazon Rekognition
--   Developer Guide.
module Network.AWS.Rekognition.DetectText

-- | <i>See:</i> <a>newDetectText</a> smart constructor.
data DetectText
DetectText' :: Maybe DetectTextFilters -> Image -> DetectText

-- | Optional parameters that let you set the criteria that the text must
--   meet to be included in your response.
[$sel:filters:DetectText'] :: DetectText -> Maybe DetectTextFilters

-- | The input image as base64-encoded bytes or an Amazon S3 object. If you
--   use the AWS CLI to call Amazon Rekognition operations, you can't pass
--   image bytes.
--   
--   If you are using an AWS SDK to call Amazon Rekognition, you might not
--   need to base64-encode image bytes passed using the <tt>Bytes</tt>
--   field. For more information, see Images in the Amazon Rekognition
--   developer guide.
[$sel:image:DetectText'] :: DetectText -> Image

-- | Create a value of <a>DetectText</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:filters:DetectText'</a>, <a>detectText_filters</a> - Optional
--   parameters that let you set the criteria that the text must meet to be
--   included in your response.
--   
--   <a>$sel:image:DetectText'</a>, <a>detectText_image</a> - The input
--   image as base64-encoded bytes or an Amazon S3 object. If you use the
--   AWS CLI to call Amazon Rekognition operations, you can't pass image
--   bytes.
--   
--   If you are using an AWS SDK to call Amazon Rekognition, you might not
--   need to base64-encode image bytes passed using the <tt>Bytes</tt>
--   field. For more information, see Images in the Amazon Rekognition
--   developer guide.
newDetectText :: Image -> DetectText

-- | Optional parameters that let you set the criteria that the text must
--   meet to be included in your response.
detectText_filters :: Lens' DetectText (Maybe DetectTextFilters)

-- | The input image as base64-encoded bytes or an Amazon S3 object. If you
--   use the AWS CLI to call Amazon Rekognition operations, you can't pass
--   image bytes.
--   
--   If you are using an AWS SDK to call Amazon Rekognition, you might not
--   need to base64-encode image bytes passed using the <tt>Bytes</tt>
--   field. For more information, see Images in the Amazon Rekognition
--   developer guide.
detectText_image :: Lens' DetectText Image

-- | <i>See:</i> <a>newDetectTextResponse</a> smart constructor.
data DetectTextResponse
DetectTextResponse' :: Maybe [TextDetection] -> Maybe Text -> Int -> DetectTextResponse

-- | An array of text that was detected in the input image.
[$sel:textDetections:DetectTextResponse'] :: DetectTextResponse -> Maybe [TextDetection]

-- | The model version used to detect text.
[$sel:textModelVersion:DetectTextResponse'] :: DetectTextResponse -> Maybe Text

-- | The response's http status code.
[$sel:httpStatus:DetectTextResponse'] :: DetectTextResponse -> Int

-- | Create a value of <a>DetectTextResponse</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:textDetections:DetectTextResponse'</a>,
--   <a>detectTextResponse_textDetections</a> - An array of text that was
--   detected in the input image.
--   
--   <a>$sel:textModelVersion:DetectTextResponse'</a>,
--   <a>detectTextResponse_textModelVersion</a> - The model version used to
--   detect text.
--   
--   <a>$sel:httpStatus:DetectTextResponse'</a>,
--   <a>detectTextResponse_httpStatus</a> - The response's http status
--   code.
newDetectTextResponse :: Int -> DetectTextResponse

-- | An array of text that was detected in the input image.
detectTextResponse_textDetections :: Lens' DetectTextResponse (Maybe [TextDetection])

-- | The model version used to detect text.
detectTextResponse_textModelVersion :: Lens' DetectTextResponse (Maybe Text)

-- | The response's http status code.
detectTextResponse_httpStatus :: Lens' DetectTextResponse Int
instance GHC.Generics.Generic Network.AWS.Rekognition.DetectText.DetectText
instance GHC.Show.Show Network.AWS.Rekognition.DetectText.DetectText
instance GHC.Read.Read Network.AWS.Rekognition.DetectText.DetectText
instance GHC.Classes.Eq Network.AWS.Rekognition.DetectText.DetectText
instance GHC.Generics.Generic Network.AWS.Rekognition.DetectText.DetectTextResponse
instance GHC.Show.Show Network.AWS.Rekognition.DetectText.DetectTextResponse
instance GHC.Read.Read Network.AWS.Rekognition.DetectText.DetectTextResponse
instance GHC.Classes.Eq Network.AWS.Rekognition.DetectText.DetectTextResponse
instance Network.AWS.Types.AWSRequest Network.AWS.Rekognition.DetectText.DetectText
instance Control.DeepSeq.NFData Network.AWS.Rekognition.DetectText.DetectTextResponse
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.DetectText.DetectText
instance Control.DeepSeq.NFData Network.AWS.Rekognition.DetectText.DetectText
instance Network.AWS.Data.Headers.ToHeaders Network.AWS.Rekognition.DetectText.DetectText
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.DetectText.DetectText
instance Network.AWS.Data.Path.ToPath Network.AWS.Rekognition.DetectText.DetectText
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.DetectText.DetectText


-- | Detects Personal Protective Equipment (PPE) worn by people detected in
--   an image. Amazon Rekognition can detect the following types of PPE.
--   
--   <ul>
--   <li>Face cover</li>
--   <li>Hand cover</li>
--   <li>Head cover</li>
--   </ul>
--   
--   You pass the input image as base64-encoded image bytes or as a
--   reference to an image in an Amazon S3 bucket. The image must be either
--   a PNG or JPG formatted file.
--   
--   <tt>DetectProtectiveEquipment</tt> detects PPE worn by up to 15
--   persons detected in an image.
--   
--   For each person detected in the image the API returns an array of body
--   parts (face, head, left-hand, right-hand). For each body part, an
--   array of detected items of PPE is returned, including an indicator of
--   whether or not the PPE covers the body part. The API returns the
--   confidence it has in each detection (person, PPE, body part and body
--   part coverage). It also returns a bounding box (BoundingBox) for each
--   detected person and each detected item of PPE.
--   
--   You can optionally request a summary of detected PPE items with the
--   <tt>SummarizationAttributes</tt> input parameter. The summary provides
--   the following information.
--   
--   <ul>
--   <li>The persons detected as wearing all of the types of PPE that you
--   specify.</li>
--   <li>The persons detected as not wearing all of the types PPE that you
--   specify.</li>
--   <li>The persons detected where PPE adornment could not be
--   determined.</li>
--   </ul>
--   
--   This is a stateless API operation. That is, the operation does not
--   persist any data.
--   
--   This operation requires permissions to perform the
--   <tt>rekognition:DetectProtectiveEquipment</tt> action.
module Network.AWS.Rekognition.DetectProtectiveEquipment

-- | <i>See:</i> <a>newDetectProtectiveEquipment</a> smart constructor.
data DetectProtectiveEquipment
DetectProtectiveEquipment' :: Maybe ProtectiveEquipmentSummarizationAttributes -> Image -> DetectProtectiveEquipment

-- | An array of PPE types that you want to summarize.
[$sel:summarizationAttributes:DetectProtectiveEquipment'] :: DetectProtectiveEquipment -> Maybe ProtectiveEquipmentSummarizationAttributes

-- | The image in which you want to detect PPE on detected persons. The
--   image can be passed as image bytes or you can reference an image
--   stored in an Amazon S3 bucket.
[$sel:image:DetectProtectiveEquipment'] :: DetectProtectiveEquipment -> Image

-- | Create a value of <a>DetectProtectiveEquipment</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:summarizationAttributes:DetectProtectiveEquipment'</a>,
--   <a>detectProtectiveEquipment_summarizationAttributes</a> - An array of
--   PPE types that you want to summarize.
--   
--   <a>$sel:image:DetectProtectiveEquipment'</a>,
--   <a>detectProtectiveEquipment_image</a> - The image in which you want
--   to detect PPE on detected persons. The image can be passed as image
--   bytes or you can reference an image stored in an Amazon S3 bucket.
newDetectProtectiveEquipment :: Image -> DetectProtectiveEquipment

-- | An array of PPE types that you want to summarize.
detectProtectiveEquipment_summarizationAttributes :: Lens' DetectProtectiveEquipment (Maybe ProtectiveEquipmentSummarizationAttributes)

-- | The image in which you want to detect PPE on detected persons. The
--   image can be passed as image bytes or you can reference an image
--   stored in an Amazon S3 bucket.
detectProtectiveEquipment_image :: Lens' DetectProtectiveEquipment Image

-- | <i>See:</i> <a>newDetectProtectiveEquipmentResponse</a> smart
--   constructor.
data DetectProtectiveEquipmentResponse
DetectProtectiveEquipmentResponse' :: Maybe ProtectiveEquipmentSummary -> Maybe Text -> Maybe [ProtectiveEquipmentPerson] -> Int -> DetectProtectiveEquipmentResponse

-- | Summary information for the types of PPE specified in the
--   <tt>SummarizationAttributes</tt> input parameter.
[$sel:summary:DetectProtectiveEquipmentResponse'] :: DetectProtectiveEquipmentResponse -> Maybe ProtectiveEquipmentSummary

-- | The version number of the PPE detection model used to detect PPE in
--   the image.
[$sel:protectiveEquipmentModelVersion:DetectProtectiveEquipmentResponse'] :: DetectProtectiveEquipmentResponse -> Maybe Text

-- | An array of persons detected in the image (including persons not
--   wearing PPE).
[$sel:persons:DetectProtectiveEquipmentResponse'] :: DetectProtectiveEquipmentResponse -> Maybe [ProtectiveEquipmentPerson]

-- | The response's http status code.
[$sel:httpStatus:DetectProtectiveEquipmentResponse'] :: DetectProtectiveEquipmentResponse -> Int

-- | Create a value of <a>DetectProtectiveEquipmentResponse</a> with all
--   optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:summary:DetectProtectiveEquipmentResponse'</a>,
--   <a>detectProtectiveEquipmentResponse_summary</a> - Summary information
--   for the types of PPE specified in the <tt>SummarizationAttributes</tt>
--   input parameter.
--   
--   
--   <a>$sel:protectiveEquipmentModelVersion:DetectProtectiveEquipmentResponse'</a>,
--   <a>detectProtectiveEquipmentResponse_protectiveEquipmentModelVersion</a>
--   - The version number of the PPE detection model used to detect PPE in
--   the image.
--   
--   <a>$sel:persons:DetectProtectiveEquipmentResponse'</a>,
--   <a>detectProtectiveEquipmentResponse_persons</a> - An array of persons
--   detected in the image (including persons not wearing PPE).
--   
--   <a>$sel:httpStatus:DetectProtectiveEquipmentResponse'</a>,
--   <a>detectProtectiveEquipmentResponse_httpStatus</a> - The response's
--   http status code.
newDetectProtectiveEquipmentResponse :: Int -> DetectProtectiveEquipmentResponse

-- | Summary information for the types of PPE specified in the
--   <tt>SummarizationAttributes</tt> input parameter.
detectProtectiveEquipmentResponse_summary :: Lens' DetectProtectiveEquipmentResponse (Maybe ProtectiveEquipmentSummary)

-- | The version number of the PPE detection model used to detect PPE in
--   the image.
detectProtectiveEquipmentResponse_protectiveEquipmentModelVersion :: Lens' DetectProtectiveEquipmentResponse (Maybe Text)

-- | An array of persons detected in the image (including persons not
--   wearing PPE).
detectProtectiveEquipmentResponse_persons :: Lens' DetectProtectiveEquipmentResponse (Maybe [ProtectiveEquipmentPerson])

-- | The response's http status code.
detectProtectiveEquipmentResponse_httpStatus :: Lens' DetectProtectiveEquipmentResponse Int
instance GHC.Generics.Generic Network.AWS.Rekognition.DetectProtectiveEquipment.DetectProtectiveEquipment
instance GHC.Show.Show Network.AWS.Rekognition.DetectProtectiveEquipment.DetectProtectiveEquipment
instance GHC.Read.Read Network.AWS.Rekognition.DetectProtectiveEquipment.DetectProtectiveEquipment
instance GHC.Classes.Eq Network.AWS.Rekognition.DetectProtectiveEquipment.DetectProtectiveEquipment
instance GHC.Generics.Generic Network.AWS.Rekognition.DetectProtectiveEquipment.DetectProtectiveEquipmentResponse
instance GHC.Show.Show Network.AWS.Rekognition.DetectProtectiveEquipment.DetectProtectiveEquipmentResponse
instance GHC.Read.Read Network.AWS.Rekognition.DetectProtectiveEquipment.DetectProtectiveEquipmentResponse
instance GHC.Classes.Eq Network.AWS.Rekognition.DetectProtectiveEquipment.DetectProtectiveEquipmentResponse
instance Network.AWS.Types.AWSRequest Network.AWS.Rekognition.DetectProtectiveEquipment.DetectProtectiveEquipment
instance Control.DeepSeq.NFData Network.AWS.Rekognition.DetectProtectiveEquipment.DetectProtectiveEquipmentResponse
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.DetectProtectiveEquipment.DetectProtectiveEquipment
instance Control.DeepSeq.NFData Network.AWS.Rekognition.DetectProtectiveEquipment.DetectProtectiveEquipment
instance Network.AWS.Data.Headers.ToHeaders Network.AWS.Rekognition.DetectProtectiveEquipment.DetectProtectiveEquipment
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.DetectProtectiveEquipment.DetectProtectiveEquipment
instance Network.AWS.Data.Path.ToPath Network.AWS.Rekognition.DetectProtectiveEquipment.DetectProtectiveEquipment
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.DetectProtectiveEquipment.DetectProtectiveEquipment


-- | Detects unsafe content in a specified JPEG or PNG format image. Use
--   <tt>DetectModerationLabels</tt> to moderate images depending on your
--   requirements. For example, you might want to filter images that
--   contain nudity, but not images containing suggestive content.
--   
--   To filter images, use the labels returned by
--   <tt>DetectModerationLabels</tt> to determine which types of content
--   are appropriate.
--   
--   For information about moderation labels, see Detecting Unsafe Content
--   in the Amazon Rekognition Developer Guide.
--   
--   You pass the input image either as base64-encoded image bytes or as a
--   reference to an image in an Amazon S3 bucket. If you use the AWS CLI
--   to call Amazon Rekognition operations, passing image bytes is not
--   supported. The image must be either a PNG or JPEG formatted file.
module Network.AWS.Rekognition.DetectModerationLabels

-- | <i>See:</i> <a>newDetectModerationLabels</a> smart constructor.
data DetectModerationLabels
DetectModerationLabels' :: Maybe HumanLoopConfig -> Maybe Double -> Image -> DetectModerationLabels

-- | Sets up the configuration for human evaluation, including the
--   FlowDefinition the image will be sent to.
[$sel:humanLoopConfig:DetectModerationLabels'] :: DetectModerationLabels -> Maybe HumanLoopConfig

-- | Specifies the minimum confidence level for the labels to return.
--   Amazon Rekognition doesn't return any labels with a confidence level
--   lower than this specified value.
--   
--   If you don't specify <tt>MinConfidence</tt>, the operation returns
--   labels with confidence values greater than or equal to 50 percent.
[$sel:minConfidence:DetectModerationLabels'] :: DetectModerationLabels -> Maybe Double

-- | The input image as base64-encoded bytes or an S3 object. If you use
--   the AWS CLI to call Amazon Rekognition operations, passing
--   base64-encoded image bytes is not supported.
--   
--   If you are using an AWS SDK to call Amazon Rekognition, you might not
--   need to base64-encode image bytes passed using the <tt>Bytes</tt>
--   field. For more information, see Images in the Amazon Rekognition
--   developer guide.
[$sel:image:DetectModerationLabels'] :: DetectModerationLabels -> Image

-- | Create a value of <a>DetectModerationLabels</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:humanLoopConfig:DetectModerationLabels'</a>,
--   <a>detectModerationLabels_humanLoopConfig</a> - Sets up the
--   configuration for human evaluation, including the FlowDefinition the
--   image will be sent to.
--   
--   <a>$sel:minConfidence:DetectModerationLabels'</a>,
--   <a>detectModerationLabels_minConfidence</a> - Specifies the minimum
--   confidence level for the labels to return. Amazon Rekognition doesn't
--   return any labels with a confidence level lower than this specified
--   value.
--   
--   If you don't specify <tt>MinConfidence</tt>, the operation returns
--   labels with confidence values greater than or equal to 50 percent.
--   
--   <a>$sel:image:DetectModerationLabels'</a>,
--   <a>detectModerationLabels_image</a> - The input image as
--   base64-encoded bytes or an S3 object. If you use the AWS CLI to call
--   Amazon Rekognition operations, passing base64-encoded image bytes is
--   not supported.
--   
--   If you are using an AWS SDK to call Amazon Rekognition, you might not
--   need to base64-encode image bytes passed using the <tt>Bytes</tt>
--   field. For more information, see Images in the Amazon Rekognition
--   developer guide.
newDetectModerationLabels :: Image -> DetectModerationLabels

-- | Sets up the configuration for human evaluation, including the
--   FlowDefinition the image will be sent to.
detectModerationLabels_humanLoopConfig :: Lens' DetectModerationLabels (Maybe HumanLoopConfig)

-- | Specifies the minimum confidence level for the labels to return.
--   Amazon Rekognition doesn't return any labels with a confidence level
--   lower than this specified value.
--   
--   If you don't specify <tt>MinConfidence</tt>, the operation returns
--   labels with confidence values greater than or equal to 50 percent.
detectModerationLabels_minConfidence :: Lens' DetectModerationLabels (Maybe Double)

-- | The input image as base64-encoded bytes or an S3 object. If you use
--   the AWS CLI to call Amazon Rekognition operations, passing
--   base64-encoded image bytes is not supported.
--   
--   If you are using an AWS SDK to call Amazon Rekognition, you might not
--   need to base64-encode image bytes passed using the <tt>Bytes</tt>
--   field. For more information, see Images in the Amazon Rekognition
--   developer guide.
detectModerationLabels_image :: Lens' DetectModerationLabels Image

-- | <i>See:</i> <a>newDetectModerationLabelsResponse</a> smart
--   constructor.
data DetectModerationLabelsResponse
DetectModerationLabelsResponse' :: Maybe HumanLoopActivationOutput -> Maybe Text -> Maybe [ModerationLabel] -> Int -> DetectModerationLabelsResponse

-- | Shows the results of the human in the loop evaluation.
[$sel:humanLoopActivationOutput:DetectModerationLabelsResponse'] :: DetectModerationLabelsResponse -> Maybe HumanLoopActivationOutput

-- | Version number of the moderation detection model that was used to
--   detect unsafe content.
[$sel:moderationModelVersion:DetectModerationLabelsResponse'] :: DetectModerationLabelsResponse -> Maybe Text

-- | Array of detected Moderation labels and the time, in milliseconds from
--   the start of the video, they were detected.
[$sel:moderationLabels:DetectModerationLabelsResponse'] :: DetectModerationLabelsResponse -> Maybe [ModerationLabel]

-- | The response's http status code.
[$sel:httpStatus:DetectModerationLabelsResponse'] :: DetectModerationLabelsResponse -> Int

-- | Create a value of <a>DetectModerationLabelsResponse</a> with all
--   optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:humanLoopActivationOutput:DetectModerationLabelsResponse'</a>,
--   <a>detectModerationLabelsResponse_humanLoopActivationOutput</a> -
--   Shows the results of the human in the loop evaluation.
--   
--   <a>$sel:moderationModelVersion:DetectModerationLabelsResponse'</a>,
--   <a>detectModerationLabelsResponse_moderationModelVersion</a> - Version
--   number of the moderation detection model that was used to detect
--   unsafe content.
--   
--   <a>$sel:moderationLabels:DetectModerationLabelsResponse'</a>,
--   <a>detectModerationLabelsResponse_moderationLabels</a> - Array of
--   detected Moderation labels and the time, in milliseconds from the
--   start of the video, they were detected.
--   
--   <a>$sel:httpStatus:DetectModerationLabelsResponse'</a>,
--   <a>detectModerationLabelsResponse_httpStatus</a> - The response's http
--   status code.
newDetectModerationLabelsResponse :: Int -> DetectModerationLabelsResponse

-- | Shows the results of the human in the loop evaluation.
detectModerationLabelsResponse_humanLoopActivationOutput :: Lens' DetectModerationLabelsResponse (Maybe HumanLoopActivationOutput)

-- | Version number of the moderation detection model that was used to
--   detect unsafe content.
detectModerationLabelsResponse_moderationModelVersion :: Lens' DetectModerationLabelsResponse (Maybe Text)

-- | Array of detected Moderation labels and the time, in milliseconds from
--   the start of the video, they were detected.
detectModerationLabelsResponse_moderationLabels :: Lens' DetectModerationLabelsResponse (Maybe [ModerationLabel])

-- | The response's http status code.
detectModerationLabelsResponse_httpStatus :: Lens' DetectModerationLabelsResponse Int
instance GHC.Generics.Generic Network.AWS.Rekognition.DetectModerationLabels.DetectModerationLabels
instance GHC.Show.Show Network.AWS.Rekognition.DetectModerationLabels.DetectModerationLabels
instance GHC.Read.Read Network.AWS.Rekognition.DetectModerationLabels.DetectModerationLabels
instance GHC.Classes.Eq Network.AWS.Rekognition.DetectModerationLabels.DetectModerationLabels
instance GHC.Generics.Generic Network.AWS.Rekognition.DetectModerationLabels.DetectModerationLabelsResponse
instance GHC.Show.Show Network.AWS.Rekognition.DetectModerationLabels.DetectModerationLabelsResponse
instance GHC.Read.Read Network.AWS.Rekognition.DetectModerationLabels.DetectModerationLabelsResponse
instance GHC.Classes.Eq Network.AWS.Rekognition.DetectModerationLabels.DetectModerationLabelsResponse
instance Network.AWS.Types.AWSRequest Network.AWS.Rekognition.DetectModerationLabels.DetectModerationLabels
instance Control.DeepSeq.NFData Network.AWS.Rekognition.DetectModerationLabels.DetectModerationLabelsResponse
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.DetectModerationLabels.DetectModerationLabels
instance Control.DeepSeq.NFData Network.AWS.Rekognition.DetectModerationLabels.DetectModerationLabels
instance Network.AWS.Data.Headers.ToHeaders Network.AWS.Rekognition.DetectModerationLabels.DetectModerationLabels
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.DetectModerationLabels.DetectModerationLabels
instance Network.AWS.Data.Path.ToPath Network.AWS.Rekognition.DetectModerationLabels.DetectModerationLabels
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.DetectModerationLabels.DetectModerationLabels


-- | Detects instances of real-world entities within an image (JPEG or PNG)
--   provided as input. This includes objects like flower, tree, and table;
--   events like wedding, graduation, and birthday party; and concepts like
--   landscape, evening, and nature.
--   
--   For an example, see Analyzing Images Stored in an Amazon S3 Bucket in
--   the Amazon Rekognition Developer Guide.
--   
--   <tt>DetectLabels</tt> does not support the detection of activities.
--   However, activity detection is supported for label detection in
--   videos. For more information, see StartLabelDetection in the Amazon
--   Rekognition Developer Guide.
--   
--   You pass the input image as base64-encoded image bytes or as a
--   reference to an image in an Amazon S3 bucket. If you use the AWS CLI
--   to call Amazon Rekognition operations, passing image bytes is not
--   supported. The image must be either a PNG or JPEG formatted file.
--   
--   For each object, scene, and concept the API returns one or more
--   labels. Each label provides the object name, and the level of
--   confidence that the image contains the object. For example, suppose
--   the input image has a lighthouse, the sea, and a rock. The response
--   includes all three labels, one for each object.
--   
--   <pre>
--   {Name: lighthouse, Confidence: 98.4629}
--   </pre>
--   
--   <pre>
--   {Name: rock,Confidence: 79.2097}
--   </pre>
--   
--   <pre>
--   {Name: sea,Confidence: 75.061}
--   </pre>
--   
--   In the preceding example, the operation returns one label for each of
--   the three objects. The operation can also return multiple labels for
--   the same object in the image. For example, if the input image shows a
--   flower (for example, a tulip), the operation might return the
--   following three labels.
--   
--   <pre>
--   {Name: flower,Confidence: 99.0562}
--   </pre>
--   
--   <pre>
--   {Name: plant,Confidence: 99.0562}
--   </pre>
--   
--   <pre>
--   {Name: tulip,Confidence: 99.0562}
--   </pre>
--   
--   In this example, the detection algorithm more precisely identifies the
--   flower as a tulip.
--   
--   In response, the API returns an array of labels. In addition, the
--   response also includes the orientation correction. Optionally, you can
--   specify <tt>MinConfidence</tt> to control the confidence threshold for
--   the labels returned. The default is 55%. You can also add the
--   <tt>MaxLabels</tt> parameter to limit the number of labels returned.
--   
--   If the object detected is a person, the operation doesn't provide the
--   same facial details that the DetectFaces operation provides.
--   
--   <tt>DetectLabels</tt> returns bounding boxes for instances of common
--   object labels in an array of Instance objects. An <tt>Instance</tt>
--   object contains a BoundingBox object, for the location of the label on
--   the image. It also includes the confidence by which the bounding box
--   was detected.
--   
--   <tt>DetectLabels</tt> also returns a hierarchical taxonomy of detected
--   labels. For example, a detected car might be assigned the label
--   <i>car</i>. The label <i>car</i> has two parent labels: <i>Vehicle</i>
--   (its parent) and <i>Transportation</i> (its grandparent). The response
--   returns the entire list of ancestors for a label. Each ancestor is a
--   unique label in the response. In the previous example, <i>Car</i>,
--   <i>Vehicle</i>, and <i>Transportation</i> are returned as unique
--   labels in the response.
--   
--   This is a stateless API operation. That is, the operation does not
--   persist any data.
--   
--   This operation requires permissions to perform the
--   <tt>rekognition:DetectLabels</tt> action.
module Network.AWS.Rekognition.DetectLabels

-- | <i>See:</i> <a>newDetectLabels</a> smart constructor.
data DetectLabels
DetectLabels' :: Maybe Double -> Maybe Natural -> Image -> DetectLabels

-- | Specifies the minimum confidence level for the labels to return.
--   Amazon Rekognition doesn't return any labels with confidence lower
--   than this specified value.
--   
--   If <tt>MinConfidence</tt> is not specified, the operation returns
--   labels with a confidence values greater than or equal to 55 percent.
[$sel:minConfidence:DetectLabels'] :: DetectLabels -> Maybe Double

-- | Maximum number of labels you want the service to return in the
--   response. The service returns the specified number of highest
--   confidence labels.
[$sel:maxLabels:DetectLabels'] :: DetectLabels -> Maybe Natural

-- | The input image as base64-encoded bytes or an S3 object. If you use
--   the AWS CLI to call Amazon Rekognition operations, passing image bytes
--   is not supported. Images stored in an S3 Bucket do not need to be
--   base64-encoded.
--   
--   If you are using an AWS SDK to call Amazon Rekognition, you might not
--   need to base64-encode image bytes passed using the <tt>Bytes</tt>
--   field. For more information, see Images in the Amazon Rekognition
--   developer guide.
[$sel:image:DetectLabels'] :: DetectLabels -> Image

-- | Create a value of <a>DetectLabels</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:minConfidence:DetectLabels'</a>,
--   <a>detectLabels_minConfidence</a> - Specifies the minimum confidence
--   level for the labels to return. Amazon Rekognition doesn't return any
--   labels with confidence lower than this specified value.
--   
--   If <tt>MinConfidence</tt> is not specified, the operation returns
--   labels with a confidence values greater than or equal to 55 percent.
--   
--   <a>$sel:maxLabels:DetectLabels'</a>, <a>detectLabels_maxLabels</a> -
--   Maximum number of labels you want the service to return in the
--   response. The service returns the specified number of highest
--   confidence labels.
--   
--   <a>$sel:image:DetectLabels'</a>, <a>detectLabels_image</a> - The input
--   image as base64-encoded bytes or an S3 object. If you use the AWS CLI
--   to call Amazon Rekognition operations, passing image bytes is not
--   supported. Images stored in an S3 Bucket do not need to be
--   base64-encoded.
--   
--   If you are using an AWS SDK to call Amazon Rekognition, you might not
--   need to base64-encode image bytes passed using the <tt>Bytes</tt>
--   field. For more information, see Images in the Amazon Rekognition
--   developer guide.
newDetectLabels :: Image -> DetectLabels

-- | Specifies the minimum confidence level for the labels to return.
--   Amazon Rekognition doesn't return any labels with confidence lower
--   than this specified value.
--   
--   If <tt>MinConfidence</tt> is not specified, the operation returns
--   labels with a confidence values greater than or equal to 55 percent.
detectLabels_minConfidence :: Lens' DetectLabels (Maybe Double)

-- | Maximum number of labels you want the service to return in the
--   response. The service returns the specified number of highest
--   confidence labels.
detectLabels_maxLabels :: Lens' DetectLabels (Maybe Natural)

-- | The input image as base64-encoded bytes or an S3 object. If you use
--   the AWS CLI to call Amazon Rekognition operations, passing image bytes
--   is not supported. Images stored in an S3 Bucket do not need to be
--   base64-encoded.
--   
--   If you are using an AWS SDK to call Amazon Rekognition, you might not
--   need to base64-encode image bytes passed using the <tt>Bytes</tt>
--   field. For more information, see Images in the Amazon Rekognition
--   developer guide.
detectLabels_image :: Lens' DetectLabels Image

-- | <i>See:</i> <a>newDetectLabelsResponse</a> smart constructor.
data DetectLabelsResponse
DetectLabelsResponse' :: Maybe [Label] -> Maybe OrientationCorrection -> Maybe Text -> Int -> DetectLabelsResponse

-- | An array of labels for the real-world objects detected.
[$sel:labels:DetectLabelsResponse'] :: DetectLabelsResponse -> Maybe [Label]

-- | The value of <tt>OrientationCorrection</tt> is always null.
--   
--   If the input image is in .jpeg format, it might contain exchangeable
--   image file format (Exif) metadata that includes the image's
--   orientation. Amazon Rekognition uses this orientation information to
--   perform image correction. The bounding box coordinates are translated
--   to represent object locations after the orientation information in the
--   Exif metadata is used to correct the image orientation. Images in .png
--   format don't contain Exif metadata.
--   
--   Amazon Rekognition doesn’t perform image correction for images in .png
--   format and .jpeg images without orientation information in the image
--   Exif metadata. The bounding box coordinates aren't translated and
--   represent the object locations before the image is rotated.
[$sel:orientationCorrection:DetectLabelsResponse'] :: DetectLabelsResponse -> Maybe OrientationCorrection

-- | Version number of the label detection model that was used to detect
--   labels.
[$sel:labelModelVersion:DetectLabelsResponse'] :: DetectLabelsResponse -> Maybe Text

-- | The response's http status code.
[$sel:httpStatus:DetectLabelsResponse'] :: DetectLabelsResponse -> Int

-- | Create a value of <a>DetectLabelsResponse</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:labels:DetectLabelsResponse'</a>,
--   <a>detectLabelsResponse_labels</a> - An array of labels for the
--   real-world objects detected.
--   
--   <a>$sel:orientationCorrection:DetectLabelsResponse'</a>,
--   <a>detectLabelsResponse_orientationCorrection</a> - The value of
--   <tt>OrientationCorrection</tt> is always null.
--   
--   If the input image is in .jpeg format, it might contain exchangeable
--   image file format (Exif) metadata that includes the image's
--   orientation. Amazon Rekognition uses this orientation information to
--   perform image correction. The bounding box coordinates are translated
--   to represent object locations after the orientation information in the
--   Exif metadata is used to correct the image orientation. Images in .png
--   format don't contain Exif metadata.
--   
--   Amazon Rekognition doesn’t perform image correction for images in .png
--   format and .jpeg images without orientation information in the image
--   Exif metadata. The bounding box coordinates aren't translated and
--   represent the object locations before the image is rotated.
--   
--   <a>$sel:labelModelVersion:DetectLabelsResponse'</a>,
--   <a>detectLabelsResponse_labelModelVersion</a> - Version number of the
--   label detection model that was used to detect labels.
--   
--   <a>$sel:httpStatus:DetectLabelsResponse'</a>,
--   <a>detectLabelsResponse_httpStatus</a> - The response's http status
--   code.
newDetectLabelsResponse :: Int -> DetectLabelsResponse

-- | An array of labels for the real-world objects detected.
detectLabelsResponse_labels :: Lens' DetectLabelsResponse (Maybe [Label])

-- | The value of <tt>OrientationCorrection</tt> is always null.
--   
--   If the input image is in .jpeg format, it might contain exchangeable
--   image file format (Exif) metadata that includes the image's
--   orientation. Amazon Rekognition uses this orientation information to
--   perform image correction. The bounding box coordinates are translated
--   to represent object locations after the orientation information in the
--   Exif metadata is used to correct the image orientation. Images in .png
--   format don't contain Exif metadata.
--   
--   Amazon Rekognition doesn’t perform image correction for images in .png
--   format and .jpeg images without orientation information in the image
--   Exif metadata. The bounding box coordinates aren't translated and
--   represent the object locations before the image is rotated.
detectLabelsResponse_orientationCorrection :: Lens' DetectLabelsResponse (Maybe OrientationCorrection)

-- | Version number of the label detection model that was used to detect
--   labels.
detectLabelsResponse_labelModelVersion :: Lens' DetectLabelsResponse (Maybe Text)

-- | The response's http status code.
detectLabelsResponse_httpStatus :: Lens' DetectLabelsResponse Int
instance GHC.Generics.Generic Network.AWS.Rekognition.DetectLabels.DetectLabels
instance GHC.Show.Show Network.AWS.Rekognition.DetectLabels.DetectLabels
instance GHC.Read.Read Network.AWS.Rekognition.DetectLabels.DetectLabels
instance GHC.Classes.Eq Network.AWS.Rekognition.DetectLabels.DetectLabels
instance GHC.Generics.Generic Network.AWS.Rekognition.DetectLabels.DetectLabelsResponse
instance GHC.Show.Show Network.AWS.Rekognition.DetectLabels.DetectLabelsResponse
instance GHC.Read.Read Network.AWS.Rekognition.DetectLabels.DetectLabelsResponse
instance GHC.Classes.Eq Network.AWS.Rekognition.DetectLabels.DetectLabelsResponse
instance Network.AWS.Types.AWSRequest Network.AWS.Rekognition.DetectLabels.DetectLabels
instance Control.DeepSeq.NFData Network.AWS.Rekognition.DetectLabels.DetectLabelsResponse
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.DetectLabels.DetectLabels
instance Control.DeepSeq.NFData Network.AWS.Rekognition.DetectLabels.DetectLabels
instance Network.AWS.Data.Headers.ToHeaders Network.AWS.Rekognition.DetectLabels.DetectLabels
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.DetectLabels.DetectLabels
instance Network.AWS.Data.Path.ToPath Network.AWS.Rekognition.DetectLabels.DetectLabels
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.DetectLabels.DetectLabels


-- | Detects faces within an image that is provided as input.
--   
--   <tt>DetectFaces</tt> detects the 100 largest faces in the image. For
--   each face detected, the operation returns face details. These details
--   include a bounding box of the face, a confidence value (that the
--   bounding box contains a face), and a fixed set of attributes such as
--   facial landmarks (for example, coordinates of eye and mouth), presence
--   of beard, sunglasses, and so on.
--   
--   The face-detection algorithm is most effective on frontal faces. For
--   non-frontal or obscured faces, the algorithm might not detect the
--   faces or might detect faces with lower confidence.
--   
--   You pass the input image either as base64-encoded image bytes or as a
--   reference to an image in an Amazon S3 bucket. If you use the AWS CLI
--   to call Amazon Rekognition operations, passing image bytes is not
--   supported. The image must be either a PNG or JPEG formatted file.
--   
--   This is a stateless API operation. That is, the operation does not
--   persist any data.
--   
--   This operation requires permissions to perform the
--   <tt>rekognition:DetectFaces</tt> action.
module Network.AWS.Rekognition.DetectFaces

-- | <i>See:</i> <a>newDetectFaces</a> smart constructor.
data DetectFaces
DetectFaces' :: Maybe [Attribute] -> Image -> DetectFaces

-- | An array of facial attributes you want to be returned. This can be the
--   default list of attributes or all attributes. If you don't specify a
--   value for <tt>Attributes</tt> or if you specify <tt>["DEFAULT"]</tt>,
--   the API returns the following subset of facial attributes:
--   <tt>BoundingBox</tt>, <tt>Confidence</tt>, <tt>Pose</tt>,
--   <tt>Quality</tt>, and <tt>Landmarks</tt>. If you provide
--   <tt>["ALL"]</tt>, all facial attributes are returned, but the
--   operation takes longer to complete.
--   
--   If you provide both, <tt>["ALL", "DEFAULT"]</tt>, the service uses a
--   logical AND operator to determine which attributes to return (in this
--   case, all attributes).
[$sel:attributes:DetectFaces'] :: DetectFaces -> Maybe [Attribute]

-- | The input image as base64-encoded bytes or an S3 object. If you use
--   the AWS CLI to call Amazon Rekognition operations, passing
--   base64-encoded image bytes is not supported.
--   
--   If you are using an AWS SDK to call Amazon Rekognition, you might not
--   need to base64-encode image bytes passed using the <tt>Bytes</tt>
--   field. For more information, see Images in the Amazon Rekognition
--   developer guide.
[$sel:image:DetectFaces'] :: DetectFaces -> Image

-- | Create a value of <a>DetectFaces</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:attributes:DetectFaces'</a>, <a>detectFaces_attributes</a> -
--   An array of facial attributes you want to be returned. This can be the
--   default list of attributes or all attributes. If you don't specify a
--   value for <tt>Attributes</tt> or if you specify <tt>["DEFAULT"]</tt>,
--   the API returns the following subset of facial attributes:
--   <tt>BoundingBox</tt>, <tt>Confidence</tt>, <tt>Pose</tt>,
--   <tt>Quality</tt>, and <tt>Landmarks</tt>. If you provide
--   <tt>["ALL"]</tt>, all facial attributes are returned, but the
--   operation takes longer to complete.
--   
--   If you provide both, <tt>["ALL", "DEFAULT"]</tt>, the service uses a
--   logical AND operator to determine which attributes to return (in this
--   case, all attributes).
--   
--   <a>$sel:image:DetectFaces'</a>, <a>detectFaces_image</a> - The input
--   image as base64-encoded bytes or an S3 object. If you use the AWS CLI
--   to call Amazon Rekognition operations, passing base64-encoded image
--   bytes is not supported.
--   
--   If you are using an AWS SDK to call Amazon Rekognition, you might not
--   need to base64-encode image bytes passed using the <tt>Bytes</tt>
--   field. For more information, see Images in the Amazon Rekognition
--   developer guide.
newDetectFaces :: Image -> DetectFaces

-- | An array of facial attributes you want to be returned. This can be the
--   default list of attributes or all attributes. If you don't specify a
--   value for <tt>Attributes</tt> or if you specify <tt>["DEFAULT"]</tt>,
--   the API returns the following subset of facial attributes:
--   <tt>BoundingBox</tt>, <tt>Confidence</tt>, <tt>Pose</tt>,
--   <tt>Quality</tt>, and <tt>Landmarks</tt>. If you provide
--   <tt>["ALL"]</tt>, all facial attributes are returned, but the
--   operation takes longer to complete.
--   
--   If you provide both, <tt>["ALL", "DEFAULT"]</tt>, the service uses a
--   logical AND operator to determine which attributes to return (in this
--   case, all attributes).
detectFaces_attributes :: Lens' DetectFaces (Maybe [Attribute])

-- | The input image as base64-encoded bytes or an S3 object. If you use
--   the AWS CLI to call Amazon Rekognition operations, passing
--   base64-encoded image bytes is not supported.
--   
--   If you are using an AWS SDK to call Amazon Rekognition, you might not
--   need to base64-encode image bytes passed using the <tt>Bytes</tt>
--   field. For more information, see Images in the Amazon Rekognition
--   developer guide.
detectFaces_image :: Lens' DetectFaces Image

-- | <i>See:</i> <a>newDetectFacesResponse</a> smart constructor.
data DetectFacesResponse
DetectFacesResponse' :: Maybe OrientationCorrection -> Maybe [FaceDetail] -> Int -> DetectFacesResponse

-- | The value of <tt>OrientationCorrection</tt> is always null.
--   
--   If the input image is in .jpeg format, it might contain exchangeable
--   image file format (Exif) metadata that includes the image's
--   orientation. Amazon Rekognition uses this orientation information to
--   perform image correction. The bounding box coordinates are translated
--   to represent object locations after the orientation information in the
--   Exif metadata is used to correct the image orientation. Images in .png
--   format don't contain Exif metadata.
--   
--   Amazon Rekognition doesn’t perform image correction for images in .png
--   format and .jpeg images without orientation information in the image
--   Exif metadata. The bounding box coordinates aren't translated and
--   represent the object locations before the image is rotated.
[$sel:orientationCorrection:DetectFacesResponse'] :: DetectFacesResponse -> Maybe OrientationCorrection

-- | Details of each face found in the image.
[$sel:faceDetails:DetectFacesResponse'] :: DetectFacesResponse -> Maybe [FaceDetail]

-- | The response's http status code.
[$sel:httpStatus:DetectFacesResponse'] :: DetectFacesResponse -> Int

-- | Create a value of <a>DetectFacesResponse</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:orientationCorrection:DetectFacesResponse'</a>,
--   <a>detectFacesResponse_orientationCorrection</a> - The value of
--   <tt>OrientationCorrection</tt> is always null.
--   
--   If the input image is in .jpeg format, it might contain exchangeable
--   image file format (Exif) metadata that includes the image's
--   orientation. Amazon Rekognition uses this orientation information to
--   perform image correction. The bounding box coordinates are translated
--   to represent object locations after the orientation information in the
--   Exif metadata is used to correct the image orientation. Images in .png
--   format don't contain Exif metadata.
--   
--   Amazon Rekognition doesn’t perform image correction for images in .png
--   format and .jpeg images without orientation information in the image
--   Exif metadata. The bounding box coordinates aren't translated and
--   represent the object locations before the image is rotated.
--   
--   <a>$sel:faceDetails:DetectFacesResponse'</a>,
--   <a>detectFacesResponse_faceDetails</a> - Details of each face found in
--   the image.
--   
--   <a>$sel:httpStatus:DetectFacesResponse'</a>,
--   <a>detectFacesResponse_httpStatus</a> - The response's http status
--   code.
newDetectFacesResponse :: Int -> DetectFacesResponse

-- | The value of <tt>OrientationCorrection</tt> is always null.
--   
--   If the input image is in .jpeg format, it might contain exchangeable
--   image file format (Exif) metadata that includes the image's
--   orientation. Amazon Rekognition uses this orientation information to
--   perform image correction. The bounding box coordinates are translated
--   to represent object locations after the orientation information in the
--   Exif metadata is used to correct the image orientation. Images in .png
--   format don't contain Exif metadata.
--   
--   Amazon Rekognition doesn’t perform image correction for images in .png
--   format and .jpeg images without orientation information in the image
--   Exif metadata. The bounding box coordinates aren't translated and
--   represent the object locations before the image is rotated.
detectFacesResponse_orientationCorrection :: Lens' DetectFacesResponse (Maybe OrientationCorrection)

-- | Details of each face found in the image.
detectFacesResponse_faceDetails :: Lens' DetectFacesResponse (Maybe [FaceDetail])

-- | The response's http status code.
detectFacesResponse_httpStatus :: Lens' DetectFacesResponse Int
instance GHC.Generics.Generic Network.AWS.Rekognition.DetectFaces.DetectFaces
instance GHC.Show.Show Network.AWS.Rekognition.DetectFaces.DetectFaces
instance GHC.Read.Read Network.AWS.Rekognition.DetectFaces.DetectFaces
instance GHC.Classes.Eq Network.AWS.Rekognition.DetectFaces.DetectFaces
instance GHC.Generics.Generic Network.AWS.Rekognition.DetectFaces.DetectFacesResponse
instance GHC.Show.Show Network.AWS.Rekognition.DetectFaces.DetectFacesResponse
instance GHC.Read.Read Network.AWS.Rekognition.DetectFaces.DetectFacesResponse
instance GHC.Classes.Eq Network.AWS.Rekognition.DetectFaces.DetectFacesResponse
instance Network.AWS.Types.AWSRequest Network.AWS.Rekognition.DetectFaces.DetectFaces
instance Control.DeepSeq.NFData Network.AWS.Rekognition.DetectFaces.DetectFacesResponse
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.DetectFaces.DetectFaces
instance Control.DeepSeq.NFData Network.AWS.Rekognition.DetectFaces.DetectFaces
instance Network.AWS.Data.Headers.ToHeaders Network.AWS.Rekognition.DetectFaces.DetectFaces
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.DetectFaces.DetectFaces
instance Network.AWS.Data.Path.ToPath Network.AWS.Rekognition.DetectFaces.DetectFaces
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.DetectFaces.DetectFaces


-- | Detects custom labels in a supplied image by using an Amazon
--   Rekognition Custom Labels model.
--   
--   You specify which version of a model version to use by using the
--   <tt>ProjectVersionArn</tt> input parameter.
--   
--   You pass the input image as base64-encoded image bytes or as a
--   reference to an image in an Amazon S3 bucket. If you use the AWS CLI
--   to call Amazon Rekognition operations, passing image bytes is not
--   supported. The image must be either a PNG or JPEG formatted file.
--   
--   For each object that the model version detects on an image, the API
--   returns a (<tt>CustomLabel</tt>) object in an array
--   (<tt>CustomLabels</tt>). Each <tt>CustomLabel</tt> object provides the
--   label name (<tt>Name</tt>), the level of confidence that the image
--   contains the object (<tt>Confidence</tt>), and object location
--   information, if it exists, for the label on the image
--   (<tt>Geometry</tt>).
--   
--   To filter labels that are returned, specify a value for
--   <tt>MinConfidence</tt>. <tt>DetectCustomLabelsLabels</tt> only returns
--   labels with a confidence that's higher than the specified value. The
--   value of <tt>MinConfidence</tt> maps to the assumed threshold values
--   created during training. For more information, see <i>Assumed
--   threshold</i> in the Amazon Rekognition Custom Labels Developer Guide.
--   Amazon Rekognition Custom Labels metrics expresses an assumed
--   threshold as a floating point value between 0-1. The range of
--   <tt>MinConfidence</tt> normalizes the threshold value to a percentage
--   value (0-100). Confidence responses from <tt>DetectCustomLabels</tt>
--   are also returned as a percentage. You can use <tt>MinConfidence</tt>
--   to change the precision and recall or your model. For more
--   information, see <i>Analyzing an image</i> in the Amazon Rekognition
--   Custom Labels Developer Guide.
--   
--   If you don't specify a value for <tt>MinConfidence</tt>,
--   <tt>DetectCustomLabels</tt> returns labels based on the assumed
--   threshold of each label.
--   
--   This is a stateless API operation. That is, the operation does not
--   persist any data.
--   
--   This operation requires permissions to perform the
--   <tt>rekognition:DetectCustomLabels</tt> action.
--   
--   For more information, see <i>Analyzing an image</i> in the Amazon
--   Rekognition Custom Labels Developer Guide.
module Network.AWS.Rekognition.DetectCustomLabels

-- | <i>See:</i> <a>newDetectCustomLabels</a> smart constructor.
data DetectCustomLabels
DetectCustomLabels' :: Maybe Double -> Maybe Natural -> Text -> Image -> DetectCustomLabels

-- | Specifies the minimum confidence level for the labels to return.
--   <tt>DetectCustomLabels</tt> doesn't return any labels with a
--   confidence value that's lower than this specified value. If you
--   specify a value of 0, <tt>DetectCustomLabels</tt> returns all labels,
--   regardless of the assumed threshold applied to each label. If you
--   don't specify a value for <tt>MinConfidence</tt>,
--   <tt>DetectCustomLabels</tt> returns labels based on the assumed
--   threshold of each label.
[$sel:minConfidence:DetectCustomLabels'] :: DetectCustomLabels -> Maybe Double

-- | Maximum number of results you want the service to return in the
--   response. The service returns the specified number of highest
--   confidence labels ranked from highest confidence to lowest.
[$sel:maxResults:DetectCustomLabels'] :: DetectCustomLabels -> Maybe Natural

-- | The ARN of the model version that you want to use.
[$sel:projectVersionArn:DetectCustomLabels'] :: DetectCustomLabels -> Text
[$sel:image:DetectCustomLabels'] :: DetectCustomLabels -> Image

-- | Create a value of <a>DetectCustomLabels</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:minConfidence:DetectCustomLabels'</a>,
--   <a>detectCustomLabels_minConfidence</a> - Specifies the minimum
--   confidence level for the labels to return. <tt>DetectCustomLabels</tt>
--   doesn't return any labels with a confidence value that's lower than
--   this specified value. If you specify a value of 0,
--   <tt>DetectCustomLabels</tt> returns all labels, regardless of the
--   assumed threshold applied to each label. If you don't specify a value
--   for <tt>MinConfidence</tt>, <tt>DetectCustomLabels</tt> returns labels
--   based on the assumed threshold of each label.
--   
--   <a>$sel:maxResults:DetectCustomLabels'</a>,
--   <a>detectCustomLabels_maxResults</a> - Maximum number of results you
--   want the service to return in the response. The service returns the
--   specified number of highest confidence labels ranked from highest
--   confidence to lowest.
--   
--   <a>$sel:projectVersionArn:DetectCustomLabels'</a>,
--   <a>detectCustomLabels_projectVersionArn</a> - The ARN of the model
--   version that you want to use.
--   
--   <a>$sel:image:DetectCustomLabels'</a>, <a>detectCustomLabels_image</a>
--   - Undocumented member.
newDetectCustomLabels :: Text -> Image -> DetectCustomLabels

-- | Specifies the minimum confidence level for the labels to return.
--   <tt>DetectCustomLabels</tt> doesn't return any labels with a
--   confidence value that's lower than this specified value. If you
--   specify a value of 0, <tt>DetectCustomLabels</tt> returns all labels,
--   regardless of the assumed threshold applied to each label. If you
--   don't specify a value for <tt>MinConfidence</tt>,
--   <tt>DetectCustomLabels</tt> returns labels based on the assumed
--   threshold of each label.
detectCustomLabels_minConfidence :: Lens' DetectCustomLabels (Maybe Double)

-- | Maximum number of results you want the service to return in the
--   response. The service returns the specified number of highest
--   confidence labels ranked from highest confidence to lowest.
detectCustomLabels_maxResults :: Lens' DetectCustomLabels (Maybe Natural)

-- | The ARN of the model version that you want to use.
detectCustomLabels_projectVersionArn :: Lens' DetectCustomLabels Text

-- | Undocumented member.
detectCustomLabels_image :: Lens' DetectCustomLabels Image

-- | <i>See:</i> <a>newDetectCustomLabelsResponse</a> smart constructor.
data DetectCustomLabelsResponse
DetectCustomLabelsResponse' :: Maybe [CustomLabel] -> Int -> DetectCustomLabelsResponse

-- | An array of custom labels detected in the input image.
[$sel:customLabels:DetectCustomLabelsResponse'] :: DetectCustomLabelsResponse -> Maybe [CustomLabel]

-- | The response's http status code.
[$sel:httpStatus:DetectCustomLabelsResponse'] :: DetectCustomLabelsResponse -> Int

-- | Create a value of <a>DetectCustomLabelsResponse</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:customLabels:DetectCustomLabelsResponse'</a>,
--   <a>detectCustomLabelsResponse_customLabels</a> - An array of custom
--   labels detected in the input image.
--   
--   <a>$sel:httpStatus:DetectCustomLabelsResponse'</a>,
--   <a>detectCustomLabelsResponse_httpStatus</a> - The response's http
--   status code.
newDetectCustomLabelsResponse :: Int -> DetectCustomLabelsResponse

-- | An array of custom labels detected in the input image.
detectCustomLabelsResponse_customLabels :: Lens' DetectCustomLabelsResponse (Maybe [CustomLabel])

-- | The response's http status code.
detectCustomLabelsResponse_httpStatus :: Lens' DetectCustomLabelsResponse Int
instance GHC.Generics.Generic Network.AWS.Rekognition.DetectCustomLabels.DetectCustomLabels
instance GHC.Show.Show Network.AWS.Rekognition.DetectCustomLabels.DetectCustomLabels
instance GHC.Read.Read Network.AWS.Rekognition.DetectCustomLabels.DetectCustomLabels
instance GHC.Classes.Eq Network.AWS.Rekognition.DetectCustomLabels.DetectCustomLabels
instance GHC.Generics.Generic Network.AWS.Rekognition.DetectCustomLabels.DetectCustomLabelsResponse
instance GHC.Show.Show Network.AWS.Rekognition.DetectCustomLabels.DetectCustomLabelsResponse
instance GHC.Read.Read Network.AWS.Rekognition.DetectCustomLabels.DetectCustomLabelsResponse
instance GHC.Classes.Eq Network.AWS.Rekognition.DetectCustomLabels.DetectCustomLabelsResponse
instance Network.AWS.Types.AWSRequest Network.AWS.Rekognition.DetectCustomLabels.DetectCustomLabels
instance Control.DeepSeq.NFData Network.AWS.Rekognition.DetectCustomLabels.DetectCustomLabelsResponse
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.DetectCustomLabels.DetectCustomLabels
instance Control.DeepSeq.NFData Network.AWS.Rekognition.DetectCustomLabels.DetectCustomLabels
instance Network.AWS.Data.Headers.ToHeaders Network.AWS.Rekognition.DetectCustomLabels.DetectCustomLabels
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.DetectCustomLabels.DetectCustomLabels
instance Network.AWS.Data.Path.ToPath Network.AWS.Rekognition.DetectCustomLabels.DetectCustomLabels
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.DetectCustomLabels.DetectCustomLabels


-- | Provides information about a stream processor created by
--   CreateStreamProcessor. You can get information about the input and
--   output streams, the input parameters for the face recognition being
--   performed, and the current status of the stream processor.
module Network.AWS.Rekognition.DescribeStreamProcessor

-- | <i>See:</i> <a>newDescribeStreamProcessor</a> smart constructor.
data DescribeStreamProcessor
DescribeStreamProcessor' :: Text -> DescribeStreamProcessor

-- | Name of the stream processor for which you want information.
[$sel:name:DescribeStreamProcessor'] :: DescribeStreamProcessor -> Text

-- | Create a value of <a>DescribeStreamProcessor</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:name:DescribeStreamProcessor'</a>,
--   <a>describeStreamProcessor_name</a> - Name of the stream processor for
--   which you want information.
newDescribeStreamProcessor :: Text -> DescribeStreamProcessor

-- | Name of the stream processor for which you want information.
describeStreamProcessor_name :: Lens' DescribeStreamProcessor Text

-- | <i>See:</i> <a>newDescribeStreamProcessorResponse</a> smart
--   constructor.
data DescribeStreamProcessorResponse
DescribeStreamProcessorResponse' :: Maybe StreamProcessorStatus -> Maybe StreamProcessorSettings -> Maybe StreamProcessorInput -> Maybe StreamProcessorOutput -> Maybe Text -> Maybe Text -> Maybe Text -> Maybe POSIX -> Maybe POSIX -> Maybe Text -> Int -> DescribeStreamProcessorResponse

-- | Current status of the stream processor.
[$sel:status:DescribeStreamProcessorResponse'] :: DescribeStreamProcessorResponse -> Maybe StreamProcessorStatus

-- | Face recognition input parameters that are being used by the stream
--   processor. Includes the collection to use for face recognition and the
--   face attributes to detect.
[$sel:settings:DescribeStreamProcessorResponse'] :: DescribeStreamProcessorResponse -> Maybe StreamProcessorSettings

-- | Kinesis video stream that provides the source streaming video.
[$sel:input:DescribeStreamProcessorResponse'] :: DescribeStreamProcessorResponse -> Maybe StreamProcessorInput

-- | Kinesis data stream to which Amazon Rekognition Video puts the
--   analysis results.
[$sel:output:DescribeStreamProcessorResponse'] :: DescribeStreamProcessorResponse -> Maybe StreamProcessorOutput

-- | ARN of the stream processor.
[$sel:streamProcessorArn:DescribeStreamProcessorResponse'] :: DescribeStreamProcessorResponse -> Maybe Text

-- | Detailed status message about the stream processor.
[$sel:statusMessage:DescribeStreamProcessorResponse'] :: DescribeStreamProcessorResponse -> Maybe Text

-- | Name of the stream processor.
[$sel:name:DescribeStreamProcessorResponse'] :: DescribeStreamProcessorResponse -> Maybe Text

-- | Date and time the stream processor was created
[$sel:creationTimestamp:DescribeStreamProcessorResponse'] :: DescribeStreamProcessorResponse -> Maybe POSIX

-- | The time, in Unix format, the stream processor was last updated. For
--   example, when the stream processor moves from a running state to a
--   failed state, or when the user starts or stops the stream processor.
[$sel:lastUpdateTimestamp:DescribeStreamProcessorResponse'] :: DescribeStreamProcessorResponse -> Maybe POSIX

-- | ARN of the IAM role that allows access to the stream processor.
[$sel:roleArn:DescribeStreamProcessorResponse'] :: DescribeStreamProcessorResponse -> Maybe Text

-- | The response's http status code.
[$sel:httpStatus:DescribeStreamProcessorResponse'] :: DescribeStreamProcessorResponse -> Int

-- | Create a value of <a>DescribeStreamProcessorResponse</a> with all
--   optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:status:DescribeStreamProcessorResponse'</a>,
--   <a>describeStreamProcessorResponse_status</a> - Current status of the
--   stream processor.
--   
--   <a>$sel:settings:DescribeStreamProcessorResponse'</a>,
--   <a>describeStreamProcessorResponse_settings</a> - Face recognition
--   input parameters that are being used by the stream processor. Includes
--   the collection to use for face recognition and the face attributes to
--   detect.
--   
--   <a>$sel:input:DescribeStreamProcessorResponse'</a>,
--   <a>describeStreamProcessorResponse_input</a> - Kinesis video stream
--   that provides the source streaming video.
--   
--   <a>$sel:output:DescribeStreamProcessorResponse'</a>,
--   <a>describeStreamProcessorResponse_output</a> - Kinesis data stream to
--   which Amazon Rekognition Video puts the analysis results.
--   
--   <a>$sel:streamProcessorArn:DescribeStreamProcessorResponse'</a>,
--   <a>describeStreamProcessorResponse_streamProcessorArn</a> - ARN of the
--   stream processor.
--   
--   <a>$sel:statusMessage:DescribeStreamProcessorResponse'</a>,
--   <a>describeStreamProcessorResponse_statusMessage</a> - Detailed status
--   message about the stream processor.
--   
--   <a>$sel:name:DescribeStreamProcessor'</a>,
--   <a>describeStreamProcessorResponse_name</a> - Name of the stream
--   processor.
--   
--   <a>$sel:creationTimestamp:DescribeStreamProcessorResponse'</a>,
--   <a>describeStreamProcessorResponse_creationTimestamp</a> - Date and
--   time the stream processor was created
--   
--   <a>$sel:lastUpdateTimestamp:DescribeStreamProcessorResponse'</a>,
--   <a>describeStreamProcessorResponse_lastUpdateTimestamp</a> - The time,
--   in Unix format, the stream processor was last updated. For example,
--   when the stream processor moves from a running state to a failed
--   state, or when the user starts or stops the stream processor.
--   
--   <a>$sel:roleArn:DescribeStreamProcessorResponse'</a>,
--   <a>describeStreamProcessorResponse_roleArn</a> - ARN of the IAM role
--   that allows access to the stream processor.
--   
--   <a>$sel:httpStatus:DescribeStreamProcessorResponse'</a>,
--   <a>describeStreamProcessorResponse_httpStatus</a> - The response's
--   http status code.
newDescribeStreamProcessorResponse :: Int -> DescribeStreamProcessorResponse

-- | Current status of the stream processor.
describeStreamProcessorResponse_status :: Lens' DescribeStreamProcessorResponse (Maybe StreamProcessorStatus)

-- | Face recognition input parameters that are being used by the stream
--   processor. Includes the collection to use for face recognition and the
--   face attributes to detect.
describeStreamProcessorResponse_settings :: Lens' DescribeStreamProcessorResponse (Maybe StreamProcessorSettings)

-- | Kinesis video stream that provides the source streaming video.
describeStreamProcessorResponse_input :: Lens' DescribeStreamProcessorResponse (Maybe StreamProcessorInput)

-- | Kinesis data stream to which Amazon Rekognition Video puts the
--   analysis results.
describeStreamProcessorResponse_output :: Lens' DescribeStreamProcessorResponse (Maybe StreamProcessorOutput)

-- | ARN of the stream processor.
describeStreamProcessorResponse_streamProcessorArn :: Lens' DescribeStreamProcessorResponse (Maybe Text)

-- | Detailed status message about the stream processor.
describeStreamProcessorResponse_statusMessage :: Lens' DescribeStreamProcessorResponse (Maybe Text)

-- | Name of the stream processor.
describeStreamProcessorResponse_name :: Lens' DescribeStreamProcessorResponse (Maybe Text)

-- | Date and time the stream processor was created
describeStreamProcessorResponse_creationTimestamp :: Lens' DescribeStreamProcessorResponse (Maybe UTCTime)

-- | The time, in Unix format, the stream processor was last updated. For
--   example, when the stream processor moves from a running state to a
--   failed state, or when the user starts or stops the stream processor.
describeStreamProcessorResponse_lastUpdateTimestamp :: Lens' DescribeStreamProcessorResponse (Maybe UTCTime)

-- | ARN of the IAM role that allows access to the stream processor.
describeStreamProcessorResponse_roleArn :: Lens' DescribeStreamProcessorResponse (Maybe Text)

-- | The response's http status code.
describeStreamProcessorResponse_httpStatus :: Lens' DescribeStreamProcessorResponse Int
instance GHC.Generics.Generic Network.AWS.Rekognition.DescribeStreamProcessor.DescribeStreamProcessor
instance GHC.Show.Show Network.AWS.Rekognition.DescribeStreamProcessor.DescribeStreamProcessor
instance GHC.Read.Read Network.AWS.Rekognition.DescribeStreamProcessor.DescribeStreamProcessor
instance GHC.Classes.Eq Network.AWS.Rekognition.DescribeStreamProcessor.DescribeStreamProcessor
instance GHC.Generics.Generic Network.AWS.Rekognition.DescribeStreamProcessor.DescribeStreamProcessorResponse
instance GHC.Show.Show Network.AWS.Rekognition.DescribeStreamProcessor.DescribeStreamProcessorResponse
instance GHC.Read.Read Network.AWS.Rekognition.DescribeStreamProcessor.DescribeStreamProcessorResponse
instance GHC.Classes.Eq Network.AWS.Rekognition.DescribeStreamProcessor.DescribeStreamProcessorResponse
instance Network.AWS.Types.AWSRequest Network.AWS.Rekognition.DescribeStreamProcessor.DescribeStreamProcessor
instance Control.DeepSeq.NFData Network.AWS.Rekognition.DescribeStreamProcessor.DescribeStreamProcessorResponse
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.DescribeStreamProcessor.DescribeStreamProcessor
instance Control.DeepSeq.NFData Network.AWS.Rekognition.DescribeStreamProcessor.DescribeStreamProcessor
instance Network.AWS.Data.Headers.ToHeaders Network.AWS.Rekognition.DescribeStreamProcessor.DescribeStreamProcessor
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.DescribeStreamProcessor.DescribeStreamProcessor
instance Network.AWS.Data.Path.ToPath Network.AWS.Rekognition.DescribeStreamProcessor.DescribeStreamProcessor
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.DescribeStreamProcessor.DescribeStreamProcessor


-- | Lists and gets information about your Amazon Rekognition Custom Labels
--   projects.
--   
--   This operation requires permissions to perform the
--   <tt>rekognition:DescribeProjects</tt> action.
--   
--   This operation returns paginated results.
module Network.AWS.Rekognition.DescribeProjects

-- | <i>See:</i> <a>newDescribeProjects</a> smart constructor.
data DescribeProjects
DescribeProjects' :: Maybe Text -> Maybe Natural -> DescribeProjects

-- | If the previous response was incomplete (because there is more results
--   to retrieve), Amazon Rekognition Custom Labels returns a pagination
--   token in the response. You can use this pagination token to retrieve
--   the next set of results.
[$sel:nextToken:DescribeProjects'] :: DescribeProjects -> Maybe Text

-- | The maximum number of results to return per paginated call. The
--   largest value you can specify is 100. If you specify a value greater
--   than 100, a ValidationException error occurs. The default value is
--   100.
[$sel:maxResults:DescribeProjects'] :: DescribeProjects -> Maybe Natural

-- | Create a value of <a>DescribeProjects</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:nextToken:DescribeProjects'</a>,
--   <a>describeProjects_nextToken</a> - If the previous response was
--   incomplete (because there is more results to retrieve), Amazon
--   Rekognition Custom Labels returns a pagination token in the response.
--   You can use this pagination token to retrieve the next set of results.
--   
--   <a>$sel:maxResults:DescribeProjects'</a>,
--   <a>describeProjects_maxResults</a> - The maximum number of results to
--   return per paginated call. The largest value you can specify is 100.
--   If you specify a value greater than 100, a ValidationException error
--   occurs. The default value is 100.
newDescribeProjects :: DescribeProjects

-- | If the previous response was incomplete (because there is more results
--   to retrieve), Amazon Rekognition Custom Labels returns a pagination
--   token in the response. You can use this pagination token to retrieve
--   the next set of results.
describeProjects_nextToken :: Lens' DescribeProjects (Maybe Text)

-- | The maximum number of results to return per paginated call. The
--   largest value you can specify is 100. If you specify a value greater
--   than 100, a ValidationException error occurs. The default value is
--   100.
describeProjects_maxResults :: Lens' DescribeProjects (Maybe Natural)

-- | <i>See:</i> <a>newDescribeProjectsResponse</a> smart constructor.
data DescribeProjectsResponse
DescribeProjectsResponse' :: Maybe Text -> Maybe [ProjectDescription] -> Int -> DescribeProjectsResponse

-- | If the previous response was incomplete (because there is more results
--   to retrieve), Amazon Rekognition Custom Labels returns a pagination
--   token in the response. You can use this pagination token to retrieve
--   the next set of results.
[$sel:nextToken:DescribeProjectsResponse'] :: DescribeProjectsResponse -> Maybe Text

-- | A list of project descriptions. The list is sorted by the date and
--   time the projects are created.
[$sel:projectDescriptions:DescribeProjectsResponse'] :: DescribeProjectsResponse -> Maybe [ProjectDescription]

-- | The response's http status code.
[$sel:httpStatus:DescribeProjectsResponse'] :: DescribeProjectsResponse -> Int

-- | Create a value of <a>DescribeProjectsResponse</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:nextToken:DescribeProjects'</a>,
--   <a>describeProjectsResponse_nextToken</a> - If the previous response
--   was incomplete (because there is more results to retrieve), Amazon
--   Rekognition Custom Labels returns a pagination token in the response.
--   You can use this pagination token to retrieve the next set of results.
--   
--   <a>$sel:projectDescriptions:DescribeProjectsResponse'</a>,
--   <a>describeProjectsResponse_projectDescriptions</a> - A list of
--   project descriptions. The list is sorted by the date and time the
--   projects are created.
--   
--   <a>$sel:httpStatus:DescribeProjectsResponse'</a>,
--   <a>describeProjectsResponse_httpStatus</a> - The response's http
--   status code.
newDescribeProjectsResponse :: Int -> DescribeProjectsResponse

-- | If the previous response was incomplete (because there is more results
--   to retrieve), Amazon Rekognition Custom Labels returns a pagination
--   token in the response. You can use this pagination token to retrieve
--   the next set of results.
describeProjectsResponse_nextToken :: Lens' DescribeProjectsResponse (Maybe Text)

-- | A list of project descriptions. The list is sorted by the date and
--   time the projects are created.
describeProjectsResponse_projectDescriptions :: Lens' DescribeProjectsResponse (Maybe [ProjectDescription])

-- | The response's http status code.
describeProjectsResponse_httpStatus :: Lens' DescribeProjectsResponse Int
instance GHC.Generics.Generic Network.AWS.Rekognition.DescribeProjects.DescribeProjects
instance GHC.Show.Show Network.AWS.Rekognition.DescribeProjects.DescribeProjects
instance GHC.Read.Read Network.AWS.Rekognition.DescribeProjects.DescribeProjects
instance GHC.Classes.Eq Network.AWS.Rekognition.DescribeProjects.DescribeProjects
instance GHC.Generics.Generic Network.AWS.Rekognition.DescribeProjects.DescribeProjectsResponse
instance GHC.Show.Show Network.AWS.Rekognition.DescribeProjects.DescribeProjectsResponse
instance GHC.Read.Read Network.AWS.Rekognition.DescribeProjects.DescribeProjectsResponse
instance GHC.Classes.Eq Network.AWS.Rekognition.DescribeProjects.DescribeProjectsResponse
instance Network.AWS.Types.AWSRequest Network.AWS.Rekognition.DescribeProjects.DescribeProjects
instance Control.DeepSeq.NFData Network.AWS.Rekognition.DescribeProjects.DescribeProjectsResponse
instance Network.AWS.Pager.AWSPager Network.AWS.Rekognition.DescribeProjects.DescribeProjects
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.DescribeProjects.DescribeProjects
instance Control.DeepSeq.NFData Network.AWS.Rekognition.DescribeProjects.DescribeProjects
instance Network.AWS.Data.Headers.ToHeaders Network.AWS.Rekognition.DescribeProjects.DescribeProjects
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.DescribeProjects.DescribeProjects
instance Network.AWS.Data.Path.ToPath Network.AWS.Rekognition.DescribeProjects.DescribeProjects
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.DescribeProjects.DescribeProjects


-- | Lists and describes the models in an Amazon Rekognition Custom Labels
--   project. You can specify up to 10 model versions in
--   <tt>ProjectVersionArns</tt>. If you don't specify a value,
--   descriptions for all models are returned.
--   
--   This operation requires permissions to perform the
--   <tt>rekognition:DescribeProjectVersions</tt> action.
--   
--   This operation returns paginated results.
module Network.AWS.Rekognition.DescribeProjectVersions

-- | <i>See:</i> <a>newDescribeProjectVersions</a> smart constructor.
data DescribeProjectVersions
DescribeProjectVersions' :: Maybe Text -> Maybe (NonEmpty Text) -> Maybe Natural -> Text -> DescribeProjectVersions

-- | If the previous response was incomplete (because there is more results
--   to retrieve), Amazon Rekognition Custom Labels returns a pagination
--   token in the response. You can use this pagination token to retrieve
--   the next set of results.
[$sel:nextToken:DescribeProjectVersions'] :: DescribeProjectVersions -> Maybe Text

-- | A list of model version names that you want to describe. You can add
--   up to 10 model version names to the list. If you don't specify a
--   value, all model descriptions are returned. A version name is part of
--   a model (ProjectVersion) ARN. For example,
--   <tt>my-model.2020-01-21T09.10.15</tt> is the version name in the
--   following ARN.
--   <tt>arn:aws:rekognition:us-east-1:123456789012:project/getting-started/version/my-model.2020-01-21T09.10.15/1234567890123</tt>.
[$sel:versionNames:DescribeProjectVersions'] :: DescribeProjectVersions -> Maybe (NonEmpty Text)

-- | The maximum number of results to return per paginated call. The
--   largest value you can specify is 100. If you specify a value greater
--   than 100, a ValidationException error occurs. The default value is
--   100.
[$sel:maxResults:DescribeProjectVersions'] :: DescribeProjectVersions -> Maybe Natural

-- | The Amazon Resource Name (ARN) of the project that contains the models
--   you want to describe.
[$sel:projectArn:DescribeProjectVersions'] :: DescribeProjectVersions -> Text

-- | Create a value of <a>DescribeProjectVersions</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:nextToken:DescribeProjectVersions'</a>,
--   <a>describeProjectVersions_nextToken</a> - If the previous response
--   was incomplete (because there is more results to retrieve), Amazon
--   Rekognition Custom Labels returns a pagination token in the response.
--   You can use this pagination token to retrieve the next set of results.
--   
--   <a>$sel:versionNames:DescribeProjectVersions'</a>,
--   <a>describeProjectVersions_versionNames</a> - A list of model version
--   names that you want to describe. You can add up to 10 model version
--   names to the list. If you don't specify a value, all model
--   descriptions are returned. A version name is part of a model
--   (ProjectVersion) ARN. For example,
--   <tt>my-model.2020-01-21T09.10.15</tt> is the version name in the
--   following ARN.
--   <tt>arn:aws:rekognition:us-east-1:123456789012:project/getting-started/version/my-model.2020-01-21T09.10.15/1234567890123</tt>.
--   
--   <a>$sel:maxResults:DescribeProjectVersions'</a>,
--   <a>describeProjectVersions_maxResults</a> - The maximum number of
--   results to return per paginated call. The largest value you can
--   specify is 100. If you specify a value greater than 100, a
--   ValidationException error occurs. The default value is 100.
--   
--   <a>$sel:projectArn:DescribeProjectVersions'</a>,
--   <a>describeProjectVersions_projectArn</a> - The Amazon Resource Name
--   (ARN) of the project that contains the models you want to describe.
newDescribeProjectVersions :: Text -> DescribeProjectVersions

-- | If the previous response was incomplete (because there is more results
--   to retrieve), Amazon Rekognition Custom Labels returns a pagination
--   token in the response. You can use this pagination token to retrieve
--   the next set of results.
describeProjectVersions_nextToken :: Lens' DescribeProjectVersions (Maybe Text)

-- | A list of model version names that you want to describe. You can add
--   up to 10 model version names to the list. If you don't specify a
--   value, all model descriptions are returned. A version name is part of
--   a model (ProjectVersion) ARN. For example,
--   <tt>my-model.2020-01-21T09.10.15</tt> is the version name in the
--   following ARN.
--   <tt>arn:aws:rekognition:us-east-1:123456789012:project/getting-started/version/my-model.2020-01-21T09.10.15/1234567890123</tt>.
describeProjectVersions_versionNames :: Lens' DescribeProjectVersions (Maybe (NonEmpty Text))

-- | The maximum number of results to return per paginated call. The
--   largest value you can specify is 100. If you specify a value greater
--   than 100, a ValidationException error occurs. The default value is
--   100.
describeProjectVersions_maxResults :: Lens' DescribeProjectVersions (Maybe Natural)

-- | The Amazon Resource Name (ARN) of the project that contains the models
--   you want to describe.
describeProjectVersions_projectArn :: Lens' DescribeProjectVersions Text

-- | <i>See:</i> <a>newDescribeProjectVersionsResponse</a> smart
--   constructor.
data DescribeProjectVersionsResponse
DescribeProjectVersionsResponse' :: Maybe Text -> Maybe [ProjectVersionDescription] -> Int -> DescribeProjectVersionsResponse

-- | If the previous response was incomplete (because there is more results
--   to retrieve), Amazon Rekognition Custom Labels returns a pagination
--   token in the response. You can use this pagination token to retrieve
--   the next set of results.
[$sel:nextToken:DescribeProjectVersionsResponse'] :: DescribeProjectVersionsResponse -> Maybe Text

-- | A list of model descriptions. The list is sorted by the creation date
--   and time of the model versions, latest to earliest.
[$sel:projectVersionDescriptions:DescribeProjectVersionsResponse'] :: DescribeProjectVersionsResponse -> Maybe [ProjectVersionDescription]

-- | The response's http status code.
[$sel:httpStatus:DescribeProjectVersionsResponse'] :: DescribeProjectVersionsResponse -> Int

-- | Create a value of <a>DescribeProjectVersionsResponse</a> with all
--   optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:nextToken:DescribeProjectVersions'</a>,
--   <a>describeProjectVersionsResponse_nextToken</a> - If the previous
--   response was incomplete (because there is more results to retrieve),
--   Amazon Rekognition Custom Labels returns a pagination token in the
--   response. You can use this pagination token to retrieve the next set
--   of results.
--   
--   
--   <a>$sel:projectVersionDescriptions:DescribeProjectVersionsResponse'</a>,
--   <a>describeProjectVersionsResponse_projectVersionDescriptions</a> - A
--   list of model descriptions. The list is sorted by the creation date
--   and time of the model versions, latest to earliest.
--   
--   <a>$sel:httpStatus:DescribeProjectVersionsResponse'</a>,
--   <a>describeProjectVersionsResponse_httpStatus</a> - The response's
--   http status code.
newDescribeProjectVersionsResponse :: Int -> DescribeProjectVersionsResponse

-- | If the previous response was incomplete (because there is more results
--   to retrieve), Amazon Rekognition Custom Labels returns a pagination
--   token in the response. You can use this pagination token to retrieve
--   the next set of results.
describeProjectVersionsResponse_nextToken :: Lens' DescribeProjectVersionsResponse (Maybe Text)

-- | A list of model descriptions. The list is sorted by the creation date
--   and time of the model versions, latest to earliest.
describeProjectVersionsResponse_projectVersionDescriptions :: Lens' DescribeProjectVersionsResponse (Maybe [ProjectVersionDescription])

-- | The response's http status code.
describeProjectVersionsResponse_httpStatus :: Lens' DescribeProjectVersionsResponse Int
instance GHC.Generics.Generic Network.AWS.Rekognition.DescribeProjectVersions.DescribeProjectVersions
instance GHC.Show.Show Network.AWS.Rekognition.DescribeProjectVersions.DescribeProjectVersions
instance GHC.Read.Read Network.AWS.Rekognition.DescribeProjectVersions.DescribeProjectVersions
instance GHC.Classes.Eq Network.AWS.Rekognition.DescribeProjectVersions.DescribeProjectVersions
instance GHC.Generics.Generic Network.AWS.Rekognition.DescribeProjectVersions.DescribeProjectVersionsResponse
instance GHC.Show.Show Network.AWS.Rekognition.DescribeProjectVersions.DescribeProjectVersionsResponse
instance GHC.Read.Read Network.AWS.Rekognition.DescribeProjectVersions.DescribeProjectVersionsResponse
instance GHC.Classes.Eq Network.AWS.Rekognition.DescribeProjectVersions.DescribeProjectVersionsResponse
instance Network.AWS.Types.AWSRequest Network.AWS.Rekognition.DescribeProjectVersions.DescribeProjectVersions
instance Control.DeepSeq.NFData Network.AWS.Rekognition.DescribeProjectVersions.DescribeProjectVersionsResponse
instance Network.AWS.Pager.AWSPager Network.AWS.Rekognition.DescribeProjectVersions.DescribeProjectVersions
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.DescribeProjectVersions.DescribeProjectVersions
instance Control.DeepSeq.NFData Network.AWS.Rekognition.DescribeProjectVersions.DescribeProjectVersions
instance Network.AWS.Data.Headers.ToHeaders Network.AWS.Rekognition.DescribeProjectVersions.DescribeProjectVersions
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.DescribeProjectVersions.DescribeProjectVersions
instance Network.AWS.Data.Path.ToPath Network.AWS.Rekognition.DescribeProjectVersions.DescribeProjectVersions
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.DescribeProjectVersions.DescribeProjectVersions


-- | Describes the specified collection. You can use
--   <tt>DescribeCollection</tt> to get information, such as the number of
--   faces indexed into a collection and the version of the model used by
--   the collection for face detection.
--   
--   For more information, see Describing a Collection in the Amazon
--   Rekognition Developer Guide.
module Network.AWS.Rekognition.DescribeCollection

-- | <i>See:</i> <a>newDescribeCollection</a> smart constructor.
data DescribeCollection
DescribeCollection' :: Text -> DescribeCollection

-- | The ID of the collection to describe.
[$sel:collectionId:DescribeCollection'] :: DescribeCollection -> Text

-- | Create a value of <a>DescribeCollection</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:collectionId:DescribeCollection'</a>,
--   <a>describeCollection_collectionId</a> - The ID of the collection to
--   describe.
newDescribeCollection :: Text -> DescribeCollection

-- | The ID of the collection to describe.
describeCollection_collectionId :: Lens' DescribeCollection Text

-- | <i>See:</i> <a>newDescribeCollectionResponse</a> smart constructor.
data DescribeCollectionResponse
DescribeCollectionResponse' :: Maybe Text -> Maybe Natural -> Maybe POSIX -> Maybe Text -> Int -> DescribeCollectionResponse

-- | The version of the face model that's used by the collection for face
--   detection.
--   
--   For more information, see Model Versioning in the Amazon Rekognition
--   Developer Guide.
[$sel:faceModelVersion:DescribeCollectionResponse'] :: DescribeCollectionResponse -> Maybe Text

-- | The number of faces that are indexed into the collection. To index
--   faces into a collection, use IndexFaces.
[$sel:faceCount:DescribeCollectionResponse'] :: DescribeCollectionResponse -> Maybe Natural

-- | The number of milliseconds since the Unix epoch time until the
--   creation of the collection. The Unix epoch time is 00:00:00
--   Coordinated Universal Time (UTC), Thursday, 1 January 1970.
[$sel:creationTimestamp:DescribeCollectionResponse'] :: DescribeCollectionResponse -> Maybe POSIX

-- | The Amazon Resource Name (ARN) of the collection.
[$sel:collectionARN:DescribeCollectionResponse'] :: DescribeCollectionResponse -> Maybe Text

-- | The response's http status code.
[$sel:httpStatus:DescribeCollectionResponse'] :: DescribeCollectionResponse -> Int

-- | Create a value of <a>DescribeCollectionResponse</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:faceModelVersion:DescribeCollectionResponse'</a>,
--   <a>describeCollectionResponse_faceModelVersion</a> - The version of
--   the face model that's used by the collection for face detection.
--   
--   For more information, see Model Versioning in the Amazon Rekognition
--   Developer Guide.
--   
--   <a>$sel:faceCount:DescribeCollectionResponse'</a>,
--   <a>describeCollectionResponse_faceCount</a> - The number of faces that
--   are indexed into the collection. To index faces into a collection, use
--   IndexFaces.
--   
--   <a>$sel:creationTimestamp:DescribeCollectionResponse'</a>,
--   <a>describeCollectionResponse_creationTimestamp</a> - The number of
--   milliseconds since the Unix epoch time until the creation of the
--   collection. The Unix epoch time is 00:00:00 Coordinated Universal Time
--   (UTC), Thursday, 1 January 1970.
--   
--   <a>$sel:collectionARN:DescribeCollectionResponse'</a>,
--   <a>describeCollectionResponse_collectionARN</a> - The Amazon Resource
--   Name (ARN) of the collection.
--   
--   <a>$sel:httpStatus:DescribeCollectionResponse'</a>,
--   <a>describeCollectionResponse_httpStatus</a> - The response's http
--   status code.
newDescribeCollectionResponse :: Int -> DescribeCollectionResponse

-- | The version of the face model that's used by the collection for face
--   detection.
--   
--   For more information, see Model Versioning in the Amazon Rekognition
--   Developer Guide.
describeCollectionResponse_faceModelVersion :: Lens' DescribeCollectionResponse (Maybe Text)

-- | The number of faces that are indexed into the collection. To index
--   faces into a collection, use IndexFaces.
describeCollectionResponse_faceCount :: Lens' DescribeCollectionResponse (Maybe Natural)

-- | The number of milliseconds since the Unix epoch time until the
--   creation of the collection. The Unix epoch time is 00:00:00
--   Coordinated Universal Time (UTC), Thursday, 1 January 1970.
describeCollectionResponse_creationTimestamp :: Lens' DescribeCollectionResponse (Maybe UTCTime)

-- | The Amazon Resource Name (ARN) of the collection.
describeCollectionResponse_collectionARN :: Lens' DescribeCollectionResponse (Maybe Text)

-- | The response's http status code.
describeCollectionResponse_httpStatus :: Lens' DescribeCollectionResponse Int
instance GHC.Generics.Generic Network.AWS.Rekognition.DescribeCollection.DescribeCollection
instance GHC.Show.Show Network.AWS.Rekognition.DescribeCollection.DescribeCollection
instance GHC.Read.Read Network.AWS.Rekognition.DescribeCollection.DescribeCollection
instance GHC.Classes.Eq Network.AWS.Rekognition.DescribeCollection.DescribeCollection
instance GHC.Generics.Generic Network.AWS.Rekognition.DescribeCollection.DescribeCollectionResponse
instance GHC.Show.Show Network.AWS.Rekognition.DescribeCollection.DescribeCollectionResponse
instance GHC.Read.Read Network.AWS.Rekognition.DescribeCollection.DescribeCollectionResponse
instance GHC.Classes.Eq Network.AWS.Rekognition.DescribeCollection.DescribeCollectionResponse
instance Network.AWS.Types.AWSRequest Network.AWS.Rekognition.DescribeCollection.DescribeCollection
instance Control.DeepSeq.NFData Network.AWS.Rekognition.DescribeCollection.DescribeCollectionResponse
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.DescribeCollection.DescribeCollection
instance Control.DeepSeq.NFData Network.AWS.Rekognition.DescribeCollection.DescribeCollection
instance Network.AWS.Data.Headers.ToHeaders Network.AWS.Rekognition.DescribeCollection.DescribeCollection
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.DescribeCollection.DescribeCollection
instance Network.AWS.Data.Path.ToPath Network.AWS.Rekognition.DescribeCollection.DescribeCollection
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.DescribeCollection.DescribeCollection


-- | Deletes the stream processor identified by <tt>Name</tt>. You assign
--   the value for <tt>Name</tt> when you create the stream processor with
--   CreateStreamProcessor. You might not be able to use the same name for
--   a stream processor for a few seconds after calling
--   <tt>DeleteStreamProcessor</tt>.
module Network.AWS.Rekognition.DeleteStreamProcessor

-- | <i>See:</i> <a>newDeleteStreamProcessor</a> smart constructor.
data DeleteStreamProcessor
DeleteStreamProcessor' :: Text -> DeleteStreamProcessor

-- | The name of the stream processor you want to delete.
[$sel:name:DeleteStreamProcessor'] :: DeleteStreamProcessor -> Text

-- | Create a value of <a>DeleteStreamProcessor</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:name:DeleteStreamProcessor'</a>,
--   <a>deleteStreamProcessor_name</a> - The name of the stream processor
--   you want to delete.
newDeleteStreamProcessor :: Text -> DeleteStreamProcessor

-- | The name of the stream processor you want to delete.
deleteStreamProcessor_name :: Lens' DeleteStreamProcessor Text

-- | <i>See:</i> <a>newDeleteStreamProcessorResponse</a> smart constructor.
data DeleteStreamProcessorResponse
DeleteStreamProcessorResponse' :: Int -> DeleteStreamProcessorResponse

-- | The response's http status code.
[$sel:httpStatus:DeleteStreamProcessorResponse'] :: DeleteStreamProcessorResponse -> Int

-- | Create a value of <a>DeleteStreamProcessorResponse</a> with all
--   optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:httpStatus:DeleteStreamProcessorResponse'</a>,
--   <a>deleteStreamProcessorResponse_httpStatus</a> - The response's http
--   status code.
newDeleteStreamProcessorResponse :: Int -> DeleteStreamProcessorResponse

-- | The response's http status code.
deleteStreamProcessorResponse_httpStatus :: Lens' DeleteStreamProcessorResponse Int
instance GHC.Generics.Generic Network.AWS.Rekognition.DeleteStreamProcessor.DeleteStreamProcessor
instance GHC.Show.Show Network.AWS.Rekognition.DeleteStreamProcessor.DeleteStreamProcessor
instance GHC.Read.Read Network.AWS.Rekognition.DeleteStreamProcessor.DeleteStreamProcessor
instance GHC.Classes.Eq Network.AWS.Rekognition.DeleteStreamProcessor.DeleteStreamProcessor
instance GHC.Generics.Generic Network.AWS.Rekognition.DeleteStreamProcessor.DeleteStreamProcessorResponse
instance GHC.Show.Show Network.AWS.Rekognition.DeleteStreamProcessor.DeleteStreamProcessorResponse
instance GHC.Read.Read Network.AWS.Rekognition.DeleteStreamProcessor.DeleteStreamProcessorResponse
instance GHC.Classes.Eq Network.AWS.Rekognition.DeleteStreamProcessor.DeleteStreamProcessorResponse
instance Network.AWS.Types.AWSRequest Network.AWS.Rekognition.DeleteStreamProcessor.DeleteStreamProcessor
instance Control.DeepSeq.NFData Network.AWS.Rekognition.DeleteStreamProcessor.DeleteStreamProcessorResponse
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.DeleteStreamProcessor.DeleteStreamProcessor
instance Control.DeepSeq.NFData Network.AWS.Rekognition.DeleteStreamProcessor.DeleteStreamProcessor
instance Network.AWS.Data.Headers.ToHeaders Network.AWS.Rekognition.DeleteStreamProcessor.DeleteStreamProcessor
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.DeleteStreamProcessor.DeleteStreamProcessor
instance Network.AWS.Data.Path.ToPath Network.AWS.Rekognition.DeleteStreamProcessor.DeleteStreamProcessor
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.DeleteStreamProcessor.DeleteStreamProcessor


-- | Deletes an Amazon Rekognition Custom Labels model.
--   
--   You can't delete a model if it is running or if it is training. To
--   check the status of a model, use the <tt>Status</tt> field returned
--   from DescribeProjectVersions. To stop a running model call
--   StopProjectVersion. If the model is training, wait until it finishes.
--   
--   This operation requires permissions to perform the
--   <tt>rekognition:DeleteProjectVersion</tt> action.
module Network.AWS.Rekognition.DeleteProjectVersion

-- | <i>See:</i> <a>newDeleteProjectVersion</a> smart constructor.
data DeleteProjectVersion
DeleteProjectVersion' :: Text -> DeleteProjectVersion

-- | The Amazon Resource Name (ARN) of the model version that you want to
--   delete.
[$sel:projectVersionArn:DeleteProjectVersion'] :: DeleteProjectVersion -> Text

-- | Create a value of <a>DeleteProjectVersion</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:projectVersionArn:DeleteProjectVersion'</a>,
--   <a>deleteProjectVersion_projectVersionArn</a> - The Amazon Resource
--   Name (ARN) of the model version that you want to delete.
newDeleteProjectVersion :: Text -> DeleteProjectVersion

-- | The Amazon Resource Name (ARN) of the model version that you want to
--   delete.
deleteProjectVersion_projectVersionArn :: Lens' DeleteProjectVersion Text

-- | <i>See:</i> <a>newDeleteProjectVersionResponse</a> smart constructor.
data DeleteProjectVersionResponse
DeleteProjectVersionResponse' :: Maybe ProjectVersionStatus -> Int -> DeleteProjectVersionResponse

-- | The status of the deletion operation.
[$sel:status:DeleteProjectVersionResponse'] :: DeleteProjectVersionResponse -> Maybe ProjectVersionStatus

-- | The response's http status code.
[$sel:httpStatus:DeleteProjectVersionResponse'] :: DeleteProjectVersionResponse -> Int

-- | Create a value of <a>DeleteProjectVersionResponse</a> with all
--   optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:status:DeleteProjectVersionResponse'</a>,
--   <a>deleteProjectVersionResponse_status</a> - The status of the
--   deletion operation.
--   
--   <a>$sel:httpStatus:DeleteProjectVersionResponse'</a>,
--   <a>deleteProjectVersionResponse_httpStatus</a> - The response's http
--   status code.
newDeleteProjectVersionResponse :: Int -> DeleteProjectVersionResponse

-- | The status of the deletion operation.
deleteProjectVersionResponse_status :: Lens' DeleteProjectVersionResponse (Maybe ProjectVersionStatus)

-- | The response's http status code.
deleteProjectVersionResponse_httpStatus :: Lens' DeleteProjectVersionResponse Int
instance GHC.Generics.Generic Network.AWS.Rekognition.DeleteProjectVersion.DeleteProjectVersion
instance GHC.Show.Show Network.AWS.Rekognition.DeleteProjectVersion.DeleteProjectVersion
instance GHC.Read.Read Network.AWS.Rekognition.DeleteProjectVersion.DeleteProjectVersion
instance GHC.Classes.Eq Network.AWS.Rekognition.DeleteProjectVersion.DeleteProjectVersion
instance GHC.Generics.Generic Network.AWS.Rekognition.DeleteProjectVersion.DeleteProjectVersionResponse
instance GHC.Show.Show Network.AWS.Rekognition.DeleteProjectVersion.DeleteProjectVersionResponse
instance GHC.Read.Read Network.AWS.Rekognition.DeleteProjectVersion.DeleteProjectVersionResponse
instance GHC.Classes.Eq Network.AWS.Rekognition.DeleteProjectVersion.DeleteProjectVersionResponse
instance Network.AWS.Types.AWSRequest Network.AWS.Rekognition.DeleteProjectVersion.DeleteProjectVersion
instance Control.DeepSeq.NFData Network.AWS.Rekognition.DeleteProjectVersion.DeleteProjectVersionResponse
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.DeleteProjectVersion.DeleteProjectVersion
instance Control.DeepSeq.NFData Network.AWS.Rekognition.DeleteProjectVersion.DeleteProjectVersion
instance Network.AWS.Data.Headers.ToHeaders Network.AWS.Rekognition.DeleteProjectVersion.DeleteProjectVersion
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.DeleteProjectVersion.DeleteProjectVersion
instance Network.AWS.Data.Path.ToPath Network.AWS.Rekognition.DeleteProjectVersion.DeleteProjectVersion
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.DeleteProjectVersion.DeleteProjectVersion


-- | Deletes an Amazon Rekognition Custom Labels project. To delete a
--   project you must first delete all models associated with the project.
--   To delete a model, see DeleteProjectVersion.
--   
--   This operation requires permissions to perform the
--   <tt>rekognition:DeleteProject</tt> action.
module Network.AWS.Rekognition.DeleteProject

-- | <i>See:</i> <a>newDeleteProject</a> smart constructor.
data DeleteProject
DeleteProject' :: Text -> DeleteProject

-- | The Amazon Resource Name (ARN) of the project that you want to delete.
[$sel:projectArn:DeleteProject'] :: DeleteProject -> Text

-- | Create a value of <a>DeleteProject</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:projectArn:DeleteProject'</a>, <a>deleteProject_projectArn</a>
--   - The Amazon Resource Name (ARN) of the project that you want to
--   delete.
newDeleteProject :: Text -> DeleteProject

-- | The Amazon Resource Name (ARN) of the project that you want to delete.
deleteProject_projectArn :: Lens' DeleteProject Text

-- | <i>See:</i> <a>newDeleteProjectResponse</a> smart constructor.
data DeleteProjectResponse
DeleteProjectResponse' :: Maybe ProjectStatus -> Int -> DeleteProjectResponse

-- | The current status of the delete project operation.
[$sel:status:DeleteProjectResponse'] :: DeleteProjectResponse -> Maybe ProjectStatus

-- | The response's http status code.
[$sel:httpStatus:DeleteProjectResponse'] :: DeleteProjectResponse -> Int

-- | Create a value of <a>DeleteProjectResponse</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:status:DeleteProjectResponse'</a>,
--   <a>deleteProjectResponse_status</a> - The current status of the delete
--   project operation.
--   
--   <a>$sel:httpStatus:DeleteProjectResponse'</a>,
--   <a>deleteProjectResponse_httpStatus</a> - The response's http status
--   code.
newDeleteProjectResponse :: Int -> DeleteProjectResponse

-- | The current status of the delete project operation.
deleteProjectResponse_status :: Lens' DeleteProjectResponse (Maybe ProjectStatus)

-- | The response's http status code.
deleteProjectResponse_httpStatus :: Lens' DeleteProjectResponse Int
instance GHC.Generics.Generic Network.AWS.Rekognition.DeleteProject.DeleteProject
instance GHC.Show.Show Network.AWS.Rekognition.DeleteProject.DeleteProject
instance GHC.Read.Read Network.AWS.Rekognition.DeleteProject.DeleteProject
instance GHC.Classes.Eq Network.AWS.Rekognition.DeleteProject.DeleteProject
instance GHC.Generics.Generic Network.AWS.Rekognition.DeleteProject.DeleteProjectResponse
instance GHC.Show.Show Network.AWS.Rekognition.DeleteProject.DeleteProjectResponse
instance GHC.Read.Read Network.AWS.Rekognition.DeleteProject.DeleteProjectResponse
instance GHC.Classes.Eq Network.AWS.Rekognition.DeleteProject.DeleteProjectResponse
instance Network.AWS.Types.AWSRequest Network.AWS.Rekognition.DeleteProject.DeleteProject
instance Control.DeepSeq.NFData Network.AWS.Rekognition.DeleteProject.DeleteProjectResponse
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.DeleteProject.DeleteProject
instance Control.DeepSeq.NFData Network.AWS.Rekognition.DeleteProject.DeleteProject
instance Network.AWS.Data.Headers.ToHeaders Network.AWS.Rekognition.DeleteProject.DeleteProject
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.DeleteProject.DeleteProject
instance Network.AWS.Data.Path.ToPath Network.AWS.Rekognition.DeleteProject.DeleteProject
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.DeleteProject.DeleteProject


-- | Deletes faces from a collection. You specify a collection ID and an
--   array of face IDs to remove from the collection.
--   
--   This operation requires permissions to perform the
--   <tt>rekognition:DeleteFaces</tt> action.
module Network.AWS.Rekognition.DeleteFaces

-- | <i>See:</i> <a>newDeleteFaces</a> smart constructor.
data DeleteFaces
DeleteFaces' :: Text -> NonEmpty Text -> DeleteFaces

-- | Collection from which to remove the specific faces.
[$sel:collectionId:DeleteFaces'] :: DeleteFaces -> Text

-- | An array of face IDs to delete.
[$sel:faceIds:DeleteFaces'] :: DeleteFaces -> NonEmpty Text

-- | Create a value of <a>DeleteFaces</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:collectionId:DeleteFaces'</a>, <a>deleteFaces_collectionId</a>
--   - Collection from which to remove the specific faces.
--   
--   <a>$sel:faceIds:DeleteFaces'</a>, <a>deleteFaces_faceIds</a> - An
--   array of face IDs to delete.
newDeleteFaces :: Text -> NonEmpty Text -> DeleteFaces

-- | Collection from which to remove the specific faces.
deleteFaces_collectionId :: Lens' DeleteFaces Text

-- | An array of face IDs to delete.
deleteFaces_faceIds :: Lens' DeleteFaces (NonEmpty Text)

-- | <i>See:</i> <a>newDeleteFacesResponse</a> smart constructor.
data DeleteFacesResponse
DeleteFacesResponse' :: Maybe (NonEmpty Text) -> Int -> DeleteFacesResponse

-- | An array of strings (face IDs) of the faces that were deleted.
[$sel:deletedFaces:DeleteFacesResponse'] :: DeleteFacesResponse -> Maybe (NonEmpty Text)

-- | The response's http status code.
[$sel:httpStatus:DeleteFacesResponse'] :: DeleteFacesResponse -> Int

-- | Create a value of <a>DeleteFacesResponse</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:deletedFaces:DeleteFacesResponse'</a>,
--   <a>deleteFacesResponse_deletedFaces</a> - An array of strings (face
--   IDs) of the faces that were deleted.
--   
--   <a>$sel:httpStatus:DeleteFacesResponse'</a>,
--   <a>deleteFacesResponse_httpStatus</a> - The response's http status
--   code.
newDeleteFacesResponse :: Int -> DeleteFacesResponse

-- | An array of strings (face IDs) of the faces that were deleted.
deleteFacesResponse_deletedFaces :: Lens' DeleteFacesResponse (Maybe (NonEmpty Text))

-- | The response's http status code.
deleteFacesResponse_httpStatus :: Lens' DeleteFacesResponse Int
instance GHC.Generics.Generic Network.AWS.Rekognition.DeleteFaces.DeleteFaces
instance GHC.Show.Show Network.AWS.Rekognition.DeleteFaces.DeleteFaces
instance GHC.Read.Read Network.AWS.Rekognition.DeleteFaces.DeleteFaces
instance GHC.Classes.Eq Network.AWS.Rekognition.DeleteFaces.DeleteFaces
instance GHC.Generics.Generic Network.AWS.Rekognition.DeleteFaces.DeleteFacesResponse
instance GHC.Show.Show Network.AWS.Rekognition.DeleteFaces.DeleteFacesResponse
instance GHC.Read.Read Network.AWS.Rekognition.DeleteFaces.DeleteFacesResponse
instance GHC.Classes.Eq Network.AWS.Rekognition.DeleteFaces.DeleteFacesResponse
instance Network.AWS.Types.AWSRequest Network.AWS.Rekognition.DeleteFaces.DeleteFaces
instance Control.DeepSeq.NFData Network.AWS.Rekognition.DeleteFaces.DeleteFacesResponse
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.DeleteFaces.DeleteFaces
instance Control.DeepSeq.NFData Network.AWS.Rekognition.DeleteFaces.DeleteFaces
instance Network.AWS.Data.Headers.ToHeaders Network.AWS.Rekognition.DeleteFaces.DeleteFaces
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.DeleteFaces.DeleteFaces
instance Network.AWS.Data.Path.ToPath Network.AWS.Rekognition.DeleteFaces.DeleteFaces
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.DeleteFaces.DeleteFaces


-- | Deletes the specified collection. Note that this operation removes all
--   faces in the collection. For an example, see
--   delete-collection-procedure.
--   
--   This operation requires permissions to perform the
--   <tt>rekognition:DeleteCollection</tt> action.
module Network.AWS.Rekognition.DeleteCollection

-- | <i>See:</i> <a>newDeleteCollection</a> smart constructor.
data DeleteCollection
DeleteCollection' :: Text -> DeleteCollection

-- | ID of the collection to delete.
[$sel:collectionId:DeleteCollection'] :: DeleteCollection -> Text

-- | Create a value of <a>DeleteCollection</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:collectionId:DeleteCollection'</a>,
--   <a>deleteCollection_collectionId</a> - ID of the collection to delete.
newDeleteCollection :: Text -> DeleteCollection

-- | ID of the collection to delete.
deleteCollection_collectionId :: Lens' DeleteCollection Text

-- | <i>See:</i> <a>newDeleteCollectionResponse</a> smart constructor.
data DeleteCollectionResponse
DeleteCollectionResponse' :: Maybe Natural -> Int -> DeleteCollectionResponse

-- | HTTP status code that indicates the result of the operation.
[$sel:statusCode:DeleteCollectionResponse'] :: DeleteCollectionResponse -> Maybe Natural

-- | The response's http status code.
[$sel:httpStatus:DeleteCollectionResponse'] :: DeleteCollectionResponse -> Int

-- | Create a value of <a>DeleteCollectionResponse</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:statusCode:DeleteCollectionResponse'</a>,
--   <a>deleteCollectionResponse_statusCode</a> - HTTP status code that
--   indicates the result of the operation.
--   
--   <a>$sel:httpStatus:DeleteCollectionResponse'</a>,
--   <a>deleteCollectionResponse_httpStatus</a> - The response's http
--   status code.
newDeleteCollectionResponse :: Int -> DeleteCollectionResponse

-- | HTTP status code that indicates the result of the operation.
deleteCollectionResponse_statusCode :: Lens' DeleteCollectionResponse (Maybe Natural)

-- | The response's http status code.
deleteCollectionResponse_httpStatus :: Lens' DeleteCollectionResponse Int
instance GHC.Generics.Generic Network.AWS.Rekognition.DeleteCollection.DeleteCollection
instance GHC.Show.Show Network.AWS.Rekognition.DeleteCollection.DeleteCollection
instance GHC.Read.Read Network.AWS.Rekognition.DeleteCollection.DeleteCollection
instance GHC.Classes.Eq Network.AWS.Rekognition.DeleteCollection.DeleteCollection
instance GHC.Generics.Generic Network.AWS.Rekognition.DeleteCollection.DeleteCollectionResponse
instance GHC.Show.Show Network.AWS.Rekognition.DeleteCollection.DeleteCollectionResponse
instance GHC.Read.Read Network.AWS.Rekognition.DeleteCollection.DeleteCollectionResponse
instance GHC.Classes.Eq Network.AWS.Rekognition.DeleteCollection.DeleteCollectionResponse
instance Network.AWS.Types.AWSRequest Network.AWS.Rekognition.DeleteCollection.DeleteCollection
instance Control.DeepSeq.NFData Network.AWS.Rekognition.DeleteCollection.DeleteCollectionResponse
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.DeleteCollection.DeleteCollection
instance Control.DeepSeq.NFData Network.AWS.Rekognition.DeleteCollection.DeleteCollection
instance Network.AWS.Data.Headers.ToHeaders Network.AWS.Rekognition.DeleteCollection.DeleteCollection
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.DeleteCollection.DeleteCollection
instance Network.AWS.Data.Path.ToPath Network.AWS.Rekognition.DeleteCollection.DeleteCollection
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.DeleteCollection.DeleteCollection


-- | Creates an Amazon Rekognition stream processor that you can use to
--   detect and recognize faces in a streaming video.
--   
--   Amazon Rekognition Video is a consumer of live video from Amazon
--   Kinesis Video Streams. Amazon Rekognition Video sends analysis results
--   to Amazon Kinesis Data Streams.
--   
--   You provide as input a Kinesis video stream (<tt>Input</tt>) and a
--   Kinesis data stream (<tt>Output</tt>) stream. You also specify the
--   face recognition criteria in <tt>Settings</tt>. For example, the
--   collection containing faces that you want to recognize. Use
--   <tt>Name</tt> to assign an identifier for the stream processor. You
--   use <tt>Name</tt> to manage the stream processor. For example, you can
--   start processing the source video by calling StartStreamProcessor with
--   the <tt>Name</tt> field.
--   
--   After you have finished analyzing a streaming video, use
--   StopStreamProcessor to stop processing. You can delete the stream
--   processor by calling DeleteStreamProcessor.
--   
--   This operation requires permissions to perform the
--   <tt>rekognition:CreateStreamProcessor</tt> action. If you want to tag
--   your stream processor, you also require permission to perform the
--   <tt>rekognition:TagResource</tt> operation.
module Network.AWS.Rekognition.CreateStreamProcessor

-- | <i>See:</i> <a>newCreateStreamProcessor</a> smart constructor.
data CreateStreamProcessor
CreateStreamProcessor' :: Maybe (HashMap Text Text) -> StreamProcessorInput -> StreamProcessorOutput -> Text -> StreamProcessorSettings -> Text -> CreateStreamProcessor

-- | A set of tags (key-value pairs) that you want to attach to the stream
--   processor.
[$sel:tags:CreateStreamProcessor'] :: CreateStreamProcessor -> Maybe (HashMap Text Text)

-- | Kinesis video stream stream that provides the source streaming video.
--   If you are using the AWS CLI, the parameter name is
--   <tt>StreamProcessorInput</tt>.
[$sel:input:CreateStreamProcessor'] :: CreateStreamProcessor -> StreamProcessorInput

-- | Kinesis data stream stream to which Amazon Rekognition Video puts the
--   analysis results. If you are using the AWS CLI, the parameter name is
--   <tt>StreamProcessorOutput</tt>.
[$sel:output:CreateStreamProcessor'] :: CreateStreamProcessor -> StreamProcessorOutput

-- | An identifier you assign to the stream processor. You can use
--   <tt>Name</tt> to manage the stream processor. For example, you can get
--   the current status of the stream processor by calling
--   DescribeStreamProcessor. <tt>Name</tt> is idempotent.
[$sel:name:CreateStreamProcessor'] :: CreateStreamProcessor -> Text

-- | Face recognition input parameters to be used by the stream processor.
--   Includes the collection to use for face recognition and the face
--   attributes to detect.
[$sel:settings:CreateStreamProcessor'] :: CreateStreamProcessor -> StreamProcessorSettings

-- | ARN of the IAM role that allows access to the stream processor.
[$sel:roleArn:CreateStreamProcessor'] :: CreateStreamProcessor -> Text

-- | Create a value of <a>CreateStreamProcessor</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:tags:CreateStreamProcessor'</a>,
--   <a>createStreamProcessor_tags</a> - A set of tags (key-value pairs)
--   that you want to attach to the stream processor.
--   
--   <a>$sel:input:CreateStreamProcessor'</a>,
--   <a>createStreamProcessor_input</a> - Kinesis video stream stream that
--   provides the source streaming video. If you are using the AWS CLI, the
--   parameter name is <tt>StreamProcessorInput</tt>.
--   
--   <a>$sel:output:CreateStreamProcessor'</a>,
--   <a>createStreamProcessor_output</a> - Kinesis data stream stream to
--   which Amazon Rekognition Video puts the analysis results. If you are
--   using the AWS CLI, the parameter name is
--   <tt>StreamProcessorOutput</tt>.
--   
--   <a>$sel:name:CreateStreamProcessor'</a>,
--   <a>createStreamProcessor_name</a> - An identifier you assign to the
--   stream processor. You can use <tt>Name</tt> to manage the stream
--   processor. For example, you can get the current status of the stream
--   processor by calling DescribeStreamProcessor. <tt>Name</tt> is
--   idempotent.
--   
--   <a>$sel:settings:CreateStreamProcessor'</a>,
--   <a>createStreamProcessor_settings</a> - Face recognition input
--   parameters to be used by the stream processor. Includes the collection
--   to use for face recognition and the face attributes to detect.
--   
--   <a>$sel:roleArn:CreateStreamProcessor'</a>,
--   <a>createStreamProcessor_roleArn</a> - ARN of the IAM role that allows
--   access to the stream processor.
newCreateStreamProcessor :: StreamProcessorInput -> StreamProcessorOutput -> Text -> StreamProcessorSettings -> Text -> CreateStreamProcessor

-- | A set of tags (key-value pairs) that you want to attach to the stream
--   processor.
createStreamProcessor_tags :: Lens' CreateStreamProcessor (Maybe (HashMap Text Text))

-- | Kinesis video stream stream that provides the source streaming video.
--   If you are using the AWS CLI, the parameter name is
--   <tt>StreamProcessorInput</tt>.
createStreamProcessor_input :: Lens' CreateStreamProcessor StreamProcessorInput

-- | Kinesis data stream stream to which Amazon Rekognition Video puts the
--   analysis results. If you are using the AWS CLI, the parameter name is
--   <tt>StreamProcessorOutput</tt>.
createStreamProcessor_output :: Lens' CreateStreamProcessor StreamProcessorOutput

-- | An identifier you assign to the stream processor. You can use
--   <tt>Name</tt> to manage the stream processor. For example, you can get
--   the current status of the stream processor by calling
--   DescribeStreamProcessor. <tt>Name</tt> is idempotent.
createStreamProcessor_name :: Lens' CreateStreamProcessor Text

-- | Face recognition input parameters to be used by the stream processor.
--   Includes the collection to use for face recognition and the face
--   attributes to detect.
createStreamProcessor_settings :: Lens' CreateStreamProcessor StreamProcessorSettings

-- | ARN of the IAM role that allows access to the stream processor.
createStreamProcessor_roleArn :: Lens' CreateStreamProcessor Text

-- | <i>See:</i> <a>newCreateStreamProcessorResponse</a> smart constructor.
data CreateStreamProcessorResponse
CreateStreamProcessorResponse' :: Maybe Text -> Int -> CreateStreamProcessorResponse

-- | ARN for the newly create stream processor.
[$sel:streamProcessorArn:CreateStreamProcessorResponse'] :: CreateStreamProcessorResponse -> Maybe Text

-- | The response's http status code.
[$sel:httpStatus:CreateStreamProcessorResponse'] :: CreateStreamProcessorResponse -> Int

-- | Create a value of <a>CreateStreamProcessorResponse</a> with all
--   optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:streamProcessorArn:CreateStreamProcessorResponse'</a>,
--   <a>createStreamProcessorResponse_streamProcessorArn</a> - ARN for the
--   newly create stream processor.
--   
--   <a>$sel:httpStatus:CreateStreamProcessorResponse'</a>,
--   <a>createStreamProcessorResponse_httpStatus</a> - The response's http
--   status code.
newCreateStreamProcessorResponse :: Int -> CreateStreamProcessorResponse

-- | ARN for the newly create stream processor.
createStreamProcessorResponse_streamProcessorArn :: Lens' CreateStreamProcessorResponse (Maybe Text)

-- | The response's http status code.
createStreamProcessorResponse_httpStatus :: Lens' CreateStreamProcessorResponse Int
instance GHC.Generics.Generic Network.AWS.Rekognition.CreateStreamProcessor.CreateStreamProcessor
instance GHC.Show.Show Network.AWS.Rekognition.CreateStreamProcessor.CreateStreamProcessor
instance GHC.Read.Read Network.AWS.Rekognition.CreateStreamProcessor.CreateStreamProcessor
instance GHC.Classes.Eq Network.AWS.Rekognition.CreateStreamProcessor.CreateStreamProcessor
instance GHC.Generics.Generic Network.AWS.Rekognition.CreateStreamProcessor.CreateStreamProcessorResponse
instance GHC.Show.Show Network.AWS.Rekognition.CreateStreamProcessor.CreateStreamProcessorResponse
instance GHC.Read.Read Network.AWS.Rekognition.CreateStreamProcessor.CreateStreamProcessorResponse
instance GHC.Classes.Eq Network.AWS.Rekognition.CreateStreamProcessor.CreateStreamProcessorResponse
instance Network.AWS.Types.AWSRequest Network.AWS.Rekognition.CreateStreamProcessor.CreateStreamProcessor
instance Control.DeepSeq.NFData Network.AWS.Rekognition.CreateStreamProcessor.CreateStreamProcessorResponse
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.CreateStreamProcessor.CreateStreamProcessor
instance Control.DeepSeq.NFData Network.AWS.Rekognition.CreateStreamProcessor.CreateStreamProcessor
instance Network.AWS.Data.Headers.ToHeaders Network.AWS.Rekognition.CreateStreamProcessor.CreateStreamProcessor
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.CreateStreamProcessor.CreateStreamProcessor
instance Network.AWS.Data.Path.ToPath Network.AWS.Rekognition.CreateStreamProcessor.CreateStreamProcessor
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.CreateStreamProcessor.CreateStreamProcessor


-- | Creates a new version of a model and begins training. Models are
--   managed as part of an Amazon Rekognition Custom Labels project. You
--   can specify one training dataset and one testing dataset. The response
--   from <tt>CreateProjectVersion</tt> is an Amazon Resource Name (ARN)
--   for the version of the model.
--   
--   Training takes a while to complete. You can get the current status by
--   calling DescribeProjectVersions.
--   
--   Once training has successfully completed, call DescribeProjectVersions
--   to get the training results and evaluate the model.
--   
--   After evaluating the model, you start the model by calling
--   StartProjectVersion.
--   
--   This operation requires permissions to perform the
--   <tt>rekognition:CreateProjectVersion</tt> action.
module Network.AWS.Rekognition.CreateProjectVersion

-- | <i>See:</i> <a>newCreateProjectVersion</a> smart constructor.
data CreateProjectVersion
CreateProjectVersion' :: Maybe Text -> Maybe (HashMap Text Text) -> Text -> Text -> OutputConfig -> TrainingData -> TestingData -> CreateProjectVersion

-- | The identifier for your AWS Key Management Service (AWS KMS) customer
--   master key (CMK). You can supply the Amazon Resource Name (ARN) of
--   your CMK, the ID of your CMK, an alias for your CMK, or an alias ARN.
--   The key is used to encrypt training and test images copied into the
--   service for model training. Your source images are unaffected. The key
--   is also used to encrypt training results and manifest files written to
--   the output Amazon S3 bucket (<tt>OutputConfig</tt>).
--   
--   If you choose to use your own CMK, you need the following permissions
--   on the CMK.
--   
--   <ul>
--   <li>kms:CreateGrant</li>
--   <li>kms:DescribeKey</li>
--   <li>kms:GenerateDataKey</li>
--   <li>kms:Decrypt</li>
--   </ul>
--   
--   If you don't specify a value for <tt>KmsKeyId</tt>, images copied into
--   the service are encrypted using a key that AWS owns and manages.
[$sel:kmsKeyId:CreateProjectVersion'] :: CreateProjectVersion -> Maybe Text

-- | A set of tags (key-value pairs) that you want to attach to the model.
[$sel:tags:CreateProjectVersion'] :: CreateProjectVersion -> Maybe (HashMap Text Text)

-- | The ARN of the Amazon Rekognition Custom Labels project that manages
--   the model that you want to train.
[$sel:projectArn:CreateProjectVersion'] :: CreateProjectVersion -> Text

-- | A name for the version of the model. This value must be unique.
[$sel:versionName:CreateProjectVersion'] :: CreateProjectVersion -> Text

-- | The Amazon S3 bucket location to store the results of training. The S3
--   bucket can be in any AWS account as long as the caller has
--   <tt>s3:PutObject</tt> permissions on the S3 bucket.
[$sel:outputConfig:CreateProjectVersion'] :: CreateProjectVersion -> OutputConfig

-- | The dataset to use for training.
[$sel:trainingData:CreateProjectVersion'] :: CreateProjectVersion -> TrainingData

-- | The dataset to use for testing.
[$sel:testingData:CreateProjectVersion'] :: CreateProjectVersion -> TestingData

-- | Create a value of <a>CreateProjectVersion</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:kmsKeyId:CreateProjectVersion'</a>,
--   <a>createProjectVersion_kmsKeyId</a> - The identifier for your AWS Key
--   Management Service (AWS KMS) customer master key (CMK). You can supply
--   the Amazon Resource Name (ARN) of your CMK, the ID of your CMK, an
--   alias for your CMK, or an alias ARN. The key is used to encrypt
--   training and test images copied into the service for model training.
--   Your source images are unaffected. The key is also used to encrypt
--   training results and manifest files written to the output Amazon S3
--   bucket (<tt>OutputConfig</tt>).
--   
--   If you choose to use your own CMK, you need the following permissions
--   on the CMK.
--   
--   <ul>
--   <li>kms:CreateGrant</li>
--   <li>kms:DescribeKey</li>
--   <li>kms:GenerateDataKey</li>
--   <li>kms:Decrypt</li>
--   </ul>
--   
--   If you don't specify a value for <tt>KmsKeyId</tt>, images copied into
--   the service are encrypted using a key that AWS owns and manages.
--   
--   <a>$sel:tags:CreateProjectVersion'</a>,
--   <a>createProjectVersion_tags</a> - A set of tags (key-value pairs)
--   that you want to attach to the model.
--   
--   <a>$sel:projectArn:CreateProjectVersion'</a>,
--   <a>createProjectVersion_projectArn</a> - The ARN of the Amazon
--   Rekognition Custom Labels project that manages the model that you want
--   to train.
--   
--   <a>$sel:versionName:CreateProjectVersion'</a>,
--   <a>createProjectVersion_versionName</a> - A name for the version of
--   the model. This value must be unique.
--   
--   <a>$sel:outputConfig:CreateProjectVersion'</a>,
--   <a>createProjectVersion_outputConfig</a> - The Amazon S3 bucket
--   location to store the results of training. The S3 bucket can be in any
--   AWS account as long as the caller has <tt>s3:PutObject</tt>
--   permissions on the S3 bucket.
--   
--   <a>$sel:trainingData:CreateProjectVersion'</a>,
--   <a>createProjectVersion_trainingData</a> - The dataset to use for
--   training.
--   
--   <a>$sel:testingData:CreateProjectVersion'</a>,
--   <a>createProjectVersion_testingData</a> - The dataset to use for
--   testing.
newCreateProjectVersion :: Text -> Text -> OutputConfig -> TrainingData -> TestingData -> CreateProjectVersion

-- | The identifier for your AWS Key Management Service (AWS KMS) customer
--   master key (CMK). You can supply the Amazon Resource Name (ARN) of
--   your CMK, the ID of your CMK, an alias for your CMK, or an alias ARN.
--   The key is used to encrypt training and test images copied into the
--   service for model training. Your source images are unaffected. The key
--   is also used to encrypt training results and manifest files written to
--   the output Amazon S3 bucket (<tt>OutputConfig</tt>).
--   
--   If you choose to use your own CMK, you need the following permissions
--   on the CMK.
--   
--   <ul>
--   <li>kms:CreateGrant</li>
--   <li>kms:DescribeKey</li>
--   <li>kms:GenerateDataKey</li>
--   <li>kms:Decrypt</li>
--   </ul>
--   
--   If you don't specify a value for <tt>KmsKeyId</tt>, images copied into
--   the service are encrypted using a key that AWS owns and manages.
createProjectVersion_kmsKeyId :: Lens' CreateProjectVersion (Maybe Text)

-- | A set of tags (key-value pairs) that you want to attach to the model.
createProjectVersion_tags :: Lens' CreateProjectVersion (Maybe (HashMap Text Text))

-- | The ARN of the Amazon Rekognition Custom Labels project that manages
--   the model that you want to train.
createProjectVersion_projectArn :: Lens' CreateProjectVersion Text

-- | A name for the version of the model. This value must be unique.
createProjectVersion_versionName :: Lens' CreateProjectVersion Text

-- | The Amazon S3 bucket location to store the results of training. The S3
--   bucket can be in any AWS account as long as the caller has
--   <tt>s3:PutObject</tt> permissions on the S3 bucket.
createProjectVersion_outputConfig :: Lens' CreateProjectVersion OutputConfig

-- | The dataset to use for training.
createProjectVersion_trainingData :: Lens' CreateProjectVersion TrainingData

-- | The dataset to use for testing.
createProjectVersion_testingData :: Lens' CreateProjectVersion TestingData

-- | <i>See:</i> <a>newCreateProjectVersionResponse</a> smart constructor.
data CreateProjectVersionResponse
CreateProjectVersionResponse' :: Maybe Text -> Int -> CreateProjectVersionResponse

-- | The ARN of the model version that was created. Use
--   <tt>DescribeProjectVersion</tt> to get the current status of the
--   training operation.
[$sel:projectVersionArn:CreateProjectVersionResponse'] :: CreateProjectVersionResponse -> Maybe Text

-- | The response's http status code.
[$sel:httpStatus:CreateProjectVersionResponse'] :: CreateProjectVersionResponse -> Int

-- | Create a value of <a>CreateProjectVersionResponse</a> with all
--   optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:projectVersionArn:CreateProjectVersionResponse'</a>,
--   <a>createProjectVersionResponse_projectVersionArn</a> - The ARN of the
--   model version that was created. Use <tt>DescribeProjectVersion</tt> to
--   get the current status of the training operation.
--   
--   <a>$sel:httpStatus:CreateProjectVersionResponse'</a>,
--   <a>createProjectVersionResponse_httpStatus</a> - The response's http
--   status code.
newCreateProjectVersionResponse :: Int -> CreateProjectVersionResponse

-- | The ARN of the model version that was created. Use
--   <tt>DescribeProjectVersion</tt> to get the current status of the
--   training operation.
createProjectVersionResponse_projectVersionArn :: Lens' CreateProjectVersionResponse (Maybe Text)

-- | The response's http status code.
createProjectVersionResponse_httpStatus :: Lens' CreateProjectVersionResponse Int
instance GHC.Generics.Generic Network.AWS.Rekognition.CreateProjectVersion.CreateProjectVersion
instance GHC.Show.Show Network.AWS.Rekognition.CreateProjectVersion.CreateProjectVersion
instance GHC.Read.Read Network.AWS.Rekognition.CreateProjectVersion.CreateProjectVersion
instance GHC.Classes.Eq Network.AWS.Rekognition.CreateProjectVersion.CreateProjectVersion
instance GHC.Generics.Generic Network.AWS.Rekognition.CreateProjectVersion.CreateProjectVersionResponse
instance GHC.Show.Show Network.AWS.Rekognition.CreateProjectVersion.CreateProjectVersionResponse
instance GHC.Read.Read Network.AWS.Rekognition.CreateProjectVersion.CreateProjectVersionResponse
instance GHC.Classes.Eq Network.AWS.Rekognition.CreateProjectVersion.CreateProjectVersionResponse
instance Network.AWS.Types.AWSRequest Network.AWS.Rekognition.CreateProjectVersion.CreateProjectVersion
instance Control.DeepSeq.NFData Network.AWS.Rekognition.CreateProjectVersion.CreateProjectVersionResponse
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.CreateProjectVersion.CreateProjectVersion
instance Control.DeepSeq.NFData Network.AWS.Rekognition.CreateProjectVersion.CreateProjectVersion
instance Network.AWS.Data.Headers.ToHeaders Network.AWS.Rekognition.CreateProjectVersion.CreateProjectVersion
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.CreateProjectVersion.CreateProjectVersion
instance Network.AWS.Data.Path.ToPath Network.AWS.Rekognition.CreateProjectVersion.CreateProjectVersion
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.CreateProjectVersion.CreateProjectVersion


-- | Creates a new Amazon Rekognition Custom Labels project. A project is a
--   logical grouping of resources (images, Labels, models) and operations
--   (training, evaluation and detection).
--   
--   This operation requires permissions to perform the
--   <tt>rekognition:CreateProject</tt> action.
module Network.AWS.Rekognition.CreateProject

-- | <i>See:</i> <a>newCreateProject</a> smart constructor.
data CreateProject
CreateProject' :: Text -> CreateProject

-- | The name of the project to create.
[$sel:projectName:CreateProject'] :: CreateProject -> Text

-- | Create a value of <a>CreateProject</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:projectName:CreateProject'</a>,
--   <a>createProject_projectName</a> - The name of the project to create.
newCreateProject :: Text -> CreateProject

-- | The name of the project to create.
createProject_projectName :: Lens' CreateProject Text

-- | <i>See:</i> <a>newCreateProjectResponse</a> smart constructor.
data CreateProjectResponse
CreateProjectResponse' :: Maybe Text -> Int -> CreateProjectResponse

-- | The Amazon Resource Name (ARN) of the new project. You can use the ARN
--   to configure IAM access to the project.
[$sel:projectArn:CreateProjectResponse'] :: CreateProjectResponse -> Maybe Text

-- | The response's http status code.
[$sel:httpStatus:CreateProjectResponse'] :: CreateProjectResponse -> Int

-- | Create a value of <a>CreateProjectResponse</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:projectArn:CreateProjectResponse'</a>,
--   <a>createProjectResponse_projectArn</a> - The Amazon Resource Name
--   (ARN) of the new project. You can use the ARN to configure IAM access
--   to the project.
--   
--   <a>$sel:httpStatus:CreateProjectResponse'</a>,
--   <a>createProjectResponse_httpStatus</a> - The response's http status
--   code.
newCreateProjectResponse :: Int -> CreateProjectResponse

-- | The Amazon Resource Name (ARN) of the new project. You can use the ARN
--   to configure IAM access to the project.
createProjectResponse_projectArn :: Lens' CreateProjectResponse (Maybe Text)

-- | The response's http status code.
createProjectResponse_httpStatus :: Lens' CreateProjectResponse Int
instance GHC.Generics.Generic Network.AWS.Rekognition.CreateProject.CreateProject
instance GHC.Show.Show Network.AWS.Rekognition.CreateProject.CreateProject
instance GHC.Read.Read Network.AWS.Rekognition.CreateProject.CreateProject
instance GHC.Classes.Eq Network.AWS.Rekognition.CreateProject.CreateProject
instance GHC.Generics.Generic Network.AWS.Rekognition.CreateProject.CreateProjectResponse
instance GHC.Show.Show Network.AWS.Rekognition.CreateProject.CreateProjectResponse
instance GHC.Read.Read Network.AWS.Rekognition.CreateProject.CreateProjectResponse
instance GHC.Classes.Eq Network.AWS.Rekognition.CreateProject.CreateProjectResponse
instance Network.AWS.Types.AWSRequest Network.AWS.Rekognition.CreateProject.CreateProject
instance Control.DeepSeq.NFData Network.AWS.Rekognition.CreateProject.CreateProjectResponse
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.CreateProject.CreateProject
instance Control.DeepSeq.NFData Network.AWS.Rekognition.CreateProject.CreateProject
instance Network.AWS.Data.Headers.ToHeaders Network.AWS.Rekognition.CreateProject.CreateProject
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.CreateProject.CreateProject
instance Network.AWS.Data.Path.ToPath Network.AWS.Rekognition.CreateProject.CreateProject
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.CreateProject.CreateProject


-- | Creates a collection in an AWS Region. You can add faces to the
--   collection using the IndexFaces operation.
--   
--   For example, you might create collections, one for each of your
--   application users. A user can then index faces using the
--   <tt>IndexFaces</tt> operation and persist results in a specific
--   collection. Then, a user can search the collection for faces in the
--   user-specific container.
--   
--   When you create a collection, it is associated with the latest version
--   of the face model version.
--   
--   Collection names are case-sensitive.
--   
--   This operation requires permissions to perform the
--   <tt>rekognition:CreateCollection</tt> action. If you want to tag your
--   collection, you also require permission to perform the
--   <tt>rekognition:TagResource</tt> operation.
module Network.AWS.Rekognition.CreateCollection

-- | <i>See:</i> <a>newCreateCollection</a> smart constructor.
data CreateCollection
CreateCollection' :: Maybe (HashMap Text Text) -> Text -> CreateCollection

-- | A set of tags (key-value pairs) that you want to attach to the
--   collection.
[$sel:tags:CreateCollection'] :: CreateCollection -> Maybe (HashMap Text Text)

-- | ID for the collection that you are creating.
[$sel:collectionId:CreateCollection'] :: CreateCollection -> Text

-- | Create a value of <a>CreateCollection</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:tags:CreateCollection'</a>, <a>createCollection_tags</a> - A
--   set of tags (key-value pairs) that you want to attach to the
--   collection.
--   
--   <a>$sel:collectionId:CreateCollection'</a>,
--   <a>createCollection_collectionId</a> - ID for the collection that you
--   are creating.
newCreateCollection :: Text -> CreateCollection

-- | A set of tags (key-value pairs) that you want to attach to the
--   collection.
createCollection_tags :: Lens' CreateCollection (Maybe (HashMap Text Text))

-- | ID for the collection that you are creating.
createCollection_collectionId :: Lens' CreateCollection Text

-- | <i>See:</i> <a>newCreateCollectionResponse</a> smart constructor.
data CreateCollectionResponse
CreateCollectionResponse' :: Maybe Text -> Maybe Text -> Maybe Natural -> Int -> CreateCollectionResponse

-- | Version number of the face detection model associated with the
--   collection you are creating.
[$sel:faceModelVersion:CreateCollectionResponse'] :: CreateCollectionResponse -> Maybe Text

-- | Amazon Resource Name (ARN) of the collection. You can use this to
--   manage permissions on your resources.
[$sel:collectionArn:CreateCollectionResponse'] :: CreateCollectionResponse -> Maybe Text

-- | HTTP status code indicating the result of the operation.
[$sel:statusCode:CreateCollectionResponse'] :: CreateCollectionResponse -> Maybe Natural

-- | The response's http status code.
[$sel:httpStatus:CreateCollectionResponse'] :: CreateCollectionResponse -> Int

-- | Create a value of <a>CreateCollectionResponse</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:faceModelVersion:CreateCollectionResponse'</a>,
--   <a>createCollectionResponse_faceModelVersion</a> - Version number of
--   the face detection model associated with the collection you are
--   creating.
--   
--   <a>$sel:collectionArn:CreateCollectionResponse'</a>,
--   <a>createCollectionResponse_collectionArn</a> - Amazon Resource Name
--   (ARN) of the collection. You can use this to manage permissions on
--   your resources.
--   
--   <a>$sel:statusCode:CreateCollectionResponse'</a>,
--   <a>createCollectionResponse_statusCode</a> - HTTP status code
--   indicating the result of the operation.
--   
--   <a>$sel:httpStatus:CreateCollectionResponse'</a>,
--   <a>createCollectionResponse_httpStatus</a> - The response's http
--   status code.
newCreateCollectionResponse :: Int -> CreateCollectionResponse

-- | Version number of the face detection model associated with the
--   collection you are creating.
createCollectionResponse_faceModelVersion :: Lens' CreateCollectionResponse (Maybe Text)

-- | Amazon Resource Name (ARN) of the collection. You can use this to
--   manage permissions on your resources.
createCollectionResponse_collectionArn :: Lens' CreateCollectionResponse (Maybe Text)

-- | HTTP status code indicating the result of the operation.
createCollectionResponse_statusCode :: Lens' CreateCollectionResponse (Maybe Natural)

-- | The response's http status code.
createCollectionResponse_httpStatus :: Lens' CreateCollectionResponse Int
instance GHC.Generics.Generic Network.AWS.Rekognition.CreateCollection.CreateCollection
instance GHC.Show.Show Network.AWS.Rekognition.CreateCollection.CreateCollection
instance GHC.Read.Read Network.AWS.Rekognition.CreateCollection.CreateCollection
instance GHC.Classes.Eq Network.AWS.Rekognition.CreateCollection.CreateCollection
instance GHC.Generics.Generic Network.AWS.Rekognition.CreateCollection.CreateCollectionResponse
instance GHC.Show.Show Network.AWS.Rekognition.CreateCollection.CreateCollectionResponse
instance GHC.Read.Read Network.AWS.Rekognition.CreateCollection.CreateCollectionResponse
instance GHC.Classes.Eq Network.AWS.Rekognition.CreateCollection.CreateCollectionResponse
instance Network.AWS.Types.AWSRequest Network.AWS.Rekognition.CreateCollection.CreateCollection
instance Control.DeepSeq.NFData Network.AWS.Rekognition.CreateCollection.CreateCollectionResponse
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.CreateCollection.CreateCollection
instance Control.DeepSeq.NFData Network.AWS.Rekognition.CreateCollection.CreateCollection
instance Network.AWS.Data.Headers.ToHeaders Network.AWS.Rekognition.CreateCollection.CreateCollection
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.CreateCollection.CreateCollection
instance Network.AWS.Data.Path.ToPath Network.AWS.Rekognition.CreateCollection.CreateCollection
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.CreateCollection.CreateCollection


-- | Compares a face in the <i>source</i> input image with each of the 100
--   largest faces detected in the <i>target</i> input image.
--   
--   If the source image contains multiple faces, the service detects the
--   largest face and compares it with each face detected in the target
--   image.
--   
--   CompareFaces uses machine learning algorithms, which are
--   probabilistic. A false negative is an incorrect prediction that a face
--   in the target image has a low similarity confidence score when
--   compared to the face in the source image. To reduce the probability of
--   false negatives, we recommend that you compare the target image
--   against multiple source images. If you plan to use
--   <tt>CompareFaces</tt> to make a decision that impacts an individual's
--   rights, privacy, or access to services, we recommend that you pass the
--   result to a human for review and further validation before taking
--   action.
--   
--   You pass the input and target images either as base64-encoded image
--   bytes or as references to images in an Amazon S3 bucket. If you use
--   the AWS CLI to call Amazon Rekognition operations, passing image bytes
--   isn't supported. The image must be formatted as a PNG or JPEG file.
--   
--   In response, the operation returns an array of face matches ordered by
--   similarity score in descending order. For each face match, the
--   response provides a bounding box of the face, facial landmarks, pose
--   details (pitch, role, and yaw), quality (brightness and sharpness),
--   and confidence value (indicating the level of confidence that the
--   bounding box contains a face). The response also provides a similarity
--   score, which indicates how closely the faces match.
--   
--   By default, only faces with a similarity score of greater than or
--   equal to 80% are returned in the response. You can change this value
--   by specifying the <tt>SimilarityThreshold</tt> parameter.
--   
--   <tt>CompareFaces</tt> also returns an array of faces that don't match
--   the source image. For each face, it returns a bounding box, confidence
--   value, landmarks, pose details, and quality. The response also returns
--   information about the face in the source image, including the bounding
--   box of the face and confidence value.
--   
--   The <tt>QualityFilter</tt> input parameter allows you to filter out
--   detected faces that don’t meet a required quality bar. The quality bar
--   is based on a variety of common use cases. Use <tt>QualityFilter</tt>
--   to set the quality bar by specifying <tt>LOW</tt>, <tt>MEDIUM</tt>, or
--   <tt>HIGH</tt>. If you do not want to filter detected faces, specify
--   <tt>NONE</tt>. The default value is <tt>NONE</tt>.
--   
--   If the image doesn't contain Exif metadata, <tt>CompareFaces</tt>
--   returns orientation information for the source and target images. Use
--   these values to display the images with the correct image orientation.
--   
--   If no faces are detected in the source or target images,
--   <tt>CompareFaces</tt> returns an <tt>InvalidParameterException</tt>
--   error.
--   
--   This is a stateless API operation. That is, data returned by this
--   operation doesn't persist.
--   
--   For an example, see Comparing Faces in Images in the Amazon
--   Rekognition Developer Guide.
--   
--   This operation requires permissions to perform the
--   <tt>rekognition:CompareFaces</tt> action.
module Network.AWS.Rekognition.CompareFaces

-- | <i>See:</i> <a>newCompareFaces</a> smart constructor.
data CompareFaces
CompareFaces' :: Maybe QualityFilter -> Maybe Double -> Image -> Image -> CompareFaces

-- | A filter that specifies a quality bar for how much filtering is done
--   to identify faces. Filtered faces aren't compared. If you specify
--   <tt>AUTO</tt>, Amazon Rekognition chooses the quality bar. If you
--   specify <tt>LOW</tt>, <tt>MEDIUM</tt>, or <tt>HIGH</tt>, filtering
--   removes all faces that don’t meet the chosen quality bar. The quality
--   bar is based on a variety of common use cases. Low-quality detections
--   can occur for a number of reasons. Some examples are an object that's
--   misidentified as a face, a face that's too blurry, or a face with a
--   pose that's too extreme to use. If you specify <tt>NONE</tt>, no
--   filtering is performed. The default value is <tt>NONE</tt>.
--   
--   To use quality filtering, the collection you are using must be
--   associated with version 3 of the face model or higher.
[$sel:qualityFilter:CompareFaces'] :: CompareFaces -> Maybe QualityFilter

-- | The minimum level of confidence in the face matches that a match must
--   meet to be included in the <tt>FaceMatches</tt> array.
[$sel:similarityThreshold:CompareFaces'] :: CompareFaces -> Maybe Double

-- | The input image as base64-encoded bytes or an S3 object. If you use
--   the AWS CLI to call Amazon Rekognition operations, passing
--   base64-encoded image bytes is not supported.
--   
--   If you are using an AWS SDK to call Amazon Rekognition, you might not
--   need to base64-encode image bytes passed using the <tt>Bytes</tt>
--   field. For more information, see Images in the Amazon Rekognition
--   developer guide.
[$sel:sourceImage:CompareFaces'] :: CompareFaces -> Image

-- | The target image as base64-encoded bytes or an S3 object. If you use
--   the AWS CLI to call Amazon Rekognition operations, passing
--   base64-encoded image bytes is not supported.
--   
--   If you are using an AWS SDK to call Amazon Rekognition, you might not
--   need to base64-encode image bytes passed using the <tt>Bytes</tt>
--   field. For more information, see Images in the Amazon Rekognition
--   developer guide.
[$sel:targetImage:CompareFaces'] :: CompareFaces -> Image

-- | Create a value of <a>CompareFaces</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:qualityFilter:CompareFaces'</a>,
--   <a>compareFaces_qualityFilter</a> - A filter that specifies a quality
--   bar for how much filtering is done to identify faces. Filtered faces
--   aren't compared. If you specify <tt>AUTO</tt>, Amazon Rekognition
--   chooses the quality bar. If you specify <tt>LOW</tt>, <tt>MEDIUM</tt>,
--   or <tt>HIGH</tt>, filtering removes all faces that don’t meet the
--   chosen quality bar. The quality bar is based on a variety of common
--   use cases. Low-quality detections can occur for a number of reasons.
--   Some examples are an object that's misidentified as a face, a face
--   that's too blurry, or a face with a pose that's too extreme to use. If
--   you specify <tt>NONE</tt>, no filtering is performed. The default
--   value is <tt>NONE</tt>.
--   
--   To use quality filtering, the collection you are using must be
--   associated with version 3 of the face model or higher.
--   
--   <a>$sel:similarityThreshold:CompareFaces'</a>,
--   <a>compareFaces_similarityThreshold</a> - The minimum level of
--   confidence in the face matches that a match must meet to be included
--   in the <tt>FaceMatches</tt> array.
--   
--   <a>$sel:sourceImage:CompareFaces'</a>, <a>compareFaces_sourceImage</a>
--   - The input image as base64-encoded bytes or an S3 object. If you use
--   the AWS CLI to call Amazon Rekognition operations, passing
--   base64-encoded image bytes is not supported.
--   
--   If you are using an AWS SDK to call Amazon Rekognition, you might not
--   need to base64-encode image bytes passed using the <tt>Bytes</tt>
--   field. For more information, see Images in the Amazon Rekognition
--   developer guide.
--   
--   <a>$sel:targetImage:CompareFaces'</a>, <a>compareFaces_targetImage</a>
--   - The target image as base64-encoded bytes or an S3 object. If you use
--   the AWS CLI to call Amazon Rekognition operations, passing
--   base64-encoded image bytes is not supported.
--   
--   If you are using an AWS SDK to call Amazon Rekognition, you might not
--   need to base64-encode image bytes passed using the <tt>Bytes</tt>
--   field. For more information, see Images in the Amazon Rekognition
--   developer guide.
newCompareFaces :: Image -> Image -> CompareFaces

-- | A filter that specifies a quality bar for how much filtering is done
--   to identify faces. Filtered faces aren't compared. If you specify
--   <tt>AUTO</tt>, Amazon Rekognition chooses the quality bar. If you
--   specify <tt>LOW</tt>, <tt>MEDIUM</tt>, or <tt>HIGH</tt>, filtering
--   removes all faces that don’t meet the chosen quality bar. The quality
--   bar is based on a variety of common use cases. Low-quality detections
--   can occur for a number of reasons. Some examples are an object that's
--   misidentified as a face, a face that's too blurry, or a face with a
--   pose that's too extreme to use. If you specify <tt>NONE</tt>, no
--   filtering is performed. The default value is <tt>NONE</tt>.
--   
--   To use quality filtering, the collection you are using must be
--   associated with version 3 of the face model or higher.
compareFaces_qualityFilter :: Lens' CompareFaces (Maybe QualityFilter)

-- | The minimum level of confidence in the face matches that a match must
--   meet to be included in the <tt>FaceMatches</tt> array.
compareFaces_similarityThreshold :: Lens' CompareFaces (Maybe Double)

-- | The input image as base64-encoded bytes or an S3 object. If you use
--   the AWS CLI to call Amazon Rekognition operations, passing
--   base64-encoded image bytes is not supported.
--   
--   If you are using an AWS SDK to call Amazon Rekognition, you might not
--   need to base64-encode image bytes passed using the <tt>Bytes</tt>
--   field. For more information, see Images in the Amazon Rekognition
--   developer guide.
compareFaces_sourceImage :: Lens' CompareFaces Image

-- | The target image as base64-encoded bytes or an S3 object. If you use
--   the AWS CLI to call Amazon Rekognition operations, passing
--   base64-encoded image bytes is not supported.
--   
--   If you are using an AWS SDK to call Amazon Rekognition, you might not
--   need to base64-encode image bytes passed using the <tt>Bytes</tt>
--   field. For more information, see Images in the Amazon Rekognition
--   developer guide.
compareFaces_targetImage :: Lens' CompareFaces Image

-- | <i>See:</i> <a>newCompareFacesResponse</a> smart constructor.
data CompareFacesResponse
CompareFacesResponse' :: Maybe [CompareFacesMatch] -> Maybe [ComparedFace] -> Maybe OrientationCorrection -> Maybe OrientationCorrection -> Maybe ComparedSourceImageFace -> Int -> CompareFacesResponse

-- | An array of faces in the target image that match the source image
--   face. Each <tt>CompareFacesMatch</tt> object provides the bounding
--   box, the confidence level that the bounding box contains a face, and
--   the similarity score for the face in the bounding box and the face in
--   the source image.
[$sel:faceMatches:CompareFacesResponse'] :: CompareFacesResponse -> Maybe [CompareFacesMatch]

-- | An array of faces in the target image that did not match the source
--   image face.
[$sel:unmatchedFaces:CompareFacesResponse'] :: CompareFacesResponse -> Maybe [ComparedFace]

-- | The value of <tt>TargetImageOrientationCorrection</tt> is always null.
--   
--   If the input image is in .jpeg format, it might contain exchangeable
--   image file format (Exif) metadata that includes the image's
--   orientation. Amazon Rekognition uses this orientation information to
--   perform image correction. The bounding box coordinates are translated
--   to represent object locations after the orientation information in the
--   Exif metadata is used to correct the image orientation. Images in .png
--   format don't contain Exif metadata.
--   
--   Amazon Rekognition doesn’t perform image correction for images in .png
--   format and .jpeg images without orientation information in the image
--   Exif metadata. The bounding box coordinates aren't translated and
--   represent the object locations before the image is rotated.
[$sel:targetImageOrientationCorrection:CompareFacesResponse'] :: CompareFacesResponse -> Maybe OrientationCorrection

-- | The value of <tt>SourceImageOrientationCorrection</tt> is always null.
--   
--   If the input image is in .jpeg format, it might contain exchangeable
--   image file format (Exif) metadata that includes the image's
--   orientation. Amazon Rekognition uses this orientation information to
--   perform image correction. The bounding box coordinates are translated
--   to represent object locations after the orientation information in the
--   Exif metadata is used to correct the image orientation. Images in .png
--   format don't contain Exif metadata.
--   
--   Amazon Rekognition doesn’t perform image correction for images in .png
--   format and .jpeg images without orientation information in the image
--   Exif metadata. The bounding box coordinates aren't translated and
--   represent the object locations before the image is rotated.
[$sel:sourceImageOrientationCorrection:CompareFacesResponse'] :: CompareFacesResponse -> Maybe OrientationCorrection

-- | The face in the source image that was used for comparison.
[$sel:sourceImageFace:CompareFacesResponse'] :: CompareFacesResponse -> Maybe ComparedSourceImageFace

-- | The response's http status code.
[$sel:httpStatus:CompareFacesResponse'] :: CompareFacesResponse -> Int

-- | Create a value of <a>CompareFacesResponse</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:faceMatches:CompareFacesResponse'</a>,
--   <a>compareFacesResponse_faceMatches</a> - An array of faces in the
--   target image that match the source image face. Each
--   <tt>CompareFacesMatch</tt> object provides the bounding box, the
--   confidence level that the bounding box contains a face, and the
--   similarity score for the face in the bounding box and the face in the
--   source image.
--   
--   <a>$sel:unmatchedFaces:CompareFacesResponse'</a>,
--   <a>compareFacesResponse_unmatchedFaces</a> - An array of faces in the
--   target image that did not match the source image face.
--   
--   <a>$sel:targetImageOrientationCorrection:CompareFacesResponse'</a>,
--   <a>compareFacesResponse_targetImageOrientationCorrection</a> - The
--   value of <tt>TargetImageOrientationCorrection</tt> is always null.
--   
--   If the input image is in .jpeg format, it might contain exchangeable
--   image file format (Exif) metadata that includes the image's
--   orientation. Amazon Rekognition uses this orientation information to
--   perform image correction. The bounding box coordinates are translated
--   to represent object locations after the orientation information in the
--   Exif metadata is used to correct the image orientation. Images in .png
--   format don't contain Exif metadata.
--   
--   Amazon Rekognition doesn’t perform image correction for images in .png
--   format and .jpeg images without orientation information in the image
--   Exif metadata. The bounding box coordinates aren't translated and
--   represent the object locations before the image is rotated.
--   
--   <a>$sel:sourceImageOrientationCorrection:CompareFacesResponse'</a>,
--   <a>compareFacesResponse_sourceImageOrientationCorrection</a> - The
--   value of <tt>SourceImageOrientationCorrection</tt> is always null.
--   
--   If the input image is in .jpeg format, it might contain exchangeable
--   image file format (Exif) metadata that includes the image's
--   orientation. Amazon Rekognition uses this orientation information to
--   perform image correction. The bounding box coordinates are translated
--   to represent object locations after the orientation information in the
--   Exif metadata is used to correct the image orientation. Images in .png
--   format don't contain Exif metadata.
--   
--   Amazon Rekognition doesn’t perform image correction for images in .png
--   format and .jpeg images without orientation information in the image
--   Exif metadata. The bounding box coordinates aren't translated and
--   represent the object locations before the image is rotated.
--   
--   <a>$sel:sourceImageFace:CompareFacesResponse'</a>,
--   <a>compareFacesResponse_sourceImageFace</a> - The face in the source
--   image that was used for comparison.
--   
--   <a>$sel:httpStatus:CompareFacesResponse'</a>,
--   <a>compareFacesResponse_httpStatus</a> - The response's http status
--   code.
newCompareFacesResponse :: Int -> CompareFacesResponse

-- | An array of faces in the target image that match the source image
--   face. Each <tt>CompareFacesMatch</tt> object provides the bounding
--   box, the confidence level that the bounding box contains a face, and
--   the similarity score for the face in the bounding box and the face in
--   the source image.
compareFacesResponse_faceMatches :: Lens' CompareFacesResponse (Maybe [CompareFacesMatch])

-- | An array of faces in the target image that did not match the source
--   image face.
compareFacesResponse_unmatchedFaces :: Lens' CompareFacesResponse (Maybe [ComparedFace])

-- | The value of <tt>TargetImageOrientationCorrection</tt> is always null.
--   
--   If the input image is in .jpeg format, it might contain exchangeable
--   image file format (Exif) metadata that includes the image's
--   orientation. Amazon Rekognition uses this orientation information to
--   perform image correction. The bounding box coordinates are translated
--   to represent object locations after the orientation information in the
--   Exif metadata is used to correct the image orientation. Images in .png
--   format don't contain Exif metadata.
--   
--   Amazon Rekognition doesn’t perform image correction for images in .png
--   format and .jpeg images without orientation information in the image
--   Exif metadata. The bounding box coordinates aren't translated and
--   represent the object locations before the image is rotated.
compareFacesResponse_targetImageOrientationCorrection :: Lens' CompareFacesResponse (Maybe OrientationCorrection)

-- | The value of <tt>SourceImageOrientationCorrection</tt> is always null.
--   
--   If the input image is in .jpeg format, it might contain exchangeable
--   image file format (Exif) metadata that includes the image's
--   orientation. Amazon Rekognition uses this orientation information to
--   perform image correction. The bounding box coordinates are translated
--   to represent object locations after the orientation information in the
--   Exif metadata is used to correct the image orientation. Images in .png
--   format don't contain Exif metadata.
--   
--   Amazon Rekognition doesn’t perform image correction for images in .png
--   format and .jpeg images without orientation information in the image
--   Exif metadata. The bounding box coordinates aren't translated and
--   represent the object locations before the image is rotated.
compareFacesResponse_sourceImageOrientationCorrection :: Lens' CompareFacesResponse (Maybe OrientationCorrection)

-- | The face in the source image that was used for comparison.
compareFacesResponse_sourceImageFace :: Lens' CompareFacesResponse (Maybe ComparedSourceImageFace)

-- | The response's http status code.
compareFacesResponse_httpStatus :: Lens' CompareFacesResponse Int
instance GHC.Generics.Generic Network.AWS.Rekognition.CompareFaces.CompareFaces
instance GHC.Show.Show Network.AWS.Rekognition.CompareFaces.CompareFaces
instance GHC.Read.Read Network.AWS.Rekognition.CompareFaces.CompareFaces
instance GHC.Classes.Eq Network.AWS.Rekognition.CompareFaces.CompareFaces
instance GHC.Generics.Generic Network.AWS.Rekognition.CompareFaces.CompareFacesResponse
instance GHC.Show.Show Network.AWS.Rekognition.CompareFaces.CompareFacesResponse
instance GHC.Read.Read Network.AWS.Rekognition.CompareFaces.CompareFacesResponse
instance GHC.Classes.Eq Network.AWS.Rekognition.CompareFaces.CompareFacesResponse
instance Network.AWS.Types.AWSRequest Network.AWS.Rekognition.CompareFaces.CompareFaces
instance Control.DeepSeq.NFData Network.AWS.Rekognition.CompareFaces.CompareFacesResponse
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.CompareFaces.CompareFaces
instance Control.DeepSeq.NFData Network.AWS.Rekognition.CompareFaces.CompareFaces
instance Network.AWS.Data.Headers.ToHeaders Network.AWS.Rekognition.CompareFaces.CompareFaces
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.CompareFaces.CompareFaces
instance Network.AWS.Data.Path.ToPath Network.AWS.Rekognition.CompareFaces.CompareFaces
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.CompareFaces.CompareFaces


-- | Removes one or more tags from an Amazon Rekognition collection, stream
--   processor, or Custom Labels model.
--   
--   This operation requires permissions to perform the
--   <tt>rekognition:UntagResource</tt> action.
module Network.AWS.Rekognition.UntagResource

-- | <i>See:</i> <a>newUntagResource</a> smart constructor.
data UntagResource
UntagResource' :: Text -> [Text] -> UntagResource

-- | Amazon Resource Name (ARN) of the model, collection, or stream
--   processor that you want to remove the tags from.
[$sel:resourceArn:UntagResource'] :: UntagResource -> Text

-- | A list of the tags that you want to remove.
[$sel:tagKeys:UntagResource'] :: UntagResource -> [Text]

-- | Create a value of <a>UntagResource</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:resourceArn:UntagResource'</a>,
--   <a>untagResource_resourceArn</a> - Amazon Resource Name (ARN) of the
--   model, collection, or stream processor that you want to remove the
--   tags from.
--   
--   <a>$sel:tagKeys:UntagResource'</a>, <a>untagResource_tagKeys</a> - A
--   list of the tags that you want to remove.
newUntagResource :: Text -> UntagResource

-- | Amazon Resource Name (ARN) of the model, collection, or stream
--   processor that you want to remove the tags from.
untagResource_resourceArn :: Lens' UntagResource Text

-- | A list of the tags that you want to remove.
untagResource_tagKeys :: Lens' UntagResource [Text]

-- | <i>See:</i> <a>newUntagResourceResponse</a> smart constructor.
data UntagResourceResponse
UntagResourceResponse' :: Int -> UntagResourceResponse

-- | The response's http status code.
[$sel:httpStatus:UntagResourceResponse'] :: UntagResourceResponse -> Int

-- | Create a value of <a>UntagResourceResponse</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:httpStatus:UntagResourceResponse'</a>,
--   <a>untagResourceResponse_httpStatus</a> - The response's http status
--   code.
newUntagResourceResponse :: Int -> UntagResourceResponse

-- | The response's http status code.
untagResourceResponse_httpStatus :: Lens' UntagResourceResponse Int
instance GHC.Generics.Generic Network.AWS.Rekognition.UntagResource.UntagResource
instance GHC.Show.Show Network.AWS.Rekognition.UntagResource.UntagResource
instance GHC.Read.Read Network.AWS.Rekognition.UntagResource.UntagResource
instance GHC.Classes.Eq Network.AWS.Rekognition.UntagResource.UntagResource
instance GHC.Generics.Generic Network.AWS.Rekognition.UntagResource.UntagResourceResponse
instance GHC.Show.Show Network.AWS.Rekognition.UntagResource.UntagResourceResponse
instance GHC.Read.Read Network.AWS.Rekognition.UntagResource.UntagResourceResponse
instance GHC.Classes.Eq Network.AWS.Rekognition.UntagResource.UntagResourceResponse
instance Network.AWS.Types.AWSRequest Network.AWS.Rekognition.UntagResource.UntagResource
instance Control.DeepSeq.NFData Network.AWS.Rekognition.UntagResource.UntagResourceResponse
instance Data.Hashable.Class.Hashable Network.AWS.Rekognition.UntagResource.UntagResource
instance Control.DeepSeq.NFData Network.AWS.Rekognition.UntagResource.UntagResource
instance Network.AWS.Data.Headers.ToHeaders Network.AWS.Rekognition.UntagResource.UntagResource
instance Data.Aeson.Types.ToJSON.ToJSON Network.AWS.Rekognition.UntagResource.UntagResource
instance Network.AWS.Data.Path.ToPath Network.AWS.Rekognition.UntagResource.UntagResource
instance Network.AWS.Data.Query.ToQuery Network.AWS.Rekognition.UntagResource.UntagResource


module Network.AWS.Rekognition.Lens

-- | An array of PPE types that you want to summarize.
detectProtectiveEquipment_summarizationAttributes :: Lens' DetectProtectiveEquipment (Maybe ProtectiveEquipmentSummarizationAttributes)

-- | The image in which you want to detect PPE on detected persons. The
--   image can be passed as image bytes or you can reference an image
--   stored in an Amazon S3 bucket.
detectProtectiveEquipment_image :: Lens' DetectProtectiveEquipment Image

-- | Summary information for the types of PPE specified in the
--   <tt>SummarizationAttributes</tt> input parameter.
detectProtectiveEquipmentResponse_summary :: Lens' DetectProtectiveEquipmentResponse (Maybe ProtectiveEquipmentSummary)

-- | The version number of the PPE detection model used to detect PPE in
--   the image.
detectProtectiveEquipmentResponse_protectiveEquipmentModelVersion :: Lens' DetectProtectiveEquipmentResponse (Maybe Text)

-- | An array of persons detected in the image (including persons not
--   wearing PPE).
detectProtectiveEquipmentResponse_persons :: Lens' DetectProtectiveEquipmentResponse (Maybe [ProtectiveEquipmentPerson])

-- | The response's http status code.
detectProtectiveEquipmentResponse_httpStatus :: Lens' DetectProtectiveEquipmentResponse Int

-- | The Amazon Resource Name (ARN) of the project that you want to delete.
deleteProject_projectArn :: Lens' DeleteProject Text

-- | The current status of the delete project operation.
deleteProjectResponse_status :: Lens' DeleteProjectResponse (Maybe ProjectStatus)

-- | The response's http status code.
deleteProjectResponse_httpStatus :: Lens' DeleteProjectResponse Int

-- | An identifier you specify that's returned in the completion
--   notification that's published to your Amazon Simple Notification
--   Service topic. For example, you can use <tt>JobTag</tt> to group
--   related jobs and identify them in the completion notification.
startCelebrityRecognition_jobTag :: Lens' StartCelebrityRecognition (Maybe Text)

-- | The Amazon SNS topic ARN that you want Amazon Rekognition Video to
--   publish the completion status of the celebrity recognition analysis
--   to. The Amazon SNS topic must have a topic name that begins with
--   <i>AmazonRekognition</i> if you are using the
--   AmazonRekognitionServiceRole permissions policy.
startCelebrityRecognition_notificationChannel :: Lens' StartCelebrityRecognition (Maybe NotificationChannel)

-- | Idempotent token used to identify the start request. If you use the
--   same token with multiple <tt>StartCelebrityRecognition</tt> requests,
--   the same <tt>JobId</tt> is returned. Use <tt>ClientRequestToken</tt>
--   to prevent the same job from being accidently started more than once.
startCelebrityRecognition_clientRequestToken :: Lens' StartCelebrityRecognition (Maybe Text)

-- | The video in which you want to recognize celebrities. The video must
--   be stored in an Amazon S3 bucket.
startCelebrityRecognition_video :: Lens' StartCelebrityRecognition Video

-- | The identifier for the celebrity recognition analysis job. Use
--   <tt>JobId</tt> to identify the job in a subsequent call to
--   <tt>GetCelebrityRecognition</tt>.
startCelebrityRecognitionResponse_jobId :: Lens' StartCelebrityRecognitionResponse (Maybe Text)

-- | The response's http status code.
startCelebrityRecognitionResponse_httpStatus :: Lens' StartCelebrityRecognitionResponse Int

-- | If the previous response was incomplete (because there are more
--   persons to retrieve), Amazon Rekognition Video returns a pagination
--   token in the response. You can use this pagination token to retrieve
--   the next set of persons.
getPersonTracking_nextToken :: Lens' GetPersonTracking (Maybe Text)

-- | Maximum number of results to return per paginated call. The largest
--   value you can specify is 1000. If you specify a value greater than
--   1000, a maximum of 1000 results is returned. The default value is
--   1000.
getPersonTracking_maxResults :: Lens' GetPersonTracking (Maybe Natural)

-- | Sort to use for elements in the <tt>Persons</tt> array. Use
--   <tt>TIMESTAMP</tt> to sort array elements by the time persons are
--   detected. Use <tt>INDEX</tt> to sort by the tracked persons. If you
--   sort by <tt>INDEX</tt>, the array elements for each person are sorted
--   by detection confidence. The default sort is by <tt>TIMESTAMP</tt>.
getPersonTracking_sortBy :: Lens' GetPersonTracking (Maybe PersonTrackingSortBy)

-- | The identifier for a job that tracks persons in a video. You get the
--   <tt>JobId</tt> from a call to <tt>StartPersonTracking</tt>.
getPersonTracking_jobId :: Lens' GetPersonTracking Text

-- | If the response is truncated, Amazon Rekognition Video returns this
--   token that you can use in the subsequent request to retrieve the next
--   set of persons.
getPersonTrackingResponse_nextToken :: Lens' GetPersonTrackingResponse (Maybe Text)

-- | Information about a video that Amazon Rekognition Video analyzed.
--   <tt>Videometadata</tt> is returned in every page of paginated
--   responses from a Amazon Rekognition Video operation.
getPersonTrackingResponse_videoMetadata :: Lens' GetPersonTrackingResponse (Maybe VideoMetadata)

-- | If the job fails, <tt>StatusMessage</tt> provides a descriptive error
--   message.
getPersonTrackingResponse_statusMessage :: Lens' GetPersonTrackingResponse (Maybe Text)

-- | The current status of the person tracking job.
getPersonTrackingResponse_jobStatus :: Lens' GetPersonTrackingResponse (Maybe VideoJobStatus)

-- | An array of the persons detected in the video and the time(s) their
--   path was tracked throughout the video. An array element will exist for
--   each time a person's path is tracked.
getPersonTrackingResponse_persons :: Lens' GetPersonTrackingResponse (Maybe [PersonDetection])

-- | The response's http status code.
getPersonTrackingResponse_httpStatus :: Lens' GetPersonTrackingResponse Int

-- | If the previous response was incomplete (because there are more labels
--   to retrieve), Amazon Rekognition Video returns a pagination token in
--   the response. You can use this pagination token to retrieve the next
--   set of text.
getTextDetection_nextToken :: Lens' GetTextDetection (Maybe Text)

-- | Maximum number of results to return per paginated call. The largest
--   value you can specify is 1000.
getTextDetection_maxResults :: Lens' GetTextDetection (Maybe Natural)

-- | Job identifier for the text detection operation for which you want
--   results returned. You get the job identifer from an initial call to
--   <tt>StartTextDetection</tt>.
getTextDetection_jobId :: Lens' GetTextDetection Text

-- | An array of text detected in the video. Each element contains the
--   detected text, the time in milliseconds from the start of the video
--   that the text was detected, and where it was detected on the screen.
getTextDetectionResponse_textDetections :: Lens' GetTextDetectionResponse (Maybe [TextDetectionResult])

-- | If the response is truncated, Amazon Rekognition Video returns this
--   token that you can use in the subsequent request to retrieve the next
--   set of text.
getTextDetectionResponse_nextToken :: Lens' GetTextDetectionResponse (Maybe Text)

-- | Undocumented member.
getTextDetectionResponse_videoMetadata :: Lens' GetTextDetectionResponse (Maybe VideoMetadata)

-- | If the job fails, <tt>StatusMessage</tt> provides a descriptive error
--   message.
getTextDetectionResponse_statusMessage :: Lens' GetTextDetectionResponse (Maybe Text)

-- | Version number of the text detection model that was used to detect
--   text.
getTextDetectionResponse_textModelVersion :: Lens' GetTextDetectionResponse (Maybe Text)

-- | Current status of the text detection job.
getTextDetectionResponse_jobStatus :: Lens' GetTextDetectionResponse (Maybe VideoJobStatus)

-- | The response's http status code.
getTextDetectionResponse_httpStatus :: Lens' GetTextDetectionResponse Int

-- | An identifier you specify that's returned in the completion
--   notification that's published to your Amazon Simple Notification
--   Service topic. For example, you can use <tt>JobTag</tt> to group
--   related jobs and identify them in the completion notification.
startSegmentDetection_jobTag :: Lens' StartSegmentDetection (Maybe Text)

-- | Filters for technical cue or shot detection.
startSegmentDetection_filters :: Lens' StartSegmentDetection (Maybe StartSegmentDetectionFilters)

-- | The ARN of the Amazon SNS topic to which you want Amazon Rekognition
--   Video to publish the completion status of the segment detection
--   operation. Note that the Amazon SNS topic must have a topic name that
--   begins with <i>AmazonRekognition</i> if you are using the
--   AmazonRekognitionServiceRole permissions policy to access the topic.
startSegmentDetection_notificationChannel :: Lens' StartSegmentDetection (Maybe NotificationChannel)

-- | Idempotent token used to identify the start request. If you use the
--   same token with multiple <tt>StartSegmentDetection</tt> requests, the
--   same <tt>JobId</tt> is returned. Use <tt>ClientRequestToken</tt> to
--   prevent the same job from being accidently started more than once.
startSegmentDetection_clientRequestToken :: Lens' StartSegmentDetection (Maybe Text)

-- | Undocumented member.
startSegmentDetection_video :: Lens' StartSegmentDetection Video

-- | An array of segment types to detect in the video. Valid values are
--   TECHNICAL_CUE and SHOT.
startSegmentDetection_segmentTypes :: Lens' StartSegmentDetection (NonEmpty SegmentType)

-- | Unique identifier for the segment detection job. The <tt>JobId</tt> is
--   returned from <tt>StartSegmentDetection</tt>.
startSegmentDetectionResponse_jobId :: Lens' StartSegmentDetectionResponse (Maybe Text)

-- | The response's http status code.
startSegmentDetectionResponse_httpStatus :: Lens' StartSegmentDetectionResponse Int

-- | Pagination token from the previous response.
listCollections_nextToken :: Lens' ListCollections (Maybe Text)

-- | Maximum number of collection IDs to return.
listCollections_maxResults :: Lens' ListCollections (Maybe Natural)

-- | An array of collection IDs.
listCollectionsResponse_collectionIds :: Lens' ListCollectionsResponse (Maybe [Text])

-- | If the result is truncated, the response provides a <tt>NextToken</tt>
--   that you can use in the subsequent request to fetch the next set of
--   collection IDs.
listCollectionsResponse_nextToken :: Lens' ListCollectionsResponse (Maybe Text)

-- | Version numbers of the face detection models associated with the
--   collections in the array <tt>CollectionIds</tt>. For example, the
--   value of <tt>FaceModelVersions[2]</tt> is the version number for the
--   face detection model used by the collection in
--   <tt>CollectionId[2]</tt>.
listCollectionsResponse_faceModelVersions :: Lens' ListCollectionsResponse (Maybe [Text])

-- | The response's http status code.
listCollectionsResponse_httpStatus :: Lens' ListCollectionsResponse Int

-- | The Amazon Resource Name(ARN) of the model version that you want to
--   start.
startProjectVersion_projectVersionArn :: Lens' StartProjectVersion Text

-- | The minimum number of inference units to use. A single inference unit
--   represents 1 hour of processing and can support up to 5 Transaction
--   Pers Second (TPS). Use a higher number to increase the TPS throughput
--   of your model. You are charged for the number of inference units that
--   you use.
startProjectVersion_minInferenceUnits :: Lens' StartProjectVersion Natural

-- | The current running status of the model.
startProjectVersionResponse_status :: Lens' StartProjectVersionResponse (Maybe ProjectVersionStatus)

-- | The response's http status code.
startProjectVersionResponse_httpStatus :: Lens' StartProjectVersionResponse Int

-- | ID of the collection to delete.
deleteCollection_collectionId :: Lens' DeleteCollection Text

-- | HTTP status code that indicates the result of the operation.
deleteCollectionResponse_statusCode :: Lens' DeleteCollectionResponse (Maybe Natural)

-- | The response's http status code.
deleteCollectionResponse_httpStatus :: Lens' DeleteCollectionResponse Int

-- | A set of tags (key-value pairs) that you want to attach to the
--   collection.
createCollection_tags :: Lens' CreateCollection (Maybe (HashMap Text Text))

-- | ID for the collection that you are creating.
createCollection_collectionId :: Lens' CreateCollection Text

-- | Version number of the face detection model associated with the
--   collection you are creating.
createCollectionResponse_faceModelVersion :: Lens' CreateCollectionResponse (Maybe Text)

-- | Amazon Resource Name (ARN) of the collection. You can use this to
--   manage permissions on your resources.
createCollectionResponse_collectionArn :: Lens' CreateCollectionResponse (Maybe Text)

-- | HTTP status code indicating the result of the operation.
createCollectionResponse_statusCode :: Lens' CreateCollectionResponse (Maybe Natural)

-- | The response's http status code.
createCollectionResponse_httpStatus :: Lens' CreateCollectionResponse Int

-- | The name of a stream processor created by CreateStreamProcessor.
stopStreamProcessor_name :: Lens' StopStreamProcessor Text

-- | The response's http status code.
stopStreamProcessorResponse_httpStatus :: Lens' StopStreamProcessorResponse Int

-- | Specifies the minimum confidence level for the labels to return.
--   Amazon Rekognition doesn't return any labels with confidence lower
--   than this specified value.
--   
--   If <tt>MinConfidence</tt> is not specified, the operation returns
--   labels with a confidence values greater than or equal to 55 percent.
detectLabels_minConfidence :: Lens' DetectLabels (Maybe Double)

-- | Maximum number of labels you want the service to return in the
--   response. The service returns the specified number of highest
--   confidence labels.
detectLabels_maxLabels :: Lens' DetectLabels (Maybe Natural)

-- | The input image as base64-encoded bytes or an S3 object. If you use
--   the AWS CLI to call Amazon Rekognition operations, passing image bytes
--   is not supported. Images stored in an S3 Bucket do not need to be
--   base64-encoded.
--   
--   If you are using an AWS SDK to call Amazon Rekognition, you might not
--   need to base64-encode image bytes passed using the <tt>Bytes</tt>
--   field. For more information, see Images in the Amazon Rekognition
--   developer guide.
detectLabels_image :: Lens' DetectLabels Image

-- | An array of labels for the real-world objects detected.
detectLabelsResponse_labels :: Lens' DetectLabelsResponse (Maybe [Label])

-- | The value of <tt>OrientationCorrection</tt> is always null.
--   
--   If the input image is in .jpeg format, it might contain exchangeable
--   image file format (Exif) metadata that includes the image's
--   orientation. Amazon Rekognition uses this orientation information to
--   perform image correction. The bounding box coordinates are translated
--   to represent object locations after the orientation information in the
--   Exif metadata is used to correct the image orientation. Images in .png
--   format don't contain Exif metadata.
--   
--   Amazon Rekognition doesn’t perform image correction for images in .png
--   format and .jpeg images without orientation information in the image
--   Exif metadata. The bounding box coordinates aren't translated and
--   represent the object locations before the image is rotated.
detectLabelsResponse_orientationCorrection :: Lens' DetectLabelsResponse (Maybe OrientationCorrection)

-- | Version number of the label detection model that was used to detect
--   labels.
detectLabelsResponse_labelModelVersion :: Lens' DetectLabelsResponse (Maybe Text)

-- | The response's http status code.
detectLabelsResponse_httpStatus :: Lens' DetectLabelsResponse Int

-- | Amazon Resource Name (ARN) of the model, collection, or stream
--   processor that contains the tags that you want a list of.
listTagsForResource_resourceArn :: Lens' ListTagsForResource Text

-- | A list of key-value tags assigned to the resource.
listTagsForResourceResponse_tags :: Lens' ListTagsForResourceResponse (Maybe (HashMap Text Text))

-- | The response's http status code.
listTagsForResourceResponse_httpStatus :: Lens' ListTagsForResourceResponse Int

-- | An identifier you specify that's returned in the completion
--   notification that's published to your Amazon Simple Notification
--   Service topic. For example, you can use <tt>JobTag</tt> to group
--   related jobs and identify them in the completion notification.
startContentModeration_jobTag :: Lens' StartContentModeration (Maybe Text)

-- | The Amazon SNS topic ARN that you want Amazon Rekognition Video to
--   publish the completion status of the content analysis to. The Amazon
--   SNS topic must have a topic name that begins with
--   <i>AmazonRekognition</i> if you are using the
--   AmazonRekognitionServiceRole permissions policy to access the topic.
startContentModeration_notificationChannel :: Lens' StartContentModeration (Maybe NotificationChannel)

-- | Idempotent token used to identify the start request. If you use the
--   same token with multiple <tt>StartContentModeration</tt> requests, the
--   same <tt>JobId</tt> is returned. Use <tt>ClientRequestToken</tt> to
--   prevent the same job from being accidently started more than once.
startContentModeration_clientRequestToken :: Lens' StartContentModeration (Maybe Text)

-- | Specifies the minimum confidence that Amazon Rekognition must have in
--   order to return a moderated content label. Confidence represents how
--   certain Amazon Rekognition is that the moderated content is correctly
--   identified. 0 is the lowest confidence. 100 is the highest confidence.
--   Amazon Rekognition doesn't return any moderated content labels with a
--   confidence level lower than this specified value. If you don't specify
--   <tt>MinConfidence</tt>, <tt>GetContentModeration</tt> returns labels
--   with confidence values greater than or equal to 50 percent.
startContentModeration_minConfidence :: Lens' StartContentModeration (Maybe Double)

-- | The video in which you want to detect inappropriate, unwanted, or
--   offensive content. The video must be stored in an Amazon S3 bucket.
startContentModeration_video :: Lens' StartContentModeration Video

-- | The identifier for the content analysis job. Use <tt>JobId</tt> to
--   identify the job in a subsequent call to
--   <tt>GetContentModeration</tt>.
startContentModerationResponse_jobId :: Lens' StartContentModerationResponse (Maybe Text)

-- | The response's http status code.
startContentModerationResponse_httpStatus :: Lens' StartContentModerationResponse Int

-- | A filter that specifies a quality bar for how much filtering is done
--   to identify faces. Filtered faces aren't searched for in the
--   collection. If you specify <tt>AUTO</tt>, Amazon Rekognition chooses
--   the quality bar. If you specify <tt>LOW</tt>, <tt>MEDIUM</tt>, or
--   <tt>HIGH</tt>, filtering removes all faces that don’t meet the chosen
--   quality bar. The quality bar is based on a variety of common use
--   cases. Low-quality detections can occur for a number of reasons. Some
--   examples are an object that's misidentified as a face, a face that's
--   too blurry, or a face with a pose that's too extreme to use. If you
--   specify <tt>NONE</tt>, no filtering is performed. The default value is
--   <tt>NONE</tt>.
--   
--   To use quality filtering, the collection you are using must be
--   associated with version 3 of the face model or higher.
searchFacesByImage_qualityFilter :: Lens' SearchFacesByImage (Maybe QualityFilter)

-- | (Optional) Specifies the minimum confidence in the face match to
--   return. For example, don't return any matches where confidence in
--   matches is less than 70%. The default value is 80%.
searchFacesByImage_faceMatchThreshold :: Lens' SearchFacesByImage (Maybe Double)

-- | Maximum number of faces to return. The operation returns the maximum
--   number of faces with the highest confidence in the match.
searchFacesByImage_maxFaces :: Lens' SearchFacesByImage (Maybe Natural)

-- | ID of the collection to search.
searchFacesByImage_collectionId :: Lens' SearchFacesByImage Text

-- | The input image as base64-encoded bytes or an S3 object. If you use
--   the AWS CLI to call Amazon Rekognition operations, passing
--   base64-encoded image bytes is not supported.
--   
--   If you are using an AWS SDK to call Amazon Rekognition, you might not
--   need to base64-encode image bytes passed using the <tt>Bytes</tt>
--   field. For more information, see Images in the Amazon Rekognition
--   developer guide.
searchFacesByImage_image :: Lens' SearchFacesByImage Image

-- | An array of faces that match the input face, along with the confidence
--   in the match.
searchFacesByImageResponse_faceMatches :: Lens' SearchFacesByImageResponse (Maybe [FaceMatch])

-- | Version number of the face detection model associated with the input
--   collection (<tt>CollectionId</tt>).
searchFacesByImageResponse_faceModelVersion :: Lens' SearchFacesByImageResponse (Maybe Text)

-- | The bounding box around the face in the input image that Amazon
--   Rekognition used for the search.
searchFacesByImageResponse_searchedFaceBoundingBox :: Lens' SearchFacesByImageResponse (Maybe BoundingBox)

-- | The level of confidence that the <tt>searchedFaceBoundingBox</tt>,
--   contains a face.
searchFacesByImageResponse_searchedFaceConfidence :: Lens' SearchFacesByImageResponse (Maybe Double)

-- | The response's http status code.
searchFacesByImageResponse_httpStatus :: Lens' SearchFacesByImageResponse Int

-- | If the previous response was incomplete (because there are more stream
--   processors to retrieve), Amazon Rekognition Video returns a pagination
--   token in the response. You can use this pagination token to retrieve
--   the next set of stream processors.
listStreamProcessors_nextToken :: Lens' ListStreamProcessors (Maybe Text)

-- | Maximum number of stream processors you want Amazon Rekognition Video
--   to return in the response. The default is 1000.
listStreamProcessors_maxResults :: Lens' ListStreamProcessors (Maybe Natural)

-- | List of stream processors that you have created.
listStreamProcessorsResponse_streamProcessors :: Lens' ListStreamProcessorsResponse (Maybe [StreamProcessor])

-- | If the response is truncated, Amazon Rekognition Video returns this
--   token that you can use in the subsequent request to retrieve the next
--   set of stream processors.
listStreamProcessorsResponse_nextToken :: Lens' ListStreamProcessorsResponse (Maybe Text)

-- | The response's http status code.
listStreamProcessorsResponse_httpStatus :: Lens' ListStreamProcessorsResponse Int

-- | The ID of the collection to describe.
describeCollection_collectionId :: Lens' DescribeCollection Text

-- | The version of the face model that's used by the collection for face
--   detection.
--   
--   For more information, see Model Versioning in the Amazon Rekognition
--   Developer Guide.
describeCollectionResponse_faceModelVersion :: Lens' DescribeCollectionResponse (Maybe Text)

-- | The number of faces that are indexed into the collection. To index
--   faces into a collection, use IndexFaces.
describeCollectionResponse_faceCount :: Lens' DescribeCollectionResponse (Maybe Natural)

-- | The number of milliseconds since the Unix epoch time until the
--   creation of the collection. The Unix epoch time is 00:00:00
--   Coordinated Universal Time (UTC), Thursday, 1 January 1970.
describeCollectionResponse_creationTimestamp :: Lens' DescribeCollectionResponse (Maybe UTCTime)

-- | The Amazon Resource Name (ARN) of the collection.
describeCollectionResponse_collectionARN :: Lens' DescribeCollectionResponse (Maybe Text)

-- | The response's http status code.
describeCollectionResponse_httpStatus :: Lens' DescribeCollectionResponse Int

-- | The Amazon Resource Name (ARN) of the model version that you want to
--   delete.
deleteProjectVersion_projectVersionArn :: Lens' DeleteProjectVersion Text

-- | The status of the deletion operation.
deleteProjectVersionResponse_status :: Lens' DeleteProjectVersionResponse (Maybe ProjectVersionStatus)

-- | The response's http status code.
deleteProjectVersionResponse_httpStatus :: Lens' DeleteProjectVersionResponse Int

-- | If the previous response was incomplete (because there is more results
--   to retrieve), Amazon Rekognition Custom Labels returns a pagination
--   token in the response. You can use this pagination token to retrieve
--   the next set of results.
describeProjectVersions_nextToken :: Lens' DescribeProjectVersions (Maybe Text)

-- | A list of model version names that you want to describe. You can add
--   up to 10 model version names to the list. If you don't specify a
--   value, all model descriptions are returned. A version name is part of
--   a model (ProjectVersion) ARN. For example,
--   <tt>my-model.2020-01-21T09.10.15</tt> is the version name in the
--   following ARN.
--   <tt>arn:aws:rekognition:us-east-1:123456789012:project/getting-started/version/my-model.2020-01-21T09.10.15/1234567890123</tt>.
describeProjectVersions_versionNames :: Lens' DescribeProjectVersions (Maybe (NonEmpty Text))

-- | The maximum number of results to return per paginated call. The
--   largest value you can specify is 100. If you specify a value greater
--   than 100, a ValidationException error occurs. The default value is
--   100.
describeProjectVersions_maxResults :: Lens' DescribeProjectVersions (Maybe Natural)

-- | The Amazon Resource Name (ARN) of the project that contains the models
--   you want to describe.
describeProjectVersions_projectArn :: Lens' DescribeProjectVersions Text

-- | If the previous response was incomplete (because there is more results
--   to retrieve), Amazon Rekognition Custom Labels returns a pagination
--   token in the response. You can use this pagination token to retrieve
--   the next set of results.
describeProjectVersionsResponse_nextToken :: Lens' DescribeProjectVersionsResponse (Maybe Text)

-- | A list of model descriptions. The list is sorted by the creation date
--   and time of the model versions, latest to earliest.
describeProjectVersionsResponse_projectVersionDescriptions :: Lens' DescribeProjectVersionsResponse (Maybe [ProjectVersionDescription])

-- | The response's http status code.
describeProjectVersionsResponse_httpStatus :: Lens' DescribeProjectVersionsResponse Int

-- | The input image as base64-encoded bytes or an S3 object. If you use
--   the AWS CLI to call Amazon Rekognition operations, passing
--   base64-encoded image bytes is not supported.
--   
--   If you are using an AWS SDK to call Amazon Rekognition, you might not
--   need to base64-encode image bytes passed using the <tt>Bytes</tt>
--   field. For more information, see Images in the Amazon Rekognition
--   developer guide.
recognizeCelebrities_image :: Lens' RecognizeCelebrities Image

-- | Details about each celebrity found in the image. Amazon Rekognition
--   can detect a maximum of 64 celebrities in an image. Each celebrity
--   object includes the following attributes: <tt>Face</tt>,
--   <tt>Confidence</tt>, <tt>Emotions</tt>, <tt>Landmarks</tt>,
--   <tt>Pose</tt>, <tt>Quality</tt>, <tt>Smile</tt>, <tt>Id</tt>,
--   <tt>KnownGender</tt>, <tt>MatchConfidence</tt>, <tt>Name</tt>,
--   <tt>Urls</tt>.
recognizeCelebritiesResponse_celebrityFaces :: Lens' RecognizeCelebritiesResponse (Maybe [Celebrity])

-- | Support for estimating image orientation using the the
--   OrientationCorrection field has ceased as of August 2021. Any returned
--   values for this field included in an API response will always be NULL.
--   
--   The orientation of the input image (counterclockwise direction). If
--   your application displays the image, you can use this value to correct
--   the orientation. The bounding box coordinates returned in
--   <tt>CelebrityFaces</tt> and <tt>UnrecognizedFaces</tt> represent face
--   locations before the image orientation is corrected.
--   
--   If the input image is in .jpeg format, it might contain exchangeable
--   image (Exif) metadata that includes the image's orientation. If so,
--   and the Exif metadata for the input image populates the orientation
--   field, the value of <tt>OrientationCorrection</tt> is null. The
--   <tt>CelebrityFaces</tt> and <tt>UnrecognizedFaces</tt> bounding box
--   coordinates represent face locations after Exif metadata is used to
--   correct the image orientation. Images in .png format don't contain
--   Exif metadata.
recognizeCelebritiesResponse_orientationCorrection :: Lens' RecognizeCelebritiesResponse (Maybe OrientationCorrection)

-- | Details about each unrecognized face in the image.
recognizeCelebritiesResponse_unrecognizedFaces :: Lens' RecognizeCelebritiesResponse (Maybe [ComparedFace])

-- | The response's http status code.
recognizeCelebritiesResponse_httpStatus :: Lens' RecognizeCelebritiesResponse Int

-- | Specifies the minimum confidence level for the labels to return.
--   <tt>DetectCustomLabels</tt> doesn't return any labels with a
--   confidence value that's lower than this specified value. If you
--   specify a value of 0, <tt>DetectCustomLabels</tt> returns all labels,
--   regardless of the assumed threshold applied to each label. If you
--   don't specify a value for <tt>MinConfidence</tt>,
--   <tt>DetectCustomLabels</tt> returns labels based on the assumed
--   threshold of each label.
detectCustomLabels_minConfidence :: Lens' DetectCustomLabels (Maybe Double)

-- | Maximum number of results you want the service to return in the
--   response. The service returns the specified number of highest
--   confidence labels ranked from highest confidence to lowest.
detectCustomLabels_maxResults :: Lens' DetectCustomLabels (Maybe Natural)

-- | The ARN of the model version that you want to use.
detectCustomLabels_projectVersionArn :: Lens' DetectCustomLabels Text

-- | Undocumented member.
detectCustomLabels_image :: Lens' DetectCustomLabels Image

-- | An array of custom labels detected in the input image.
detectCustomLabelsResponse_customLabels :: Lens' DetectCustomLabelsResponse (Maybe [CustomLabel])

-- | The response's http status code.
detectCustomLabelsResponse_httpStatus :: Lens' DetectCustomLabelsResponse Int

-- | If the previous response was incomplete (because there is more search
--   results to retrieve), Amazon Rekognition Video returns a pagination
--   token in the response. You can use this pagination token to retrieve
--   the next set of search results.
getFaceSearch_nextToken :: Lens' GetFaceSearch (Maybe Text)

-- | Maximum number of results to return per paginated call. The largest
--   value you can specify is 1000. If you specify a value greater than
--   1000, a maximum of 1000 results is returned. The default value is
--   1000.
getFaceSearch_maxResults :: Lens' GetFaceSearch (Maybe Natural)

-- | Sort to use for grouping faces in the response. Use <tt>TIMESTAMP</tt>
--   to group faces by the time that they are recognized. Use
--   <tt>INDEX</tt> to sort by recognized faces.
getFaceSearch_sortBy :: Lens' GetFaceSearch (Maybe FaceSearchSortBy)

-- | The job identifer for the search request. You get the job identifier
--   from an initial call to <tt>StartFaceSearch</tt>.
getFaceSearch_jobId :: Lens' GetFaceSearch Text

-- | If the response is truncated, Amazon Rekognition Video returns this
--   token that you can use in the subsequent request to retrieve the next
--   set of search results.
getFaceSearchResponse_nextToken :: Lens' GetFaceSearchResponse (Maybe Text)

-- | Information about a video that Amazon Rekognition analyzed.
--   <tt>Videometadata</tt> is returned in every page of paginated
--   responses from a Amazon Rekognition Video operation.
getFaceSearchResponse_videoMetadata :: Lens' GetFaceSearchResponse (Maybe VideoMetadata)

-- | If the job fails, <tt>StatusMessage</tt> provides a descriptive error
--   message.
getFaceSearchResponse_statusMessage :: Lens' GetFaceSearchResponse (Maybe Text)

-- | The current status of the face search job.
getFaceSearchResponse_jobStatus :: Lens' GetFaceSearchResponse (Maybe VideoJobStatus)

-- | An array of persons, PersonMatch, in the video whose face(s) match the
--   face(s) in an Amazon Rekognition collection. It also includes time
--   information for when persons are matched in the video. You specify the
--   input collection in an initial call to <tt>StartFaceSearch</tt>. Each
--   <tt>Persons</tt> element includes a time the person was matched, face
--   match details (<tt>FaceMatches</tt>) for matching faces in the
--   collection, and person information (<tt>Person</tt>) for the matched
--   person.
getFaceSearchResponse_persons :: Lens' GetFaceSearchResponse (Maybe [PersonMatch])

-- | The response's http status code.
getFaceSearchResponse_httpStatus :: Lens' GetFaceSearchResponse Int

-- | An identifier you specify that's returned in the completion
--   notification that's published to your Amazon Simple Notification
--   Service topic. For example, you can use <tt>JobTag</tt> to group
--   related jobs and identify them in the completion notification.
startLabelDetection_jobTag :: Lens' StartLabelDetection (Maybe Text)

-- | The Amazon SNS topic ARN you want Amazon Rekognition Video to publish
--   the completion status of the label detection operation to. The Amazon
--   SNS topic must have a topic name that begins with
--   <i>AmazonRekognition</i> if you are using the
--   AmazonRekognitionServiceRole permissions policy.
startLabelDetection_notificationChannel :: Lens' StartLabelDetection (Maybe NotificationChannel)

-- | Idempotent token used to identify the start request. If you use the
--   same token with multiple <tt>StartLabelDetection</tt> requests, the
--   same <tt>JobId</tt> is returned. Use <tt>ClientRequestToken</tt> to
--   prevent the same job from being accidently started more than once.
startLabelDetection_clientRequestToken :: Lens' StartLabelDetection (Maybe Text)

-- | Specifies the minimum confidence that Amazon Rekognition Video must
--   have in order to return a detected label. Confidence represents how
--   certain Amazon Rekognition is that a label is correctly identified.0
--   is the lowest confidence. 100 is the highest confidence. Amazon
--   Rekognition Video doesn't return any labels with a confidence level
--   lower than this specified value.
--   
--   If you don't specify <tt>MinConfidence</tt>, the operation returns
--   labels with confidence values greater than or equal to 50 percent.
startLabelDetection_minConfidence :: Lens' StartLabelDetection (Maybe Double)

-- | The video in which you want to detect labels. The video must be stored
--   in an Amazon S3 bucket.
startLabelDetection_video :: Lens' StartLabelDetection Video

-- | The identifier for the label detection job. Use <tt>JobId</tt> to
--   identify the job in a subsequent call to <tt>GetLabelDetection</tt>.
startLabelDetectionResponse_jobId :: Lens' StartLabelDetectionResponse (Maybe Text)

-- | The response's http status code.
startLabelDetectionResponse_httpStatus :: Lens' StartLabelDetectionResponse Int

-- | Optional value specifying the minimum confidence in the face match to
--   return. For example, don't return any matches where confidence in
--   matches is less than 70%. The default value is 80%.
searchFaces_faceMatchThreshold :: Lens' SearchFaces (Maybe Double)

-- | Maximum number of faces to return. The operation returns the maximum
--   number of faces with the highest confidence in the match.
searchFaces_maxFaces :: Lens' SearchFaces (Maybe Natural)

-- | ID of the collection the face belongs to.
searchFaces_collectionId :: Lens' SearchFaces Text

-- | ID of a face to find matches for in the collection.
searchFaces_faceId :: Lens' SearchFaces Text

-- | An array of faces that matched the input face, along with the
--   confidence in the match.
searchFacesResponse_faceMatches :: Lens' SearchFacesResponse (Maybe [FaceMatch])

-- | Version number of the face detection model associated with the input
--   collection (<tt>CollectionId</tt>).
searchFacesResponse_faceModelVersion :: Lens' SearchFacesResponse (Maybe Text)

-- | ID of the face that was searched for matches in a collection.
searchFacesResponse_searchedFaceId :: Lens' SearchFacesResponse (Maybe Text)

-- | The response's http status code.
searchFacesResponse_httpStatus :: Lens' SearchFacesResponse Int

-- | The ID you want to assign to all the faces detected in the image.
indexFaces_externalImageId :: Lens' IndexFaces (Maybe Text)

-- | A filter that specifies a quality bar for how much filtering is done
--   to identify faces. Filtered faces aren't indexed. If you specify
--   <tt>AUTO</tt>, Amazon Rekognition chooses the quality bar. If you
--   specify <tt>LOW</tt>, <tt>MEDIUM</tt>, or <tt>HIGH</tt>, filtering
--   removes all faces that don’t meet the chosen quality bar. The default
--   value is <tt>AUTO</tt>. The quality bar is based on a variety of
--   common use cases. Low-quality detections can occur for a number of
--   reasons. Some examples are an object that's misidentified as a face, a
--   face that's too blurry, or a face with a pose that's too extreme to
--   use. If you specify <tt>NONE</tt>, no filtering is performed.
--   
--   To use quality filtering, the collection you are using must be
--   associated with version 3 of the face model or higher.
indexFaces_qualityFilter :: Lens' IndexFaces (Maybe QualityFilter)

-- | The maximum number of faces to index. The value of <tt>MaxFaces</tt>
--   must be greater than or equal to 1. <tt>IndexFaces</tt> returns no
--   more than 100 detected faces in an image, even if you specify a larger
--   value for <tt>MaxFaces</tt>.
--   
--   If <tt>IndexFaces</tt> detects more faces than the value of
--   <tt>MaxFaces</tt>, the faces with the lowest quality are filtered out
--   first. If there are still more faces than the value of
--   <tt>MaxFaces</tt>, the faces with the smallest bounding boxes are
--   filtered out (up to the number that's needed to satisfy the value of
--   <tt>MaxFaces</tt>). Information about the unindexed faces is available
--   in the <tt>UnindexedFaces</tt> array.
--   
--   The faces that are returned by <tt>IndexFaces</tt> are sorted by the
--   largest face bounding box size to the smallest size, in descending
--   order.
--   
--   <tt>MaxFaces</tt> can be used with a collection associated with any
--   version of the face model.
indexFaces_maxFaces :: Lens' IndexFaces (Maybe Natural)

-- | An array of facial attributes that you want to be returned. This can
--   be the default list of attributes or all attributes. If you don't
--   specify a value for <tt>Attributes</tt> or if you specify
--   <tt>["DEFAULT"]</tt>, the API returns the following subset of facial
--   attributes: <tt>BoundingBox</tt>, <tt>Confidence</tt>, <tt>Pose</tt>,
--   <tt>Quality</tt>, and <tt>Landmarks</tt>. If you provide
--   <tt>["ALL"]</tt>, all facial attributes are returned, but the
--   operation takes longer to complete.
--   
--   If you provide both, <tt>["ALL", "DEFAULT"]</tt>, the service uses a
--   logical AND operator to determine which attributes to return (in this
--   case, all attributes).
indexFaces_detectionAttributes :: Lens' IndexFaces (Maybe [Attribute])

-- | The ID of an existing collection to which you want to add the faces
--   that are detected in the input images.
indexFaces_collectionId :: Lens' IndexFaces Text

-- | The input image as base64-encoded bytes or an S3 object. If you use
--   the AWS CLI to call Amazon Rekognition operations, passing
--   base64-encoded image bytes isn't supported.
--   
--   If you are using an AWS SDK to call Amazon Rekognition, you might not
--   need to base64-encode image bytes passed using the <tt>Bytes</tt>
--   field. For more information, see Images in the Amazon Rekognition
--   developer guide.
indexFaces_image :: Lens' IndexFaces Image

-- | The version number of the face detection model that's associated with
--   the input collection (<tt>CollectionId</tt>).
indexFacesResponse_faceModelVersion :: Lens' IndexFacesResponse (Maybe Text)

-- | An array of faces detected and added to the collection. For more
--   information, see Searching Faces in a Collection in the Amazon
--   Rekognition Developer Guide.
indexFacesResponse_faceRecords :: Lens' IndexFacesResponse (Maybe [FaceRecord])

-- | If your collection is associated with a face detection model that's
--   later than version 3.0, the value of <tt>OrientationCorrection</tt> is
--   always null and no orientation information is returned.
--   
--   If your collection is associated with a face detection model that's
--   version 3.0 or earlier, the following applies:
--   
--   <ul>
--   <li>If the input image is in .jpeg format, it might contain
--   exchangeable image file format (Exif) metadata that includes the
--   image's orientation. Amazon Rekognition uses this orientation
--   information to perform image correction - the bounding box coordinates
--   are translated to represent object locations after the orientation
--   information in the Exif metadata is used to correct the image
--   orientation. Images in .png format don't contain Exif metadata. The
--   value of <tt>OrientationCorrection</tt> is null.</li>
--   <li>If the image doesn't contain orientation information in its Exif
--   metadata, Amazon Rekognition returns an estimated orientation
--   (ROTATE_0, ROTATE_90, ROTATE_180, ROTATE_270). Amazon Rekognition
--   doesn’t perform image correction for images. The bounding box
--   coordinates aren't translated and represent the object locations
--   before the image is rotated.</li>
--   </ul>
--   
--   Bounding box information is returned in the <tt>FaceRecords</tt>
--   array. You can get the version of the face detection model by calling
--   DescribeCollection.
indexFacesResponse_orientationCorrection :: Lens' IndexFacesResponse (Maybe OrientationCorrection)

-- | An array of faces that were detected in the image but weren't indexed.
--   They weren't indexed because the quality filter identified them as low
--   quality, or the <tt>MaxFaces</tt> request parameter filtered them out.
--   To use the quality filter, you specify the <tt>QualityFilter</tt>
--   request parameter.
indexFacesResponse_unindexedFaces :: Lens' IndexFacesResponse (Maybe [UnindexedFace])

-- | The response's http status code.
indexFacesResponse_httpStatus :: Lens' IndexFacesResponse Int

-- | If the previous response was incomplete (because there are more labels
--   to retrieve), Amazon Rekognition Video returns a pagination token in
--   the response. You can use this pagination token to retrieve the next
--   set of labels.
getLabelDetection_nextToken :: Lens' GetLabelDetection (Maybe Text)

-- | Maximum number of results to return per paginated call. The largest
--   value you can specify is 1000. If you specify a value greater than
--   1000, a maximum of 1000 results is returned. The default value is
--   1000.
getLabelDetection_maxResults :: Lens' GetLabelDetection (Maybe Natural)

-- | Sort to use for elements in the <tt>Labels</tt> array. Use
--   <tt>TIMESTAMP</tt> to sort array elements by the time labels are
--   detected. Use <tt>NAME</tt> to alphabetically group elements for a
--   label together. Within each label group, the array element are sorted
--   by detection confidence. The default sort is by <tt>TIMESTAMP</tt>.
getLabelDetection_sortBy :: Lens' GetLabelDetection (Maybe LabelDetectionSortBy)

-- | Job identifier for the label detection operation for which you want
--   results returned. You get the job identifer from an initial call to
--   <tt>StartlabelDetection</tt>.
getLabelDetection_jobId :: Lens' GetLabelDetection Text

-- | If the response is truncated, Amazon Rekognition Video returns this
--   token that you can use in the subsequent request to retrieve the next
--   set of labels.
getLabelDetectionResponse_nextToken :: Lens' GetLabelDetectionResponse (Maybe Text)

-- | Information about a video that Amazon Rekognition Video analyzed.
--   <tt>Videometadata</tt> is returned in every page of paginated
--   responses from a Amazon Rekognition video operation.
getLabelDetectionResponse_videoMetadata :: Lens' GetLabelDetectionResponse (Maybe VideoMetadata)

-- | If the job fails, <tt>StatusMessage</tt> provides a descriptive error
--   message.
getLabelDetectionResponse_statusMessage :: Lens' GetLabelDetectionResponse (Maybe Text)

-- | An array of labels detected in the video. Each element contains the
--   detected label and the time, in milliseconds from the start of the
--   video, that the label was detected.
getLabelDetectionResponse_labels :: Lens' GetLabelDetectionResponse (Maybe [LabelDetection])

-- | The current status of the label detection job.
getLabelDetectionResponse_jobStatus :: Lens' GetLabelDetectionResponse (Maybe VideoJobStatus)

-- | Version number of the label detection model that was used to detect
--   labels.
getLabelDetectionResponse_labelModelVersion :: Lens' GetLabelDetectionResponse (Maybe Text)

-- | The response's http status code.
getLabelDetectionResponse_httpStatus :: Lens' GetLabelDetectionResponse Int

-- | The Amazon Resource Name (ARN) of the model version that you want to
--   delete.
--   
--   This operation requires permissions to perform the
--   <tt>rekognition:StopProjectVersion</tt> action.
stopProjectVersion_projectVersionArn :: Lens' StopProjectVersion Text

-- | The current status of the stop operation.
stopProjectVersionResponse_status :: Lens' StopProjectVersionResponse (Maybe ProjectVersionStatus)

-- | The response's http status code.
stopProjectVersionResponse_httpStatus :: Lens' StopProjectVersionResponse Int

-- | Name of the stream processor for which you want information.
describeStreamProcessor_name :: Lens' DescribeStreamProcessor Text

-- | Current status of the stream processor.
describeStreamProcessorResponse_status :: Lens' DescribeStreamProcessorResponse (Maybe StreamProcessorStatus)

-- | Face recognition input parameters that are being used by the stream
--   processor. Includes the collection to use for face recognition and the
--   face attributes to detect.
describeStreamProcessorResponse_settings :: Lens' DescribeStreamProcessorResponse (Maybe StreamProcessorSettings)

-- | Kinesis video stream that provides the source streaming video.
describeStreamProcessorResponse_input :: Lens' DescribeStreamProcessorResponse (Maybe StreamProcessorInput)

-- | Kinesis data stream to which Amazon Rekognition Video puts the
--   analysis results.
describeStreamProcessorResponse_output :: Lens' DescribeStreamProcessorResponse (Maybe StreamProcessorOutput)

-- | ARN of the stream processor.
describeStreamProcessorResponse_streamProcessorArn :: Lens' DescribeStreamProcessorResponse (Maybe Text)

-- | Detailed status message about the stream processor.
describeStreamProcessorResponse_statusMessage :: Lens' DescribeStreamProcessorResponse (Maybe Text)

-- | Name of the stream processor.
describeStreamProcessorResponse_name :: Lens' DescribeStreamProcessorResponse (Maybe Text)

-- | Date and time the stream processor was created
describeStreamProcessorResponse_creationTimestamp :: Lens' DescribeStreamProcessorResponse (Maybe UTCTime)

-- | The time, in Unix format, the stream processor was last updated. For
--   example, when the stream processor moves from a running state to a
--   failed state, or when the user starts or stops the stream processor.
describeStreamProcessorResponse_lastUpdateTimestamp :: Lens' DescribeStreamProcessorResponse (Maybe UTCTime)

-- | ARN of the IAM role that allows access to the stream processor.
describeStreamProcessorResponse_roleArn :: Lens' DescribeStreamProcessorResponse (Maybe Text)

-- | The response's http status code.
describeStreamProcessorResponse_httpStatus :: Lens' DescribeStreamProcessorResponse Int

-- | The minimum confidence in the person match to return. For example,
--   don't return any matches where confidence in matches is less than 70%.
--   The default value is 80%.
startFaceSearch_faceMatchThreshold :: Lens' StartFaceSearch (Maybe Double)

-- | An identifier you specify that's returned in the completion
--   notification that's published to your Amazon Simple Notification
--   Service topic. For example, you can use <tt>JobTag</tt> to group
--   related jobs and identify them in the completion notification.
startFaceSearch_jobTag :: Lens' StartFaceSearch (Maybe Text)

-- | The ARN of the Amazon SNS topic to which you want Amazon Rekognition
--   Video to publish the completion status of the search. The Amazon SNS
--   topic must have a topic name that begins with <i>AmazonRekognition</i>
--   if you are using the AmazonRekognitionServiceRole permissions policy
--   to access the topic.
startFaceSearch_notificationChannel :: Lens' StartFaceSearch (Maybe NotificationChannel)

-- | Idempotent token used to identify the start request. If you use the
--   same token with multiple <tt>StartFaceSearch</tt> requests, the same
--   <tt>JobId</tt> is returned. Use <tt>ClientRequestToken</tt> to prevent
--   the same job from being accidently started more than once.
startFaceSearch_clientRequestToken :: Lens' StartFaceSearch (Maybe Text)

-- | The video you want to search. The video must be stored in an Amazon S3
--   bucket.
startFaceSearch_video :: Lens' StartFaceSearch Video

-- | ID of the collection that contains the faces you want to search for.
startFaceSearch_collectionId :: Lens' StartFaceSearch Text

-- | The identifier for the search job. Use <tt>JobId</tt> to identify the
--   job in a subsequent call to <tt>GetFaceSearch</tt>.
startFaceSearchResponse_jobId :: Lens' StartFaceSearchResponse (Maybe Text)

-- | The response's http status code.
startFaceSearchResponse_httpStatus :: Lens' StartFaceSearchResponse Int

-- | An identifier returned in the completion status published by your
--   Amazon Simple Notification Service topic. For example, you can use
--   <tt>JobTag</tt> to group related jobs and identify them in the
--   completion notification.
startTextDetection_jobTag :: Lens' StartTextDetection (Maybe Text)

-- | Optional parameters that let you set criteria the text must meet to be
--   included in your response.
startTextDetection_filters :: Lens' StartTextDetection (Maybe StartTextDetectionFilters)

-- | Undocumented member.
startTextDetection_notificationChannel :: Lens' StartTextDetection (Maybe NotificationChannel)

-- | Idempotent token used to identify the start request. If you use the
--   same token with multiple <tt>StartTextDetection</tt> requests, the
--   same <tt>JobId</tt> is returned. Use <tt>ClientRequestToken</tt> to
--   prevent the same job from being accidentaly started more than once.
startTextDetection_clientRequestToken :: Lens' StartTextDetection (Maybe Text)

-- | Undocumented member.
startTextDetection_video :: Lens' StartTextDetection Video

-- | Identifier for the text detection job. Use <tt>JobId</tt> to identify
--   the job in a subsequent call to <tt>GetTextDetection</tt>.
startTextDetectionResponse_jobId :: Lens' StartTextDetectionResponse (Maybe Text)

-- | The response's http status code.
startTextDetectionResponse_httpStatus :: Lens' StartTextDetectionResponse Int

-- | An identifier you specify that's returned in the completion
--   notification that's published to your Amazon Simple Notification
--   Service topic. For example, you can use <tt>JobTag</tt> to group
--   related jobs and identify them in the completion notification.
startPersonTracking_jobTag :: Lens' StartPersonTracking (Maybe Text)

-- | The Amazon SNS topic ARN you want Amazon Rekognition Video to publish
--   the completion status of the people detection operation to. The Amazon
--   SNS topic must have a topic name that begins with
--   <i>AmazonRekognition</i> if you are using the
--   AmazonRekognitionServiceRole permissions policy.
startPersonTracking_notificationChannel :: Lens' StartPersonTracking (Maybe NotificationChannel)

-- | Idempotent token used to identify the start request. If you use the
--   same token with multiple <tt>StartPersonTracking</tt> requests, the
--   same <tt>JobId</tt> is returned. Use <tt>ClientRequestToken</tt> to
--   prevent the same job from being accidently started more than once.
startPersonTracking_clientRequestToken :: Lens' StartPersonTracking (Maybe Text)

-- | The video in which you want to detect people. The video must be stored
--   in an Amazon S3 bucket.
startPersonTracking_video :: Lens' StartPersonTracking Video

-- | The identifier for the person detection job. Use <tt>JobId</tt> to
--   identify the job in a subsequent call to <tt>GetPersonTracking</tt>.
startPersonTrackingResponse_jobId :: Lens' StartPersonTrackingResponse (Maybe Text)

-- | The response's http status code.
startPersonTrackingResponse_httpStatus :: Lens' StartPersonTrackingResponse Int

-- | If the previous response was incomplete (because there is more
--   recognized celebrities to retrieve), Amazon Rekognition Video returns
--   a pagination token in the response. You can use this pagination token
--   to retrieve the next set of celebrities.
getCelebrityRecognition_nextToken :: Lens' GetCelebrityRecognition (Maybe Text)

-- | Maximum number of results to return per paginated call. The largest
--   value you can specify is 1000. If you specify a value greater than
--   1000, a maximum of 1000 results is returned. The default value is
--   1000.
getCelebrityRecognition_maxResults :: Lens' GetCelebrityRecognition (Maybe Natural)

-- | Sort to use for celebrities returned in <tt>Celebrities</tt> field.
--   Specify <tt>ID</tt> to sort by the celebrity identifier, specify
--   <tt>TIMESTAMP</tt> to sort by the time the celebrity was recognized.
getCelebrityRecognition_sortBy :: Lens' GetCelebrityRecognition (Maybe CelebrityRecognitionSortBy)

-- | Job identifier for the required celebrity recognition analysis. You
--   can get the job identifer from a call to
--   <tt>StartCelebrityRecognition</tt>.
getCelebrityRecognition_jobId :: Lens' GetCelebrityRecognition Text

-- | If the response is truncated, Amazon Rekognition Video returns this
--   token that you can use in the subsequent request to retrieve the next
--   set of celebrities.
getCelebrityRecognitionResponse_nextToken :: Lens' GetCelebrityRecognitionResponse (Maybe Text)

-- | Information about a video that Amazon Rekognition Video analyzed.
--   <tt>Videometadata</tt> is returned in every page of paginated
--   responses from a Amazon Rekognition Video operation.
getCelebrityRecognitionResponse_videoMetadata :: Lens' GetCelebrityRecognitionResponse (Maybe VideoMetadata)

-- | If the job fails, <tt>StatusMessage</tt> provides a descriptive error
--   message.
getCelebrityRecognitionResponse_statusMessage :: Lens' GetCelebrityRecognitionResponse (Maybe Text)

-- | Array of celebrities recognized in the video.
getCelebrityRecognitionResponse_celebrities :: Lens' GetCelebrityRecognitionResponse (Maybe [CelebrityRecognition])

-- | The current status of the celebrity recognition job.
getCelebrityRecognitionResponse_jobStatus :: Lens' GetCelebrityRecognitionResponse (Maybe VideoJobStatus)

-- | The response's http status code.
getCelebrityRecognitionResponse_httpStatus :: Lens' GetCelebrityRecognitionResponse Int

-- | The name of the stream processor to start processing.
startStreamProcessor_name :: Lens' StartStreamProcessor Text

-- | The response's http status code.
startStreamProcessorResponse_httpStatus :: Lens' StartStreamProcessorResponse Int

-- | Optional parameters that let you set the criteria that the text must
--   meet to be included in your response.
detectText_filters :: Lens' DetectText (Maybe DetectTextFilters)

-- | The input image as base64-encoded bytes or an Amazon S3 object. If you
--   use the AWS CLI to call Amazon Rekognition operations, you can't pass
--   image bytes.
--   
--   If you are using an AWS SDK to call Amazon Rekognition, you might not
--   need to base64-encode image bytes passed using the <tt>Bytes</tt>
--   field. For more information, see Images in the Amazon Rekognition
--   developer guide.
detectText_image :: Lens' DetectText Image

-- | An array of text that was detected in the input image.
detectTextResponse_textDetections :: Lens' DetectTextResponse (Maybe [TextDetection])

-- | The model version used to detect text.
detectTextResponse_textModelVersion :: Lens' DetectTextResponse (Maybe Text)

-- | The response's http status code.
detectTextResponse_httpStatus :: Lens' DetectTextResponse Int

-- | If the response is truncated, Amazon Rekognition Video returns this
--   token that you can use in the subsequent request to retrieve the next
--   set of text.
getSegmentDetection_nextToken :: Lens' GetSegmentDetection (Maybe Text)

-- | Maximum number of results to return per paginated call. The largest
--   value you can specify is 1000.
getSegmentDetection_maxResults :: Lens' GetSegmentDetection (Maybe Natural)

-- | Job identifier for the text detection operation for which you want
--   results returned. You get the job identifer from an initial call to
--   <tt>StartSegmentDetection</tt>.
getSegmentDetection_jobId :: Lens' GetSegmentDetection Text

-- | An array containing the segment types requested in the call to
--   <tt>StartSegmentDetection</tt>.
getSegmentDetectionResponse_selectedSegmentTypes :: Lens' GetSegmentDetectionResponse (Maybe [SegmentTypeInfo])

-- | If the previous response was incomplete (because there are more labels
--   to retrieve), Amazon Rekognition Video returns a pagination token in
--   the response. You can use this pagination token to retrieve the next
--   set of text.
getSegmentDetectionResponse_nextToken :: Lens' GetSegmentDetectionResponse (Maybe Text)

-- | Currently, Amazon Rekognition Video returns a single object in the
--   <tt>VideoMetadata</tt> array. The object contains information about
--   the video stream in the input file that Amazon Rekognition Video chose
--   to analyze. The <tt>VideoMetadata</tt> object includes the video
--   codec, video format and other information. Video metadata is returned
--   in each page of information returned by <tt>GetSegmentDetection</tt>.
getSegmentDetectionResponse_videoMetadata :: Lens' GetSegmentDetectionResponse (Maybe [VideoMetadata])

-- | If the job fails, <tt>StatusMessage</tt> provides a descriptive error
--   message.
getSegmentDetectionResponse_statusMessage :: Lens' GetSegmentDetectionResponse (Maybe Text)

-- | An array of segments detected in a video. The array is sorted by the
--   segment types (TECHNICAL_CUE or SHOT) specified in the
--   <tt>SegmentTypes</tt> input parameter of
--   <tt>StartSegmentDetection</tt>. Within each segment type the array is
--   sorted by timestamp values.
getSegmentDetectionResponse_segments :: Lens' GetSegmentDetectionResponse (Maybe [SegmentDetection])

-- | Current status of the segment detection job.
getSegmentDetectionResponse_jobStatus :: Lens' GetSegmentDetectionResponse (Maybe VideoJobStatus)

-- | An array of objects. There can be multiple audio streams. Each
--   <tt>AudioMetadata</tt> object contains metadata for a single audio
--   stream. Audio information in an <tt>AudioMetadata</tt> objects
--   includes the audio codec, the number of audio channels, the duration
--   of the audio stream, and the sample rate. Audio metadata is returned
--   in each page of information returned by <tt>GetSegmentDetection</tt>.
getSegmentDetectionResponse_audioMetadata :: Lens' GetSegmentDetectionResponse (Maybe [AudioMetadata])

-- | The response's http status code.
getSegmentDetectionResponse_httpStatus :: Lens' GetSegmentDetectionResponse Int

-- | A filter that specifies a quality bar for how much filtering is done
--   to identify faces. Filtered faces aren't compared. If you specify
--   <tt>AUTO</tt>, Amazon Rekognition chooses the quality bar. If you
--   specify <tt>LOW</tt>, <tt>MEDIUM</tt>, or <tt>HIGH</tt>, filtering
--   removes all faces that don’t meet the chosen quality bar. The quality
--   bar is based on a variety of common use cases. Low-quality detections
--   can occur for a number of reasons. Some examples are an object that's
--   misidentified as a face, a face that's too blurry, or a face with a
--   pose that's too extreme to use. If you specify <tt>NONE</tt>, no
--   filtering is performed. The default value is <tt>NONE</tt>.
--   
--   To use quality filtering, the collection you are using must be
--   associated with version 3 of the face model or higher.
compareFaces_qualityFilter :: Lens' CompareFaces (Maybe QualityFilter)

-- | The minimum level of confidence in the face matches that a match must
--   meet to be included in the <tt>FaceMatches</tt> array.
compareFaces_similarityThreshold :: Lens' CompareFaces (Maybe Double)

-- | The input image as base64-encoded bytes or an S3 object. If you use
--   the AWS CLI to call Amazon Rekognition operations, passing
--   base64-encoded image bytes is not supported.
--   
--   If you are using an AWS SDK to call Amazon Rekognition, you might not
--   need to base64-encode image bytes passed using the <tt>Bytes</tt>
--   field. For more information, see Images in the Amazon Rekognition
--   developer guide.
compareFaces_sourceImage :: Lens' CompareFaces Image

-- | The target image as base64-encoded bytes or an S3 object. If you use
--   the AWS CLI to call Amazon Rekognition operations, passing
--   base64-encoded image bytes is not supported.
--   
--   If you are using an AWS SDK to call Amazon Rekognition, you might not
--   need to base64-encode image bytes passed using the <tt>Bytes</tt>
--   field. For more information, see Images in the Amazon Rekognition
--   developer guide.
compareFaces_targetImage :: Lens' CompareFaces Image

-- | An array of faces in the target image that match the source image
--   face. Each <tt>CompareFacesMatch</tt> object provides the bounding
--   box, the confidence level that the bounding box contains a face, and
--   the similarity score for the face in the bounding box and the face in
--   the source image.
compareFacesResponse_faceMatches :: Lens' CompareFacesResponse (Maybe [CompareFacesMatch])

-- | An array of faces in the target image that did not match the source
--   image face.
compareFacesResponse_unmatchedFaces :: Lens' CompareFacesResponse (Maybe [ComparedFace])

-- | The value of <tt>TargetImageOrientationCorrection</tt> is always null.
--   
--   If the input image is in .jpeg format, it might contain exchangeable
--   image file format (Exif) metadata that includes the image's
--   orientation. Amazon Rekognition uses this orientation information to
--   perform image correction. The bounding box coordinates are translated
--   to represent object locations after the orientation information in the
--   Exif metadata is used to correct the image orientation. Images in .png
--   format don't contain Exif metadata.
--   
--   Amazon Rekognition doesn’t perform image correction for images in .png
--   format and .jpeg images without orientation information in the image
--   Exif metadata. The bounding box coordinates aren't translated and
--   represent the object locations before the image is rotated.
compareFacesResponse_targetImageOrientationCorrection :: Lens' CompareFacesResponse (Maybe OrientationCorrection)

-- | The value of <tt>SourceImageOrientationCorrection</tt> is always null.
--   
--   If the input image is in .jpeg format, it might contain exchangeable
--   image file format (Exif) metadata that includes the image's
--   orientation. Amazon Rekognition uses this orientation information to
--   perform image correction. The bounding box coordinates are translated
--   to represent object locations after the orientation information in the
--   Exif metadata is used to correct the image orientation. Images in .png
--   format don't contain Exif metadata.
--   
--   Amazon Rekognition doesn’t perform image correction for images in .png
--   format and .jpeg images without orientation information in the image
--   Exif metadata. The bounding box coordinates aren't translated and
--   represent the object locations before the image is rotated.
compareFacesResponse_sourceImageOrientationCorrection :: Lens' CompareFacesResponse (Maybe OrientationCorrection)

-- | The face in the source image that was used for comparison.
compareFacesResponse_sourceImageFace :: Lens' CompareFacesResponse (Maybe ComparedSourceImageFace)

-- | The response's http status code.
compareFacesResponse_httpStatus :: Lens' CompareFacesResponse Int

-- | An array of facial attributes you want to be returned. This can be the
--   default list of attributes or all attributes. If you don't specify a
--   value for <tt>Attributes</tt> or if you specify <tt>["DEFAULT"]</tt>,
--   the API returns the following subset of facial attributes:
--   <tt>BoundingBox</tt>, <tt>Confidence</tt>, <tt>Pose</tt>,
--   <tt>Quality</tt>, and <tt>Landmarks</tt>. If you provide
--   <tt>["ALL"]</tt>, all facial attributes are returned, but the
--   operation takes longer to complete.
--   
--   If you provide both, <tt>["ALL", "DEFAULT"]</tt>, the service uses a
--   logical AND operator to determine which attributes to return (in this
--   case, all attributes).
detectFaces_attributes :: Lens' DetectFaces (Maybe [Attribute])

-- | The input image as base64-encoded bytes or an S3 object. If you use
--   the AWS CLI to call Amazon Rekognition operations, passing
--   base64-encoded image bytes is not supported.
--   
--   If you are using an AWS SDK to call Amazon Rekognition, you might not
--   need to base64-encode image bytes passed using the <tt>Bytes</tt>
--   field. For more information, see Images in the Amazon Rekognition
--   developer guide.
detectFaces_image :: Lens' DetectFaces Image

-- | The value of <tt>OrientationCorrection</tt> is always null.
--   
--   If the input image is in .jpeg format, it might contain exchangeable
--   image file format (Exif) metadata that includes the image's
--   orientation. Amazon Rekognition uses this orientation information to
--   perform image correction. The bounding box coordinates are translated
--   to represent object locations after the orientation information in the
--   Exif metadata is used to correct the image orientation. Images in .png
--   format don't contain Exif metadata.
--   
--   Amazon Rekognition doesn’t perform image correction for images in .png
--   format and .jpeg images without orientation information in the image
--   Exif metadata. The bounding box coordinates aren't translated and
--   represent the object locations before the image is rotated.
detectFacesResponse_orientationCorrection :: Lens' DetectFacesResponse (Maybe OrientationCorrection)

-- | Details of each face found in the image.
detectFacesResponse_faceDetails :: Lens' DetectFacesResponse (Maybe [FaceDetail])

-- | The response's http status code.
detectFacesResponse_httpStatus :: Lens' DetectFacesResponse Int

-- | If the previous response was incomplete (because there are more faces
--   to retrieve), Amazon Rekognition Video returns a pagination token in
--   the response. You can use this pagination token to retrieve the next
--   set of faces.
getFaceDetection_nextToken :: Lens' GetFaceDetection (Maybe Text)

-- | Maximum number of results to return per paginated call. The largest
--   value you can specify is 1000. If you specify a value greater than
--   1000, a maximum of 1000 results is returned. The default value is
--   1000.
getFaceDetection_maxResults :: Lens' GetFaceDetection (Maybe Natural)

-- | Unique identifier for the face detection job. The <tt>JobId</tt> is
--   returned from <tt>StartFaceDetection</tt>.
getFaceDetection_jobId :: Lens' GetFaceDetection Text

-- | If the response is truncated, Amazon Rekognition returns this token
--   that you can use in the subsequent request to retrieve the next set of
--   faces.
getFaceDetectionResponse_nextToken :: Lens' GetFaceDetectionResponse (Maybe Text)

-- | Information about a video that Amazon Rekognition Video analyzed.
--   <tt>Videometadata</tt> is returned in every page of paginated
--   responses from a Amazon Rekognition video operation.
getFaceDetectionResponse_videoMetadata :: Lens' GetFaceDetectionResponse (Maybe VideoMetadata)

-- | If the job fails, <tt>StatusMessage</tt> provides a descriptive error
--   message.
getFaceDetectionResponse_statusMessage :: Lens' GetFaceDetectionResponse (Maybe Text)

-- | An array of faces detected in the video. Each element contains a
--   detected face's details and the time, in milliseconds from the start
--   of the video, the face was detected.
getFaceDetectionResponse_faces :: Lens' GetFaceDetectionResponse (Maybe [FaceDetection])

-- | The current status of the face detection job.
getFaceDetectionResponse_jobStatus :: Lens' GetFaceDetectionResponse (Maybe VideoJobStatus)

-- | The response's http status code.
getFaceDetectionResponse_httpStatus :: Lens' GetFaceDetectionResponse Int

-- | Amazon Resource Name (ARN) of the model, collection, or stream
--   processor that you want to assign the tags to.
tagResource_resourceArn :: Lens' TagResource Text

-- | The key-value tags to assign to the resource.
tagResource_tags :: Lens' TagResource (HashMap Text Text)

-- | The response's http status code.
tagResourceResponse_httpStatus :: Lens' TagResourceResponse Int

-- | If the previous response was incomplete (because there is more data to
--   retrieve), Amazon Rekognition returns a pagination token in the
--   response. You can use this pagination token to retrieve the next set
--   of faces.
listFaces_nextToken :: Lens' ListFaces (Maybe Text)

-- | Maximum number of faces to return.
listFaces_maxResults :: Lens' ListFaces (Maybe Natural)

-- | ID of the collection from which to list the faces.
listFaces_collectionId :: Lens' ListFaces Text

-- | Version number of the face detection model associated with the input
--   collection (<tt>CollectionId</tt>).
listFacesResponse_faceModelVersion :: Lens' ListFacesResponse (Maybe Text)

-- | If the response is truncated, Amazon Rekognition returns this token
--   that you can use in the subsequent request to retrieve the next set of
--   faces.
listFacesResponse_nextToken :: Lens' ListFacesResponse (Maybe Text)

-- | An array of <tt>Face</tt> objects.
listFacesResponse_faces :: Lens' ListFacesResponse (Maybe [Face])

-- | The response's http status code.
listFacesResponse_httpStatus :: Lens' ListFacesResponse Int

-- | The identifier for your AWS Key Management Service (AWS KMS) customer
--   master key (CMK). You can supply the Amazon Resource Name (ARN) of
--   your CMK, the ID of your CMK, an alias for your CMK, or an alias ARN.
--   The key is used to encrypt training and test images copied into the
--   service for model training. Your source images are unaffected. The key
--   is also used to encrypt training results and manifest files written to
--   the output Amazon S3 bucket (<tt>OutputConfig</tt>).
--   
--   If you choose to use your own CMK, you need the following permissions
--   on the CMK.
--   
--   <ul>
--   <li>kms:CreateGrant</li>
--   <li>kms:DescribeKey</li>
--   <li>kms:GenerateDataKey</li>
--   <li>kms:Decrypt</li>
--   </ul>
--   
--   If you don't specify a value for <tt>KmsKeyId</tt>, images copied into
--   the service are encrypted using a key that AWS owns and manages.
createProjectVersion_kmsKeyId :: Lens' CreateProjectVersion (Maybe Text)

-- | A set of tags (key-value pairs) that you want to attach to the model.
createProjectVersion_tags :: Lens' CreateProjectVersion (Maybe (HashMap Text Text))

-- | The ARN of the Amazon Rekognition Custom Labels project that manages
--   the model that you want to train.
createProjectVersion_projectArn :: Lens' CreateProjectVersion Text

-- | A name for the version of the model. This value must be unique.
createProjectVersion_versionName :: Lens' CreateProjectVersion Text

-- | The Amazon S3 bucket location to store the results of training. The S3
--   bucket can be in any AWS account as long as the caller has
--   <tt>s3:PutObject</tt> permissions on the S3 bucket.
createProjectVersion_outputConfig :: Lens' CreateProjectVersion OutputConfig

-- | The dataset to use for training.
createProjectVersion_trainingData :: Lens' CreateProjectVersion TrainingData

-- | The dataset to use for testing.
createProjectVersion_testingData :: Lens' CreateProjectVersion TestingData

-- | The ARN of the model version that was created. Use
--   <tt>DescribeProjectVersion</tt> to get the current status of the
--   training operation.
createProjectVersionResponse_projectVersionArn :: Lens' CreateProjectVersionResponse (Maybe Text)

-- | The response's http status code.
createProjectVersionResponse_httpStatus :: Lens' CreateProjectVersionResponse Int

-- | If the previous response was incomplete (because there is more results
--   to retrieve), Amazon Rekognition Custom Labels returns a pagination
--   token in the response. You can use this pagination token to retrieve
--   the next set of results.
describeProjects_nextToken :: Lens' DescribeProjects (Maybe Text)

-- | The maximum number of results to return per paginated call. The
--   largest value you can specify is 100. If you specify a value greater
--   than 100, a ValidationException error occurs. The default value is
--   100.
describeProjects_maxResults :: Lens' DescribeProjects (Maybe Natural)

-- | If the previous response was incomplete (because there is more results
--   to retrieve), Amazon Rekognition Custom Labels returns a pagination
--   token in the response. You can use this pagination token to retrieve
--   the next set of results.
describeProjectsResponse_nextToken :: Lens' DescribeProjectsResponse (Maybe Text)

-- | A list of project descriptions. The list is sorted by the date and
--   time the projects are created.
describeProjectsResponse_projectDescriptions :: Lens' DescribeProjectsResponse (Maybe [ProjectDescription])

-- | The response's http status code.
describeProjectsResponse_httpStatus :: Lens' DescribeProjectsResponse Int

-- | If the previous response was incomplete (because there is more data to
--   retrieve), Amazon Rekognition returns a pagination token in the
--   response. You can use this pagination token to retrieve the next set
--   of content moderation labels.
getContentModeration_nextToken :: Lens' GetContentModeration (Maybe Text)

-- | Maximum number of results to return per paginated call. The largest
--   value you can specify is 1000. If you specify a value greater than
--   1000, a maximum of 1000 results is returned. The default value is
--   1000.
getContentModeration_maxResults :: Lens' GetContentModeration (Maybe Natural)

-- | Sort to use for elements in the <tt>ModerationLabelDetections</tt>
--   array. Use <tt>TIMESTAMP</tt> to sort array elements by the time
--   labels are detected. Use <tt>NAME</tt> to alphabetically group
--   elements for a label together. Within each label group, the array
--   element are sorted by detection confidence. The default sort is by
--   <tt>TIMESTAMP</tt>.
getContentModeration_sortBy :: Lens' GetContentModeration (Maybe ContentModerationSortBy)

-- | The identifier for the inappropriate, unwanted, or offensive content
--   moderation job. Use <tt>JobId</tt> to identify the job in a subsequent
--   call to <tt>GetContentModeration</tt>.
getContentModeration_jobId :: Lens' GetContentModeration Text

-- | If the response is truncated, Amazon Rekognition Video returns this
--   token that you can use in the subsequent request to retrieve the next
--   set of content moderation labels.
getContentModerationResponse_nextToken :: Lens' GetContentModerationResponse (Maybe Text)

-- | Information about a video that Amazon Rekognition analyzed.
--   <tt>Videometadata</tt> is returned in every page of paginated
--   responses from <tt>GetContentModeration</tt>.
getContentModerationResponse_videoMetadata :: Lens' GetContentModerationResponse (Maybe VideoMetadata)

-- | If the job fails, <tt>StatusMessage</tt> provides a descriptive error
--   message.
getContentModerationResponse_statusMessage :: Lens' GetContentModerationResponse (Maybe Text)

-- | The current status of the content moderation analysis job.
getContentModerationResponse_jobStatus :: Lens' GetContentModerationResponse (Maybe VideoJobStatus)

-- | Version number of the moderation detection model that was used to
--   detect inappropriate, unwanted, or offensive content.
getContentModerationResponse_moderationModelVersion :: Lens' GetContentModerationResponse (Maybe Text)

-- | The detected inappropriate, unwanted, or offensive content moderation
--   labels and the time(s) they were detected.
getContentModerationResponse_moderationLabels :: Lens' GetContentModerationResponse (Maybe [ContentModerationDetection])

-- | The response's http status code.
getContentModerationResponse_httpStatus :: Lens' GetContentModerationResponse Int

-- | Collection from which to remove the specific faces.
deleteFaces_collectionId :: Lens' DeleteFaces Text

-- | An array of face IDs to delete.
deleteFaces_faceIds :: Lens' DeleteFaces (NonEmpty Text)

-- | An array of strings (face IDs) of the faces that were deleted.
deleteFacesResponse_deletedFaces :: Lens' DeleteFacesResponse (Maybe (NonEmpty Text))

-- | The response's http status code.
deleteFacesResponse_httpStatus :: Lens' DeleteFacesResponse Int

-- | The ID for the celebrity. You get the celebrity ID from a call to the
--   RecognizeCelebrities operation, which recognizes celebrities in an
--   image.
getCelebrityInfo_id :: Lens' GetCelebrityInfo Text

-- | An array of URLs pointing to additional celebrity information.
getCelebrityInfoResponse_urls :: Lens' GetCelebrityInfoResponse (Maybe [Text])

-- | Retrieves the known gender for the celebrity.
getCelebrityInfoResponse_knownGender :: Lens' GetCelebrityInfoResponse (Maybe KnownGender)

-- | The name of the celebrity.
getCelebrityInfoResponse_name :: Lens' GetCelebrityInfoResponse (Maybe Text)

-- | The response's http status code.
getCelebrityInfoResponse_httpStatus :: Lens' GetCelebrityInfoResponse Int

-- | The name of the stream processor you want to delete.
deleteStreamProcessor_name :: Lens' DeleteStreamProcessor Text

-- | The response's http status code.
deleteStreamProcessorResponse_httpStatus :: Lens' DeleteStreamProcessorResponse Int

-- | Amazon Resource Name (ARN) of the model, collection, or stream
--   processor that you want to remove the tags from.
untagResource_resourceArn :: Lens' UntagResource Text

-- | A list of the tags that you want to remove.
untagResource_tagKeys :: Lens' UntagResource [Text]

-- | The response's http status code.
untagResourceResponse_httpStatus :: Lens' UntagResourceResponse Int

-- | Sets up the configuration for human evaluation, including the
--   FlowDefinition the image will be sent to.
detectModerationLabels_humanLoopConfig :: Lens' DetectModerationLabels (Maybe HumanLoopConfig)

-- | Specifies the minimum confidence level for the labels to return.
--   Amazon Rekognition doesn't return any labels with a confidence level
--   lower than this specified value.
--   
--   If you don't specify <tt>MinConfidence</tt>, the operation returns
--   labels with confidence values greater than or equal to 50 percent.
detectModerationLabels_minConfidence :: Lens' DetectModerationLabels (Maybe Double)

-- | The input image as base64-encoded bytes or an S3 object. If you use
--   the AWS CLI to call Amazon Rekognition operations, passing
--   base64-encoded image bytes is not supported.
--   
--   If you are using an AWS SDK to call Amazon Rekognition, you might not
--   need to base64-encode image bytes passed using the <tt>Bytes</tt>
--   field. For more information, see Images in the Amazon Rekognition
--   developer guide.
detectModerationLabels_image :: Lens' DetectModerationLabels Image

-- | Shows the results of the human in the loop evaluation.
detectModerationLabelsResponse_humanLoopActivationOutput :: Lens' DetectModerationLabelsResponse (Maybe HumanLoopActivationOutput)

-- | Version number of the moderation detection model that was used to
--   detect unsafe content.
detectModerationLabelsResponse_moderationModelVersion :: Lens' DetectModerationLabelsResponse (Maybe Text)

-- | Array of detected Moderation labels and the time, in milliseconds from
--   the start of the video, they were detected.
detectModerationLabelsResponse_moderationLabels :: Lens' DetectModerationLabelsResponse (Maybe [ModerationLabel])

-- | The response's http status code.
detectModerationLabelsResponse_httpStatus :: Lens' DetectModerationLabelsResponse Int

-- | A set of tags (key-value pairs) that you want to attach to the stream
--   processor.
createStreamProcessor_tags :: Lens' CreateStreamProcessor (Maybe (HashMap Text Text))

-- | Kinesis video stream stream that provides the source streaming video.
--   If you are using the AWS CLI, the parameter name is
--   <tt>StreamProcessorInput</tt>.
createStreamProcessor_input :: Lens' CreateStreamProcessor StreamProcessorInput

-- | Kinesis data stream stream to which Amazon Rekognition Video puts the
--   analysis results. If you are using the AWS CLI, the parameter name is
--   <tt>StreamProcessorOutput</tt>.
createStreamProcessor_output :: Lens' CreateStreamProcessor StreamProcessorOutput

-- | An identifier you assign to the stream processor. You can use
--   <tt>Name</tt> to manage the stream processor. For example, you can get
--   the current status of the stream processor by calling
--   DescribeStreamProcessor. <tt>Name</tt> is idempotent.
createStreamProcessor_name :: Lens' CreateStreamProcessor Text

-- | Face recognition input parameters to be used by the stream processor.
--   Includes the collection to use for face recognition and the face
--   attributes to detect.
createStreamProcessor_settings :: Lens' CreateStreamProcessor StreamProcessorSettings

-- | ARN of the IAM role that allows access to the stream processor.
createStreamProcessor_roleArn :: Lens' CreateStreamProcessor Text

-- | ARN for the newly create stream processor.
createStreamProcessorResponse_streamProcessorArn :: Lens' CreateStreamProcessorResponse (Maybe Text)

-- | The response's http status code.
createStreamProcessorResponse_httpStatus :: Lens' CreateStreamProcessorResponse Int

-- | An identifier you specify that's returned in the completion
--   notification that's published to your Amazon Simple Notification
--   Service topic. For example, you can use <tt>JobTag</tt> to group
--   related jobs and identify them in the completion notification.
startFaceDetection_jobTag :: Lens' StartFaceDetection (Maybe Text)

-- | The ARN of the Amazon SNS topic to which you want Amazon Rekognition
--   Video to publish the completion status of the face detection
--   operation. The Amazon SNS topic must have a topic name that begins
--   with <i>AmazonRekognition</i> if you are using the
--   AmazonRekognitionServiceRole permissions policy.
startFaceDetection_notificationChannel :: Lens' StartFaceDetection (Maybe NotificationChannel)

-- | Idempotent token used to identify the start request. If you use the
--   same token with multiple <tt>StartFaceDetection</tt> requests, the
--   same <tt>JobId</tt> is returned. Use <tt>ClientRequestToken</tt> to
--   prevent the same job from being accidently started more than once.
startFaceDetection_clientRequestToken :: Lens' StartFaceDetection (Maybe Text)

-- | The face attributes you want returned.
--   
--   <tt>DEFAULT</tt> - The following subset of facial attributes are
--   returned: BoundingBox, Confidence, Pose, Quality and Landmarks.
--   
--   <tt>ALL</tt> - All facial attributes are returned.
startFaceDetection_faceAttributes :: Lens' StartFaceDetection (Maybe FaceAttributes)

-- | The video in which you want to detect faces. The video must be stored
--   in an Amazon S3 bucket.
startFaceDetection_video :: Lens' StartFaceDetection Video

-- | The identifier for the face detection job. Use <tt>JobId</tt> to
--   identify the job in a subsequent call to <tt>GetFaceDetection</tt>.
startFaceDetectionResponse_jobId :: Lens' StartFaceDetectionResponse (Maybe Text)

-- | The response's http status code.
startFaceDetectionResponse_httpStatus :: Lens' StartFaceDetectionResponse Int

-- | The name of the project to create.
createProject_projectName :: Lens' CreateProject Text

-- | The Amazon Resource Name (ARN) of the new project. You can use the ARN
--   to configure IAM access to the project.
createProjectResponse_projectArn :: Lens' CreateProjectResponse (Maybe Text)

-- | The response's http status code.
createProjectResponse_httpStatus :: Lens' CreateProjectResponse Int

-- | The lowest estimated age.
ageRange_low :: Lens' AgeRange (Maybe Natural)

-- | The highest estimated age.
ageRange_high :: Lens' AgeRange (Maybe Natural)

-- | Undocumented member.
asset_groundTruthManifest :: Lens' Asset (Maybe GroundTruthManifest)

-- | The audio codec used to encode or decode the audio stream.
audioMetadata_codec :: Lens' AudioMetadata (Maybe Text)

-- | The sample rate for the audio stream.
audioMetadata_sampleRate :: Lens' AudioMetadata (Maybe Natural)

-- | The number of audio channels in the segment.
audioMetadata_numberOfChannels :: Lens' AudioMetadata (Maybe Natural)

-- | The duration of the audio stream in milliseconds.
audioMetadata_durationMillis :: Lens' AudioMetadata (Maybe Natural)

-- | Boolean value that indicates whether the face has beard or not.
beard_value :: Lens' Beard (Maybe Bool)

-- | Level of confidence in the determination.
beard_confidence :: Lens' Beard (Maybe Double)

-- | A threshold used to determine the maximum luminance value for a pixel
--   to be considered black. In a full color range video, luminance values
--   range from 0-255. A pixel value of 0 is pure black, and the most
--   strict filter. The maximum black pixel value is computed as follows:
--   max_black_pixel_value = minimum_luminance + MaxPixelThreshold
--   *luminance_range.
--   
--   For example, for a full range video with BlackPixelThreshold = 0.1,
--   max_black_pixel_value is 0 + 0.1 * (255-0) = 25.5.
--   
--   The default value of MaxPixelThreshold is 0.2, which maps to a
--   max_black_pixel_value of 51 for a full range video. You can lower this
--   threshold to be more strict on black levels.
blackFrame_maxPixelThreshold :: Lens' BlackFrame (Maybe Double)

-- | The minimum percentage of pixels in a frame that need to have a
--   luminance below the max_black_pixel_value for a frame to be considered
--   a black frame. Luminance is calculated using the BT.709 matrix.
--   
--   The default value is 99, which means at least 99% of all pixels in the
--   frame are black pixels as per the <tt>MaxPixelThreshold</tt> set. You
--   can reduce this value to allow more noise on the black frame.
blackFrame_minCoveragePercentage :: Lens' BlackFrame (Maybe Double)

-- | Height of the bounding box as a ratio of the overall image height.
boundingBox_height :: Lens' BoundingBox (Maybe Double)

-- | Left coordinate of the bounding box as a ratio of overall image width.
boundingBox_left :: Lens' BoundingBox (Maybe Double)

-- | Width of the bounding box as a ratio of the overall image width.
boundingBox_width :: Lens' BoundingBox (Maybe Double)

-- | Top coordinate of the bounding box as a ratio of overall image height.
boundingBox_top :: Lens' BoundingBox (Maybe Double)

-- | The confidence, in percentage, that Amazon Rekognition has that the
--   recognized face is the celebrity.
celebrity_matchConfidence :: Lens' Celebrity (Maybe Double)

-- | An array of URLs pointing to additional information about the
--   celebrity. If there is no additional information about the celebrity,
--   this list is empty.
celebrity_urls :: Lens' Celebrity (Maybe [Text])

-- | Undocumented member.
celebrity_knownGender :: Lens' Celebrity (Maybe KnownGender)

-- | The name of the celebrity.
celebrity_name :: Lens' Celebrity (Maybe Text)

-- | A unique identifier for the celebrity.
celebrity_id :: Lens' Celebrity (Maybe Text)

-- | Provides information about the celebrity's face, such as its location
--   on the image.
celebrity_face :: Lens' Celebrity (Maybe ComparedFace)

-- | Bounding box around the body of a celebrity.
celebrityDetail_boundingBox :: Lens' CelebrityDetail (Maybe BoundingBox)

-- | An array of URLs pointing to additional celebrity information.
celebrityDetail_urls :: Lens' CelebrityDetail (Maybe [Text])

-- | The confidence, in percentage, that Amazon Rekognition has that the
--   recognized face is the celebrity.
celebrityDetail_confidence :: Lens' CelebrityDetail (Maybe Double)

-- | The name of the celebrity.
celebrityDetail_name :: Lens' CelebrityDetail (Maybe Text)

-- | The unique identifier for the celebrity.
celebrityDetail_id :: Lens' CelebrityDetail (Maybe Text)

-- | Face details for the recognized celebrity.
celebrityDetail_face :: Lens' CelebrityDetail (Maybe FaceDetail)

-- | Information about a recognized celebrity.
celebrityRecognition_celebrity :: Lens' CelebrityRecognition (Maybe CelebrityDetail)

-- | The time, in milliseconds from the start of the video, that the
--   celebrity was recognized.
celebrityRecognition_timestamp :: Lens' CelebrityRecognition (Maybe Integer)

-- | Level of confidence that the faces match.
compareFacesMatch_similarity :: Lens' CompareFacesMatch (Maybe Double)

-- | Provides face metadata (bounding box and confidence that the bounding
--   box actually contains a face).
compareFacesMatch_face :: Lens' CompareFacesMatch (Maybe ComparedFace)

-- | Bounding box of the face.
comparedFace_boundingBox :: Lens' ComparedFace (Maybe BoundingBox)

-- | The emotions that appear to be expressed on the face, and the
--   confidence level in the determination. Valid values include "Happy",
--   "Sad", "Angry", "Confused", "Disgusted", "Surprised", "Calm",
--   "Unknown", and "Fear".
comparedFace_emotions :: Lens' ComparedFace (Maybe [Emotion])

-- | Indicates the pose of the face as determined by its pitch, roll, and
--   yaw.
comparedFace_pose :: Lens' ComparedFace (Maybe Pose)

-- | Level of confidence that what the bounding box contains is a face.
comparedFace_confidence :: Lens' ComparedFace (Maybe Double)

-- | Identifies face image brightness and sharpness.
comparedFace_quality :: Lens' ComparedFace (Maybe ImageQuality)

-- | Indicates whether or not the face is smiling, and the confidence level
--   in the determination.
comparedFace_smile :: Lens' ComparedFace (Maybe Smile)

-- | An array of facial landmarks.
comparedFace_landmarks :: Lens' ComparedFace (Maybe [Landmark])

-- | Bounding box of the face.
comparedSourceImageFace_boundingBox :: Lens' ComparedSourceImageFace (Maybe BoundingBox)

-- | Confidence level that the selected bounding box contains a face.
comparedSourceImageFace_confidence :: Lens' ComparedSourceImageFace (Maybe Double)

-- | The content moderation label detected by in the stored video.
contentModerationDetection_moderationLabel :: Lens' ContentModerationDetection (Maybe ModerationLabel)

-- | Time, in milliseconds from the beginning of the video, that the
--   content moderation label was detected.
contentModerationDetection_timestamp :: Lens' ContentModerationDetection (Maybe Integer)

-- | True if the PPE covers the corresponding body part, otherwise false.
coversBodyPart_value :: Lens' CoversBodyPart (Maybe Bool)

-- | The confidence that Amazon Rekognition has in the value of
--   <tt>Value</tt>.
coversBodyPart_confidence :: Lens' CoversBodyPart (Maybe Double)

-- | The confidence that the model has in the detection of the custom
--   label. The range is 0-100. A higher value indicates a higher
--   confidence.
customLabel_confidence :: Lens' CustomLabel (Maybe Double)

-- | The name of the custom label.
customLabel_name :: Lens' CustomLabel (Maybe Text)

-- | The location of the detected object on the image that corresponds to
--   the custom label. Includes an axis aligned coarse bounding box
--   surrounding the object and a finer grain polygon for more accurate
--   spatial information.
customLabel_geometry :: Lens' CustomLabel (Maybe Geometry)

-- | A Filter focusing on a certain area of the image. Uses a
--   <tt>BoundingBox</tt> object to set the region of the image.
detectTextFilters_regionsOfInterest :: Lens' DetectTextFilters (Maybe [RegionOfInterest])

-- | Undocumented member.
detectTextFilters_wordFilter :: Lens' DetectTextFilters (Maybe DetectionFilter)

-- | Sets the minimum height of the word bounding box. Words with bounding
--   box heights lesser than this value will be excluded from the result.
--   Value is relative to the video frame height.
detectionFilter_minBoundingBoxHeight :: Lens' DetectionFilter (Maybe Double)

-- | Sets the minimum width of the word bounding box. Words with bounding
--   boxes widths lesser than this value will be excluded from the result.
--   Value is relative to the video frame width.
detectionFilter_minBoundingBoxWidth :: Lens' DetectionFilter (Maybe Double)

-- | Sets the confidence of word detection. Words with detection confidence
--   below this will be excluded from the result. Values should be between
--   50 and 100 as Text in Video will not return any result below 50.
detectionFilter_minConfidence :: Lens' DetectionFilter (Maybe Double)

-- | Level of confidence in the determination.
emotion_confidence :: Lens' Emotion (Maybe Double)

-- | Type of emotion detected.
emotion_type :: Lens' Emotion (Maybe EmotionName)

-- | A bounding box surrounding the item of detected PPE.
equipmentDetection_boundingBox :: Lens' EquipmentDetection (Maybe BoundingBox)

-- | Information about the body part covered by the detected PPE.
equipmentDetection_coversBodyPart :: Lens' EquipmentDetection (Maybe CoversBodyPart)

-- | The confidence that Amazon Rekognition has that the bounding box
--   (<tt>BoundingBox</tt>) contains an item of PPE.
equipmentDetection_confidence :: Lens' EquipmentDetection (Maybe Double)

-- | The type of detected PPE.
equipmentDetection_type :: Lens' EquipmentDetection (Maybe ProtectiveEquipmentType)

-- | The S3 bucket that contains the training summary.
evaluationResult_summary :: Lens' EvaluationResult (Maybe Summary)

-- | The F1 score for the evaluation of all labels. The F1 score metric
--   evaluates the overall precision and recall performance of the model as
--   a single value. A higher value indicates better precision and recall
--   performance. A lower score indicates that precision, recall, or both
--   are performing poorly.
evaluationResult_f1Score :: Lens' EvaluationResult (Maybe Double)

-- | Boolean value that indicates whether the eyes on the face are open.
eyeOpen_value :: Lens' EyeOpen (Maybe Bool)

-- | Level of confidence in the determination.
eyeOpen_confidence :: Lens' EyeOpen (Maybe Double)

-- | Boolean value that indicates whether the face is wearing eye glasses
--   or not.
eyeglasses_value :: Lens' Eyeglasses (Maybe Bool)

-- | Level of confidence in the determination.
eyeglasses_confidence :: Lens' Eyeglasses (Maybe Double)

-- | Unique identifier that Amazon Rekognition assigns to the face.
face_faceId :: Lens' Face (Maybe Text)

-- | Bounding box of the face.
face_boundingBox :: Lens' Face (Maybe BoundingBox)

-- | Identifier that you assign to all the faces in the input image.
face_externalImageId :: Lens' Face (Maybe Text)

-- | Confidence level that the bounding box contains a face (and not a
--   different object such as a tree).
face_confidence :: Lens' Face (Maybe Double)

-- | Unique identifier that Amazon Rekognition assigns to the input image.
face_imageId :: Lens' Face (Maybe Text)

-- | The estimated age range, in years, for the face. Low represents the
--   lowest estimated age and High represents the highest estimated age.
faceDetail_ageRange :: Lens' FaceDetail (Maybe AgeRange)

-- | Indicates whether or not the face is wearing sunglasses, and the
--   confidence level in the determination.
faceDetail_sunglasses :: Lens' FaceDetail (Maybe Sunglasses)

-- | Indicates whether or not the mouth on the face is open, and the
--   confidence level in the determination.
faceDetail_mouthOpen :: Lens' FaceDetail (Maybe MouthOpen)

-- | Bounding box of the face. Default attribute.
faceDetail_boundingBox :: Lens' FaceDetail (Maybe BoundingBox)

-- | The emotions that appear to be expressed on the face, and the
--   confidence level in the determination. The API is only making a
--   determination of the physical appearance of a person's face. It is not
--   a determination of the person’s internal emotional state and should
--   not be used in such a way. For example, a person pretending to have a
--   sad face might not be sad emotionally.
faceDetail_emotions :: Lens' FaceDetail (Maybe [Emotion])

-- | Indicates whether or not the eyes on the face are open, and the
--   confidence level in the determination.
faceDetail_eyesOpen :: Lens' FaceDetail (Maybe EyeOpen)

-- | Indicates the pose of the face as determined by its pitch, roll, and
--   yaw. Default attribute.
faceDetail_pose :: Lens' FaceDetail (Maybe Pose)

-- | Confidence level that the bounding box contains a face (and not a
--   different object such as a tree). Default attribute.
faceDetail_confidence :: Lens' FaceDetail (Maybe Double)

-- | The predicted gender of a detected face.
faceDetail_gender :: Lens' FaceDetail (Maybe Gender)

-- | Identifies image brightness and sharpness. Default attribute.
faceDetail_quality :: Lens' FaceDetail (Maybe ImageQuality)

-- | Indicates whether or not the face is wearing eye glasses, and the
--   confidence level in the determination.
faceDetail_eyeglasses :: Lens' FaceDetail (Maybe Eyeglasses)

-- | Indicates whether or not the face has a beard, and the confidence
--   level in the determination.
faceDetail_beard :: Lens' FaceDetail (Maybe Beard)

-- | Indicates whether or not the face has a mustache, and the confidence
--   level in the determination.
faceDetail_mustache :: Lens' FaceDetail (Maybe Mustache)

-- | Indicates whether or not the face is smiling, and the confidence level
--   in the determination.
faceDetail_smile :: Lens' FaceDetail (Maybe Smile)

-- | Indicates the location of landmarks on the face. Default attribute.
faceDetail_landmarks :: Lens' FaceDetail (Maybe [Landmark])

-- | Time, in milliseconds from the start of the video, that the face was
--   detected.
faceDetection_timestamp :: Lens' FaceDetection (Maybe Integer)

-- | The face properties for the detected face.
faceDetection_face :: Lens' FaceDetection (Maybe FaceDetail)

-- | Confidence in the match of this face with the input face.
faceMatch_similarity :: Lens' FaceMatch (Maybe Double)

-- | Describes the face properties such as the bounding box, face ID, image
--   ID of the source image, and external image ID that you assigned.
faceMatch_face :: Lens' FaceMatch (Maybe Face)

-- | Structure containing attributes of the face that the algorithm
--   detected.
faceRecord_faceDetail :: Lens' FaceRecord (Maybe FaceDetail)

-- | Describes the face properties such as the bounding box, face ID, image
--   ID of the input image, and external image ID that you assigned.
faceRecord_face :: Lens' FaceRecord (Maybe Face)

-- | Minimum face match confidence score that must be met to return a
--   result for a recognized face. Default is 80. 0 is the lowest
--   confidence. 100 is the highest confidence.
faceSearchSettings_faceMatchThreshold :: Lens' FaceSearchSettings (Maybe Double)

-- | The ID of a collection that contains faces that you want to search
--   for.
faceSearchSettings_collectionId :: Lens' FaceSearchSettings (Maybe Text)

-- | The predicted gender of the face.
gender_value :: Lens' Gender (Maybe GenderType)

-- | Level of confidence in the prediction.
gender_confidence :: Lens' Gender (Maybe Double)

-- | An axis-aligned coarse representation of the detected item's location
--   on the image.
geometry_boundingBox :: Lens' Geometry (Maybe BoundingBox)

-- | Within the bounding box, a fine-grained polygon around the detected
--   item.
geometry_polygon :: Lens' Geometry (Maybe [Point])

-- | Undocumented member.
groundTruthManifest_s3Object :: Lens' GroundTruthManifest (Maybe S3Object)

-- | Shows if and why human review was needed.
humanLoopActivationOutput_humanLoopActivationReasons :: Lens' HumanLoopActivationOutput (Maybe (NonEmpty Text))

-- | The Amazon Resource Name (ARN) of the HumanLoop created.
humanLoopActivationOutput_humanLoopArn :: Lens' HumanLoopActivationOutput (Maybe Text)

-- | Shows the result of condition evaluations, including those conditions
--   which activated a human review.
humanLoopActivationOutput_humanLoopActivationConditionsEvaluationResults :: Lens' HumanLoopActivationOutput (Maybe Text)

-- | Sets attributes of the input data.
humanLoopConfig_dataAttributes :: Lens' HumanLoopConfig (Maybe HumanLoopDataAttributes)

-- | The name of the human review used for this image. This should be kept
--   unique within a region.
humanLoopConfig_humanLoopName :: Lens' HumanLoopConfig Text

-- | The Amazon Resource Name (ARN) of the flow definition. You can create
--   a flow definition by using the Amazon Sagemaker
--   <a>CreateFlowDefinition</a> Operation.
humanLoopConfig_flowDefinitionArn :: Lens' HumanLoopConfig Text

-- | Sets whether the input image is free of personally identifiable
--   information.
humanLoopDataAttributes_contentClassifiers :: Lens' HumanLoopDataAttributes (Maybe [ContentClassifier])

-- | Identifies an S3 object as the image source.
image_s3Object :: Lens' Image (Maybe S3Object)

-- | Blob of image bytes up to 5 MBs.-- -- <i>Note:</i> This <tt>Lens</tt>
--   automatically encodes and decodes Base64 data. -- The underlying
--   isomorphism will encode to Base64 representation during --
--   serialisation, and decode from Base64 representation during
--   deserialisation. -- This <tt>Lens</tt> accepts and returns only raw
--   unencoded data.
image_bytes :: Lens' Image (Maybe ByteString)

-- | Value representing sharpness of the face. The service returns a value
--   between 0 and 100 (inclusive). A higher value indicates a sharper face
--   image.
imageQuality_sharpness :: Lens' ImageQuality (Maybe Double)

-- | Value representing brightness of the face. The service returns a value
--   between 0 and 100 (inclusive). A higher value indicates a brighter
--   face image.
imageQuality_brightness :: Lens' ImageQuality (Maybe Double)

-- | The position of the label instance on the image.
instance_boundingBox :: Lens' Instance (Maybe BoundingBox)

-- | The confidence that Amazon Rekognition has in the accuracy of the
--   bounding box.
instance_confidence :: Lens' Instance (Maybe Double)

-- | ARN of the output Amazon Kinesis Data Streams stream.
kinesisDataStream_arn :: Lens' KinesisDataStream (Maybe Text)

-- | ARN of the Kinesis video stream stream that streams the source video.
kinesisVideoStream_arn :: Lens' KinesisVideoStream (Maybe Text)

-- | A string value of the KnownGender info about the Celebrity.
knownGender_type :: Lens' KnownGender (Maybe KnownGenderType)

-- | Level of confidence.
label_confidence :: Lens' Label (Maybe Double)

-- | The parent labels for a label. The response includes all ancestor
--   labels.
label_parents :: Lens' Label (Maybe [Parent])

-- | The name (label) of the object or scene.
label_name :: Lens' Label (Maybe Text)

-- | If <tt>Label</tt> represents an object, <tt>Instances</tt> contains
--   the bounding boxes for each instance of the detected object. Bounding
--   boxes are returned for common object labels such as people, cars,
--   furniture, apparel or pets.
label_instances :: Lens' Label (Maybe [Instance])

-- | Details about the detected label.
labelDetection_label :: Lens' LabelDetection (Maybe Label)

-- | Time, in milliseconds from the start of the video, that the label was
--   detected.
labelDetection_timestamp :: Lens' LabelDetection (Maybe Integer)

-- | Type of landmark.
landmark_type :: Lens' Landmark (Maybe LandmarkType)

-- | The x-coordinate of the landmark expressed as a ratio of the width of
--   the image. The x-coordinate is measured from the left-side of the
--   image. For example, if the image is 700 pixels wide and the
--   x-coordinate of the landmark is at 350 pixels, this value is 0.5.
landmark_x :: Lens' Landmark (Maybe Double)

-- | The y-coordinate of the landmark expressed as a ratio of the height of
--   the image. The y-coordinate is measured from the top of the image. For
--   example, if the image height is 200 pixels and the y-coordinate of the
--   landmark is at 50 pixels, this value is 0.25.
landmark_y :: Lens' Landmark (Maybe Double)

-- | Specifies the confidence that Amazon Rekognition has that the label
--   has been correctly identified.
--   
--   If you don't specify the <tt>MinConfidence</tt> parameter in the call
--   to <tt>DetectModerationLabels</tt>, the operation returns labels with
--   a confidence value greater than or equal to 50 percent.
moderationLabel_confidence :: Lens' ModerationLabel (Maybe Double)

-- | The label name for the type of unsafe content detected in the image.
moderationLabel_name :: Lens' ModerationLabel (Maybe Text)

-- | The name for the parent label. Labels at the top level of the
--   hierarchy have the parent label <tt>""</tt>.
moderationLabel_parentName :: Lens' ModerationLabel (Maybe Text)

-- | Boolean value that indicates whether the mouth on the face is open or
--   not.
mouthOpen_value :: Lens' MouthOpen (Maybe Bool)

-- | Level of confidence in the determination.
mouthOpen_confidence :: Lens' MouthOpen (Maybe Double)

-- | Boolean value that indicates whether the face has mustache or not.
mustache_value :: Lens' Mustache (Maybe Bool)

-- | Level of confidence in the determination.
mustache_confidence :: Lens' Mustache (Maybe Double)

-- | The Amazon SNS topic to which Amazon Rekognition to posts the
--   completion status.
notificationChannel_sNSTopicArn :: Lens' NotificationChannel Text

-- | The ARN of an IAM role that gives Amazon Rekognition publishing
--   permissions to the Amazon SNS topic.
notificationChannel_roleArn :: Lens' NotificationChannel Text

-- | The prefix applied to the training output files.
outputConfig_s3KeyPrefix :: Lens' OutputConfig (Maybe Text)

-- | The S3 bucket where training output is placed.
outputConfig_s3Bucket :: Lens' OutputConfig (Maybe Text)

-- | The name of the parent label.
parent_name :: Lens' Parent (Maybe Text)

-- | Bounding box around the detected person.
personDetail_boundingBox :: Lens' PersonDetail (Maybe BoundingBox)

-- | Identifier for the person detected person within a video. Use to keep
--   track of the person throughout the video. The identifier is not stored
--   by Amazon Rekognition.
personDetail_index :: Lens' PersonDetail (Maybe Integer)

-- | Face details for the detected person.
personDetail_face :: Lens' PersonDetail (Maybe FaceDetail)

-- | Details about a person whose path was tracked in a video.
personDetection_person :: Lens' PersonDetection (Maybe PersonDetail)

-- | The time, in milliseconds from the start of the video, that the
--   person's path was tracked.
personDetection_timestamp :: Lens' PersonDetection (Maybe Integer)

-- | Information about the faces in the input collection that match the
--   face of a person in the video.
personMatch_faceMatches :: Lens' PersonMatch (Maybe [FaceMatch])

-- | Information about the matched person.
personMatch_person :: Lens' PersonMatch (Maybe PersonDetail)

-- | The time, in milliseconds from the beginning of the video, that the
--   person was matched in the video.
personMatch_timestamp :: Lens' PersonMatch (Maybe Integer)

-- | The value of the X coordinate for a point on a <tt>Polygon</tt>.
point_x :: Lens' Point (Maybe Double)

-- | The value of the Y coordinate for a point on a <tt>Polygon</tt>.
point_y :: Lens' Point (Maybe Double)

-- | Value representing the face rotation on the yaw axis.
pose_yaw :: Lens' Pose (Maybe Double)

-- | Value representing the face rotation on the roll axis.
pose_roll :: Lens' Pose (Maybe Double)

-- | Value representing the face rotation on the pitch axis.
pose_pitch :: Lens' Pose (Maybe Double)

-- | The current status of the project.
projectDescription_status :: Lens' ProjectDescription (Maybe ProjectStatus)

-- | The Unix timestamp for the date and time that the project was created.
projectDescription_creationTimestamp :: Lens' ProjectDescription (Maybe UTCTime)

-- | The Amazon Resource Name (ARN) of the project.
projectDescription_projectArn :: Lens' ProjectDescription (Maybe Text)

-- | The minimum number of inference units used by the model. For more
--   information, see StartProjectVersion.
projectVersionDescription_minInferenceUnits :: Lens' ProjectVersionDescription (Maybe Natural)

-- | The current status of the model version.
projectVersionDescription_status :: Lens' ProjectVersionDescription (Maybe ProjectVersionStatus)

-- | The training results. <tt>EvaluationResult</tt> is only returned if
--   training is successful.
projectVersionDescription_evaluationResult :: Lens' ProjectVersionDescription (Maybe EvaluationResult)

-- | The location of the summary manifest. The summary manifest provides
--   aggregate data validation results for the training and test datasets.
projectVersionDescription_manifestSummary :: Lens' ProjectVersionDescription (Maybe GroundTruthManifest)

-- | The identifer for the AWS Key Management Service (AWS KMS) customer
--   master key that was used to encrypt the model during training.
projectVersionDescription_kmsKeyId :: Lens' ProjectVersionDescription (Maybe Text)

-- | Contains information about the testing results.
projectVersionDescription_testingDataResult :: Lens' ProjectVersionDescription (Maybe TestingDataResult)

-- | A descriptive message for an error or warning that occurred.
projectVersionDescription_statusMessage :: Lens' ProjectVersionDescription (Maybe Text)

-- | The Unix datetime for the date and time that training started.
projectVersionDescription_creationTimestamp :: Lens' ProjectVersionDescription (Maybe UTCTime)

-- | The Amazon Resource Name (ARN) of the model version.
projectVersionDescription_projectVersionArn :: Lens' ProjectVersionDescription (Maybe Text)

-- | The location where training results are saved.
projectVersionDescription_outputConfig :: Lens' ProjectVersionDescription (Maybe OutputConfig)

-- | The duration, in seconds, that the model version has been billed for
--   training. This value is only returned if the model version has been
--   successfully trained.
projectVersionDescription_billableTrainingTimeInSeconds :: Lens' ProjectVersionDescription (Maybe Natural)

-- | The Unix date and time that training of the model ended.
projectVersionDescription_trainingEndTimestamp :: Lens' ProjectVersionDescription (Maybe UTCTime)

-- | Contains information about the training results.
projectVersionDescription_trainingDataResult :: Lens' ProjectVersionDescription (Maybe TrainingDataResult)

-- | An array of Personal Protective Equipment items detected around a body
--   part.
protectiveEquipmentBodyPart_equipmentDetections :: Lens' ProtectiveEquipmentBodyPart (Maybe [EquipmentDetection])

-- | The confidence that Amazon Rekognition has in the detection accuracy
--   of the detected body part.
protectiveEquipmentBodyPart_confidence :: Lens' ProtectiveEquipmentBodyPart (Maybe Double)

-- | The detected body part.
protectiveEquipmentBodyPart_name :: Lens' ProtectiveEquipmentBodyPart (Maybe BodyPart)

-- | An array of body parts detected on a person's body (including body
--   parts without PPE).
protectiveEquipmentPerson_bodyParts :: Lens' ProtectiveEquipmentPerson (Maybe [ProtectiveEquipmentBodyPart])

-- | A bounding box around the detected person.
protectiveEquipmentPerson_boundingBox :: Lens' ProtectiveEquipmentPerson (Maybe BoundingBox)

-- | The confidence that Amazon Rekognition has that the bounding box
--   contains a person.
protectiveEquipmentPerson_confidence :: Lens' ProtectiveEquipmentPerson (Maybe Double)

-- | The identifier for the detected person. The identifier is only unique
--   for a single call to <tt>DetectProtectiveEquipment</tt>.
protectiveEquipmentPerson_id :: Lens' ProtectiveEquipmentPerson (Maybe Natural)

-- | The minimum confidence level for which you want summary information.
--   The confidence level applies to person detection, body part detection,
--   equipment detection, and body part coverage. Amazon Rekognition
--   doesn't return summary information with a confidence than this
--   specified value. There isn't a default value.
--   
--   Specify a <tt>MinConfidence</tt> value that is between 50-100% as
--   <tt>DetectProtectiveEquipment</tt> returns predictions only where the
--   detection confidence is between 50% - 100%. If you specify a value
--   that is less than 50%, the results are the same specifying a value of
--   50%.
protectiveEquipmentSummarizationAttributes_minConfidence :: Lens' ProtectiveEquipmentSummarizationAttributes Double

-- | An array of personal protective equipment types for which you want
--   summary information. If a person is detected wearing a required
--   requipment type, the person's ID is added to the
--   <tt>PersonsWithRequiredEquipment</tt> array field returned in
--   ProtectiveEquipmentSummary by <tt>DetectProtectiveEquipment</tt>.
protectiveEquipmentSummarizationAttributes_requiredEquipmentTypes :: Lens' ProtectiveEquipmentSummarizationAttributes [ProtectiveEquipmentType]

-- | An array of IDs for persons who are wearing detected personal
--   protective equipment.
protectiveEquipmentSummary_personsWithRequiredEquipment :: Lens' ProtectiveEquipmentSummary (Maybe [Natural])

-- | An array of IDs for persons who are not wearing all of the types of
--   PPE specified in the <tt>RequiredEquipmentTypes</tt> field of the
--   detected personal protective equipment.
protectiveEquipmentSummary_personsWithoutRequiredEquipment :: Lens' ProtectiveEquipmentSummary (Maybe [Natural])

-- | An array of IDs for persons where it was not possible to determine if
--   they are wearing personal protective equipment.
protectiveEquipmentSummary_personsIndeterminate :: Lens' ProtectiveEquipmentSummary (Maybe [Natural])

-- | The box representing a region of interest on screen.
regionOfInterest_boundingBox :: Lens' RegionOfInterest (Maybe BoundingBox)

-- | Name of the S3 bucket.
s3Object_bucket :: Lens' S3Object (Maybe Text)

-- | S3 object key name.
s3Object_name :: Lens' S3Object (Maybe Text)

-- | If the bucket is versioning enabled, you can specify the object
--   version.
s3Object_version :: Lens' S3Object (Maybe Text)

-- | If the segment is a technical cue, contains information about the
--   technical cue.
segmentDetection_technicalCueSegment :: Lens' SegmentDetection (Maybe TechnicalCueSegment)

-- | The frame number at the end of a video segment, using a frame index
--   that starts with 0.
segmentDetection_endFrameNumber :: Lens' SegmentDetection (Maybe Natural)

-- | The duration of the timecode for the detected segment in SMPTE format.
segmentDetection_durationSMPTE :: Lens' SegmentDetection (Maybe Text)

-- | The end time of the detected segment, in milliseconds, from the start
--   of the video. This value is rounded down.
segmentDetection_endTimestampMillis :: Lens' SegmentDetection (Maybe Integer)

-- | The frame-accurate SMPTE timecode, from the start of a video, for the
--   start of a detected segment. <tt>StartTimecode</tt> is in
--   <i>HH:MM:SS:fr</i> format (and <i>;fr</i> for drop frame-rates).
segmentDetection_startTimecodeSMPTE :: Lens' SegmentDetection (Maybe Text)

-- | The frame-accurate SMPTE timecode, from the start of a video, for the
--   end of a detected segment. <tt>EndTimecode</tt> is in
--   <i>HH:MM:SS:fr</i> format (and <i>;fr</i> for drop frame-rates).
segmentDetection_endTimecodeSMPTE :: Lens' SegmentDetection (Maybe Text)

-- | The duration of the detected segment in milliseconds.
segmentDetection_durationMillis :: Lens' SegmentDetection (Maybe Natural)

-- | The duration of a video segment, expressed in frames.
segmentDetection_durationFrames :: Lens' SegmentDetection (Maybe Natural)

-- | The start time of the detected segment in milliseconds from the start
--   of the video. This value is rounded down. For example, if the actual
--   timestamp is 100.6667 milliseconds, Amazon Rekognition Video returns a
--   value of 100 millis.
segmentDetection_startTimestampMillis :: Lens' SegmentDetection (Maybe Integer)

-- | The type of the segment. Valid values are <tt>TECHNICAL_CUE</tt> and
--   <tt>SHOT</tt>.
segmentDetection_type :: Lens' SegmentDetection (Maybe SegmentType)

-- | If the segment is a shot detection, contains information about the
--   shot detection.
segmentDetection_shotSegment :: Lens' SegmentDetection (Maybe ShotSegment)

-- | The frame number of the start of a video segment, using a frame index
--   that starts with 0.
segmentDetection_startFrameNumber :: Lens' SegmentDetection (Maybe Natural)

-- | The version of the model used to detect segments.
segmentTypeInfo_modelVersion :: Lens' SegmentTypeInfo (Maybe Text)

-- | The type of a segment (technical cue or shot detection).
segmentTypeInfo_type :: Lens' SegmentTypeInfo (Maybe SegmentType)

-- | The confidence that Amazon Rekognition Video has in the accuracy of
--   the detected segment.
shotSegment_confidence :: Lens' ShotSegment (Maybe Double)

-- | An Identifier for a shot detection segment detected in a video.
shotSegment_index :: Lens' ShotSegment (Maybe Natural)

-- | Boolean value that indicates whether the face is smiling or not.
smile_value :: Lens' Smile (Maybe Bool)

-- | Level of confidence in the determination.
smile_confidence :: Lens' Smile (Maybe Double)

-- | Filters that are specific to technical cues.
startSegmentDetectionFilters_technicalCueFilter :: Lens' StartSegmentDetectionFilters (Maybe StartTechnicalCueDetectionFilter)

-- | Filters that are specific to shot detections.
startSegmentDetectionFilters_shotFilter :: Lens' StartSegmentDetectionFilters (Maybe StartShotDetectionFilter)

-- | Specifies the minimum confidence that Amazon Rekognition Video must
--   have in order to return a detected segment. Confidence represents how
--   certain Amazon Rekognition is that a segment is correctly identified.
--   0 is the lowest confidence. 100 is the highest confidence. Amazon
--   Rekognition Video doesn't return any segments with a confidence level
--   lower than this specified value.
--   
--   If you don't specify <tt>MinSegmentConfidence</tt>, the
--   <tt>GetSegmentDetection</tt> returns segments with confidence values
--   greater than or equal to 50 percent.
startShotDetectionFilter_minSegmentConfidence :: Lens' StartShotDetectionFilter (Maybe Double)

-- | A filter that allows you to control the black frame detection by
--   specifying the black levels and pixel coverage of black pixels in a
--   frame. Videos can come from multiple sources, formats, and time
--   periods, with different standards and varying noise levels for black
--   frames that need to be accounted for.
startTechnicalCueDetectionFilter_blackFrame :: Lens' StartTechnicalCueDetectionFilter (Maybe BlackFrame)

-- | Specifies the minimum confidence that Amazon Rekognition Video must
--   have in order to return a detected segment. Confidence represents how
--   certain Amazon Rekognition is that a segment is correctly identified.
--   0 is the lowest confidence. 100 is the highest confidence. Amazon
--   Rekognition Video doesn't return any segments with a confidence level
--   lower than this specified value.
--   
--   If you don't specify <tt>MinSegmentConfidence</tt>,
--   <tt>GetSegmentDetection</tt> returns segments with confidence values
--   greater than or equal to 50 percent.
startTechnicalCueDetectionFilter_minSegmentConfidence :: Lens' StartTechnicalCueDetectionFilter (Maybe Double)

-- | Filter focusing on a certain area of the frame. Uses a
--   <tt>BoundingBox</tt> object to set the region of the screen.
startTextDetectionFilters_regionsOfInterest :: Lens' StartTextDetectionFilters (Maybe [RegionOfInterest])

-- | Filters focusing on qualities of the text, such as confidence or size.
startTextDetectionFilters_wordFilter :: Lens' StartTextDetectionFilters (Maybe DetectionFilter)

-- | Current status of the Amazon Rekognition stream processor.
streamProcessor_status :: Lens' StreamProcessor (Maybe StreamProcessorStatus)

-- | Name of the Amazon Rekognition stream processor.
streamProcessor_name :: Lens' StreamProcessor (Maybe Text)

-- | The Kinesis video stream input stream for the source streaming video.
streamProcessorInput_kinesisVideoStream :: Lens' StreamProcessorInput (Maybe KinesisVideoStream)

-- | The Amazon Kinesis Data Streams stream to which the Amazon Rekognition
--   stream processor streams the analysis results.
streamProcessorOutput_kinesisDataStream :: Lens' StreamProcessorOutput (Maybe KinesisDataStream)

-- | Face search settings to use on a streaming video.
streamProcessorSettings_faceSearch :: Lens' StreamProcessorSettings (Maybe FaceSearchSettings)

-- | Undocumented member.
summary_s3Object :: Lens' Summary (Maybe S3Object)

-- | Boolean value that indicates whether the face is wearing sunglasses or
--   not.
sunglasses_value :: Lens' Sunglasses (Maybe Bool)

-- | Level of confidence in the determination.
sunglasses_confidence :: Lens' Sunglasses (Maybe Double)

-- | The confidence that Amazon Rekognition Video has in the accuracy of
--   the detected segment.
technicalCueSegment_confidence :: Lens' TechnicalCueSegment (Maybe Double)

-- | The type of the technical cue.
technicalCueSegment_type :: Lens' TechnicalCueSegment (Maybe TechnicalCueType)

-- | The assets used for testing.
testingData_assets :: Lens' TestingData (Maybe [Asset])

-- | If specified, Amazon Rekognition Custom Labels creates a testing
--   dataset with an 80/20 split of the training dataset.
testingData_autoCreate :: Lens' TestingData (Maybe Bool)

-- | The testing dataset that was supplied for training.
testingDataResult_input :: Lens' TestingDataResult (Maybe TestingData)

-- | The subset of the dataset that was actually tested. Some images
--   (assets) might not be tested due to file formatting and other issues.
testingDataResult_output :: Lens' TestingDataResult (Maybe TestingData)

-- | The location of the data validation manifest. The data validation
--   manifest is created for the test dataset during model training.
testingDataResult_validation :: Lens' TestingDataResult (Maybe ValidationData)

-- | The word or line of text recognized by Amazon Rekognition.
textDetection_detectedText :: Lens' TextDetection (Maybe Text)

-- | The confidence that Amazon Rekognition has in the accuracy of the
--   detected text and the accuracy of the geometry points around the
--   detected text.
textDetection_confidence :: Lens' TextDetection (Maybe Double)

-- | The location of the detected text on the image. Includes an axis
--   aligned coarse bounding box surrounding the text and a finer grain
--   polygon for more accurate spatial information.
textDetection_geometry :: Lens' TextDetection (Maybe Geometry)

-- | The identifier for the detected text. The identifier is only unique
--   for a single call to <tt>DetectText</tt>.
textDetection_id :: Lens' TextDetection (Maybe Natural)

-- | The type of text that was detected.
textDetection_type :: Lens' TextDetection (Maybe TextTypes)

-- | The Parent identifier for the detected text identified by the value of
--   <tt>ID</tt>. If the type of detected text is <tt>LINE</tt>, the value
--   of <tt>ParentId</tt> is <tt>Null</tt>.
textDetection_parentId :: Lens' TextDetection (Maybe Natural)

-- | Details about text detected in a video.
textDetectionResult_textDetection :: Lens' TextDetectionResult (Maybe TextDetection)

-- | The time, in milliseconds from the start of the video, that the text
--   was detected.
textDetectionResult_timestamp :: Lens' TextDetectionResult (Maybe Integer)

-- | A Sagemaker GroundTruth manifest file that contains the training
--   images (assets).
trainingData_assets :: Lens' TrainingData (Maybe [Asset])

-- | The training assets that you supplied for training.
trainingDataResult_input :: Lens' TrainingDataResult (Maybe TrainingData)

-- | The images (assets) that were actually trained by Amazon Rekognition
--   Custom Labels.
trainingDataResult_output :: Lens' TrainingDataResult (Maybe TrainingData)

-- | The location of the data validation manifest. The data validation
--   manifest is created for the training dataset during model training.
trainingDataResult_validation :: Lens' TrainingDataResult (Maybe ValidationData)

-- | An array of reasons that specify why a face wasn't indexed.
--   
--   <ul>
--   <li>EXTREME_POSE - The face is at a pose that can't be detected. For
--   example, the head is turned too far away from the camera.</li>
--   <li>EXCEEDS_MAX_FACES - The number of faces detected is already higher
--   than that specified by the <tt>MaxFaces</tt> input parameter for
--   <tt>IndexFaces</tt>.</li>
--   <li>LOW_BRIGHTNESS - The image is too dark.</li>
--   <li>LOW_SHARPNESS - The image is too blurry.</li>
--   <li>LOW_CONFIDENCE - The face was detected with a low confidence.</li>
--   <li>SMALL_BOUNDING_BOX - The bounding box around the face is too
--   small.</li>
--   </ul>
unindexedFace_reasons :: Lens' UnindexedFace (Maybe [Reason])

-- | The structure that contains attributes of a face that
--   <tt>IndexFaces</tt>detected, but didn't index.
unindexedFace_faceDetail :: Lens' UnindexedFace (Maybe FaceDetail)

-- | The assets that comprise the validation data.
validationData_assets :: Lens' ValidationData (Maybe [Asset])

-- | The Amazon S3 bucket name and file name for the video.
video_s3Object :: Lens' Video (Maybe S3Object)

-- | Number of frames per second in the video.
videoMetadata_frameRate :: Lens' VideoMetadata (Maybe Double)

-- | A description of the range of luminance values in a video, either
--   LIMITED (16 to 235) or FULL (0 to 255).
videoMetadata_colorRange :: Lens' VideoMetadata (Maybe VideoColorRange)

-- | Format of the analyzed video. Possible values are MP4, MOV and AVI.
videoMetadata_format :: Lens' VideoMetadata (Maybe Text)

-- | Type of compression used in the analyzed video.
videoMetadata_codec :: Lens' VideoMetadata (Maybe Text)

-- | Vertical pixel dimension of the video.
videoMetadata_frameHeight :: Lens' VideoMetadata (Maybe Natural)

-- | Length of the video in milliseconds.
videoMetadata_durationMillis :: Lens' VideoMetadata (Maybe Natural)

-- | Horizontal pixel dimension of the video.
videoMetadata_frameWidth :: Lens' VideoMetadata (Maybe Natural)


module Network.AWS.Rekognition.Waiters

-- | Polls <a>DescribeProjectVersions</a> every 30 seconds until a
--   successful state is reached. An error is returned after 40 failed
--   checks.
newProjectVersionRunning :: Wait DescribeProjectVersions

-- | Polls <a>DescribeProjectVersions</a> every 120 seconds until a
--   successful state is reached. An error is returned after 360 failed
--   checks.
newProjectVersionTrainingCompleted :: Wait DescribeProjectVersions


-- | Derived from API version <tt>2016-06-27</tt> of the AWS service
--   descriptions, licensed under Apache 2.0.
--   
--   This is the Amazon Rekognition API reference.
module Network.AWS.Rekognition

-- | API version <tt>2016-06-27</tt> of the Amazon Rekognition SDK
--   configuration.
defaultService :: Service

-- | You are not authorized to perform the action.
_AccessDeniedException :: AsError a => Getting (First ServiceError) a ServiceError

-- | The file size or duration of the supplied media is too large. The
--   maximum file size is 10GB. The maximum duration is 6 hours.
_VideoTooLargeException :: AsError a => Getting (First ServiceError) a ServiceError

-- | Input parameter violated a constraint. Validate your parameter before
--   calling the API operation again.
_InvalidParameterException :: AsError a => Getting (First ServiceError) a ServiceError

-- | The provided image format is not supported.
_InvalidImageFormatException :: AsError a => Getting (First ServiceError) a ServiceError

-- | A resource with the specified ID already exists.
_ResourceAlreadyExistsException :: AsError a => Getting (First ServiceError) a ServiceError

-- | Amazon Rekognition is unable to access the S3 object specified in the
--   request.
_InvalidS3ObjectException :: AsError a => Getting (First ServiceError) a ServiceError

-- | The number of requests exceeded your throughput limit. If you want to
--   increase this limit, contact Amazon Rekognition.
_ProvisionedThroughputExceededException :: AsError a => Getting (First ServiceError) a ServiceError

-- | The input image size exceeds the allowed limit. If you are calling
--   DetectProtectiveEquipment, the image size or resolution exceeds the
--   allowed limit. For more information, see Limits in Amazon Rekognition
--   in the Amazon Rekognition Developer Guide.
_ImageTooLargeException :: AsError a => Getting (First ServiceError) a ServiceError

-- | The size of the collection exceeds the allowed limit. For more
--   information, see Limits in Amazon Rekognition in the Amazon
--   Rekognition Developer Guide.
_ServiceQuotaExceededException :: AsError a => Getting (First ServiceError) a ServiceError

-- | Amazon Rekognition is temporarily unable to process the request. Try
--   your call again.
_ThrottlingException :: AsError a => Getting (First ServiceError) a ServiceError

-- | Amazon Rekognition experienced a service issue. Try your call again.
_InternalServerError :: AsError a => Getting (First ServiceError) a ServiceError

-- | A <tt>ClientRequestToken</tt> input parameter was reused with an
--   operation, but at least one of the other input parameters is different
--   from the previous call to the operation.
_IdempotentParameterMismatchException :: AsError a => Getting (First ServiceError) a ServiceError

-- | The requested resource isn't ready. For example, this exception occurs
--   when you call <tt>DetectCustomLabels</tt> with a model version that
--   isn't deployed.
_ResourceNotReadyException :: AsError a => Getting (First ServiceError) a ServiceError

-- | The resource specified in the request cannot be found.
_ResourceNotFoundException :: AsError a => Getting (First ServiceError) a ServiceError

-- | The number of in-progress human reviews you have has exceeded the
--   number allowed.
_HumanLoopQuotaExceededException :: AsError a => Getting (First ServiceError) a ServiceError

-- | Pagination token in the request is not valid.
_InvalidPaginationTokenException :: AsError a => Getting (First ServiceError) a ServiceError

-- | An Amazon Rekognition service limit was exceeded. For example, if you
--   start too many Amazon Rekognition Video jobs concurrently, calls to
--   start operations (<tt>StartLabelDetection</tt>, for example) will
--   raise a <tt>LimitExceededException</tt> exception (HTTP status code:
--   400) until the number of concurrently running jobs is below the Amazon
--   Rekognition service limit.
_LimitExceededException :: AsError a => Getting (First ServiceError) a ServiceError

-- | The specified resource is already being used.
_ResourceInUseException :: AsError a => Getting (First ServiceError) a ServiceError

-- | Polls <a>DescribeProjectVersions</a> every 30 seconds until a
--   successful state is reached. An error is returned after 40 failed
--   checks.
newProjectVersionRunning :: Wait DescribeProjectVersions

-- | Polls <a>DescribeProjectVersions</a> every 120 seconds until a
--   successful state is reached. An error is returned after 360 failed
--   checks.
newProjectVersionTrainingCompleted :: Wait DescribeProjectVersions

-- | <i>See:</i> <a>newDetectProtectiveEquipment</a> smart constructor.
data DetectProtectiveEquipment
DetectProtectiveEquipment' :: Maybe ProtectiveEquipmentSummarizationAttributes -> Image -> DetectProtectiveEquipment

-- | Create a value of <a>DetectProtectiveEquipment</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:summarizationAttributes:DetectProtectiveEquipment'</a>,
--   <a>detectProtectiveEquipment_summarizationAttributes</a> - An array of
--   PPE types that you want to summarize.
--   
--   <a>$sel:image:DetectProtectiveEquipment'</a>,
--   <a>detectProtectiveEquipment_image</a> - The image in which you want
--   to detect PPE on detected persons. The image can be passed as image
--   bytes or you can reference an image stored in an Amazon S3 bucket.
newDetectProtectiveEquipment :: Image -> DetectProtectiveEquipment

-- | <i>See:</i> <a>newDetectProtectiveEquipmentResponse</a> smart
--   constructor.
data DetectProtectiveEquipmentResponse
DetectProtectiveEquipmentResponse' :: Maybe ProtectiveEquipmentSummary -> Maybe Text -> Maybe [ProtectiveEquipmentPerson] -> Int -> DetectProtectiveEquipmentResponse

-- | Create a value of <a>DetectProtectiveEquipmentResponse</a> with all
--   optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:summary:DetectProtectiveEquipmentResponse'</a>,
--   <a>detectProtectiveEquipmentResponse_summary</a> - Summary information
--   for the types of PPE specified in the <tt>SummarizationAttributes</tt>
--   input parameter.
--   
--   
--   <a>$sel:protectiveEquipmentModelVersion:DetectProtectiveEquipmentResponse'</a>,
--   <a>detectProtectiveEquipmentResponse_protectiveEquipmentModelVersion</a>
--   - The version number of the PPE detection model used to detect PPE in
--   the image.
--   
--   <a>$sel:persons:DetectProtectiveEquipmentResponse'</a>,
--   <a>detectProtectiveEquipmentResponse_persons</a> - An array of persons
--   detected in the image (including persons not wearing PPE).
--   
--   <a>$sel:httpStatus:DetectProtectiveEquipmentResponse'</a>,
--   <a>detectProtectiveEquipmentResponse_httpStatus</a> - The response's
--   http status code.
newDetectProtectiveEquipmentResponse :: Int -> DetectProtectiveEquipmentResponse

-- | <i>See:</i> <a>newDeleteProject</a> smart constructor.
data DeleteProject
DeleteProject' :: Text -> DeleteProject

-- | Create a value of <a>DeleteProject</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:projectArn:DeleteProject'</a>, <a>deleteProject_projectArn</a>
--   - The Amazon Resource Name (ARN) of the project that you want to
--   delete.
newDeleteProject :: Text -> DeleteProject

-- | <i>See:</i> <a>newDeleteProjectResponse</a> smart constructor.
data DeleteProjectResponse
DeleteProjectResponse' :: Maybe ProjectStatus -> Int -> DeleteProjectResponse

-- | Create a value of <a>DeleteProjectResponse</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:status:DeleteProjectResponse'</a>,
--   <a>deleteProjectResponse_status</a> - The current status of the delete
--   project operation.
--   
--   <a>$sel:httpStatus:DeleteProjectResponse'</a>,
--   <a>deleteProjectResponse_httpStatus</a> - The response's http status
--   code.
newDeleteProjectResponse :: Int -> DeleteProjectResponse

-- | <i>See:</i> <a>newStartCelebrityRecognition</a> smart constructor.
data StartCelebrityRecognition
StartCelebrityRecognition' :: Maybe Text -> Maybe NotificationChannel -> Maybe Text -> Video -> StartCelebrityRecognition

-- | Create a value of <a>StartCelebrityRecognition</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:jobTag:StartCelebrityRecognition'</a>,
--   <a>startCelebrityRecognition_jobTag</a> - An identifier you specify
--   that's returned in the completion notification that's published to
--   your Amazon Simple Notification Service topic. For example, you can
--   use <tt>JobTag</tt> to group related jobs and identify them in the
--   completion notification.
--   
--   <a>$sel:notificationChannel:StartCelebrityRecognition'</a>,
--   <a>startCelebrityRecognition_notificationChannel</a> - The Amazon SNS
--   topic ARN that you want Amazon Rekognition Video to publish the
--   completion status of the celebrity recognition analysis to. The Amazon
--   SNS topic must have a topic name that begins with
--   <i>AmazonRekognition</i> if you are using the
--   AmazonRekognitionServiceRole permissions policy.
--   
--   <a>$sel:clientRequestToken:StartCelebrityRecognition'</a>,
--   <a>startCelebrityRecognition_clientRequestToken</a> - Idempotent token
--   used to identify the start request. If you use the same token with
--   multiple <tt>StartCelebrityRecognition</tt> requests, the same
--   <tt>JobId</tt> is returned. Use <tt>ClientRequestToken</tt> to prevent
--   the same job from being accidently started more than once.
--   
--   <a>$sel:video:StartCelebrityRecognition'</a>,
--   <a>startCelebrityRecognition_video</a> - The video in which you want
--   to recognize celebrities. The video must be stored in an Amazon S3
--   bucket.
newStartCelebrityRecognition :: Video -> StartCelebrityRecognition

-- | <i>See:</i> <a>newStartCelebrityRecognitionResponse</a> smart
--   constructor.
data StartCelebrityRecognitionResponse
StartCelebrityRecognitionResponse' :: Maybe Text -> Int -> StartCelebrityRecognitionResponse

-- | Create a value of <a>StartCelebrityRecognitionResponse</a> with all
--   optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:jobId:StartCelebrityRecognitionResponse'</a>,
--   <a>startCelebrityRecognitionResponse_jobId</a> - The identifier for
--   the celebrity recognition analysis job. Use <tt>JobId</tt> to identify
--   the job in a subsequent call to <tt>GetCelebrityRecognition</tt>.
--   
--   <a>$sel:httpStatus:StartCelebrityRecognitionResponse'</a>,
--   <a>startCelebrityRecognitionResponse_httpStatus</a> - The response's
--   http status code.
newStartCelebrityRecognitionResponse :: Int -> StartCelebrityRecognitionResponse

-- | <i>See:</i> <a>newGetPersonTracking</a> smart constructor.
data GetPersonTracking
GetPersonTracking' :: Maybe Text -> Maybe Natural -> Maybe PersonTrackingSortBy -> Text -> GetPersonTracking

-- | Create a value of <a>GetPersonTracking</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:nextToken:GetPersonTracking'</a>,
--   <a>getPersonTracking_nextToken</a> - If the previous response was
--   incomplete (because there are more persons to retrieve), Amazon
--   Rekognition Video returns a pagination token in the response. You can
--   use this pagination token to retrieve the next set of persons.
--   
--   <a>$sel:maxResults:GetPersonTracking'</a>,
--   <a>getPersonTracking_maxResults</a> - Maximum number of results to
--   return per paginated call. The largest value you can specify is 1000.
--   If you specify a value greater than 1000, a maximum of 1000 results is
--   returned. The default value is 1000.
--   
--   <a>$sel:sortBy:GetPersonTracking'</a>, <a>getPersonTracking_sortBy</a>
--   - Sort to use for elements in the <tt>Persons</tt> array. Use
--   <tt>TIMESTAMP</tt> to sort array elements by the time persons are
--   detected. Use <tt>INDEX</tt> to sort by the tracked persons. If you
--   sort by <tt>INDEX</tt>, the array elements for each person are sorted
--   by detection confidence. The default sort is by <tt>TIMESTAMP</tt>.
--   
--   <a>$sel:jobId:GetPersonTracking'</a>, <a>getPersonTracking_jobId</a> -
--   The identifier for a job that tracks persons in a video. You get the
--   <tt>JobId</tt> from a call to <tt>StartPersonTracking</tt>.
newGetPersonTracking :: Text -> GetPersonTracking

-- | <i>See:</i> <a>newGetPersonTrackingResponse</a> smart constructor.
data GetPersonTrackingResponse
GetPersonTrackingResponse' :: Maybe Text -> Maybe VideoMetadata -> Maybe Text -> Maybe VideoJobStatus -> Maybe [PersonDetection] -> Int -> GetPersonTrackingResponse

-- | Create a value of <a>GetPersonTrackingResponse</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:nextToken:GetPersonTracking'</a>,
--   <a>getPersonTrackingResponse_nextToken</a> - If the response is
--   truncated, Amazon Rekognition Video returns this token that you can
--   use in the subsequent request to retrieve the next set of persons.
--   
--   <a>$sel:videoMetadata:GetPersonTrackingResponse'</a>,
--   <a>getPersonTrackingResponse_videoMetadata</a> - Information about a
--   video that Amazon Rekognition Video analyzed. <tt>Videometadata</tt>
--   is returned in every page of paginated responses from a Amazon
--   Rekognition Video operation.
--   
--   <a>$sel:statusMessage:GetPersonTrackingResponse'</a>,
--   <a>getPersonTrackingResponse_statusMessage</a> - If the job fails,
--   <tt>StatusMessage</tt> provides a descriptive error message.
--   
--   <a>$sel:jobStatus:GetPersonTrackingResponse'</a>,
--   <a>getPersonTrackingResponse_jobStatus</a> - The current status of the
--   person tracking job.
--   
--   <a>$sel:persons:GetPersonTrackingResponse'</a>,
--   <a>getPersonTrackingResponse_persons</a> - An array of the persons
--   detected in the video and the time(s) their path was tracked
--   throughout the video. An array element will exist for each time a
--   person's path is tracked.
--   
--   <a>$sel:httpStatus:GetPersonTrackingResponse'</a>,
--   <a>getPersonTrackingResponse_httpStatus</a> - The response's http
--   status code.
newGetPersonTrackingResponse :: Int -> GetPersonTrackingResponse

-- | <i>See:</i> <a>newGetTextDetection</a> smart constructor.
data GetTextDetection
GetTextDetection' :: Maybe Text -> Maybe Natural -> Text -> GetTextDetection

-- | Create a value of <a>GetTextDetection</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:nextToken:GetTextDetection'</a>,
--   <a>getTextDetection_nextToken</a> - If the previous response was
--   incomplete (because there are more labels to retrieve), Amazon
--   Rekognition Video returns a pagination token in the response. You can
--   use this pagination token to retrieve the next set of text.
--   
--   <a>$sel:maxResults:GetTextDetection'</a>,
--   <a>getTextDetection_maxResults</a> - Maximum number of results to
--   return per paginated call. The largest value you can specify is 1000.
--   
--   <a>$sel:jobId:GetTextDetection'</a>, <a>getTextDetection_jobId</a> -
--   Job identifier for the text detection operation for which you want
--   results returned. You get the job identifer from an initial call to
--   <tt>StartTextDetection</tt>.
newGetTextDetection :: Text -> GetTextDetection

-- | <i>See:</i> <a>newGetTextDetectionResponse</a> smart constructor.
data GetTextDetectionResponse
GetTextDetectionResponse' :: Maybe [TextDetectionResult] -> Maybe Text -> Maybe VideoMetadata -> Maybe Text -> Maybe Text -> Maybe VideoJobStatus -> Int -> GetTextDetectionResponse

-- | Create a value of <a>GetTextDetectionResponse</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:textDetections:GetTextDetectionResponse'</a>,
--   <a>getTextDetectionResponse_textDetections</a> - An array of text
--   detected in the video. Each element contains the detected text, the
--   time in milliseconds from the start of the video that the text was
--   detected, and where it was detected on the screen.
--   
--   <a>$sel:nextToken:GetTextDetection'</a>,
--   <a>getTextDetectionResponse_nextToken</a> - If the response is
--   truncated, Amazon Rekognition Video returns this token that you can
--   use in the subsequent request to retrieve the next set of text.
--   
--   <a>$sel:videoMetadata:GetTextDetectionResponse'</a>,
--   <a>getTextDetectionResponse_videoMetadata</a> - Undocumented member.
--   
--   <a>$sel:statusMessage:GetTextDetectionResponse'</a>,
--   <a>getTextDetectionResponse_statusMessage</a> - If the job fails,
--   <tt>StatusMessage</tt> provides a descriptive error message.
--   
--   <a>$sel:textModelVersion:GetTextDetectionResponse'</a>,
--   <a>getTextDetectionResponse_textModelVersion</a> - Version number of
--   the text detection model that was used to detect text.
--   
--   <a>$sel:jobStatus:GetTextDetectionResponse'</a>,
--   <a>getTextDetectionResponse_jobStatus</a> - Current status of the text
--   detection job.
--   
--   <a>$sel:httpStatus:GetTextDetectionResponse'</a>,
--   <a>getTextDetectionResponse_httpStatus</a> - The response's http
--   status code.
newGetTextDetectionResponse :: Int -> GetTextDetectionResponse

-- | <i>See:</i> <a>newStartSegmentDetection</a> smart constructor.
data StartSegmentDetection
StartSegmentDetection' :: Maybe Text -> Maybe StartSegmentDetectionFilters -> Maybe NotificationChannel -> Maybe Text -> Video -> NonEmpty SegmentType -> StartSegmentDetection

-- | Create a value of <a>StartSegmentDetection</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:jobTag:StartSegmentDetection'</a>,
--   <a>startSegmentDetection_jobTag</a> - An identifier you specify that's
--   returned in the completion notification that's published to your
--   Amazon Simple Notification Service topic. For example, you can use
--   <tt>JobTag</tt> to group related jobs and identify them in the
--   completion notification.
--   
--   <a>$sel:filters:StartSegmentDetection'</a>,
--   <a>startSegmentDetection_filters</a> - Filters for technical cue or
--   shot detection.
--   
--   <a>$sel:notificationChannel:StartSegmentDetection'</a>,
--   <a>startSegmentDetection_notificationChannel</a> - The ARN of the
--   Amazon SNS topic to which you want Amazon Rekognition Video to publish
--   the completion status of the segment detection operation. Note that
--   the Amazon SNS topic must have a topic name that begins with
--   <i>AmazonRekognition</i> if you are using the
--   AmazonRekognitionServiceRole permissions policy to access the topic.
--   
--   <a>$sel:clientRequestToken:StartSegmentDetection'</a>,
--   <a>startSegmentDetection_clientRequestToken</a> - Idempotent token
--   used to identify the start request. If you use the same token with
--   multiple <tt>StartSegmentDetection</tt> requests, the same
--   <tt>JobId</tt> is returned. Use <tt>ClientRequestToken</tt> to prevent
--   the same job from being accidently started more than once.
--   
--   <a>$sel:video:StartSegmentDetection'</a>,
--   <a>startSegmentDetection_video</a> - Undocumented member.
--   
--   <a>$sel:segmentTypes:StartSegmentDetection'</a>,
--   <a>startSegmentDetection_segmentTypes</a> - An array of segment types
--   to detect in the video. Valid values are TECHNICAL_CUE and SHOT.
newStartSegmentDetection :: Video -> NonEmpty SegmentType -> StartSegmentDetection

-- | <i>See:</i> <a>newStartSegmentDetectionResponse</a> smart constructor.
data StartSegmentDetectionResponse
StartSegmentDetectionResponse' :: Maybe Text -> Int -> StartSegmentDetectionResponse

-- | Create a value of <a>StartSegmentDetectionResponse</a> with all
--   optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:jobId:StartSegmentDetectionResponse'</a>,
--   <a>startSegmentDetectionResponse_jobId</a> - Unique identifier for the
--   segment detection job. The <tt>JobId</tt> is returned from
--   <tt>StartSegmentDetection</tt>.
--   
--   <a>$sel:httpStatus:StartSegmentDetectionResponse'</a>,
--   <a>startSegmentDetectionResponse_httpStatus</a> - The response's http
--   status code.
newStartSegmentDetectionResponse :: Int -> StartSegmentDetectionResponse

-- | <i>See:</i> <a>newListCollections</a> smart constructor.
data ListCollections
ListCollections' :: Maybe Text -> Maybe Natural -> ListCollections

-- | Create a value of <a>ListCollections</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:nextToken:ListCollections'</a>,
--   <a>listCollections_nextToken</a> - Pagination token from the previous
--   response.
--   
--   <a>$sel:maxResults:ListCollections'</a>,
--   <a>listCollections_maxResults</a> - Maximum number of collection IDs
--   to return.
newListCollections :: ListCollections

-- | <i>See:</i> <a>newListCollectionsResponse</a> smart constructor.
data ListCollectionsResponse
ListCollectionsResponse' :: Maybe [Text] -> Maybe Text -> Maybe [Text] -> Int -> ListCollectionsResponse

-- | Create a value of <a>ListCollectionsResponse</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:collectionIds:ListCollectionsResponse'</a>,
--   <a>listCollectionsResponse_collectionIds</a> - An array of collection
--   IDs.
--   
--   <a>$sel:nextToken:ListCollections'</a>,
--   <a>listCollectionsResponse_nextToken</a> - If the result is truncated,
--   the response provides a <tt>NextToken</tt> that you can use in the
--   subsequent request to fetch the next set of collection IDs.
--   
--   <a>$sel:faceModelVersions:ListCollectionsResponse'</a>,
--   <a>listCollectionsResponse_faceModelVersions</a> - Version numbers of
--   the face detection models associated with the collections in the array
--   <tt>CollectionIds</tt>. For example, the value of
--   <tt>FaceModelVersions[2]</tt> is the version number for the face
--   detection model used by the collection in <tt>CollectionId[2]</tt>.
--   
--   <a>$sel:httpStatus:ListCollectionsResponse'</a>,
--   <a>listCollectionsResponse_httpStatus</a> - The response's http status
--   code.
newListCollectionsResponse :: Int -> ListCollectionsResponse

-- | <i>See:</i> <a>newStartProjectVersion</a> smart constructor.
data StartProjectVersion
StartProjectVersion' :: Text -> Natural -> StartProjectVersion

-- | Create a value of <a>StartProjectVersion</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:projectVersionArn:StartProjectVersion'</a>,
--   <a>startProjectVersion_projectVersionArn</a> - The Amazon Resource
--   Name(ARN) of the model version that you want to start.
--   
--   <a>$sel:minInferenceUnits:StartProjectVersion'</a>,
--   <a>startProjectVersion_minInferenceUnits</a> - The minimum number of
--   inference units to use. A single inference unit represents 1 hour of
--   processing and can support up to 5 Transaction Pers Second (TPS). Use
--   a higher number to increase the TPS throughput of your model. You are
--   charged for the number of inference units that you use.
newStartProjectVersion :: Text -> Natural -> StartProjectVersion

-- | <i>See:</i> <a>newStartProjectVersionResponse</a> smart constructor.
data StartProjectVersionResponse
StartProjectVersionResponse' :: Maybe ProjectVersionStatus -> Int -> StartProjectVersionResponse

-- | Create a value of <a>StartProjectVersionResponse</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:status:StartProjectVersionResponse'</a>,
--   <a>startProjectVersionResponse_status</a> - The current running status
--   of the model.
--   
--   <a>$sel:httpStatus:StartProjectVersionResponse'</a>,
--   <a>startProjectVersionResponse_httpStatus</a> - The response's http
--   status code.
newStartProjectVersionResponse :: Int -> StartProjectVersionResponse

-- | <i>See:</i> <a>newDeleteCollection</a> smart constructor.
data DeleteCollection
DeleteCollection' :: Text -> DeleteCollection

-- | Create a value of <a>DeleteCollection</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:collectionId:DeleteCollection'</a>,
--   <a>deleteCollection_collectionId</a> - ID of the collection to delete.
newDeleteCollection :: Text -> DeleteCollection

-- | <i>See:</i> <a>newDeleteCollectionResponse</a> smart constructor.
data DeleteCollectionResponse
DeleteCollectionResponse' :: Maybe Natural -> Int -> DeleteCollectionResponse

-- | Create a value of <a>DeleteCollectionResponse</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:statusCode:DeleteCollectionResponse'</a>,
--   <a>deleteCollectionResponse_statusCode</a> - HTTP status code that
--   indicates the result of the operation.
--   
--   <a>$sel:httpStatus:DeleteCollectionResponse'</a>,
--   <a>deleteCollectionResponse_httpStatus</a> - The response's http
--   status code.
newDeleteCollectionResponse :: Int -> DeleteCollectionResponse

-- | <i>See:</i> <a>newCreateCollection</a> smart constructor.
data CreateCollection
CreateCollection' :: Maybe (HashMap Text Text) -> Text -> CreateCollection

-- | Create a value of <a>CreateCollection</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:tags:CreateCollection'</a>, <a>createCollection_tags</a> - A
--   set of tags (key-value pairs) that you want to attach to the
--   collection.
--   
--   <a>$sel:collectionId:CreateCollection'</a>,
--   <a>createCollection_collectionId</a> - ID for the collection that you
--   are creating.
newCreateCollection :: Text -> CreateCollection

-- | <i>See:</i> <a>newCreateCollectionResponse</a> smart constructor.
data CreateCollectionResponse
CreateCollectionResponse' :: Maybe Text -> Maybe Text -> Maybe Natural -> Int -> CreateCollectionResponse

-- | Create a value of <a>CreateCollectionResponse</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:faceModelVersion:CreateCollectionResponse'</a>,
--   <a>createCollectionResponse_faceModelVersion</a> - Version number of
--   the face detection model associated with the collection you are
--   creating.
--   
--   <a>$sel:collectionArn:CreateCollectionResponse'</a>,
--   <a>createCollectionResponse_collectionArn</a> - Amazon Resource Name
--   (ARN) of the collection. You can use this to manage permissions on
--   your resources.
--   
--   <a>$sel:statusCode:CreateCollectionResponse'</a>,
--   <a>createCollectionResponse_statusCode</a> - HTTP status code
--   indicating the result of the operation.
--   
--   <a>$sel:httpStatus:CreateCollectionResponse'</a>,
--   <a>createCollectionResponse_httpStatus</a> - The response's http
--   status code.
newCreateCollectionResponse :: Int -> CreateCollectionResponse

-- | <i>See:</i> <a>newStopStreamProcessor</a> smart constructor.
data StopStreamProcessor
StopStreamProcessor' :: Text -> StopStreamProcessor

-- | Create a value of <a>StopStreamProcessor</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:name:StopStreamProcessor'</a>, <a>stopStreamProcessor_name</a>
--   - The name of a stream processor created by CreateStreamProcessor.
newStopStreamProcessor :: Text -> StopStreamProcessor

-- | <i>See:</i> <a>newStopStreamProcessorResponse</a> smart constructor.
data StopStreamProcessorResponse
StopStreamProcessorResponse' :: Int -> StopStreamProcessorResponse

-- | Create a value of <a>StopStreamProcessorResponse</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:httpStatus:StopStreamProcessorResponse'</a>,
--   <a>stopStreamProcessorResponse_httpStatus</a> - The response's http
--   status code.
newStopStreamProcessorResponse :: Int -> StopStreamProcessorResponse

-- | <i>See:</i> <a>newDetectLabels</a> smart constructor.
data DetectLabels
DetectLabels' :: Maybe Double -> Maybe Natural -> Image -> DetectLabels

-- | Create a value of <a>DetectLabels</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:minConfidence:DetectLabels'</a>,
--   <a>detectLabels_minConfidence</a> - Specifies the minimum confidence
--   level for the labels to return. Amazon Rekognition doesn't return any
--   labels with confidence lower than this specified value.
--   
--   If <tt>MinConfidence</tt> is not specified, the operation returns
--   labels with a confidence values greater than or equal to 55 percent.
--   
--   <a>$sel:maxLabels:DetectLabels'</a>, <a>detectLabels_maxLabels</a> -
--   Maximum number of labels you want the service to return in the
--   response. The service returns the specified number of highest
--   confidence labels.
--   
--   <a>$sel:image:DetectLabels'</a>, <a>detectLabels_image</a> - The input
--   image as base64-encoded bytes or an S3 object. If you use the AWS CLI
--   to call Amazon Rekognition operations, passing image bytes is not
--   supported. Images stored in an S3 Bucket do not need to be
--   base64-encoded.
--   
--   If you are using an AWS SDK to call Amazon Rekognition, you might not
--   need to base64-encode image bytes passed using the <tt>Bytes</tt>
--   field. For more information, see Images in the Amazon Rekognition
--   developer guide.
newDetectLabels :: Image -> DetectLabels

-- | <i>See:</i> <a>newDetectLabelsResponse</a> smart constructor.
data DetectLabelsResponse
DetectLabelsResponse' :: Maybe [Label] -> Maybe OrientationCorrection -> Maybe Text -> Int -> DetectLabelsResponse

-- | Create a value of <a>DetectLabelsResponse</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:labels:DetectLabelsResponse'</a>,
--   <a>detectLabelsResponse_labels</a> - An array of labels for the
--   real-world objects detected.
--   
--   <a>$sel:orientationCorrection:DetectLabelsResponse'</a>,
--   <a>detectLabelsResponse_orientationCorrection</a> - The value of
--   <tt>OrientationCorrection</tt> is always null.
--   
--   If the input image is in .jpeg format, it might contain exchangeable
--   image file format (Exif) metadata that includes the image's
--   orientation. Amazon Rekognition uses this orientation information to
--   perform image correction. The bounding box coordinates are translated
--   to represent object locations after the orientation information in the
--   Exif metadata is used to correct the image orientation. Images in .png
--   format don't contain Exif metadata.
--   
--   Amazon Rekognition doesn’t perform image correction for images in .png
--   format and .jpeg images without orientation information in the image
--   Exif metadata. The bounding box coordinates aren't translated and
--   represent the object locations before the image is rotated.
--   
--   <a>$sel:labelModelVersion:DetectLabelsResponse'</a>,
--   <a>detectLabelsResponse_labelModelVersion</a> - Version number of the
--   label detection model that was used to detect labels.
--   
--   <a>$sel:httpStatus:DetectLabelsResponse'</a>,
--   <a>detectLabelsResponse_httpStatus</a> - The response's http status
--   code.
newDetectLabelsResponse :: Int -> DetectLabelsResponse

-- | <i>See:</i> <a>newListTagsForResource</a> smart constructor.
data ListTagsForResource
ListTagsForResource' :: Text -> ListTagsForResource

-- | Create a value of <a>ListTagsForResource</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:resourceArn:ListTagsForResource'</a>,
--   <a>listTagsForResource_resourceArn</a> - Amazon Resource Name (ARN) of
--   the model, collection, or stream processor that contains the tags that
--   you want a list of.
newListTagsForResource :: Text -> ListTagsForResource

-- | <i>See:</i> <a>newListTagsForResourceResponse</a> smart constructor.
data ListTagsForResourceResponse
ListTagsForResourceResponse' :: Maybe (HashMap Text Text) -> Int -> ListTagsForResourceResponse

-- | Create a value of <a>ListTagsForResourceResponse</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:tags:ListTagsForResourceResponse'</a>,
--   <a>listTagsForResourceResponse_tags</a> - A list of key-value tags
--   assigned to the resource.
--   
--   <a>$sel:httpStatus:ListTagsForResourceResponse'</a>,
--   <a>listTagsForResourceResponse_httpStatus</a> - The response's http
--   status code.
newListTagsForResourceResponse :: Int -> ListTagsForResourceResponse

-- | <i>See:</i> <a>newStartContentModeration</a> smart constructor.
data StartContentModeration
StartContentModeration' :: Maybe Text -> Maybe NotificationChannel -> Maybe Text -> Maybe Double -> Video -> StartContentModeration

-- | Create a value of <a>StartContentModeration</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:jobTag:StartContentModeration'</a>,
--   <a>startContentModeration_jobTag</a> - An identifier you specify
--   that's returned in the completion notification that's published to
--   your Amazon Simple Notification Service topic. For example, you can
--   use <tt>JobTag</tt> to group related jobs and identify them in the
--   completion notification.
--   
--   <a>$sel:notificationChannel:StartContentModeration'</a>,
--   <a>startContentModeration_notificationChannel</a> - The Amazon SNS
--   topic ARN that you want Amazon Rekognition Video to publish the
--   completion status of the content analysis to. The Amazon SNS topic
--   must have a topic name that begins with <i>AmazonRekognition</i> if
--   you are using the AmazonRekognitionServiceRole permissions policy to
--   access the topic.
--   
--   <a>$sel:clientRequestToken:StartContentModeration'</a>,
--   <a>startContentModeration_clientRequestToken</a> - Idempotent token
--   used to identify the start request. If you use the same token with
--   multiple <tt>StartContentModeration</tt> requests, the same
--   <tt>JobId</tt> is returned. Use <tt>ClientRequestToken</tt> to prevent
--   the same job from being accidently started more than once.
--   
--   <a>$sel:minConfidence:StartContentModeration'</a>,
--   <a>startContentModeration_minConfidence</a> - Specifies the minimum
--   confidence that Amazon Rekognition must have in order to return a
--   moderated content label. Confidence represents how certain Amazon
--   Rekognition is that the moderated content is correctly identified. 0
--   is the lowest confidence. 100 is the highest confidence. Amazon
--   Rekognition doesn't return any moderated content labels with a
--   confidence level lower than this specified value. If you don't specify
--   <tt>MinConfidence</tt>, <tt>GetContentModeration</tt> returns labels
--   with confidence values greater than or equal to 50 percent.
--   
--   <a>$sel:video:StartContentModeration'</a>,
--   <a>startContentModeration_video</a> - The video in which you want to
--   detect inappropriate, unwanted, or offensive content. The video must
--   be stored in an Amazon S3 bucket.
newStartContentModeration :: Video -> StartContentModeration

-- | <i>See:</i> <a>newStartContentModerationResponse</a> smart
--   constructor.
data StartContentModerationResponse
StartContentModerationResponse' :: Maybe Text -> Int -> StartContentModerationResponse

-- | Create a value of <a>StartContentModerationResponse</a> with all
--   optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:jobId:StartContentModerationResponse'</a>,
--   <a>startContentModerationResponse_jobId</a> - The identifier for the
--   content analysis job. Use <tt>JobId</tt> to identify the job in a
--   subsequent call to <tt>GetContentModeration</tt>.
--   
--   <a>$sel:httpStatus:StartContentModerationResponse'</a>,
--   <a>startContentModerationResponse_httpStatus</a> - The response's http
--   status code.
newStartContentModerationResponse :: Int -> StartContentModerationResponse

-- | <i>See:</i> <a>newSearchFacesByImage</a> smart constructor.
data SearchFacesByImage
SearchFacesByImage' :: Maybe QualityFilter -> Maybe Double -> Maybe Natural -> Text -> Image -> SearchFacesByImage

-- | Create a value of <a>SearchFacesByImage</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:qualityFilter:SearchFacesByImage'</a>,
--   <a>searchFacesByImage_qualityFilter</a> - A filter that specifies a
--   quality bar for how much filtering is done to identify faces. Filtered
--   faces aren't searched for in the collection. If you specify
--   <tt>AUTO</tt>, Amazon Rekognition chooses the quality bar. If you
--   specify <tt>LOW</tt>, <tt>MEDIUM</tt>, or <tt>HIGH</tt>, filtering
--   removes all faces that don’t meet the chosen quality bar. The quality
--   bar is based on a variety of common use cases. Low-quality detections
--   can occur for a number of reasons. Some examples are an object that's
--   misidentified as a face, a face that's too blurry, or a face with a
--   pose that's too extreme to use. If you specify <tt>NONE</tt>, no
--   filtering is performed. The default value is <tt>NONE</tt>.
--   
--   To use quality filtering, the collection you are using must be
--   associated with version 3 of the face model or higher.
--   
--   <a>$sel:faceMatchThreshold:SearchFacesByImage'</a>,
--   <a>searchFacesByImage_faceMatchThreshold</a> - (Optional) Specifies
--   the minimum confidence in the face match to return. For example, don't
--   return any matches where confidence in matches is less than 70%. The
--   default value is 80%.
--   
--   <a>$sel:maxFaces:SearchFacesByImage'</a>,
--   <a>searchFacesByImage_maxFaces</a> - Maximum number of faces to
--   return. The operation returns the maximum number of faces with the
--   highest confidence in the match.
--   
--   <a>$sel:collectionId:SearchFacesByImage'</a>,
--   <a>searchFacesByImage_collectionId</a> - ID of the collection to
--   search.
--   
--   <a>$sel:image:SearchFacesByImage'</a>, <a>searchFacesByImage_image</a>
--   - The input image as base64-encoded bytes or an S3 object. If you use
--   the AWS CLI to call Amazon Rekognition operations, passing
--   base64-encoded image bytes is not supported.
--   
--   If you are using an AWS SDK to call Amazon Rekognition, you might not
--   need to base64-encode image bytes passed using the <tt>Bytes</tt>
--   field. For more information, see Images in the Amazon Rekognition
--   developer guide.
newSearchFacesByImage :: Text -> Image -> SearchFacesByImage

-- | <i>See:</i> <a>newSearchFacesByImageResponse</a> smart constructor.
data SearchFacesByImageResponse
SearchFacesByImageResponse' :: Maybe [FaceMatch] -> Maybe Text -> Maybe BoundingBox -> Maybe Double -> Int -> SearchFacesByImageResponse

-- | Create a value of <a>SearchFacesByImageResponse</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:faceMatches:SearchFacesByImageResponse'</a>,
--   <a>searchFacesByImageResponse_faceMatches</a> - An array of faces that
--   match the input face, along with the confidence in the match.
--   
--   <a>$sel:faceModelVersion:SearchFacesByImageResponse'</a>,
--   <a>searchFacesByImageResponse_faceModelVersion</a> - Version number of
--   the face detection model associated with the input collection
--   (<tt>CollectionId</tt>).
--   
--   <a>$sel:searchedFaceBoundingBox:SearchFacesByImageResponse'</a>,
--   <a>searchFacesByImageResponse_searchedFaceBoundingBox</a> - The
--   bounding box around the face in the input image that Amazon
--   Rekognition used for the search.
--   
--   <a>$sel:searchedFaceConfidence:SearchFacesByImageResponse'</a>,
--   <a>searchFacesByImageResponse_searchedFaceConfidence</a> - The level
--   of confidence that the <tt>searchedFaceBoundingBox</tt>, contains a
--   face.
--   
--   <a>$sel:httpStatus:SearchFacesByImageResponse'</a>,
--   <a>searchFacesByImageResponse_httpStatus</a> - The response's http
--   status code.
newSearchFacesByImageResponse :: Int -> SearchFacesByImageResponse

-- | <i>See:</i> <a>newListStreamProcessors</a> smart constructor.
data ListStreamProcessors
ListStreamProcessors' :: Maybe Text -> Maybe Natural -> ListStreamProcessors

-- | Create a value of <a>ListStreamProcessors</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:nextToken:ListStreamProcessors'</a>,
--   <a>listStreamProcessors_nextToken</a> - If the previous response was
--   incomplete (because there are more stream processors to retrieve),
--   Amazon Rekognition Video returns a pagination token in the response.
--   You can use this pagination token to retrieve the next set of stream
--   processors.
--   
--   <a>$sel:maxResults:ListStreamProcessors'</a>,
--   <a>listStreamProcessors_maxResults</a> - Maximum number of stream
--   processors you want Amazon Rekognition Video to return in the
--   response. The default is 1000.
newListStreamProcessors :: ListStreamProcessors

-- | <i>See:</i> <a>newListStreamProcessorsResponse</a> smart constructor.
data ListStreamProcessorsResponse
ListStreamProcessorsResponse' :: Maybe [StreamProcessor] -> Maybe Text -> Int -> ListStreamProcessorsResponse

-- | Create a value of <a>ListStreamProcessorsResponse</a> with all
--   optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:streamProcessors:ListStreamProcessorsResponse'</a>,
--   <a>listStreamProcessorsResponse_streamProcessors</a> - List of stream
--   processors that you have created.
--   
--   <a>$sel:nextToken:ListStreamProcessors'</a>,
--   <a>listStreamProcessorsResponse_nextToken</a> - If the response is
--   truncated, Amazon Rekognition Video returns this token that you can
--   use in the subsequent request to retrieve the next set of stream
--   processors.
--   
--   <a>$sel:httpStatus:ListStreamProcessorsResponse'</a>,
--   <a>listStreamProcessorsResponse_httpStatus</a> - The response's http
--   status code.
newListStreamProcessorsResponse :: Int -> ListStreamProcessorsResponse

-- | <i>See:</i> <a>newDescribeCollection</a> smart constructor.
data DescribeCollection
DescribeCollection' :: Text -> DescribeCollection

-- | Create a value of <a>DescribeCollection</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:collectionId:DescribeCollection'</a>,
--   <a>describeCollection_collectionId</a> - The ID of the collection to
--   describe.
newDescribeCollection :: Text -> DescribeCollection

-- | <i>See:</i> <a>newDescribeCollectionResponse</a> smart constructor.
data DescribeCollectionResponse
DescribeCollectionResponse' :: Maybe Text -> Maybe Natural -> Maybe POSIX -> Maybe Text -> Int -> DescribeCollectionResponse

-- | Create a value of <a>DescribeCollectionResponse</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:faceModelVersion:DescribeCollectionResponse'</a>,
--   <a>describeCollectionResponse_faceModelVersion</a> - The version of
--   the face model that's used by the collection for face detection.
--   
--   For more information, see Model Versioning in the Amazon Rekognition
--   Developer Guide.
--   
--   <a>$sel:faceCount:DescribeCollectionResponse'</a>,
--   <a>describeCollectionResponse_faceCount</a> - The number of faces that
--   are indexed into the collection. To index faces into a collection, use
--   IndexFaces.
--   
--   <a>$sel:creationTimestamp:DescribeCollectionResponse'</a>,
--   <a>describeCollectionResponse_creationTimestamp</a> - The number of
--   milliseconds since the Unix epoch time until the creation of the
--   collection. The Unix epoch time is 00:00:00 Coordinated Universal Time
--   (UTC), Thursday, 1 January 1970.
--   
--   <a>$sel:collectionARN:DescribeCollectionResponse'</a>,
--   <a>describeCollectionResponse_collectionARN</a> - The Amazon Resource
--   Name (ARN) of the collection.
--   
--   <a>$sel:httpStatus:DescribeCollectionResponse'</a>,
--   <a>describeCollectionResponse_httpStatus</a> - The response's http
--   status code.
newDescribeCollectionResponse :: Int -> DescribeCollectionResponse

-- | <i>See:</i> <a>newDeleteProjectVersion</a> smart constructor.
data DeleteProjectVersion
DeleteProjectVersion' :: Text -> DeleteProjectVersion

-- | Create a value of <a>DeleteProjectVersion</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:projectVersionArn:DeleteProjectVersion'</a>,
--   <a>deleteProjectVersion_projectVersionArn</a> - The Amazon Resource
--   Name (ARN) of the model version that you want to delete.
newDeleteProjectVersion :: Text -> DeleteProjectVersion

-- | <i>See:</i> <a>newDeleteProjectVersionResponse</a> smart constructor.
data DeleteProjectVersionResponse
DeleteProjectVersionResponse' :: Maybe ProjectVersionStatus -> Int -> DeleteProjectVersionResponse

-- | Create a value of <a>DeleteProjectVersionResponse</a> with all
--   optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:status:DeleteProjectVersionResponse'</a>,
--   <a>deleteProjectVersionResponse_status</a> - The status of the
--   deletion operation.
--   
--   <a>$sel:httpStatus:DeleteProjectVersionResponse'</a>,
--   <a>deleteProjectVersionResponse_httpStatus</a> - The response's http
--   status code.
newDeleteProjectVersionResponse :: Int -> DeleteProjectVersionResponse

-- | <i>See:</i> <a>newDescribeProjectVersions</a> smart constructor.
data DescribeProjectVersions
DescribeProjectVersions' :: Maybe Text -> Maybe (NonEmpty Text) -> Maybe Natural -> Text -> DescribeProjectVersions

-- | Create a value of <a>DescribeProjectVersions</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:nextToken:DescribeProjectVersions'</a>,
--   <a>describeProjectVersions_nextToken</a> - If the previous response
--   was incomplete (because there is more results to retrieve), Amazon
--   Rekognition Custom Labels returns a pagination token in the response.
--   You can use this pagination token to retrieve the next set of results.
--   
--   <a>$sel:versionNames:DescribeProjectVersions'</a>,
--   <a>describeProjectVersions_versionNames</a> - A list of model version
--   names that you want to describe. You can add up to 10 model version
--   names to the list. If you don't specify a value, all model
--   descriptions are returned. A version name is part of a model
--   (ProjectVersion) ARN. For example,
--   <tt>my-model.2020-01-21T09.10.15</tt> is the version name in the
--   following ARN.
--   <tt>arn:aws:rekognition:us-east-1:123456789012:project/getting-started/version/my-model.2020-01-21T09.10.15/1234567890123</tt>.
--   
--   <a>$sel:maxResults:DescribeProjectVersions'</a>,
--   <a>describeProjectVersions_maxResults</a> - The maximum number of
--   results to return per paginated call. The largest value you can
--   specify is 100. If you specify a value greater than 100, a
--   ValidationException error occurs. The default value is 100.
--   
--   <a>$sel:projectArn:DescribeProjectVersions'</a>,
--   <a>describeProjectVersions_projectArn</a> - The Amazon Resource Name
--   (ARN) of the project that contains the models you want to describe.
newDescribeProjectVersions :: Text -> DescribeProjectVersions

-- | <i>See:</i> <a>newDescribeProjectVersionsResponse</a> smart
--   constructor.
data DescribeProjectVersionsResponse
DescribeProjectVersionsResponse' :: Maybe Text -> Maybe [ProjectVersionDescription] -> Int -> DescribeProjectVersionsResponse

-- | Create a value of <a>DescribeProjectVersionsResponse</a> with all
--   optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:nextToken:DescribeProjectVersions'</a>,
--   <a>describeProjectVersionsResponse_nextToken</a> - If the previous
--   response was incomplete (because there is more results to retrieve),
--   Amazon Rekognition Custom Labels returns a pagination token in the
--   response. You can use this pagination token to retrieve the next set
--   of results.
--   
--   
--   <a>$sel:projectVersionDescriptions:DescribeProjectVersionsResponse'</a>,
--   <a>describeProjectVersionsResponse_projectVersionDescriptions</a> - A
--   list of model descriptions. The list is sorted by the creation date
--   and time of the model versions, latest to earliest.
--   
--   <a>$sel:httpStatus:DescribeProjectVersionsResponse'</a>,
--   <a>describeProjectVersionsResponse_httpStatus</a> - The response's
--   http status code.
newDescribeProjectVersionsResponse :: Int -> DescribeProjectVersionsResponse

-- | <i>See:</i> <a>newRecognizeCelebrities</a> smart constructor.
data RecognizeCelebrities
RecognizeCelebrities' :: Image -> RecognizeCelebrities

-- | Create a value of <a>RecognizeCelebrities</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:image:RecognizeCelebrities'</a>,
--   <a>recognizeCelebrities_image</a> - The input image as base64-encoded
--   bytes or an S3 object. If you use the AWS CLI to call Amazon
--   Rekognition operations, passing base64-encoded image bytes is not
--   supported.
--   
--   If you are using an AWS SDK to call Amazon Rekognition, you might not
--   need to base64-encode image bytes passed using the <tt>Bytes</tt>
--   field. For more information, see Images in the Amazon Rekognition
--   developer guide.
newRecognizeCelebrities :: Image -> RecognizeCelebrities

-- | <i>See:</i> <a>newRecognizeCelebritiesResponse</a> smart constructor.
data RecognizeCelebritiesResponse
RecognizeCelebritiesResponse' :: Maybe [Celebrity] -> Maybe OrientationCorrection -> Maybe [ComparedFace] -> Int -> RecognizeCelebritiesResponse

-- | Create a value of <a>RecognizeCelebritiesResponse</a> with all
--   optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:celebrityFaces:RecognizeCelebritiesResponse'</a>,
--   <a>recognizeCelebritiesResponse_celebrityFaces</a> - Details about
--   each celebrity found in the image. Amazon Rekognition can detect a
--   maximum of 64 celebrities in an image. Each celebrity object includes
--   the following attributes: <tt>Face</tt>, <tt>Confidence</tt>,
--   <tt>Emotions</tt>, <tt>Landmarks</tt>, <tt>Pose</tt>,
--   <tt>Quality</tt>, <tt>Smile</tt>, <tt>Id</tt>, <tt>KnownGender</tt>,
--   <tt>MatchConfidence</tt>, <tt>Name</tt>, <tt>Urls</tt>.
--   
--   <a>$sel:orientationCorrection:RecognizeCelebritiesResponse'</a>,
--   <a>recognizeCelebritiesResponse_orientationCorrection</a> - Support
--   for estimating image orientation using the the OrientationCorrection
--   field has ceased as of August 2021. Any returned values for this field
--   included in an API response will always be NULL.
--   
--   The orientation of the input image (counterclockwise direction). If
--   your application displays the image, you can use this value to correct
--   the orientation. The bounding box coordinates returned in
--   <tt>CelebrityFaces</tt> and <tt>UnrecognizedFaces</tt> represent face
--   locations before the image orientation is corrected.
--   
--   If the input image is in .jpeg format, it might contain exchangeable
--   image (Exif) metadata that includes the image's orientation. If so,
--   and the Exif metadata for the input image populates the orientation
--   field, the value of <tt>OrientationCorrection</tt> is null. The
--   <tt>CelebrityFaces</tt> and <tt>UnrecognizedFaces</tt> bounding box
--   coordinates represent face locations after Exif metadata is used to
--   correct the image orientation. Images in .png format don't contain
--   Exif metadata.
--   
--   <a>$sel:unrecognizedFaces:RecognizeCelebritiesResponse'</a>,
--   <a>recognizeCelebritiesResponse_unrecognizedFaces</a> - Details about
--   each unrecognized face in the image.
--   
--   <a>$sel:httpStatus:RecognizeCelebritiesResponse'</a>,
--   <a>recognizeCelebritiesResponse_httpStatus</a> - The response's http
--   status code.
newRecognizeCelebritiesResponse :: Int -> RecognizeCelebritiesResponse

-- | <i>See:</i> <a>newDetectCustomLabels</a> smart constructor.
data DetectCustomLabels
DetectCustomLabels' :: Maybe Double -> Maybe Natural -> Text -> Image -> DetectCustomLabels

-- | Create a value of <a>DetectCustomLabels</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:minConfidence:DetectCustomLabels'</a>,
--   <a>detectCustomLabels_minConfidence</a> - Specifies the minimum
--   confidence level for the labels to return. <tt>DetectCustomLabels</tt>
--   doesn't return any labels with a confidence value that's lower than
--   this specified value. If you specify a value of 0,
--   <tt>DetectCustomLabels</tt> returns all labels, regardless of the
--   assumed threshold applied to each label. If you don't specify a value
--   for <tt>MinConfidence</tt>, <tt>DetectCustomLabels</tt> returns labels
--   based on the assumed threshold of each label.
--   
--   <a>$sel:maxResults:DetectCustomLabels'</a>,
--   <a>detectCustomLabels_maxResults</a> - Maximum number of results you
--   want the service to return in the response. The service returns the
--   specified number of highest confidence labels ranked from highest
--   confidence to lowest.
--   
--   <a>$sel:projectVersionArn:DetectCustomLabels'</a>,
--   <a>detectCustomLabels_projectVersionArn</a> - The ARN of the model
--   version that you want to use.
--   
--   <a>$sel:image:DetectCustomLabels'</a>, <a>detectCustomLabels_image</a>
--   - Undocumented member.
newDetectCustomLabels :: Text -> Image -> DetectCustomLabels

-- | <i>See:</i> <a>newDetectCustomLabelsResponse</a> smart constructor.
data DetectCustomLabelsResponse
DetectCustomLabelsResponse' :: Maybe [CustomLabel] -> Int -> DetectCustomLabelsResponse

-- | Create a value of <a>DetectCustomLabelsResponse</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:customLabels:DetectCustomLabelsResponse'</a>,
--   <a>detectCustomLabelsResponse_customLabels</a> - An array of custom
--   labels detected in the input image.
--   
--   <a>$sel:httpStatus:DetectCustomLabelsResponse'</a>,
--   <a>detectCustomLabelsResponse_httpStatus</a> - The response's http
--   status code.
newDetectCustomLabelsResponse :: Int -> DetectCustomLabelsResponse

-- | <i>See:</i> <a>newGetFaceSearch</a> smart constructor.
data GetFaceSearch
GetFaceSearch' :: Maybe Text -> Maybe Natural -> Maybe FaceSearchSortBy -> Text -> GetFaceSearch

-- | Create a value of <a>GetFaceSearch</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:nextToken:GetFaceSearch'</a>, <a>getFaceSearch_nextToken</a> -
--   If the previous response was incomplete (because there is more search
--   results to retrieve), Amazon Rekognition Video returns a pagination
--   token in the response. You can use this pagination token to retrieve
--   the next set of search results.
--   
--   <a>$sel:maxResults:GetFaceSearch'</a>, <a>getFaceSearch_maxResults</a>
--   - Maximum number of results to return per paginated call. The largest
--   value you can specify is 1000. If you specify a value greater than
--   1000, a maximum of 1000 results is returned. The default value is
--   1000.
--   
--   <a>$sel:sortBy:GetFaceSearch'</a>, <a>getFaceSearch_sortBy</a> - Sort
--   to use for grouping faces in the response. Use <tt>TIMESTAMP</tt> to
--   group faces by the time that they are recognized. Use <tt>INDEX</tt>
--   to sort by recognized faces.
--   
--   <a>$sel:jobId:GetFaceSearch'</a>, <a>getFaceSearch_jobId</a> - The job
--   identifer for the search request. You get the job identifier from an
--   initial call to <tt>StartFaceSearch</tt>.
newGetFaceSearch :: Text -> GetFaceSearch

-- | <i>See:</i> <a>newGetFaceSearchResponse</a> smart constructor.
data GetFaceSearchResponse
GetFaceSearchResponse' :: Maybe Text -> Maybe VideoMetadata -> Maybe Text -> Maybe VideoJobStatus -> Maybe [PersonMatch] -> Int -> GetFaceSearchResponse

-- | Create a value of <a>GetFaceSearchResponse</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:nextToken:GetFaceSearch'</a>,
--   <a>getFaceSearchResponse_nextToken</a> - If the response is truncated,
--   Amazon Rekognition Video returns this token that you can use in the
--   subsequent request to retrieve the next set of search results.
--   
--   <a>$sel:videoMetadata:GetFaceSearchResponse'</a>,
--   <a>getFaceSearchResponse_videoMetadata</a> - Information about a video
--   that Amazon Rekognition analyzed. <tt>Videometadata</tt> is returned
--   in every page of paginated responses from a Amazon Rekognition Video
--   operation.
--   
--   <a>$sel:statusMessage:GetFaceSearchResponse'</a>,
--   <a>getFaceSearchResponse_statusMessage</a> - If the job fails,
--   <tt>StatusMessage</tt> provides a descriptive error message.
--   
--   <a>$sel:jobStatus:GetFaceSearchResponse'</a>,
--   <a>getFaceSearchResponse_jobStatus</a> - The current status of the
--   face search job.
--   
--   <a>$sel:persons:GetFaceSearchResponse'</a>,
--   <a>getFaceSearchResponse_persons</a> - An array of persons,
--   PersonMatch, in the video whose face(s) match the face(s) in an Amazon
--   Rekognition collection. It also includes time information for when
--   persons are matched in the video. You specify the input collection in
--   an initial call to <tt>StartFaceSearch</tt>. Each <tt>Persons</tt>
--   element includes a time the person was matched, face match details
--   (<tt>FaceMatches</tt>) for matching faces in the collection, and
--   person information (<tt>Person</tt>) for the matched person.
--   
--   <a>$sel:httpStatus:GetFaceSearchResponse'</a>,
--   <a>getFaceSearchResponse_httpStatus</a> - The response's http status
--   code.
newGetFaceSearchResponse :: Int -> GetFaceSearchResponse

-- | <i>See:</i> <a>newStartLabelDetection</a> smart constructor.
data StartLabelDetection
StartLabelDetection' :: Maybe Text -> Maybe NotificationChannel -> Maybe Text -> Maybe Double -> Video -> StartLabelDetection

-- | Create a value of <a>StartLabelDetection</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:jobTag:StartLabelDetection'</a>,
--   <a>startLabelDetection_jobTag</a> - An identifier you specify that's
--   returned in the completion notification that's published to your
--   Amazon Simple Notification Service topic. For example, you can use
--   <tt>JobTag</tt> to group related jobs and identify them in the
--   completion notification.
--   
--   <a>$sel:notificationChannel:StartLabelDetection'</a>,
--   <a>startLabelDetection_notificationChannel</a> - The Amazon SNS topic
--   ARN you want Amazon Rekognition Video to publish the completion status
--   of the label detection operation to. The Amazon SNS topic must have a
--   topic name that begins with <i>AmazonRekognition</i> if you are using
--   the AmazonRekognitionServiceRole permissions policy.
--   
--   <a>$sel:clientRequestToken:StartLabelDetection'</a>,
--   <a>startLabelDetection_clientRequestToken</a> - Idempotent token used
--   to identify the start request. If you use the same token with multiple
--   <tt>StartLabelDetection</tt> requests, the same <tt>JobId</tt> is
--   returned. Use <tt>ClientRequestToken</tt> to prevent the same job from
--   being accidently started more than once.
--   
--   <a>$sel:minConfidence:StartLabelDetection'</a>,
--   <a>startLabelDetection_minConfidence</a> - Specifies the minimum
--   confidence that Amazon Rekognition Video must have in order to return
--   a detected label. Confidence represents how certain Amazon Rekognition
--   is that a label is correctly identified.0 is the lowest confidence.
--   100 is the highest confidence. Amazon Rekognition Video doesn't return
--   any labels with a confidence level lower than this specified value.
--   
--   If you don't specify <tt>MinConfidence</tt>, the operation returns
--   labels with confidence values greater than or equal to 50 percent.
--   
--   <a>$sel:video:StartLabelDetection'</a>,
--   <a>startLabelDetection_video</a> - The video in which you want to
--   detect labels. The video must be stored in an Amazon S3 bucket.
newStartLabelDetection :: Video -> StartLabelDetection

-- | <i>See:</i> <a>newStartLabelDetectionResponse</a> smart constructor.
data StartLabelDetectionResponse
StartLabelDetectionResponse' :: Maybe Text -> Int -> StartLabelDetectionResponse

-- | Create a value of <a>StartLabelDetectionResponse</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:jobId:StartLabelDetectionResponse'</a>,
--   <a>startLabelDetectionResponse_jobId</a> - The identifier for the
--   label detection job. Use <tt>JobId</tt> to identify the job in a
--   subsequent call to <tt>GetLabelDetection</tt>.
--   
--   <a>$sel:httpStatus:StartLabelDetectionResponse'</a>,
--   <a>startLabelDetectionResponse_httpStatus</a> - The response's http
--   status code.
newStartLabelDetectionResponse :: Int -> StartLabelDetectionResponse

-- | <i>See:</i> <a>newSearchFaces</a> smart constructor.
data SearchFaces
SearchFaces' :: Maybe Double -> Maybe Natural -> Text -> Text -> SearchFaces

-- | Create a value of <a>SearchFaces</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:faceMatchThreshold:SearchFaces'</a>,
--   <a>searchFaces_faceMatchThreshold</a> - Optional value specifying the
--   minimum confidence in the face match to return. For example, don't
--   return any matches where confidence in matches is less than 70%. The
--   default value is 80%.
--   
--   <a>$sel:maxFaces:SearchFaces'</a>, <a>searchFaces_maxFaces</a> -
--   Maximum number of faces to return. The operation returns the maximum
--   number of faces with the highest confidence in the match.
--   
--   <a>$sel:collectionId:SearchFaces'</a>, <a>searchFaces_collectionId</a>
--   - ID of the collection the face belongs to.
--   
--   <a>$sel:faceId:SearchFaces'</a>, <a>searchFaces_faceId</a> - ID of a
--   face to find matches for in the collection.
newSearchFaces :: Text -> Text -> SearchFaces

-- | <i>See:</i> <a>newSearchFacesResponse</a> smart constructor.
data SearchFacesResponse
SearchFacesResponse' :: Maybe [FaceMatch] -> Maybe Text -> Maybe Text -> Int -> SearchFacesResponse

-- | Create a value of <a>SearchFacesResponse</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:faceMatches:SearchFacesResponse'</a>,
--   <a>searchFacesResponse_faceMatches</a> - An array of faces that
--   matched the input face, along with the confidence in the match.
--   
--   <a>$sel:faceModelVersion:SearchFacesResponse'</a>,
--   <a>searchFacesResponse_faceModelVersion</a> - Version number of the
--   face detection model associated with the input collection
--   (<tt>CollectionId</tt>).
--   
--   <a>$sel:searchedFaceId:SearchFacesResponse'</a>,
--   <a>searchFacesResponse_searchedFaceId</a> - ID of the face that was
--   searched for matches in a collection.
--   
--   <a>$sel:httpStatus:SearchFacesResponse'</a>,
--   <a>searchFacesResponse_httpStatus</a> - The response's http status
--   code.
newSearchFacesResponse :: Int -> SearchFacesResponse

-- | <i>See:</i> <a>newIndexFaces</a> smart constructor.
data IndexFaces
IndexFaces' :: Maybe Text -> Maybe QualityFilter -> Maybe Natural -> Maybe [Attribute] -> Text -> Image -> IndexFaces

-- | Create a value of <a>IndexFaces</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:externalImageId:IndexFaces'</a>,
--   <a>indexFaces_externalImageId</a> - The ID you want to assign to all
--   the faces detected in the image.
--   
--   <a>$sel:qualityFilter:IndexFaces'</a>, <a>indexFaces_qualityFilter</a>
--   - A filter that specifies a quality bar for how much filtering is done
--   to identify faces. Filtered faces aren't indexed. If you specify
--   <tt>AUTO</tt>, Amazon Rekognition chooses the quality bar. If you
--   specify <tt>LOW</tt>, <tt>MEDIUM</tt>, or <tt>HIGH</tt>, filtering
--   removes all faces that don’t meet the chosen quality bar. The default
--   value is <tt>AUTO</tt>. The quality bar is based on a variety of
--   common use cases. Low-quality detections can occur for a number of
--   reasons. Some examples are an object that's misidentified as a face, a
--   face that's too blurry, or a face with a pose that's too extreme to
--   use. If you specify <tt>NONE</tt>, no filtering is performed.
--   
--   To use quality filtering, the collection you are using must be
--   associated with version 3 of the face model or higher.
--   
--   <a>$sel:maxFaces:IndexFaces'</a>, <a>indexFaces_maxFaces</a> - The
--   maximum number of faces to index. The value of <tt>MaxFaces</tt> must
--   be greater than or equal to 1. <tt>IndexFaces</tt> returns no more
--   than 100 detected faces in an image, even if you specify a larger
--   value for <tt>MaxFaces</tt>.
--   
--   If <tt>IndexFaces</tt> detects more faces than the value of
--   <tt>MaxFaces</tt>, the faces with the lowest quality are filtered out
--   first. If there are still more faces than the value of
--   <tt>MaxFaces</tt>, the faces with the smallest bounding boxes are
--   filtered out (up to the number that's needed to satisfy the value of
--   <tt>MaxFaces</tt>). Information about the unindexed faces is available
--   in the <tt>UnindexedFaces</tt> array.
--   
--   The faces that are returned by <tt>IndexFaces</tt> are sorted by the
--   largest face bounding box size to the smallest size, in descending
--   order.
--   
--   <tt>MaxFaces</tt> can be used with a collection associated with any
--   version of the face model.
--   
--   <a>$sel:detectionAttributes:IndexFaces'</a>,
--   <a>indexFaces_detectionAttributes</a> - An array of facial attributes
--   that you want to be returned. This can be the default list of
--   attributes or all attributes. If you don't specify a value for
--   <tt>Attributes</tt> or if you specify <tt>["DEFAULT"]</tt>, the API
--   returns the following subset of facial attributes:
--   <tt>BoundingBox</tt>, <tt>Confidence</tt>, <tt>Pose</tt>,
--   <tt>Quality</tt>, and <tt>Landmarks</tt>. If you provide
--   <tt>["ALL"]</tt>, all facial attributes are returned, but the
--   operation takes longer to complete.
--   
--   If you provide both, <tt>["ALL", "DEFAULT"]</tt>, the service uses a
--   logical AND operator to determine which attributes to return (in this
--   case, all attributes).
--   
--   <a>$sel:collectionId:IndexFaces'</a>, <a>indexFaces_collectionId</a> -
--   The ID of an existing collection to which you want to add the faces
--   that are detected in the input images.
--   
--   <a>$sel:image:IndexFaces'</a>, <a>indexFaces_image</a> - The input
--   image as base64-encoded bytes or an S3 object. If you use the AWS CLI
--   to call Amazon Rekognition operations, passing base64-encoded image
--   bytes isn't supported.
--   
--   If you are using an AWS SDK to call Amazon Rekognition, you might not
--   need to base64-encode image bytes passed using the <tt>Bytes</tt>
--   field. For more information, see Images in the Amazon Rekognition
--   developer guide.
newIndexFaces :: Text -> Image -> IndexFaces

-- | <i>See:</i> <a>newIndexFacesResponse</a> smart constructor.
data IndexFacesResponse
IndexFacesResponse' :: Maybe Text -> Maybe [FaceRecord] -> Maybe OrientationCorrection -> Maybe [UnindexedFace] -> Int -> IndexFacesResponse

-- | Create a value of <a>IndexFacesResponse</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:faceModelVersion:IndexFacesResponse'</a>,
--   <a>indexFacesResponse_faceModelVersion</a> - The version number of the
--   face detection model that's associated with the input collection
--   (<tt>CollectionId</tt>).
--   
--   <a>$sel:faceRecords:IndexFacesResponse'</a>,
--   <a>indexFacesResponse_faceRecords</a> - An array of faces detected and
--   added to the collection. For more information, see Searching Faces in
--   a Collection in the Amazon Rekognition Developer Guide.
--   
--   <a>$sel:orientationCorrection:IndexFacesResponse'</a>,
--   <a>indexFacesResponse_orientationCorrection</a> - If your collection
--   is associated with a face detection model that's later than version
--   3.0, the value of <tt>OrientationCorrection</tt> is always null and no
--   orientation information is returned.
--   
--   If your collection is associated with a face detection model that's
--   version 3.0 or earlier, the following applies:
--   
--   <ul>
--   <li>If the input image is in .jpeg format, it might contain
--   exchangeable image file format (Exif) metadata that includes the
--   image's orientation. Amazon Rekognition uses this orientation
--   information to perform image correction - the bounding box coordinates
--   are translated to represent object locations after the orientation
--   information in the Exif metadata is used to correct the image
--   orientation. Images in .png format don't contain Exif metadata. The
--   value of <tt>OrientationCorrection</tt> is null.</li>
--   <li>If the image doesn't contain orientation information in its Exif
--   metadata, Amazon Rekognition returns an estimated orientation
--   (ROTATE_0, ROTATE_90, ROTATE_180, ROTATE_270). Amazon Rekognition
--   doesn’t perform image correction for images. The bounding box
--   coordinates aren't translated and represent the object locations
--   before the image is rotated.</li>
--   </ul>
--   
--   Bounding box information is returned in the <tt>FaceRecords</tt>
--   array. You can get the version of the face detection model by calling
--   DescribeCollection.
--   
--   <a>$sel:unindexedFaces:IndexFacesResponse'</a>,
--   <a>indexFacesResponse_unindexedFaces</a> - An array of faces that were
--   detected in the image but weren't indexed. They weren't indexed
--   because the quality filter identified them as low quality, or the
--   <tt>MaxFaces</tt> request parameter filtered them out. To use the
--   quality filter, you specify the <tt>QualityFilter</tt> request
--   parameter.
--   
--   <a>$sel:httpStatus:IndexFacesResponse'</a>,
--   <a>indexFacesResponse_httpStatus</a> - The response's http status
--   code.
newIndexFacesResponse :: Int -> IndexFacesResponse

-- | <i>See:</i> <a>newGetLabelDetection</a> smart constructor.
data GetLabelDetection
GetLabelDetection' :: Maybe Text -> Maybe Natural -> Maybe LabelDetectionSortBy -> Text -> GetLabelDetection

-- | Create a value of <a>GetLabelDetection</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:nextToken:GetLabelDetection'</a>,
--   <a>getLabelDetection_nextToken</a> - If the previous response was
--   incomplete (because there are more labels to retrieve), Amazon
--   Rekognition Video returns a pagination token in the response. You can
--   use this pagination token to retrieve the next set of labels.
--   
--   <a>$sel:maxResults:GetLabelDetection'</a>,
--   <a>getLabelDetection_maxResults</a> - Maximum number of results to
--   return per paginated call. The largest value you can specify is 1000.
--   If you specify a value greater than 1000, a maximum of 1000 results is
--   returned. The default value is 1000.
--   
--   <a>$sel:sortBy:GetLabelDetection'</a>, <a>getLabelDetection_sortBy</a>
--   - Sort to use for elements in the <tt>Labels</tt> array. Use
--   <tt>TIMESTAMP</tt> to sort array elements by the time labels are
--   detected. Use <tt>NAME</tt> to alphabetically group elements for a
--   label together. Within each label group, the array element are sorted
--   by detection confidence. The default sort is by <tt>TIMESTAMP</tt>.
--   
--   <a>$sel:jobId:GetLabelDetection'</a>, <a>getLabelDetection_jobId</a> -
--   Job identifier for the label detection operation for which you want
--   results returned. You get the job identifer from an initial call to
--   <tt>StartlabelDetection</tt>.
newGetLabelDetection :: Text -> GetLabelDetection

-- | <i>See:</i> <a>newGetLabelDetectionResponse</a> smart constructor.
data GetLabelDetectionResponse
GetLabelDetectionResponse' :: Maybe Text -> Maybe VideoMetadata -> Maybe Text -> Maybe [LabelDetection] -> Maybe VideoJobStatus -> Maybe Text -> Int -> GetLabelDetectionResponse

-- | Create a value of <a>GetLabelDetectionResponse</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:nextToken:GetLabelDetection'</a>,
--   <a>getLabelDetectionResponse_nextToken</a> - If the response is
--   truncated, Amazon Rekognition Video returns this token that you can
--   use in the subsequent request to retrieve the next set of labels.
--   
--   <a>$sel:videoMetadata:GetLabelDetectionResponse'</a>,
--   <a>getLabelDetectionResponse_videoMetadata</a> - Information about a
--   video that Amazon Rekognition Video analyzed. <tt>Videometadata</tt>
--   is returned in every page of paginated responses from a Amazon
--   Rekognition video operation.
--   
--   <a>$sel:statusMessage:GetLabelDetectionResponse'</a>,
--   <a>getLabelDetectionResponse_statusMessage</a> - If the job fails,
--   <tt>StatusMessage</tt> provides a descriptive error message.
--   
--   <a>$sel:labels:GetLabelDetectionResponse'</a>,
--   <a>getLabelDetectionResponse_labels</a> - An array of labels detected
--   in the video. Each element contains the detected label and the time,
--   in milliseconds from the start of the video, that the label was
--   detected.
--   
--   <a>$sel:jobStatus:GetLabelDetectionResponse'</a>,
--   <a>getLabelDetectionResponse_jobStatus</a> - The current status of the
--   label detection job.
--   
--   <a>$sel:labelModelVersion:GetLabelDetectionResponse'</a>,
--   <a>getLabelDetectionResponse_labelModelVersion</a> - Version number of
--   the label detection model that was used to detect labels.
--   
--   <a>$sel:httpStatus:GetLabelDetectionResponse'</a>,
--   <a>getLabelDetectionResponse_httpStatus</a> - The response's http
--   status code.
newGetLabelDetectionResponse :: Int -> GetLabelDetectionResponse

-- | <i>See:</i> <a>newStopProjectVersion</a> smart constructor.
data StopProjectVersion
StopProjectVersion' :: Text -> StopProjectVersion

-- | Create a value of <a>StopProjectVersion</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:projectVersionArn:StopProjectVersion'</a>,
--   <a>stopProjectVersion_projectVersionArn</a> - The Amazon Resource Name
--   (ARN) of the model version that you want to delete.
--   
--   This operation requires permissions to perform the
--   <tt>rekognition:StopProjectVersion</tt> action.
newStopProjectVersion :: Text -> StopProjectVersion

-- | <i>See:</i> <a>newStopProjectVersionResponse</a> smart constructor.
data StopProjectVersionResponse
StopProjectVersionResponse' :: Maybe ProjectVersionStatus -> Int -> StopProjectVersionResponse

-- | Create a value of <a>StopProjectVersionResponse</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:status:StopProjectVersionResponse'</a>,
--   <a>stopProjectVersionResponse_status</a> - The current status of the
--   stop operation.
--   
--   <a>$sel:httpStatus:StopProjectVersionResponse'</a>,
--   <a>stopProjectVersionResponse_httpStatus</a> - The response's http
--   status code.
newStopProjectVersionResponse :: Int -> StopProjectVersionResponse

-- | <i>See:</i> <a>newDescribeStreamProcessor</a> smart constructor.
data DescribeStreamProcessor
DescribeStreamProcessor' :: Text -> DescribeStreamProcessor

-- | Create a value of <a>DescribeStreamProcessor</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:name:DescribeStreamProcessor'</a>,
--   <a>describeStreamProcessor_name</a> - Name of the stream processor for
--   which you want information.
newDescribeStreamProcessor :: Text -> DescribeStreamProcessor

-- | <i>See:</i> <a>newDescribeStreamProcessorResponse</a> smart
--   constructor.
data DescribeStreamProcessorResponse
DescribeStreamProcessorResponse' :: Maybe StreamProcessorStatus -> Maybe StreamProcessorSettings -> Maybe StreamProcessorInput -> Maybe StreamProcessorOutput -> Maybe Text -> Maybe Text -> Maybe Text -> Maybe POSIX -> Maybe POSIX -> Maybe Text -> Int -> DescribeStreamProcessorResponse

-- | Create a value of <a>DescribeStreamProcessorResponse</a> with all
--   optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:status:DescribeStreamProcessorResponse'</a>,
--   <a>describeStreamProcessorResponse_status</a> - Current status of the
--   stream processor.
--   
--   <a>$sel:settings:DescribeStreamProcessorResponse'</a>,
--   <a>describeStreamProcessorResponse_settings</a> - Face recognition
--   input parameters that are being used by the stream processor. Includes
--   the collection to use for face recognition and the face attributes to
--   detect.
--   
--   <a>$sel:input:DescribeStreamProcessorResponse'</a>,
--   <a>describeStreamProcessorResponse_input</a> - Kinesis video stream
--   that provides the source streaming video.
--   
--   <a>$sel:output:DescribeStreamProcessorResponse'</a>,
--   <a>describeStreamProcessorResponse_output</a> - Kinesis data stream to
--   which Amazon Rekognition Video puts the analysis results.
--   
--   <a>$sel:streamProcessorArn:DescribeStreamProcessorResponse'</a>,
--   <a>describeStreamProcessorResponse_streamProcessorArn</a> - ARN of the
--   stream processor.
--   
--   <a>$sel:statusMessage:DescribeStreamProcessorResponse'</a>,
--   <a>describeStreamProcessorResponse_statusMessage</a> - Detailed status
--   message about the stream processor.
--   
--   <a>$sel:name:DescribeStreamProcessor'</a>,
--   <a>describeStreamProcessorResponse_name</a> - Name of the stream
--   processor.
--   
--   <a>$sel:creationTimestamp:DescribeStreamProcessorResponse'</a>,
--   <a>describeStreamProcessorResponse_creationTimestamp</a> - Date and
--   time the stream processor was created
--   
--   <a>$sel:lastUpdateTimestamp:DescribeStreamProcessorResponse'</a>,
--   <a>describeStreamProcessorResponse_lastUpdateTimestamp</a> - The time,
--   in Unix format, the stream processor was last updated. For example,
--   when the stream processor moves from a running state to a failed
--   state, or when the user starts or stops the stream processor.
--   
--   <a>$sel:roleArn:DescribeStreamProcessorResponse'</a>,
--   <a>describeStreamProcessorResponse_roleArn</a> - ARN of the IAM role
--   that allows access to the stream processor.
--   
--   <a>$sel:httpStatus:DescribeStreamProcessorResponse'</a>,
--   <a>describeStreamProcessorResponse_httpStatus</a> - The response's
--   http status code.
newDescribeStreamProcessorResponse :: Int -> DescribeStreamProcessorResponse

-- | <i>See:</i> <a>newStartFaceSearch</a> smart constructor.
data StartFaceSearch
StartFaceSearch' :: Maybe Double -> Maybe Text -> Maybe NotificationChannel -> Maybe Text -> Video -> Text -> StartFaceSearch

-- | Create a value of <a>StartFaceSearch</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:faceMatchThreshold:StartFaceSearch'</a>,
--   <a>startFaceSearch_faceMatchThreshold</a> - The minimum confidence in
--   the person match to return. For example, don't return any matches
--   where confidence in matches is less than 70%. The default value is
--   80%.
--   
--   <a>$sel:jobTag:StartFaceSearch'</a>, <a>startFaceSearch_jobTag</a> -
--   An identifier you specify that's returned in the completion
--   notification that's published to your Amazon Simple Notification
--   Service topic. For example, you can use <tt>JobTag</tt> to group
--   related jobs and identify them in the completion notification.
--   
--   <a>$sel:notificationChannel:StartFaceSearch'</a>,
--   <a>startFaceSearch_notificationChannel</a> - The ARN of the Amazon SNS
--   topic to which you want Amazon Rekognition Video to publish the
--   completion status of the search. The Amazon SNS topic must have a
--   topic name that begins with <i>AmazonRekognition</i> if you are using
--   the AmazonRekognitionServiceRole permissions policy to access the
--   topic.
--   
--   <a>$sel:clientRequestToken:StartFaceSearch'</a>,
--   <a>startFaceSearch_clientRequestToken</a> - Idempotent token used to
--   identify the start request. If you use the same token with multiple
--   <tt>StartFaceSearch</tt> requests, the same <tt>JobId</tt> is
--   returned. Use <tt>ClientRequestToken</tt> to prevent the same job from
--   being accidently started more than once.
--   
--   <a>$sel:video:StartFaceSearch'</a>, <a>startFaceSearch_video</a> - The
--   video you want to search. The video must be stored in an Amazon S3
--   bucket.
--   
--   <a>$sel:collectionId:StartFaceSearch'</a>,
--   <a>startFaceSearch_collectionId</a> - ID of the collection that
--   contains the faces you want to search for.
newStartFaceSearch :: Video -> Text -> StartFaceSearch

-- | <i>See:</i> <a>newStartFaceSearchResponse</a> smart constructor.
data StartFaceSearchResponse
StartFaceSearchResponse' :: Maybe Text -> Int -> StartFaceSearchResponse

-- | Create a value of <a>StartFaceSearchResponse</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:jobId:StartFaceSearchResponse'</a>,
--   <a>startFaceSearchResponse_jobId</a> - The identifier for the search
--   job. Use <tt>JobId</tt> to identify the job in a subsequent call to
--   <tt>GetFaceSearch</tt>.
--   
--   <a>$sel:httpStatus:StartFaceSearchResponse'</a>,
--   <a>startFaceSearchResponse_httpStatus</a> - The response's http status
--   code.
newStartFaceSearchResponse :: Int -> StartFaceSearchResponse

-- | <i>See:</i> <a>newStartTextDetection</a> smart constructor.
data StartTextDetection
StartTextDetection' :: Maybe Text -> Maybe StartTextDetectionFilters -> Maybe NotificationChannel -> Maybe Text -> Video -> StartTextDetection

-- | Create a value of <a>StartTextDetection</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:jobTag:StartTextDetection'</a>,
--   <a>startTextDetection_jobTag</a> - An identifier returned in the
--   completion status published by your Amazon Simple Notification Service
--   topic. For example, you can use <tt>JobTag</tt> to group related jobs
--   and identify them in the completion notification.
--   
--   <a>$sel:filters:StartTextDetection'</a>,
--   <a>startTextDetection_filters</a> - Optional parameters that let you
--   set criteria the text must meet to be included in your response.
--   
--   <a>$sel:notificationChannel:StartTextDetection'</a>,
--   <a>startTextDetection_notificationChannel</a> - Undocumented member.
--   
--   <a>$sel:clientRequestToken:StartTextDetection'</a>,
--   <a>startTextDetection_clientRequestToken</a> - Idempotent token used
--   to identify the start request. If you use the same token with multiple
--   <tt>StartTextDetection</tt> requests, the same <tt>JobId</tt> is
--   returned. Use <tt>ClientRequestToken</tt> to prevent the same job from
--   being accidentaly started more than once.
--   
--   <a>$sel:video:StartTextDetection'</a>, <a>startTextDetection_video</a>
--   - Undocumented member.
newStartTextDetection :: Video -> StartTextDetection

-- | <i>See:</i> <a>newStartTextDetectionResponse</a> smart constructor.
data StartTextDetectionResponse
StartTextDetectionResponse' :: Maybe Text -> Int -> StartTextDetectionResponse

-- | Create a value of <a>StartTextDetectionResponse</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:jobId:StartTextDetectionResponse'</a>,
--   <a>startTextDetectionResponse_jobId</a> - Identifier for the text
--   detection job. Use <tt>JobId</tt> to identify the job in a subsequent
--   call to <tt>GetTextDetection</tt>.
--   
--   <a>$sel:httpStatus:StartTextDetectionResponse'</a>,
--   <a>startTextDetectionResponse_httpStatus</a> - The response's http
--   status code.
newStartTextDetectionResponse :: Int -> StartTextDetectionResponse

-- | <i>See:</i> <a>newStartPersonTracking</a> smart constructor.
data StartPersonTracking
StartPersonTracking' :: Maybe Text -> Maybe NotificationChannel -> Maybe Text -> Video -> StartPersonTracking

-- | Create a value of <a>StartPersonTracking</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:jobTag:StartPersonTracking'</a>,
--   <a>startPersonTracking_jobTag</a> - An identifier you specify that's
--   returned in the completion notification that's published to your
--   Amazon Simple Notification Service topic. For example, you can use
--   <tt>JobTag</tt> to group related jobs and identify them in the
--   completion notification.
--   
--   <a>$sel:notificationChannel:StartPersonTracking'</a>,
--   <a>startPersonTracking_notificationChannel</a> - The Amazon SNS topic
--   ARN you want Amazon Rekognition Video to publish the completion status
--   of the people detection operation to. The Amazon SNS topic must have a
--   topic name that begins with <i>AmazonRekognition</i> if you are using
--   the AmazonRekognitionServiceRole permissions policy.
--   
--   <a>$sel:clientRequestToken:StartPersonTracking'</a>,
--   <a>startPersonTracking_clientRequestToken</a> - Idempotent token used
--   to identify the start request. If you use the same token with multiple
--   <tt>StartPersonTracking</tt> requests, the same <tt>JobId</tt> is
--   returned. Use <tt>ClientRequestToken</tt> to prevent the same job from
--   being accidently started more than once.
--   
--   <a>$sel:video:StartPersonTracking'</a>,
--   <a>startPersonTracking_video</a> - The video in which you want to
--   detect people. The video must be stored in an Amazon S3 bucket.
newStartPersonTracking :: Video -> StartPersonTracking

-- | <i>See:</i> <a>newStartPersonTrackingResponse</a> smart constructor.
data StartPersonTrackingResponse
StartPersonTrackingResponse' :: Maybe Text -> Int -> StartPersonTrackingResponse

-- | Create a value of <a>StartPersonTrackingResponse</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:jobId:StartPersonTrackingResponse'</a>,
--   <a>startPersonTrackingResponse_jobId</a> - The identifier for the
--   person detection job. Use <tt>JobId</tt> to identify the job in a
--   subsequent call to <tt>GetPersonTracking</tt>.
--   
--   <a>$sel:httpStatus:StartPersonTrackingResponse'</a>,
--   <a>startPersonTrackingResponse_httpStatus</a> - The response's http
--   status code.
newStartPersonTrackingResponse :: Int -> StartPersonTrackingResponse

-- | <i>See:</i> <a>newGetCelebrityRecognition</a> smart constructor.
data GetCelebrityRecognition
GetCelebrityRecognition' :: Maybe Text -> Maybe Natural -> Maybe CelebrityRecognitionSortBy -> Text -> GetCelebrityRecognition

-- | Create a value of <a>GetCelebrityRecognition</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:nextToken:GetCelebrityRecognition'</a>,
--   <a>getCelebrityRecognition_nextToken</a> - If the previous response
--   was incomplete (because there is more recognized celebrities to
--   retrieve), Amazon Rekognition Video returns a pagination token in the
--   response. You can use this pagination token to retrieve the next set
--   of celebrities.
--   
--   <a>$sel:maxResults:GetCelebrityRecognition'</a>,
--   <a>getCelebrityRecognition_maxResults</a> - Maximum number of results
--   to return per paginated call. The largest value you can specify is
--   1000. If you specify a value greater than 1000, a maximum of 1000
--   results is returned. The default value is 1000.
--   
--   <a>$sel:sortBy:GetCelebrityRecognition'</a>,
--   <a>getCelebrityRecognition_sortBy</a> - Sort to use for celebrities
--   returned in <tt>Celebrities</tt> field. Specify <tt>ID</tt> to sort by
--   the celebrity identifier, specify <tt>TIMESTAMP</tt> to sort by the
--   time the celebrity was recognized.
--   
--   <a>$sel:jobId:GetCelebrityRecognition'</a>,
--   <a>getCelebrityRecognition_jobId</a> - Job identifier for the required
--   celebrity recognition analysis. You can get the job identifer from a
--   call to <tt>StartCelebrityRecognition</tt>.
newGetCelebrityRecognition :: Text -> GetCelebrityRecognition

-- | <i>See:</i> <a>newGetCelebrityRecognitionResponse</a> smart
--   constructor.
data GetCelebrityRecognitionResponse
GetCelebrityRecognitionResponse' :: Maybe Text -> Maybe VideoMetadata -> Maybe Text -> Maybe [CelebrityRecognition] -> Maybe VideoJobStatus -> Int -> GetCelebrityRecognitionResponse

-- | Create a value of <a>GetCelebrityRecognitionResponse</a> with all
--   optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:nextToken:GetCelebrityRecognition'</a>,
--   <a>getCelebrityRecognitionResponse_nextToken</a> - If the response is
--   truncated, Amazon Rekognition Video returns this token that you can
--   use in the subsequent request to retrieve the next set of celebrities.
--   
--   <a>$sel:videoMetadata:GetCelebrityRecognitionResponse'</a>,
--   <a>getCelebrityRecognitionResponse_videoMetadata</a> - Information
--   about a video that Amazon Rekognition Video analyzed.
--   <tt>Videometadata</tt> is returned in every page of paginated
--   responses from a Amazon Rekognition Video operation.
--   
--   <a>$sel:statusMessage:GetCelebrityRecognitionResponse'</a>,
--   <a>getCelebrityRecognitionResponse_statusMessage</a> - If the job
--   fails, <tt>StatusMessage</tt> provides a descriptive error message.
--   
--   <a>$sel:celebrities:GetCelebrityRecognitionResponse'</a>,
--   <a>getCelebrityRecognitionResponse_celebrities</a> - Array of
--   celebrities recognized in the video.
--   
--   <a>$sel:jobStatus:GetCelebrityRecognitionResponse'</a>,
--   <a>getCelebrityRecognitionResponse_jobStatus</a> - The current status
--   of the celebrity recognition job.
--   
--   <a>$sel:httpStatus:GetCelebrityRecognitionResponse'</a>,
--   <a>getCelebrityRecognitionResponse_httpStatus</a> - The response's
--   http status code.
newGetCelebrityRecognitionResponse :: Int -> GetCelebrityRecognitionResponse

-- | <i>See:</i> <a>newStartStreamProcessor</a> smart constructor.
data StartStreamProcessor
StartStreamProcessor' :: Text -> StartStreamProcessor

-- | Create a value of <a>StartStreamProcessor</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:name:StartStreamProcessor'</a>,
--   <a>startStreamProcessor_name</a> - The name of the stream processor to
--   start processing.
newStartStreamProcessor :: Text -> StartStreamProcessor

-- | <i>See:</i> <a>newStartStreamProcessorResponse</a> smart constructor.
data StartStreamProcessorResponse
StartStreamProcessorResponse' :: Int -> StartStreamProcessorResponse

-- | Create a value of <a>StartStreamProcessorResponse</a> with all
--   optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:httpStatus:StartStreamProcessorResponse'</a>,
--   <a>startStreamProcessorResponse_httpStatus</a> - The response's http
--   status code.
newStartStreamProcessorResponse :: Int -> StartStreamProcessorResponse

-- | <i>See:</i> <a>newDetectText</a> smart constructor.
data DetectText
DetectText' :: Maybe DetectTextFilters -> Image -> DetectText

-- | Create a value of <a>DetectText</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:filters:DetectText'</a>, <a>detectText_filters</a> - Optional
--   parameters that let you set the criteria that the text must meet to be
--   included in your response.
--   
--   <a>$sel:image:DetectText'</a>, <a>detectText_image</a> - The input
--   image as base64-encoded bytes or an Amazon S3 object. If you use the
--   AWS CLI to call Amazon Rekognition operations, you can't pass image
--   bytes.
--   
--   If you are using an AWS SDK to call Amazon Rekognition, you might not
--   need to base64-encode image bytes passed using the <tt>Bytes</tt>
--   field. For more information, see Images in the Amazon Rekognition
--   developer guide.
newDetectText :: Image -> DetectText

-- | <i>See:</i> <a>newDetectTextResponse</a> smart constructor.
data DetectTextResponse
DetectTextResponse' :: Maybe [TextDetection] -> Maybe Text -> Int -> DetectTextResponse

-- | Create a value of <a>DetectTextResponse</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:textDetections:DetectTextResponse'</a>,
--   <a>detectTextResponse_textDetections</a> - An array of text that was
--   detected in the input image.
--   
--   <a>$sel:textModelVersion:DetectTextResponse'</a>,
--   <a>detectTextResponse_textModelVersion</a> - The model version used to
--   detect text.
--   
--   <a>$sel:httpStatus:DetectTextResponse'</a>,
--   <a>detectTextResponse_httpStatus</a> - The response's http status
--   code.
newDetectTextResponse :: Int -> DetectTextResponse

-- | <i>See:</i> <a>newGetSegmentDetection</a> smart constructor.
data GetSegmentDetection
GetSegmentDetection' :: Maybe Text -> Maybe Natural -> Text -> GetSegmentDetection

-- | Create a value of <a>GetSegmentDetection</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:nextToken:GetSegmentDetection'</a>,
--   <a>getSegmentDetection_nextToken</a> - If the response is truncated,
--   Amazon Rekognition Video returns this token that you can use in the
--   subsequent request to retrieve the next set of text.
--   
--   <a>$sel:maxResults:GetSegmentDetection'</a>,
--   <a>getSegmentDetection_maxResults</a> - Maximum number of results to
--   return per paginated call. The largest value you can specify is 1000.
--   
--   <a>$sel:jobId:GetSegmentDetection'</a>,
--   <a>getSegmentDetection_jobId</a> - Job identifier for the text
--   detection operation for which you want results returned. You get the
--   job identifer from an initial call to <tt>StartSegmentDetection</tt>.
newGetSegmentDetection :: Text -> GetSegmentDetection

-- | <i>See:</i> <a>newGetSegmentDetectionResponse</a> smart constructor.
data GetSegmentDetectionResponse
GetSegmentDetectionResponse' :: Maybe [SegmentTypeInfo] -> Maybe Text -> Maybe [VideoMetadata] -> Maybe Text -> Maybe [SegmentDetection] -> Maybe VideoJobStatus -> Maybe [AudioMetadata] -> Int -> GetSegmentDetectionResponse

-- | Create a value of <a>GetSegmentDetectionResponse</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:selectedSegmentTypes:GetSegmentDetectionResponse'</a>,
--   <a>getSegmentDetectionResponse_selectedSegmentTypes</a> - An array
--   containing the segment types requested in the call to
--   <tt>StartSegmentDetection</tt>.
--   
--   <a>$sel:nextToken:GetSegmentDetection'</a>,
--   <a>getSegmentDetectionResponse_nextToken</a> - If the previous
--   response was incomplete (because there are more labels to retrieve),
--   Amazon Rekognition Video returns a pagination token in the response.
--   You can use this pagination token to retrieve the next set of text.
--   
--   <a>$sel:videoMetadata:GetSegmentDetectionResponse'</a>,
--   <a>getSegmentDetectionResponse_videoMetadata</a> - Currently, Amazon
--   Rekognition Video returns a single object in the
--   <tt>VideoMetadata</tt> array. The object contains information about
--   the video stream in the input file that Amazon Rekognition Video chose
--   to analyze. The <tt>VideoMetadata</tt> object includes the video
--   codec, video format and other information. Video metadata is returned
--   in each page of information returned by <tt>GetSegmentDetection</tt>.
--   
--   <a>$sel:statusMessage:GetSegmentDetectionResponse'</a>,
--   <a>getSegmentDetectionResponse_statusMessage</a> - If the job fails,
--   <tt>StatusMessage</tt> provides a descriptive error message.
--   
--   <a>$sel:segments:GetSegmentDetectionResponse'</a>,
--   <a>getSegmentDetectionResponse_segments</a> - An array of segments
--   detected in a video. The array is sorted by the segment types
--   (TECHNICAL_CUE or SHOT) specified in the <tt>SegmentTypes</tt> input
--   parameter of <tt>StartSegmentDetection</tt>. Within each segment type
--   the array is sorted by timestamp values.
--   
--   <a>$sel:jobStatus:GetSegmentDetectionResponse'</a>,
--   <a>getSegmentDetectionResponse_jobStatus</a> - Current status of the
--   segment detection job.
--   
--   <a>$sel:audioMetadata:GetSegmentDetectionResponse'</a>,
--   <a>getSegmentDetectionResponse_audioMetadata</a> - An array of
--   objects. There can be multiple audio streams. Each
--   <tt>AudioMetadata</tt> object contains metadata for a single audio
--   stream. Audio information in an <tt>AudioMetadata</tt> objects
--   includes the audio codec, the number of audio channels, the duration
--   of the audio stream, and the sample rate. Audio metadata is returned
--   in each page of information returned by <tt>GetSegmentDetection</tt>.
--   
--   <a>$sel:httpStatus:GetSegmentDetectionResponse'</a>,
--   <a>getSegmentDetectionResponse_httpStatus</a> - The response's http
--   status code.
newGetSegmentDetectionResponse :: Int -> GetSegmentDetectionResponse

-- | <i>See:</i> <a>newCompareFaces</a> smart constructor.
data CompareFaces
CompareFaces' :: Maybe QualityFilter -> Maybe Double -> Image -> Image -> CompareFaces

-- | Create a value of <a>CompareFaces</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:qualityFilter:CompareFaces'</a>,
--   <a>compareFaces_qualityFilter</a> - A filter that specifies a quality
--   bar for how much filtering is done to identify faces. Filtered faces
--   aren't compared. If you specify <tt>AUTO</tt>, Amazon Rekognition
--   chooses the quality bar. If you specify <tt>LOW</tt>, <tt>MEDIUM</tt>,
--   or <tt>HIGH</tt>, filtering removes all faces that don’t meet the
--   chosen quality bar. The quality bar is based on a variety of common
--   use cases. Low-quality detections can occur for a number of reasons.
--   Some examples are an object that's misidentified as a face, a face
--   that's too blurry, or a face with a pose that's too extreme to use. If
--   you specify <tt>NONE</tt>, no filtering is performed. The default
--   value is <tt>NONE</tt>.
--   
--   To use quality filtering, the collection you are using must be
--   associated with version 3 of the face model or higher.
--   
--   <a>$sel:similarityThreshold:CompareFaces'</a>,
--   <a>compareFaces_similarityThreshold</a> - The minimum level of
--   confidence in the face matches that a match must meet to be included
--   in the <tt>FaceMatches</tt> array.
--   
--   <a>$sel:sourceImage:CompareFaces'</a>, <a>compareFaces_sourceImage</a>
--   - The input image as base64-encoded bytes or an S3 object. If you use
--   the AWS CLI to call Amazon Rekognition operations, passing
--   base64-encoded image bytes is not supported.
--   
--   If you are using an AWS SDK to call Amazon Rekognition, you might not
--   need to base64-encode image bytes passed using the <tt>Bytes</tt>
--   field. For more information, see Images in the Amazon Rekognition
--   developer guide.
--   
--   <a>$sel:targetImage:CompareFaces'</a>, <a>compareFaces_targetImage</a>
--   - The target image as base64-encoded bytes or an S3 object. If you use
--   the AWS CLI to call Amazon Rekognition operations, passing
--   base64-encoded image bytes is not supported.
--   
--   If you are using an AWS SDK to call Amazon Rekognition, you might not
--   need to base64-encode image bytes passed using the <tt>Bytes</tt>
--   field. For more information, see Images in the Amazon Rekognition
--   developer guide.
newCompareFaces :: Image -> Image -> CompareFaces

-- | <i>See:</i> <a>newCompareFacesResponse</a> smart constructor.
data CompareFacesResponse
CompareFacesResponse' :: Maybe [CompareFacesMatch] -> Maybe [ComparedFace] -> Maybe OrientationCorrection -> Maybe OrientationCorrection -> Maybe ComparedSourceImageFace -> Int -> CompareFacesResponse

-- | Create a value of <a>CompareFacesResponse</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:faceMatches:CompareFacesResponse'</a>,
--   <a>compareFacesResponse_faceMatches</a> - An array of faces in the
--   target image that match the source image face. Each
--   <tt>CompareFacesMatch</tt> object provides the bounding box, the
--   confidence level that the bounding box contains a face, and the
--   similarity score for the face in the bounding box and the face in the
--   source image.
--   
--   <a>$sel:unmatchedFaces:CompareFacesResponse'</a>,
--   <a>compareFacesResponse_unmatchedFaces</a> - An array of faces in the
--   target image that did not match the source image face.
--   
--   <a>$sel:targetImageOrientationCorrection:CompareFacesResponse'</a>,
--   <a>compareFacesResponse_targetImageOrientationCorrection</a> - The
--   value of <tt>TargetImageOrientationCorrection</tt> is always null.
--   
--   If the input image is in .jpeg format, it might contain exchangeable
--   image file format (Exif) metadata that includes the image's
--   orientation. Amazon Rekognition uses this orientation information to
--   perform image correction. The bounding box coordinates are translated
--   to represent object locations after the orientation information in the
--   Exif metadata is used to correct the image orientation. Images in .png
--   format don't contain Exif metadata.
--   
--   Amazon Rekognition doesn’t perform image correction for images in .png
--   format and .jpeg images without orientation information in the image
--   Exif metadata. The bounding box coordinates aren't translated and
--   represent the object locations before the image is rotated.
--   
--   <a>$sel:sourceImageOrientationCorrection:CompareFacesResponse'</a>,
--   <a>compareFacesResponse_sourceImageOrientationCorrection</a> - The
--   value of <tt>SourceImageOrientationCorrection</tt> is always null.
--   
--   If the input image is in .jpeg format, it might contain exchangeable
--   image file format (Exif) metadata that includes the image's
--   orientation. Amazon Rekognition uses this orientation information to
--   perform image correction. The bounding box coordinates are translated
--   to represent object locations after the orientation information in the
--   Exif metadata is used to correct the image orientation. Images in .png
--   format don't contain Exif metadata.
--   
--   Amazon Rekognition doesn’t perform image correction for images in .png
--   format and .jpeg images without orientation information in the image
--   Exif metadata. The bounding box coordinates aren't translated and
--   represent the object locations before the image is rotated.
--   
--   <a>$sel:sourceImageFace:CompareFacesResponse'</a>,
--   <a>compareFacesResponse_sourceImageFace</a> - The face in the source
--   image that was used for comparison.
--   
--   <a>$sel:httpStatus:CompareFacesResponse'</a>,
--   <a>compareFacesResponse_httpStatus</a> - The response's http status
--   code.
newCompareFacesResponse :: Int -> CompareFacesResponse

-- | <i>See:</i> <a>newDetectFaces</a> smart constructor.
data DetectFaces
DetectFaces' :: Maybe [Attribute] -> Image -> DetectFaces

-- | Create a value of <a>DetectFaces</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:attributes:DetectFaces'</a>, <a>detectFaces_attributes</a> -
--   An array of facial attributes you want to be returned. This can be the
--   default list of attributes or all attributes. If you don't specify a
--   value for <tt>Attributes</tt> or if you specify <tt>["DEFAULT"]</tt>,
--   the API returns the following subset of facial attributes:
--   <tt>BoundingBox</tt>, <tt>Confidence</tt>, <tt>Pose</tt>,
--   <tt>Quality</tt>, and <tt>Landmarks</tt>. If you provide
--   <tt>["ALL"]</tt>, all facial attributes are returned, but the
--   operation takes longer to complete.
--   
--   If you provide both, <tt>["ALL", "DEFAULT"]</tt>, the service uses a
--   logical AND operator to determine which attributes to return (in this
--   case, all attributes).
--   
--   <a>$sel:image:DetectFaces'</a>, <a>detectFaces_image</a> - The input
--   image as base64-encoded bytes or an S3 object. If you use the AWS CLI
--   to call Amazon Rekognition operations, passing base64-encoded image
--   bytes is not supported.
--   
--   If you are using an AWS SDK to call Amazon Rekognition, you might not
--   need to base64-encode image bytes passed using the <tt>Bytes</tt>
--   field. For more information, see Images in the Amazon Rekognition
--   developer guide.
newDetectFaces :: Image -> DetectFaces

-- | <i>See:</i> <a>newDetectFacesResponse</a> smart constructor.
data DetectFacesResponse
DetectFacesResponse' :: Maybe OrientationCorrection -> Maybe [FaceDetail] -> Int -> DetectFacesResponse

-- | Create a value of <a>DetectFacesResponse</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:orientationCorrection:DetectFacesResponse'</a>,
--   <a>detectFacesResponse_orientationCorrection</a> - The value of
--   <tt>OrientationCorrection</tt> is always null.
--   
--   If the input image is in .jpeg format, it might contain exchangeable
--   image file format (Exif) metadata that includes the image's
--   orientation. Amazon Rekognition uses this orientation information to
--   perform image correction. The bounding box coordinates are translated
--   to represent object locations after the orientation information in the
--   Exif metadata is used to correct the image orientation. Images in .png
--   format don't contain Exif metadata.
--   
--   Amazon Rekognition doesn’t perform image correction for images in .png
--   format and .jpeg images without orientation information in the image
--   Exif metadata. The bounding box coordinates aren't translated and
--   represent the object locations before the image is rotated.
--   
--   <a>$sel:faceDetails:DetectFacesResponse'</a>,
--   <a>detectFacesResponse_faceDetails</a> - Details of each face found in
--   the image.
--   
--   <a>$sel:httpStatus:DetectFacesResponse'</a>,
--   <a>detectFacesResponse_httpStatus</a> - The response's http status
--   code.
newDetectFacesResponse :: Int -> DetectFacesResponse

-- | <i>See:</i> <a>newGetFaceDetection</a> smart constructor.
data GetFaceDetection
GetFaceDetection' :: Maybe Text -> Maybe Natural -> Text -> GetFaceDetection

-- | Create a value of <a>GetFaceDetection</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:nextToken:GetFaceDetection'</a>,
--   <a>getFaceDetection_nextToken</a> - If the previous response was
--   incomplete (because there are more faces to retrieve), Amazon
--   Rekognition Video returns a pagination token in the response. You can
--   use this pagination token to retrieve the next set of faces.
--   
--   <a>$sel:maxResults:GetFaceDetection'</a>,
--   <a>getFaceDetection_maxResults</a> - Maximum number of results to
--   return per paginated call. The largest value you can specify is 1000.
--   If you specify a value greater than 1000, a maximum of 1000 results is
--   returned. The default value is 1000.
--   
--   <a>$sel:jobId:GetFaceDetection'</a>, <a>getFaceDetection_jobId</a> -
--   Unique identifier for the face detection job. The <tt>JobId</tt> is
--   returned from <tt>StartFaceDetection</tt>.
newGetFaceDetection :: Text -> GetFaceDetection

-- | <i>See:</i> <a>newGetFaceDetectionResponse</a> smart constructor.
data GetFaceDetectionResponse
GetFaceDetectionResponse' :: Maybe Text -> Maybe VideoMetadata -> Maybe Text -> Maybe [FaceDetection] -> Maybe VideoJobStatus -> Int -> GetFaceDetectionResponse

-- | Create a value of <a>GetFaceDetectionResponse</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:nextToken:GetFaceDetection'</a>,
--   <a>getFaceDetectionResponse_nextToken</a> - If the response is
--   truncated, Amazon Rekognition returns this token that you can use in
--   the subsequent request to retrieve the next set of faces.
--   
--   <a>$sel:videoMetadata:GetFaceDetectionResponse'</a>,
--   <a>getFaceDetectionResponse_videoMetadata</a> - Information about a
--   video that Amazon Rekognition Video analyzed. <tt>Videometadata</tt>
--   is returned in every page of paginated responses from a Amazon
--   Rekognition video operation.
--   
--   <a>$sel:statusMessage:GetFaceDetectionResponse'</a>,
--   <a>getFaceDetectionResponse_statusMessage</a> - If the job fails,
--   <tt>StatusMessage</tt> provides a descriptive error message.
--   
--   <a>$sel:faces:GetFaceDetectionResponse'</a>,
--   <a>getFaceDetectionResponse_faces</a> - An array of faces detected in
--   the video. Each element contains a detected face's details and the
--   time, in milliseconds from the start of the video, the face was
--   detected.
--   
--   <a>$sel:jobStatus:GetFaceDetectionResponse'</a>,
--   <a>getFaceDetectionResponse_jobStatus</a> - The current status of the
--   face detection job.
--   
--   <a>$sel:httpStatus:GetFaceDetectionResponse'</a>,
--   <a>getFaceDetectionResponse_httpStatus</a> - The response's http
--   status code.
newGetFaceDetectionResponse :: Int -> GetFaceDetectionResponse

-- | <i>See:</i> <a>newTagResource</a> smart constructor.
data TagResource
TagResource' :: Text -> HashMap Text Text -> TagResource

-- | Create a value of <a>TagResource</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:resourceArn:TagResource'</a>, <a>tagResource_resourceArn</a> -
--   Amazon Resource Name (ARN) of the model, collection, or stream
--   processor that you want to assign the tags to.
--   
--   <a>$sel:tags:TagResource'</a>, <a>tagResource_tags</a> - The key-value
--   tags to assign to the resource.
newTagResource :: Text -> TagResource

-- | <i>See:</i> <a>newTagResourceResponse</a> smart constructor.
data TagResourceResponse
TagResourceResponse' :: Int -> TagResourceResponse

-- | Create a value of <a>TagResourceResponse</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:httpStatus:TagResourceResponse'</a>,
--   <a>tagResourceResponse_httpStatus</a> - The response's http status
--   code.
newTagResourceResponse :: Int -> TagResourceResponse

-- | <i>See:</i> <a>newListFaces</a> smart constructor.
data ListFaces
ListFaces' :: Maybe Text -> Maybe Natural -> Text -> ListFaces

-- | Create a value of <a>ListFaces</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:nextToken:ListFaces'</a>, <a>listFaces_nextToken</a> - If the
--   previous response was incomplete (because there is more data to
--   retrieve), Amazon Rekognition returns a pagination token in the
--   response. You can use this pagination token to retrieve the next set
--   of faces.
--   
--   <a>$sel:maxResults:ListFaces'</a>, <a>listFaces_maxResults</a> -
--   Maximum number of faces to return.
--   
--   <a>$sel:collectionId:ListFaces'</a>, <a>listFaces_collectionId</a> -
--   ID of the collection from which to list the faces.
newListFaces :: Text -> ListFaces

-- | <i>See:</i> <a>newListFacesResponse</a> smart constructor.
data ListFacesResponse
ListFacesResponse' :: Maybe Text -> Maybe Text -> Maybe [Face] -> Int -> ListFacesResponse

-- | Create a value of <a>ListFacesResponse</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:faceModelVersion:ListFacesResponse'</a>,
--   <a>listFacesResponse_faceModelVersion</a> - Version number of the face
--   detection model associated with the input collection
--   (<tt>CollectionId</tt>).
--   
--   <a>$sel:nextToken:ListFaces'</a>, <a>listFacesResponse_nextToken</a> -
--   If the response is truncated, Amazon Rekognition returns this token
--   that you can use in the subsequent request to retrieve the next set of
--   faces.
--   
--   <a>$sel:faces:ListFacesResponse'</a>, <a>listFacesResponse_faces</a> -
--   An array of <tt>Face</tt> objects.
--   
--   <a>$sel:httpStatus:ListFacesResponse'</a>,
--   <a>listFacesResponse_httpStatus</a> - The response's http status code.
newListFacesResponse :: Int -> ListFacesResponse

-- | <i>See:</i> <a>newCreateProjectVersion</a> smart constructor.
data CreateProjectVersion
CreateProjectVersion' :: Maybe Text -> Maybe (HashMap Text Text) -> Text -> Text -> OutputConfig -> TrainingData -> TestingData -> CreateProjectVersion

-- | Create a value of <a>CreateProjectVersion</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:kmsKeyId:CreateProjectVersion'</a>,
--   <a>createProjectVersion_kmsKeyId</a> - The identifier for your AWS Key
--   Management Service (AWS KMS) customer master key (CMK). You can supply
--   the Amazon Resource Name (ARN) of your CMK, the ID of your CMK, an
--   alias for your CMK, or an alias ARN. The key is used to encrypt
--   training and test images copied into the service for model training.
--   Your source images are unaffected. The key is also used to encrypt
--   training results and manifest files written to the output Amazon S3
--   bucket (<tt>OutputConfig</tt>).
--   
--   If you choose to use your own CMK, you need the following permissions
--   on the CMK.
--   
--   <ul>
--   <li>kms:CreateGrant</li>
--   <li>kms:DescribeKey</li>
--   <li>kms:GenerateDataKey</li>
--   <li>kms:Decrypt</li>
--   </ul>
--   
--   If you don't specify a value for <tt>KmsKeyId</tt>, images copied into
--   the service are encrypted using a key that AWS owns and manages.
--   
--   <a>$sel:tags:CreateProjectVersion'</a>,
--   <a>createProjectVersion_tags</a> - A set of tags (key-value pairs)
--   that you want to attach to the model.
--   
--   <a>$sel:projectArn:CreateProjectVersion'</a>,
--   <a>createProjectVersion_projectArn</a> - The ARN of the Amazon
--   Rekognition Custom Labels project that manages the model that you want
--   to train.
--   
--   <a>$sel:versionName:CreateProjectVersion'</a>,
--   <a>createProjectVersion_versionName</a> - A name for the version of
--   the model. This value must be unique.
--   
--   <a>$sel:outputConfig:CreateProjectVersion'</a>,
--   <a>createProjectVersion_outputConfig</a> - The Amazon S3 bucket
--   location to store the results of training. The S3 bucket can be in any
--   AWS account as long as the caller has <tt>s3:PutObject</tt>
--   permissions on the S3 bucket.
--   
--   <a>$sel:trainingData:CreateProjectVersion'</a>,
--   <a>createProjectVersion_trainingData</a> - The dataset to use for
--   training.
--   
--   <a>$sel:testingData:CreateProjectVersion'</a>,
--   <a>createProjectVersion_testingData</a> - The dataset to use for
--   testing.
newCreateProjectVersion :: Text -> Text -> OutputConfig -> TrainingData -> TestingData -> CreateProjectVersion

-- | <i>See:</i> <a>newCreateProjectVersionResponse</a> smart constructor.
data CreateProjectVersionResponse
CreateProjectVersionResponse' :: Maybe Text -> Int -> CreateProjectVersionResponse

-- | Create a value of <a>CreateProjectVersionResponse</a> with all
--   optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:projectVersionArn:CreateProjectVersionResponse'</a>,
--   <a>createProjectVersionResponse_projectVersionArn</a> - The ARN of the
--   model version that was created. Use <tt>DescribeProjectVersion</tt> to
--   get the current status of the training operation.
--   
--   <a>$sel:httpStatus:CreateProjectVersionResponse'</a>,
--   <a>createProjectVersionResponse_httpStatus</a> - The response's http
--   status code.
newCreateProjectVersionResponse :: Int -> CreateProjectVersionResponse

-- | <i>See:</i> <a>newDescribeProjects</a> smart constructor.
data DescribeProjects
DescribeProjects' :: Maybe Text -> Maybe Natural -> DescribeProjects

-- | Create a value of <a>DescribeProjects</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:nextToken:DescribeProjects'</a>,
--   <a>describeProjects_nextToken</a> - If the previous response was
--   incomplete (because there is more results to retrieve), Amazon
--   Rekognition Custom Labels returns a pagination token in the response.
--   You can use this pagination token to retrieve the next set of results.
--   
--   <a>$sel:maxResults:DescribeProjects'</a>,
--   <a>describeProjects_maxResults</a> - The maximum number of results to
--   return per paginated call. The largest value you can specify is 100.
--   If you specify a value greater than 100, a ValidationException error
--   occurs. The default value is 100.
newDescribeProjects :: DescribeProjects

-- | <i>See:</i> <a>newDescribeProjectsResponse</a> smart constructor.
data DescribeProjectsResponse
DescribeProjectsResponse' :: Maybe Text -> Maybe [ProjectDescription] -> Int -> DescribeProjectsResponse

-- | Create a value of <a>DescribeProjectsResponse</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:nextToken:DescribeProjects'</a>,
--   <a>describeProjectsResponse_nextToken</a> - If the previous response
--   was incomplete (because there is more results to retrieve), Amazon
--   Rekognition Custom Labels returns a pagination token in the response.
--   You can use this pagination token to retrieve the next set of results.
--   
--   <a>$sel:projectDescriptions:DescribeProjectsResponse'</a>,
--   <a>describeProjectsResponse_projectDescriptions</a> - A list of
--   project descriptions. The list is sorted by the date and time the
--   projects are created.
--   
--   <a>$sel:httpStatus:DescribeProjectsResponse'</a>,
--   <a>describeProjectsResponse_httpStatus</a> - The response's http
--   status code.
newDescribeProjectsResponse :: Int -> DescribeProjectsResponse

-- | <i>See:</i> <a>newGetContentModeration</a> smart constructor.
data GetContentModeration
GetContentModeration' :: Maybe Text -> Maybe Natural -> Maybe ContentModerationSortBy -> Text -> GetContentModeration

-- | Create a value of <a>GetContentModeration</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:nextToken:GetContentModeration'</a>,
--   <a>getContentModeration_nextToken</a> - If the previous response was
--   incomplete (because there is more data to retrieve), Amazon
--   Rekognition returns a pagination token in the response. You can use
--   this pagination token to retrieve the next set of content moderation
--   labels.
--   
--   <a>$sel:maxResults:GetContentModeration'</a>,
--   <a>getContentModeration_maxResults</a> - Maximum number of results to
--   return per paginated call. The largest value you can specify is 1000.
--   If you specify a value greater than 1000, a maximum of 1000 results is
--   returned. The default value is 1000.
--   
--   <a>$sel:sortBy:GetContentModeration'</a>,
--   <a>getContentModeration_sortBy</a> - Sort to use for elements in the
--   <tt>ModerationLabelDetections</tt> array. Use <tt>TIMESTAMP</tt> to
--   sort array elements by the time labels are detected. Use <tt>NAME</tt>
--   to alphabetically group elements for a label together. Within each
--   label group, the array element are sorted by detection confidence. The
--   default sort is by <tt>TIMESTAMP</tt>.
--   
--   <a>$sel:jobId:GetContentModeration'</a>,
--   <a>getContentModeration_jobId</a> - The identifier for the
--   inappropriate, unwanted, or offensive content moderation job. Use
--   <tt>JobId</tt> to identify the job in a subsequent call to
--   <tt>GetContentModeration</tt>.
newGetContentModeration :: Text -> GetContentModeration

-- | <i>See:</i> <a>newGetContentModerationResponse</a> smart constructor.
data GetContentModerationResponse
GetContentModerationResponse' :: Maybe Text -> Maybe VideoMetadata -> Maybe Text -> Maybe VideoJobStatus -> Maybe Text -> Maybe [ContentModerationDetection] -> Int -> GetContentModerationResponse

-- | Create a value of <a>GetContentModerationResponse</a> with all
--   optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:nextToken:GetContentModeration'</a>,
--   <a>getContentModerationResponse_nextToken</a> - If the response is
--   truncated, Amazon Rekognition Video returns this token that you can
--   use in the subsequent request to retrieve the next set of content
--   moderation labels.
--   
--   <a>$sel:videoMetadata:GetContentModerationResponse'</a>,
--   <a>getContentModerationResponse_videoMetadata</a> - Information about
--   a video that Amazon Rekognition analyzed. <tt>Videometadata</tt> is
--   returned in every page of paginated responses from
--   <tt>GetContentModeration</tt>.
--   
--   <a>$sel:statusMessage:GetContentModerationResponse'</a>,
--   <a>getContentModerationResponse_statusMessage</a> - If the job fails,
--   <tt>StatusMessage</tt> provides a descriptive error message.
--   
--   <a>$sel:jobStatus:GetContentModerationResponse'</a>,
--   <a>getContentModerationResponse_jobStatus</a> - The current status of
--   the content moderation analysis job.
--   
--   <a>$sel:moderationModelVersion:GetContentModerationResponse'</a>,
--   <a>getContentModerationResponse_moderationModelVersion</a> - Version
--   number of the moderation detection model that was used to detect
--   inappropriate, unwanted, or offensive content.
--   
--   <a>$sel:moderationLabels:GetContentModerationResponse'</a>,
--   <a>getContentModerationResponse_moderationLabels</a> - The detected
--   inappropriate, unwanted, or offensive content moderation labels and
--   the time(s) they were detected.
--   
--   <a>$sel:httpStatus:GetContentModerationResponse'</a>,
--   <a>getContentModerationResponse_httpStatus</a> - The response's http
--   status code.
newGetContentModerationResponse :: Int -> GetContentModerationResponse

-- | <i>See:</i> <a>newDeleteFaces</a> smart constructor.
data DeleteFaces
DeleteFaces' :: Text -> NonEmpty Text -> DeleteFaces

-- | Create a value of <a>DeleteFaces</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:collectionId:DeleteFaces'</a>, <a>deleteFaces_collectionId</a>
--   - Collection from which to remove the specific faces.
--   
--   <a>$sel:faceIds:DeleteFaces'</a>, <a>deleteFaces_faceIds</a> - An
--   array of face IDs to delete.
newDeleteFaces :: Text -> NonEmpty Text -> DeleteFaces

-- | <i>See:</i> <a>newDeleteFacesResponse</a> smart constructor.
data DeleteFacesResponse
DeleteFacesResponse' :: Maybe (NonEmpty Text) -> Int -> DeleteFacesResponse

-- | Create a value of <a>DeleteFacesResponse</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:deletedFaces:DeleteFacesResponse'</a>,
--   <a>deleteFacesResponse_deletedFaces</a> - An array of strings (face
--   IDs) of the faces that were deleted.
--   
--   <a>$sel:httpStatus:DeleteFacesResponse'</a>,
--   <a>deleteFacesResponse_httpStatus</a> - The response's http status
--   code.
newDeleteFacesResponse :: Int -> DeleteFacesResponse

-- | <i>See:</i> <a>newGetCelebrityInfo</a> smart constructor.
data GetCelebrityInfo
GetCelebrityInfo' :: Text -> GetCelebrityInfo

-- | Create a value of <a>GetCelebrityInfo</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:id:GetCelebrityInfo'</a>, <a>getCelebrityInfo_id</a> - The ID
--   for the celebrity. You get the celebrity ID from a call to the
--   RecognizeCelebrities operation, which recognizes celebrities in an
--   image.
newGetCelebrityInfo :: Text -> GetCelebrityInfo

-- | <i>See:</i> <a>newGetCelebrityInfoResponse</a> smart constructor.
data GetCelebrityInfoResponse
GetCelebrityInfoResponse' :: Maybe [Text] -> Maybe KnownGender -> Maybe Text -> Int -> GetCelebrityInfoResponse

-- | Create a value of <a>GetCelebrityInfoResponse</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:urls:GetCelebrityInfoResponse'</a>,
--   <a>getCelebrityInfoResponse_urls</a> - An array of URLs pointing to
--   additional celebrity information.
--   
--   <a>$sel:knownGender:GetCelebrityInfoResponse'</a>,
--   <a>getCelebrityInfoResponse_knownGender</a> - Retrieves the known
--   gender for the celebrity.
--   
--   <a>$sel:name:GetCelebrityInfoResponse'</a>,
--   <a>getCelebrityInfoResponse_name</a> - The name of the celebrity.
--   
--   <a>$sel:httpStatus:GetCelebrityInfoResponse'</a>,
--   <a>getCelebrityInfoResponse_httpStatus</a> - The response's http
--   status code.
newGetCelebrityInfoResponse :: Int -> GetCelebrityInfoResponse

-- | <i>See:</i> <a>newDeleteStreamProcessor</a> smart constructor.
data DeleteStreamProcessor
DeleteStreamProcessor' :: Text -> DeleteStreamProcessor

-- | Create a value of <a>DeleteStreamProcessor</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:name:DeleteStreamProcessor'</a>,
--   <a>deleteStreamProcessor_name</a> - The name of the stream processor
--   you want to delete.
newDeleteStreamProcessor :: Text -> DeleteStreamProcessor

-- | <i>See:</i> <a>newDeleteStreamProcessorResponse</a> smart constructor.
data DeleteStreamProcessorResponse
DeleteStreamProcessorResponse' :: Int -> DeleteStreamProcessorResponse

-- | Create a value of <a>DeleteStreamProcessorResponse</a> with all
--   optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:httpStatus:DeleteStreamProcessorResponse'</a>,
--   <a>deleteStreamProcessorResponse_httpStatus</a> - The response's http
--   status code.
newDeleteStreamProcessorResponse :: Int -> DeleteStreamProcessorResponse

-- | <i>See:</i> <a>newUntagResource</a> smart constructor.
data UntagResource
UntagResource' :: Text -> [Text] -> UntagResource

-- | Create a value of <a>UntagResource</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:resourceArn:UntagResource'</a>,
--   <a>untagResource_resourceArn</a> - Amazon Resource Name (ARN) of the
--   model, collection, or stream processor that you want to remove the
--   tags from.
--   
--   <a>$sel:tagKeys:UntagResource'</a>, <a>untagResource_tagKeys</a> - A
--   list of the tags that you want to remove.
newUntagResource :: Text -> UntagResource

-- | <i>See:</i> <a>newUntagResourceResponse</a> smart constructor.
data UntagResourceResponse
UntagResourceResponse' :: Int -> UntagResourceResponse

-- | Create a value of <a>UntagResourceResponse</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:httpStatus:UntagResourceResponse'</a>,
--   <a>untagResourceResponse_httpStatus</a> - The response's http status
--   code.
newUntagResourceResponse :: Int -> UntagResourceResponse

-- | <i>See:</i> <a>newDetectModerationLabels</a> smart constructor.
data DetectModerationLabels
DetectModerationLabels' :: Maybe HumanLoopConfig -> Maybe Double -> Image -> DetectModerationLabels

-- | Create a value of <a>DetectModerationLabels</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:humanLoopConfig:DetectModerationLabels'</a>,
--   <a>detectModerationLabels_humanLoopConfig</a> - Sets up the
--   configuration for human evaluation, including the FlowDefinition the
--   image will be sent to.
--   
--   <a>$sel:minConfidence:DetectModerationLabels'</a>,
--   <a>detectModerationLabels_minConfidence</a> - Specifies the minimum
--   confidence level for the labels to return. Amazon Rekognition doesn't
--   return any labels with a confidence level lower than this specified
--   value.
--   
--   If you don't specify <tt>MinConfidence</tt>, the operation returns
--   labels with confidence values greater than or equal to 50 percent.
--   
--   <a>$sel:image:DetectModerationLabels'</a>,
--   <a>detectModerationLabels_image</a> - The input image as
--   base64-encoded bytes or an S3 object. If you use the AWS CLI to call
--   Amazon Rekognition operations, passing base64-encoded image bytes is
--   not supported.
--   
--   If you are using an AWS SDK to call Amazon Rekognition, you might not
--   need to base64-encode image bytes passed using the <tt>Bytes</tt>
--   field. For more information, see Images in the Amazon Rekognition
--   developer guide.
newDetectModerationLabels :: Image -> DetectModerationLabels

-- | <i>See:</i> <a>newDetectModerationLabelsResponse</a> smart
--   constructor.
data DetectModerationLabelsResponse
DetectModerationLabelsResponse' :: Maybe HumanLoopActivationOutput -> Maybe Text -> Maybe [ModerationLabel] -> Int -> DetectModerationLabelsResponse

-- | Create a value of <a>DetectModerationLabelsResponse</a> with all
--   optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:humanLoopActivationOutput:DetectModerationLabelsResponse'</a>,
--   <a>detectModerationLabelsResponse_humanLoopActivationOutput</a> -
--   Shows the results of the human in the loop evaluation.
--   
--   <a>$sel:moderationModelVersion:DetectModerationLabelsResponse'</a>,
--   <a>detectModerationLabelsResponse_moderationModelVersion</a> - Version
--   number of the moderation detection model that was used to detect
--   unsafe content.
--   
--   <a>$sel:moderationLabels:DetectModerationLabelsResponse'</a>,
--   <a>detectModerationLabelsResponse_moderationLabels</a> - Array of
--   detected Moderation labels and the time, in milliseconds from the
--   start of the video, they were detected.
--   
--   <a>$sel:httpStatus:DetectModerationLabelsResponse'</a>,
--   <a>detectModerationLabelsResponse_httpStatus</a> - The response's http
--   status code.
newDetectModerationLabelsResponse :: Int -> DetectModerationLabelsResponse

-- | <i>See:</i> <a>newCreateStreamProcessor</a> smart constructor.
data CreateStreamProcessor
CreateStreamProcessor' :: Maybe (HashMap Text Text) -> StreamProcessorInput -> StreamProcessorOutput -> Text -> StreamProcessorSettings -> Text -> CreateStreamProcessor

-- | Create a value of <a>CreateStreamProcessor</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:tags:CreateStreamProcessor'</a>,
--   <a>createStreamProcessor_tags</a> - A set of tags (key-value pairs)
--   that you want to attach to the stream processor.
--   
--   <a>$sel:input:CreateStreamProcessor'</a>,
--   <a>createStreamProcessor_input</a> - Kinesis video stream stream that
--   provides the source streaming video. If you are using the AWS CLI, the
--   parameter name is <tt>StreamProcessorInput</tt>.
--   
--   <a>$sel:output:CreateStreamProcessor'</a>,
--   <a>createStreamProcessor_output</a> - Kinesis data stream stream to
--   which Amazon Rekognition Video puts the analysis results. If you are
--   using the AWS CLI, the parameter name is
--   <tt>StreamProcessorOutput</tt>.
--   
--   <a>$sel:name:CreateStreamProcessor'</a>,
--   <a>createStreamProcessor_name</a> - An identifier you assign to the
--   stream processor. You can use <tt>Name</tt> to manage the stream
--   processor. For example, you can get the current status of the stream
--   processor by calling DescribeStreamProcessor. <tt>Name</tt> is
--   idempotent.
--   
--   <a>$sel:settings:CreateStreamProcessor'</a>,
--   <a>createStreamProcessor_settings</a> - Face recognition input
--   parameters to be used by the stream processor. Includes the collection
--   to use for face recognition and the face attributes to detect.
--   
--   <a>$sel:roleArn:CreateStreamProcessor'</a>,
--   <a>createStreamProcessor_roleArn</a> - ARN of the IAM role that allows
--   access to the stream processor.
newCreateStreamProcessor :: StreamProcessorInput -> StreamProcessorOutput -> Text -> StreamProcessorSettings -> Text -> CreateStreamProcessor

-- | <i>See:</i> <a>newCreateStreamProcessorResponse</a> smart constructor.
data CreateStreamProcessorResponse
CreateStreamProcessorResponse' :: Maybe Text -> Int -> CreateStreamProcessorResponse

-- | Create a value of <a>CreateStreamProcessorResponse</a> with all
--   optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:streamProcessorArn:CreateStreamProcessorResponse'</a>,
--   <a>createStreamProcessorResponse_streamProcessorArn</a> - ARN for the
--   newly create stream processor.
--   
--   <a>$sel:httpStatus:CreateStreamProcessorResponse'</a>,
--   <a>createStreamProcessorResponse_httpStatus</a> - The response's http
--   status code.
newCreateStreamProcessorResponse :: Int -> CreateStreamProcessorResponse

-- | <i>See:</i> <a>newStartFaceDetection</a> smart constructor.
data StartFaceDetection
StartFaceDetection' :: Maybe Text -> Maybe NotificationChannel -> Maybe Text -> Maybe FaceAttributes -> Video -> StartFaceDetection

-- | Create a value of <a>StartFaceDetection</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:jobTag:StartFaceDetection'</a>,
--   <a>startFaceDetection_jobTag</a> - An identifier you specify that's
--   returned in the completion notification that's published to your
--   Amazon Simple Notification Service topic. For example, you can use
--   <tt>JobTag</tt> to group related jobs and identify them in the
--   completion notification.
--   
--   <a>$sel:notificationChannel:StartFaceDetection'</a>,
--   <a>startFaceDetection_notificationChannel</a> - The ARN of the Amazon
--   SNS topic to which you want Amazon Rekognition Video to publish the
--   completion status of the face detection operation. The Amazon SNS
--   topic must have a topic name that begins with <i>AmazonRekognition</i>
--   if you are using the AmazonRekognitionServiceRole permissions policy.
--   
--   <a>$sel:clientRequestToken:StartFaceDetection'</a>,
--   <a>startFaceDetection_clientRequestToken</a> - Idempotent token used
--   to identify the start request. If you use the same token with multiple
--   <tt>StartFaceDetection</tt> requests, the same <tt>JobId</tt> is
--   returned. Use <tt>ClientRequestToken</tt> to prevent the same job from
--   being accidently started more than once.
--   
--   <a>$sel:faceAttributes:StartFaceDetection'</a>,
--   <a>startFaceDetection_faceAttributes</a> - The face attributes you
--   want returned.
--   
--   <tt>DEFAULT</tt> - The following subset of facial attributes are
--   returned: BoundingBox, Confidence, Pose, Quality and Landmarks.
--   
--   <tt>ALL</tt> - All facial attributes are returned.
--   
--   <a>$sel:video:StartFaceDetection'</a>, <a>startFaceDetection_video</a>
--   - The video in which you want to detect faces. The video must be
--   stored in an Amazon S3 bucket.
newStartFaceDetection :: Video -> StartFaceDetection

-- | <i>See:</i> <a>newStartFaceDetectionResponse</a> smart constructor.
data StartFaceDetectionResponse
StartFaceDetectionResponse' :: Maybe Text -> Int -> StartFaceDetectionResponse

-- | Create a value of <a>StartFaceDetectionResponse</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:jobId:StartFaceDetectionResponse'</a>,
--   <a>startFaceDetectionResponse_jobId</a> - The identifier for the face
--   detection job. Use <tt>JobId</tt> to identify the job in a subsequent
--   call to <tt>GetFaceDetection</tt>.
--   
--   <a>$sel:httpStatus:StartFaceDetectionResponse'</a>,
--   <a>startFaceDetectionResponse_httpStatus</a> - The response's http
--   status code.
newStartFaceDetectionResponse :: Int -> StartFaceDetectionResponse

-- | <i>See:</i> <a>newCreateProject</a> smart constructor.
data CreateProject
CreateProject' :: Text -> CreateProject

-- | Create a value of <a>CreateProject</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:projectName:CreateProject'</a>,
--   <a>createProject_projectName</a> - The name of the project to create.
newCreateProject :: Text -> CreateProject

-- | <i>See:</i> <a>newCreateProjectResponse</a> smart constructor.
data CreateProjectResponse
CreateProjectResponse' :: Maybe Text -> Int -> CreateProjectResponse

-- | Create a value of <a>CreateProjectResponse</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:projectArn:CreateProjectResponse'</a>,
--   <a>createProjectResponse_projectArn</a> - The Amazon Resource Name
--   (ARN) of the new project. You can use the ARN to configure IAM access
--   to the project.
--   
--   <a>$sel:httpStatus:CreateProjectResponse'</a>,
--   <a>createProjectResponse_httpStatus</a> - The response's http status
--   code.
newCreateProjectResponse :: Int -> CreateProjectResponse
newtype Attribute
Attribute' :: Text -> Attribute
[fromAttribute] :: Attribute -> Text
pattern Attribute_ALL :: Attribute
pattern Attribute_DEFAULT :: Attribute
newtype BodyPart
BodyPart' :: Text -> BodyPart
[fromBodyPart] :: BodyPart -> Text
pattern BodyPart_FACE :: BodyPart
pattern BodyPart_HEAD :: BodyPart
pattern BodyPart_LEFT_HAND :: BodyPart
pattern BodyPart_RIGHT_HAND :: BodyPart
newtype CelebrityRecognitionSortBy
CelebrityRecognitionSortBy' :: Text -> CelebrityRecognitionSortBy
[fromCelebrityRecognitionSortBy] :: CelebrityRecognitionSortBy -> Text
pattern CelebrityRecognitionSortBy_ID :: CelebrityRecognitionSortBy
pattern CelebrityRecognitionSortBy_TIMESTAMP :: CelebrityRecognitionSortBy
newtype ContentClassifier
ContentClassifier' :: Text -> ContentClassifier
[fromContentClassifier] :: ContentClassifier -> Text
pattern ContentClassifier_FreeOfAdultContent :: ContentClassifier
pattern ContentClassifier_FreeOfPersonallyIdentifiableInformation :: ContentClassifier
newtype ContentModerationSortBy
ContentModerationSortBy' :: Text -> ContentModerationSortBy
[fromContentModerationSortBy] :: ContentModerationSortBy -> Text
pattern ContentModerationSortBy_NAME :: ContentModerationSortBy
pattern ContentModerationSortBy_TIMESTAMP :: ContentModerationSortBy
newtype EmotionName
EmotionName' :: Text -> EmotionName
[fromEmotionName] :: EmotionName -> Text
pattern EmotionName_ANGRY :: EmotionName
pattern EmotionName_CALM :: EmotionName
pattern EmotionName_CONFUSED :: EmotionName
pattern EmotionName_DISGUSTED :: EmotionName
pattern EmotionName_FEAR :: EmotionName
pattern EmotionName_HAPPY :: EmotionName
pattern EmotionName_SAD :: EmotionName
pattern EmotionName_SURPRISED :: EmotionName
pattern EmotionName_UNKNOWN :: EmotionName
newtype FaceAttributes
FaceAttributes' :: Text -> FaceAttributes
[fromFaceAttributes] :: FaceAttributes -> Text
pattern FaceAttributes_ALL :: FaceAttributes
pattern FaceAttributes_DEFAULT :: FaceAttributes
newtype FaceSearchSortBy
FaceSearchSortBy' :: Text -> FaceSearchSortBy
[fromFaceSearchSortBy] :: FaceSearchSortBy -> Text
pattern FaceSearchSortBy_INDEX :: FaceSearchSortBy
pattern FaceSearchSortBy_TIMESTAMP :: FaceSearchSortBy
newtype GenderType
GenderType' :: Text -> GenderType
[fromGenderType] :: GenderType -> Text
pattern GenderType_Female :: GenderType
pattern GenderType_Male :: GenderType

-- | A list of enum string of possible gender values that Celebrity
--   returns.
newtype KnownGenderType
KnownGenderType' :: Text -> KnownGenderType
[fromKnownGenderType] :: KnownGenderType -> Text
pattern KnownGenderType_Female :: KnownGenderType
pattern KnownGenderType_Male :: KnownGenderType
newtype LabelDetectionSortBy
LabelDetectionSortBy' :: Text -> LabelDetectionSortBy
[fromLabelDetectionSortBy] :: LabelDetectionSortBy -> Text
pattern LabelDetectionSortBy_NAME :: LabelDetectionSortBy
pattern LabelDetectionSortBy_TIMESTAMP :: LabelDetectionSortBy
newtype LandmarkType
LandmarkType' :: Text -> LandmarkType
[fromLandmarkType] :: LandmarkType -> Text
pattern LandmarkType_ChinBottom :: LandmarkType
pattern LandmarkType_EyeLeft :: LandmarkType
pattern LandmarkType_EyeRight :: LandmarkType
pattern LandmarkType_LeftEyeBrowLeft :: LandmarkType
pattern LandmarkType_LeftEyeBrowRight :: LandmarkType
pattern LandmarkType_LeftEyeBrowUp :: LandmarkType
pattern LandmarkType_LeftEyeDown :: LandmarkType
pattern LandmarkType_LeftEyeLeft :: LandmarkType
pattern LandmarkType_LeftEyeRight :: LandmarkType
pattern LandmarkType_LeftEyeUp :: LandmarkType
pattern LandmarkType_LeftPupil :: LandmarkType
pattern LandmarkType_MidJawlineLeft :: LandmarkType
pattern LandmarkType_MidJawlineRight :: LandmarkType
pattern LandmarkType_MouthDown :: LandmarkType
pattern LandmarkType_MouthLeft :: LandmarkType
pattern LandmarkType_MouthRight :: LandmarkType
pattern LandmarkType_MouthUp :: LandmarkType
pattern LandmarkType_Nose :: LandmarkType
pattern LandmarkType_NoseLeft :: LandmarkType
pattern LandmarkType_NoseRight :: LandmarkType
pattern LandmarkType_RightEyeBrowLeft :: LandmarkType
pattern LandmarkType_RightEyeBrowRight :: LandmarkType
pattern LandmarkType_RightEyeBrowUp :: LandmarkType
pattern LandmarkType_RightEyeDown :: LandmarkType
pattern LandmarkType_RightEyeLeft :: LandmarkType
pattern LandmarkType_RightEyeRight :: LandmarkType
pattern LandmarkType_RightEyeUp :: LandmarkType
pattern LandmarkType_RightPupil :: LandmarkType
pattern LandmarkType_UpperJawlineLeft :: LandmarkType
pattern LandmarkType_UpperJawlineRight :: LandmarkType
newtype OrientationCorrection
OrientationCorrection' :: Text -> OrientationCorrection
[fromOrientationCorrection] :: OrientationCorrection -> Text
pattern OrientationCorrection_ROTATE_0 :: OrientationCorrection
pattern OrientationCorrection_ROTATE_180 :: OrientationCorrection
pattern OrientationCorrection_ROTATE_270 :: OrientationCorrection
pattern OrientationCorrection_ROTATE_90 :: OrientationCorrection
newtype PersonTrackingSortBy
PersonTrackingSortBy' :: Text -> PersonTrackingSortBy
[fromPersonTrackingSortBy] :: PersonTrackingSortBy -> Text
pattern PersonTrackingSortBy_INDEX :: PersonTrackingSortBy
pattern PersonTrackingSortBy_TIMESTAMP :: PersonTrackingSortBy
newtype ProjectStatus
ProjectStatus' :: Text -> ProjectStatus
[fromProjectStatus] :: ProjectStatus -> Text
pattern ProjectStatus_CREATED :: ProjectStatus
pattern ProjectStatus_CREATING :: ProjectStatus
pattern ProjectStatus_DELETING :: ProjectStatus
newtype ProjectVersionStatus
ProjectVersionStatus' :: Text -> ProjectVersionStatus
[fromProjectVersionStatus] :: ProjectVersionStatus -> Text
pattern ProjectVersionStatus_DELETING :: ProjectVersionStatus
pattern ProjectVersionStatus_FAILED :: ProjectVersionStatus
pattern ProjectVersionStatus_RUNNING :: ProjectVersionStatus
pattern ProjectVersionStatus_STARTING :: ProjectVersionStatus
pattern ProjectVersionStatus_STOPPED :: ProjectVersionStatus
pattern ProjectVersionStatus_STOPPING :: ProjectVersionStatus
pattern ProjectVersionStatus_TRAINING_COMPLETED :: ProjectVersionStatus
pattern ProjectVersionStatus_TRAINING_FAILED :: ProjectVersionStatus
pattern ProjectVersionStatus_TRAINING_IN_PROGRESS :: ProjectVersionStatus
newtype ProtectiveEquipmentType
ProtectiveEquipmentType' :: Text -> ProtectiveEquipmentType
[fromProtectiveEquipmentType] :: ProtectiveEquipmentType -> Text
pattern ProtectiveEquipmentType_FACE_COVER :: ProtectiveEquipmentType
pattern ProtectiveEquipmentType_HAND_COVER :: ProtectiveEquipmentType
pattern ProtectiveEquipmentType_HEAD_COVER :: ProtectiveEquipmentType
newtype QualityFilter
QualityFilter' :: Text -> QualityFilter
[fromQualityFilter] :: QualityFilter -> Text
pattern QualityFilter_AUTO :: QualityFilter
pattern QualityFilter_HIGH :: QualityFilter
pattern QualityFilter_LOW :: QualityFilter
pattern QualityFilter_MEDIUM :: QualityFilter
pattern QualityFilter_NONE :: QualityFilter
newtype Reason
Reason' :: Text -> Reason
[fromReason] :: Reason -> Text
pattern Reason_EXCEEDS_MAX_FACES :: Reason
pattern Reason_EXTREME_POSE :: Reason
pattern Reason_LOW_BRIGHTNESS :: Reason
pattern Reason_LOW_CONFIDENCE :: Reason
pattern Reason_LOW_FACE_QUALITY :: Reason
pattern Reason_LOW_SHARPNESS :: Reason
pattern Reason_SMALL_BOUNDING_BOX :: Reason
newtype SegmentType
SegmentType' :: Text -> SegmentType
[fromSegmentType] :: SegmentType -> Text
pattern SegmentType_SHOT :: SegmentType
pattern SegmentType_TECHNICAL_CUE :: SegmentType
newtype StreamProcessorStatus
StreamProcessorStatus' :: Text -> StreamProcessorStatus
[fromStreamProcessorStatus] :: StreamProcessorStatus -> Text
pattern StreamProcessorStatus_FAILED :: StreamProcessorStatus
pattern StreamProcessorStatus_RUNNING :: StreamProcessorStatus
pattern StreamProcessorStatus_STARTING :: StreamProcessorStatus
pattern StreamProcessorStatus_STOPPED :: StreamProcessorStatus
pattern StreamProcessorStatus_STOPPING :: StreamProcessorStatus
newtype TechnicalCueType
TechnicalCueType' :: Text -> TechnicalCueType
[fromTechnicalCueType] :: TechnicalCueType -> Text
pattern TechnicalCueType_BlackFrames :: TechnicalCueType
pattern TechnicalCueType_ColorBars :: TechnicalCueType
pattern TechnicalCueType_Content :: TechnicalCueType
pattern TechnicalCueType_EndCredits :: TechnicalCueType
pattern TechnicalCueType_OpeningCredits :: TechnicalCueType
pattern TechnicalCueType_Slate :: TechnicalCueType
pattern TechnicalCueType_StudioLogo :: TechnicalCueType
newtype TextTypes
TextTypes' :: Text -> TextTypes
[fromTextTypes] :: TextTypes -> Text
pattern TextTypes_LINE :: TextTypes
pattern TextTypes_WORD :: TextTypes
newtype VideoColorRange
VideoColorRange' :: Text -> VideoColorRange
[fromVideoColorRange] :: VideoColorRange -> Text
pattern VideoColorRange_FULL :: VideoColorRange
pattern VideoColorRange_LIMITED :: VideoColorRange
newtype VideoJobStatus
VideoJobStatus' :: Text -> VideoJobStatus
[fromVideoJobStatus] :: VideoJobStatus -> Text
pattern VideoJobStatus_FAILED :: VideoJobStatus
pattern VideoJobStatus_IN_PROGRESS :: VideoJobStatus
pattern VideoJobStatus_SUCCEEDED :: VideoJobStatus

-- | Structure containing the estimated age range, in years, for a face.
--   
--   Amazon Rekognition estimates an age range for faces detected in the
--   input image. Estimated age ranges can overlap. A face of a 5-year-old
--   might have an estimated range of 4-6, while the face of a 6-year-old
--   might have an estimated range of 4-8.
--   
--   <i>See:</i> <a>newAgeRange</a> smart constructor.
data AgeRange
AgeRange' :: Maybe Natural -> Maybe Natural -> AgeRange

-- | Create a value of <a>AgeRange</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:low:AgeRange'</a>, <a>ageRange_low</a> - The lowest estimated
--   age.
--   
--   <a>$sel:high:AgeRange'</a>, <a>ageRange_high</a> - The highest
--   estimated age.
newAgeRange :: AgeRange

-- | Assets are the images that you use to train and evaluate a model
--   version. Assets can also contain validation information that you use
--   to debug a failed model training.
--   
--   <i>See:</i> <a>newAsset</a> smart constructor.
data Asset
Asset' :: Maybe GroundTruthManifest -> Asset

-- | Create a value of <a>Asset</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:groundTruthManifest:Asset'</a>,
--   <a>asset_groundTruthManifest</a> - Undocumented member.
newAsset :: Asset

-- | Metadata information about an audio stream. An array of
--   <tt>AudioMetadata</tt> objects for the audio streams found in a stored
--   video is returned by GetSegmentDetection.
--   
--   <i>See:</i> <a>newAudioMetadata</a> smart constructor.
data AudioMetadata
AudioMetadata' :: Maybe Text -> Maybe Natural -> Maybe Natural -> Maybe Natural -> AudioMetadata

-- | Create a value of <a>AudioMetadata</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:codec:AudioMetadata'</a>, <a>audioMetadata_codec</a> - The
--   audio codec used to encode or decode the audio stream.
--   
--   <a>$sel:sampleRate:AudioMetadata'</a>, <a>audioMetadata_sampleRate</a>
--   - The sample rate for the audio stream.
--   
--   <a>$sel:numberOfChannels:AudioMetadata'</a>,
--   <a>audioMetadata_numberOfChannels</a> - The number of audio channels
--   in the segment.
--   
--   <a>$sel:durationMillis:AudioMetadata'</a>,
--   <a>audioMetadata_durationMillis</a> - The duration of the audio stream
--   in milliseconds.
newAudioMetadata :: AudioMetadata

-- | Indicates whether or not the face has a beard, and the confidence
--   level in the determination.
--   
--   <i>See:</i> <a>newBeard</a> smart constructor.
data Beard
Beard' :: Maybe Bool -> Maybe Double -> Beard

-- | Create a value of <a>Beard</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:value:Beard'</a>, <a>beard_value</a> - Boolean value that
--   indicates whether the face has beard or not.
--   
--   <a>$sel:confidence:Beard'</a>, <a>beard_confidence</a> - Level of
--   confidence in the determination.
newBeard :: Beard

-- | A filter that allows you to control the black frame detection by
--   specifying the black levels and pixel coverage of black pixels in a
--   frame. As videos can come from multiple sources, formats, and time
--   periods, they may contain different standards and varying noise levels
--   for black frames that need to be accounted for. For more information,
--   see StartSegmentDetection.
--   
--   <i>See:</i> <a>newBlackFrame</a> smart constructor.
data BlackFrame
BlackFrame' :: Maybe Double -> Maybe Double -> BlackFrame

-- | Create a value of <a>BlackFrame</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:maxPixelThreshold:BlackFrame'</a>,
--   <a>blackFrame_maxPixelThreshold</a> - A threshold used to determine
--   the maximum luminance value for a pixel to be considered black. In a
--   full color range video, luminance values range from 0-255. A pixel
--   value of 0 is pure black, and the most strict filter. The maximum
--   black pixel value is computed as follows: max_black_pixel_value =
--   minimum_luminance + MaxPixelThreshold *luminance_range.
--   
--   For example, for a full range video with BlackPixelThreshold = 0.1,
--   max_black_pixel_value is 0 + 0.1 * (255-0) = 25.5.
--   
--   The default value of MaxPixelThreshold is 0.2, which maps to a
--   max_black_pixel_value of 51 for a full range video. You can lower this
--   threshold to be more strict on black levels.
--   
--   <a>$sel:minCoveragePercentage:BlackFrame'</a>,
--   <a>blackFrame_minCoveragePercentage</a> - The minimum percentage of
--   pixels in a frame that need to have a luminance below the
--   max_black_pixel_value for a frame to be considered a black frame.
--   Luminance is calculated using the BT.709 matrix.
--   
--   The default value is 99, which means at least 99% of all pixels in the
--   frame are black pixels as per the <tt>MaxPixelThreshold</tt> set. You
--   can reduce this value to allow more noise on the black frame.
newBlackFrame :: BlackFrame

-- | Identifies the bounding box around the label, face, text or personal
--   protective equipment. The <tt>left</tt> (x-coordinate) and
--   <tt>top</tt> (y-coordinate) are coordinates representing the top and
--   left sides of the bounding box. Note that the upper-left corner of the
--   image is the origin (0,0).
--   
--   The <tt>top</tt> and <tt>left</tt> values returned are ratios of the
--   overall image size. For example, if the input image is 700x200 pixels,
--   and the top-left coordinate of the bounding box is 350x50 pixels, the
--   API returns a <tt>left</tt> value of 0.5 (350/700) and a <tt>top</tt>
--   value of 0.25 (50/200).
--   
--   The <tt>width</tt> and <tt>height</tt> values represent the dimensions
--   of the bounding box as a ratio of the overall image dimension. For
--   example, if the input image is 700x200 pixels, and the bounding box
--   width is 70 pixels, the width returned is 0.1.
--   
--   The bounding box coordinates can have negative values. For example, if
--   Amazon Rekognition is able to detect a face that is at the image edge
--   and is only partially visible, the service can return coordinates that
--   are outside the image bounds and, depending on the image edge, you
--   might get negative values or values greater than 1 for the
--   <tt>left</tt> or <tt>top</tt> values.
--   
--   <i>See:</i> <a>newBoundingBox</a> smart constructor.
data BoundingBox
BoundingBox' :: Maybe Double -> Maybe Double -> Maybe Double -> Maybe Double -> BoundingBox

-- | Create a value of <a>BoundingBox</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:height:BoundingBox'</a>, <a>boundingBox_height</a> - Height of
--   the bounding box as a ratio of the overall image height.
--   
--   <a>$sel:left:BoundingBox'</a>, <a>boundingBox_left</a> - Left
--   coordinate of the bounding box as a ratio of overall image width.
--   
--   <a>$sel:width:BoundingBox'</a>, <a>boundingBox_width</a> - Width of
--   the bounding box as a ratio of the overall image width.
--   
--   <a>$sel:top:BoundingBox'</a>, <a>boundingBox_top</a> - Top coordinate
--   of the bounding box as a ratio of overall image height.
newBoundingBox :: BoundingBox

-- | Provides information about a celebrity recognized by the
--   RecognizeCelebrities operation.
--   
--   <i>See:</i> <a>newCelebrity</a> smart constructor.
data Celebrity
Celebrity' :: Maybe Double -> Maybe [Text] -> Maybe KnownGender -> Maybe Text -> Maybe Text -> Maybe ComparedFace -> Celebrity

-- | Create a value of <a>Celebrity</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:matchConfidence:Celebrity'</a>,
--   <a>celebrity_matchConfidence</a> - The confidence, in percentage, that
--   Amazon Rekognition has that the recognized face is the celebrity.
--   
--   <a>$sel:urls:Celebrity'</a>, <a>celebrity_urls</a> - An array of URLs
--   pointing to additional information about the celebrity. If there is no
--   additional information about the celebrity, this list is empty.
--   
--   <a>$sel:knownGender:Celebrity'</a>, <a>celebrity_knownGender</a> -
--   Undocumented member.
--   
--   <a>$sel:name:Celebrity'</a>, <a>celebrity_name</a> - The name of the
--   celebrity.
--   
--   <a>$sel:id:Celebrity'</a>, <a>celebrity_id</a> - A unique identifier
--   for the celebrity.
--   
--   <a>$sel:face:Celebrity'</a>, <a>celebrity_face</a> - Provides
--   information about the celebrity's face, such as its location on the
--   image.
newCelebrity :: Celebrity

-- | Information about a recognized celebrity.
--   
--   <i>See:</i> <a>newCelebrityDetail</a> smart constructor.
data CelebrityDetail
CelebrityDetail' :: Maybe BoundingBox -> Maybe [Text] -> Maybe Double -> Maybe Text -> Maybe Text -> Maybe FaceDetail -> CelebrityDetail

-- | Create a value of <a>CelebrityDetail</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:boundingBox:CelebrityDetail'</a>,
--   <a>celebrityDetail_boundingBox</a> - Bounding box around the body of a
--   celebrity.
--   
--   <a>$sel:urls:CelebrityDetail'</a>, <a>celebrityDetail_urls</a> - An
--   array of URLs pointing to additional celebrity information.
--   
--   <a>$sel:confidence:CelebrityDetail'</a>,
--   <a>celebrityDetail_confidence</a> - The confidence, in percentage,
--   that Amazon Rekognition has that the recognized face is the celebrity.
--   
--   <a>$sel:name:CelebrityDetail'</a>, <a>celebrityDetail_name</a> - The
--   name of the celebrity.
--   
--   <a>$sel:id:CelebrityDetail'</a>, <a>celebrityDetail_id</a> - The
--   unique identifier for the celebrity.
--   
--   <a>$sel:face:CelebrityDetail'</a>, <a>celebrityDetail_face</a> - Face
--   details for the recognized celebrity.
newCelebrityDetail :: CelebrityDetail

-- | Information about a detected celebrity and the time the celebrity was
--   detected in a stored video. For more information, see
--   GetCelebrityRecognition in the Amazon Rekognition Developer Guide.
--   
--   <i>See:</i> <a>newCelebrityRecognition</a> smart constructor.
data CelebrityRecognition
CelebrityRecognition' :: Maybe CelebrityDetail -> Maybe Integer -> CelebrityRecognition

-- | Create a value of <a>CelebrityRecognition</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:celebrity:CelebrityRecognition'</a>,
--   <a>celebrityRecognition_celebrity</a> - Information about a recognized
--   celebrity.
--   
--   <a>$sel:timestamp:CelebrityRecognition'</a>,
--   <a>celebrityRecognition_timestamp</a> - The time, in milliseconds from
--   the start of the video, that the celebrity was recognized.
newCelebrityRecognition :: CelebrityRecognition

-- | Provides information about a face in a target image that matches the
--   source image face analyzed by <tt>CompareFaces</tt>. The <tt>Face</tt>
--   property contains the bounding box of the face in the target image.
--   The <tt>Similarity</tt> property is the confidence that the source
--   image face matches the face in the bounding box.
--   
--   <i>See:</i> <a>newCompareFacesMatch</a> smart constructor.
data CompareFacesMatch
CompareFacesMatch' :: Maybe Double -> Maybe ComparedFace -> CompareFacesMatch

-- | Create a value of <a>CompareFacesMatch</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:similarity:CompareFacesMatch'</a>,
--   <a>compareFacesMatch_similarity</a> - Level of confidence that the
--   faces match.
--   
--   <a>$sel:face:CompareFacesMatch'</a>, <a>compareFacesMatch_face</a> -
--   Provides face metadata (bounding box and confidence that the bounding
--   box actually contains a face).
newCompareFacesMatch :: CompareFacesMatch

-- | Provides face metadata for target image faces that are analyzed by
--   <tt>CompareFaces</tt> and <tt>RecognizeCelebrities</tt>.
--   
--   <i>See:</i> <a>newComparedFace</a> smart constructor.
data ComparedFace
ComparedFace' :: Maybe BoundingBox -> Maybe [Emotion] -> Maybe Pose -> Maybe Double -> Maybe ImageQuality -> Maybe Smile -> Maybe [Landmark] -> ComparedFace

-- | Create a value of <a>ComparedFace</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:boundingBox:ComparedFace'</a>, <a>comparedFace_boundingBox</a>
--   - Bounding box of the face.
--   
--   <a>$sel:emotions:ComparedFace'</a>, <a>comparedFace_emotions</a> - The
--   emotions that appear to be expressed on the face, and the confidence
--   level in the determination. Valid values include "Happy", "Sad",
--   "Angry", "Confused", "Disgusted", "Surprised", "Calm", "Unknown", and
--   "Fear".
--   
--   <a>$sel:pose:ComparedFace'</a>, <a>comparedFace_pose</a> - Indicates
--   the pose of the face as determined by its pitch, roll, and yaw.
--   
--   <a>$sel:confidence:ComparedFace'</a>, <a>comparedFace_confidence</a> -
--   Level of confidence that what the bounding box contains is a face.
--   
--   <a>$sel:quality:ComparedFace'</a>, <a>comparedFace_quality</a> -
--   Identifies face image brightness and sharpness.
--   
--   <a>$sel:smile:ComparedFace'</a>, <a>comparedFace_smile</a> - Indicates
--   whether or not the face is smiling, and the confidence level in the
--   determination.
--   
--   <a>$sel:landmarks:ComparedFace'</a>, <a>comparedFace_landmarks</a> -
--   An array of facial landmarks.
newComparedFace :: ComparedFace

-- | Type that describes the face Amazon Rekognition chose to compare with
--   the faces in the target. This contains a bounding box for the selected
--   face and confidence level that the bounding box contains a face. Note
--   that Amazon Rekognition selects the largest face in the source image
--   for this comparison.
--   
--   <i>See:</i> <a>newComparedSourceImageFace</a> smart constructor.
data ComparedSourceImageFace
ComparedSourceImageFace' :: Maybe BoundingBox -> Maybe Double -> ComparedSourceImageFace

-- | Create a value of <a>ComparedSourceImageFace</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:boundingBox:ComparedSourceImageFace'</a>,
--   <a>comparedSourceImageFace_boundingBox</a> - Bounding box of the face.
--   
--   <a>$sel:confidence:ComparedSourceImageFace'</a>,
--   <a>comparedSourceImageFace_confidence</a> - Confidence level that the
--   selected bounding box contains a face.
newComparedSourceImageFace :: ComparedSourceImageFace

-- | Information about an inappropriate, unwanted, or offensive content
--   label detection in a stored video.
--   
--   <i>See:</i> <a>newContentModerationDetection</a> smart constructor.
data ContentModerationDetection
ContentModerationDetection' :: Maybe ModerationLabel -> Maybe Integer -> ContentModerationDetection

-- | Create a value of <a>ContentModerationDetection</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:moderationLabel:ContentModerationDetection'</a>,
--   <a>contentModerationDetection_moderationLabel</a> - The content
--   moderation label detected by in the stored video.
--   
--   <a>$sel:timestamp:ContentModerationDetection'</a>,
--   <a>contentModerationDetection_timestamp</a> - Time, in milliseconds
--   from the beginning of the video, that the content moderation label was
--   detected.
newContentModerationDetection :: ContentModerationDetection

-- | Information about an item of Personal Protective Equipment covering a
--   corresponding body part. For more information, see
--   DetectProtectiveEquipment.
--   
--   <i>See:</i> <a>newCoversBodyPart</a> smart constructor.
data CoversBodyPart
CoversBodyPart' :: Maybe Bool -> Maybe Double -> CoversBodyPart

-- | Create a value of <a>CoversBodyPart</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:value:CoversBodyPart'</a>, <a>coversBodyPart_value</a> - True
--   if the PPE covers the corresponding body part, otherwise false.
--   
--   <a>$sel:confidence:CoversBodyPart'</a>,
--   <a>coversBodyPart_confidence</a> - The confidence that Amazon
--   Rekognition has in the value of <tt>Value</tt>.
newCoversBodyPart :: CoversBodyPart

-- | A custom label detected in an image by a call to DetectCustomLabels.
--   
--   <i>See:</i> <a>newCustomLabel</a> smart constructor.
data CustomLabel
CustomLabel' :: Maybe Double -> Maybe Text -> Maybe Geometry -> CustomLabel

-- | Create a value of <a>CustomLabel</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:confidence:CustomLabel'</a>, <a>customLabel_confidence</a> -
--   The confidence that the model has in the detection of the custom
--   label. The range is 0-100. A higher value indicates a higher
--   confidence.
--   
--   <a>$sel:name:CustomLabel'</a>, <a>customLabel_name</a> - The name of
--   the custom label.
--   
--   <a>$sel:geometry:CustomLabel'</a>, <a>customLabel_geometry</a> - The
--   location of the detected object on the image that corresponds to the
--   custom label. Includes an axis aligned coarse bounding box surrounding
--   the object and a finer grain polygon for more accurate spatial
--   information.
newCustomLabel :: CustomLabel

-- | A set of optional parameters that you can use to set the criteria that
--   the text must meet to be included in your response.
--   <tt>WordFilter</tt> looks at a word’s height, width, and minimum
--   confidence. <tt>RegionOfInterest</tt> lets you set a specific region
--   of the image to look for text in.
--   
--   <i>See:</i> <a>newDetectTextFilters</a> smart constructor.
data DetectTextFilters
DetectTextFilters' :: Maybe [RegionOfInterest] -> Maybe DetectionFilter -> DetectTextFilters

-- | Create a value of <a>DetectTextFilters</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:regionsOfInterest:DetectTextFilters'</a>,
--   <a>detectTextFilters_regionsOfInterest</a> - A Filter focusing on a
--   certain area of the image. Uses a <tt>BoundingBox</tt> object to set
--   the region of the image.
--   
--   <a>$sel:wordFilter:DetectTextFilters'</a>,
--   <a>detectTextFilters_wordFilter</a> - Undocumented member.
newDetectTextFilters :: DetectTextFilters

-- | A set of parameters that allow you to filter out certain results from
--   your returned results.
--   
--   <i>See:</i> <a>newDetectionFilter</a> smart constructor.
data DetectionFilter
DetectionFilter' :: Maybe Double -> Maybe Double -> Maybe Double -> DetectionFilter

-- | Create a value of <a>DetectionFilter</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:minBoundingBoxHeight:DetectionFilter'</a>,
--   <a>detectionFilter_minBoundingBoxHeight</a> - Sets the minimum height
--   of the word bounding box. Words with bounding box heights lesser than
--   this value will be excluded from the result. Value is relative to the
--   video frame height.
--   
--   <a>$sel:minBoundingBoxWidth:DetectionFilter'</a>,
--   <a>detectionFilter_minBoundingBoxWidth</a> - Sets the minimum width of
--   the word bounding box. Words with bounding boxes widths lesser than
--   this value will be excluded from the result. Value is relative to the
--   video frame width.
--   
--   <a>$sel:minConfidence:DetectionFilter'</a>,
--   <a>detectionFilter_minConfidence</a> - Sets the confidence of word
--   detection. Words with detection confidence below this will be excluded
--   from the result. Values should be between 50 and 100 as Text in Video
--   will not return any result below 50.
newDetectionFilter :: DetectionFilter

-- | The emotions that appear to be expressed on the face, and the
--   confidence level in the determination. The API is only making a
--   determination of the physical appearance of a person's face. It is not
--   a determination of the person’s internal emotional state and should
--   not be used in such a way. For example, a person pretending to have a
--   sad face might not be sad emotionally.
--   
--   <i>See:</i> <a>newEmotion</a> smart constructor.
data Emotion
Emotion' :: Maybe Double -> Maybe EmotionName -> Emotion

-- | Create a value of <a>Emotion</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:confidence:Emotion'</a>, <a>emotion_confidence</a> - Level of
--   confidence in the determination.
--   
--   <a>$sel:type':Emotion'</a>, <a>emotion_type</a> - Type of emotion
--   detected.
newEmotion :: Emotion

-- | Information about an item of Personal Protective Equipment (PPE)
--   detected by DetectProtectiveEquipment. For more information, see
--   DetectProtectiveEquipment.
--   
--   <i>See:</i> <a>newEquipmentDetection</a> smart constructor.
data EquipmentDetection
EquipmentDetection' :: Maybe BoundingBox -> Maybe CoversBodyPart -> Maybe Double -> Maybe ProtectiveEquipmentType -> EquipmentDetection

-- | Create a value of <a>EquipmentDetection</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:boundingBox:EquipmentDetection'</a>,
--   <a>equipmentDetection_boundingBox</a> - A bounding box surrounding the
--   item of detected PPE.
--   
--   <a>$sel:coversBodyPart:EquipmentDetection'</a>,
--   <a>equipmentDetection_coversBodyPart</a> - Information about the body
--   part covered by the detected PPE.
--   
--   <a>$sel:confidence:EquipmentDetection'</a>,
--   <a>equipmentDetection_confidence</a> - The confidence that Amazon
--   Rekognition has that the bounding box (<tt>BoundingBox</tt>) contains
--   an item of PPE.
--   
--   <a>$sel:type':EquipmentDetection'</a>, <a>equipmentDetection_type</a>
--   - The type of detected PPE.
newEquipmentDetection :: EquipmentDetection

-- | The evaluation results for the training of a model.
--   
--   <i>See:</i> <a>newEvaluationResult</a> smart constructor.
data EvaluationResult
EvaluationResult' :: Maybe Summary -> Maybe Double -> EvaluationResult

-- | Create a value of <a>EvaluationResult</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:summary:EvaluationResult'</a>, <a>evaluationResult_summary</a>
--   - The S3 bucket that contains the training summary.
--   
--   <a>$sel:f1Score:EvaluationResult'</a>, <a>evaluationResult_f1Score</a>
--   - The F1 score for the evaluation of all labels. The F1 score metric
--   evaluates the overall precision and recall performance of the model as
--   a single value. A higher value indicates better precision and recall
--   performance. A lower score indicates that precision, recall, or both
--   are performing poorly.
newEvaluationResult :: EvaluationResult

-- | Indicates whether or not the eyes on the face are open, and the
--   confidence level in the determination.
--   
--   <i>See:</i> <a>newEyeOpen</a> smart constructor.
data EyeOpen
EyeOpen' :: Maybe Bool -> Maybe Double -> EyeOpen

-- | Create a value of <a>EyeOpen</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:value:EyeOpen'</a>, <a>eyeOpen_value</a> - Boolean value that
--   indicates whether the eyes on the face are open.
--   
--   <a>$sel:confidence:EyeOpen'</a>, <a>eyeOpen_confidence</a> - Level of
--   confidence in the determination.
newEyeOpen :: EyeOpen

-- | Indicates whether or not the face is wearing eye glasses, and the
--   confidence level in the determination.
--   
--   <i>See:</i> <a>newEyeglasses</a> smart constructor.
data Eyeglasses
Eyeglasses' :: Maybe Bool -> Maybe Double -> Eyeglasses

-- | Create a value of <a>Eyeglasses</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:value:Eyeglasses'</a>, <a>eyeglasses_value</a> - Boolean value
--   that indicates whether the face is wearing eye glasses or not.
--   
--   <a>$sel:confidence:Eyeglasses'</a>, <a>eyeglasses_confidence</a> -
--   Level of confidence in the determination.
newEyeglasses :: Eyeglasses

-- | Describes the face properties such as the bounding box, face ID, image
--   ID of the input image, and external image ID that you assigned.
--   
--   <i>See:</i> <a>newFace</a> smart constructor.
data Face
Face' :: Maybe Text -> Maybe BoundingBox -> Maybe Text -> Maybe Double -> Maybe Text -> Face

-- | Create a value of <a>Face</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:faceId:Face'</a>, <a>face_faceId</a> - Unique identifier that
--   Amazon Rekognition assigns to the face.
--   
--   <a>$sel:boundingBox:Face'</a>, <a>face_boundingBox</a> - Bounding box
--   of the face.
--   
--   <a>$sel:externalImageId:Face'</a>, <a>face_externalImageId</a> -
--   Identifier that you assign to all the faces in the input image.
--   
--   <a>$sel:confidence:Face'</a>, <a>face_confidence</a> - Confidence
--   level that the bounding box contains a face (and not a different
--   object such as a tree).
--   
--   <a>$sel:imageId:Face'</a>, <a>face_imageId</a> - Unique identifier
--   that Amazon Rekognition assigns to the input image.
newFace :: Face

-- | Structure containing attributes of the face that the algorithm
--   detected.
--   
--   A <tt>FaceDetail</tt> object contains either the default facial
--   attributes or all facial attributes. The default attributes are
--   <tt>BoundingBox</tt>, <tt>Confidence</tt>, <tt>Landmarks</tt>,
--   <tt>Pose</tt>, and <tt>Quality</tt>.
--   
--   GetFaceDetection is the only Amazon Rekognition Video stored video
--   operation that can return a <tt>FaceDetail</tt> object with all
--   attributes. To specify which attributes to return, use the
--   <tt>FaceAttributes</tt> input parameter for StartFaceDetection. The
--   following Amazon Rekognition Video operations return only the default
--   attributes. The corresponding Start operations don't have a
--   <tt>FaceAttributes</tt> input parameter.
--   
--   <ul>
--   <li>GetCelebrityRecognition</li>
--   <li>GetPersonTracking</li>
--   <li>GetFaceSearch</li>
--   </ul>
--   
--   The Amazon Rekognition Image DetectFaces and IndexFaces operations can
--   return all facial attributes. To specify which attributes to return,
--   use the <tt>Attributes</tt> input parameter for <tt>DetectFaces</tt>.
--   For <tt>IndexFaces</tt>, use the <tt>DetectAttributes</tt> input
--   parameter.
--   
--   <i>See:</i> <a>newFaceDetail</a> smart constructor.
data FaceDetail
FaceDetail' :: Maybe AgeRange -> Maybe Sunglasses -> Maybe MouthOpen -> Maybe BoundingBox -> Maybe [Emotion] -> Maybe EyeOpen -> Maybe Pose -> Maybe Double -> Maybe Gender -> Maybe ImageQuality -> Maybe Eyeglasses -> Maybe Beard -> Maybe Mustache -> Maybe Smile -> Maybe [Landmark] -> FaceDetail

-- | Create a value of <a>FaceDetail</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:ageRange:FaceDetail'</a>, <a>faceDetail_ageRange</a> - The
--   estimated age range, in years, for the face. Low represents the lowest
--   estimated age and High represents the highest estimated age.
--   
--   <a>$sel:sunglasses:FaceDetail'</a>, <a>faceDetail_sunglasses</a> -
--   Indicates whether or not the face is wearing sunglasses, and the
--   confidence level in the determination.
--   
--   <a>$sel:mouthOpen:FaceDetail'</a>, <a>faceDetail_mouthOpen</a> -
--   Indicates whether or not the mouth on the face is open, and the
--   confidence level in the determination.
--   
--   <a>$sel:boundingBox:FaceDetail'</a>, <a>faceDetail_boundingBox</a> -
--   Bounding box of the face. Default attribute.
--   
--   <a>$sel:emotions:FaceDetail'</a>, <a>faceDetail_emotions</a> - The
--   emotions that appear to be expressed on the face, and the confidence
--   level in the determination. The API is only making a determination of
--   the physical appearance of a person's face. It is not a determination
--   of the person’s internal emotional state and should not be used in
--   such a way. For example, a person pretending to have a sad face might
--   not be sad emotionally.
--   
--   <a>$sel:eyesOpen:FaceDetail'</a>, <a>faceDetail_eyesOpen</a> -
--   Indicates whether or not the eyes on the face are open, and the
--   confidence level in the determination.
--   
--   <a>$sel:pose:FaceDetail'</a>, <a>faceDetail_pose</a> - Indicates the
--   pose of the face as determined by its pitch, roll, and yaw. Default
--   attribute.
--   
--   <a>$sel:confidence:FaceDetail'</a>, <a>faceDetail_confidence</a> -
--   Confidence level that the bounding box contains a face (and not a
--   different object such as a tree). Default attribute.
--   
--   <a>$sel:gender:FaceDetail'</a>, <a>faceDetail_gender</a> - The
--   predicted gender of a detected face.
--   
--   <a>$sel:quality:FaceDetail'</a>, <a>faceDetail_quality</a> -
--   Identifies image brightness and sharpness. Default attribute.
--   
--   <a>$sel:eyeglasses:FaceDetail'</a>, <a>faceDetail_eyeglasses</a> -
--   Indicates whether or not the face is wearing eye glasses, and the
--   confidence level in the determination.
--   
--   <a>$sel:beard:FaceDetail'</a>, <a>faceDetail_beard</a> - Indicates
--   whether or not the face has a beard, and the confidence level in the
--   determination.
--   
--   <a>$sel:mustache:FaceDetail'</a>, <a>faceDetail_mustache</a> -
--   Indicates whether or not the face has a mustache, and the confidence
--   level in the determination.
--   
--   <a>$sel:smile:FaceDetail'</a>, <a>faceDetail_smile</a> - Indicates
--   whether or not the face is smiling, and the confidence level in the
--   determination.
--   
--   <a>$sel:landmarks:FaceDetail'</a>, <a>faceDetail_landmarks</a> -
--   Indicates the location of landmarks on the face. Default attribute.
newFaceDetail :: FaceDetail

-- | Information about a face detected in a video analysis request and the
--   time the face was detected in the video.
--   
--   <i>See:</i> <a>newFaceDetection</a> smart constructor.
data FaceDetection
FaceDetection' :: Maybe Integer -> Maybe FaceDetail -> FaceDetection

-- | Create a value of <a>FaceDetection</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:timestamp:FaceDetection'</a>, <a>faceDetection_timestamp</a> -
--   Time, in milliseconds from the start of the video, that the face was
--   detected.
--   
--   <a>$sel:face:FaceDetection'</a>, <a>faceDetection_face</a> - The face
--   properties for the detected face.
newFaceDetection :: FaceDetection

-- | Provides face metadata. In addition, it also provides the confidence
--   in the match of this face with the input face.
--   
--   <i>See:</i> <a>newFaceMatch</a> smart constructor.
data FaceMatch
FaceMatch' :: Maybe Double -> Maybe Face -> FaceMatch

-- | Create a value of <a>FaceMatch</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:similarity:FaceMatch'</a>, <a>faceMatch_similarity</a> -
--   Confidence in the match of this face with the input face.
--   
--   <a>$sel:face:FaceMatch'</a>, <a>faceMatch_face</a> - Describes the
--   face properties such as the bounding box, face ID, image ID of the
--   source image, and external image ID that you assigned.
newFaceMatch :: FaceMatch

-- | Object containing both the face metadata (stored in the backend
--   database), and facial attributes that are detected but aren't stored
--   in the database.
--   
--   <i>See:</i> <a>newFaceRecord</a> smart constructor.
data FaceRecord
FaceRecord' :: Maybe FaceDetail -> Maybe Face -> FaceRecord

-- | Create a value of <a>FaceRecord</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:faceDetail:FaceRecord'</a>, <a>faceRecord_faceDetail</a> -
--   Structure containing attributes of the face that the algorithm
--   detected.
--   
--   <a>$sel:face:FaceRecord'</a>, <a>faceRecord_face</a> - Describes the
--   face properties such as the bounding box, face ID, image ID of the
--   input image, and external image ID that you assigned.
newFaceRecord :: FaceRecord

-- | Input face recognition parameters for an Amazon Rekognition stream
--   processor. <tt>FaceRecognitionSettings</tt> is a request parameter for
--   CreateStreamProcessor.
--   
--   <i>See:</i> <a>newFaceSearchSettings</a> smart constructor.
data FaceSearchSettings
FaceSearchSettings' :: Maybe Double -> Maybe Text -> FaceSearchSettings

-- | Create a value of <a>FaceSearchSettings</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:faceMatchThreshold:FaceSearchSettings'</a>,
--   <a>faceSearchSettings_faceMatchThreshold</a> - Minimum face match
--   confidence score that must be met to return a result for a recognized
--   face. Default is 80. 0 is the lowest confidence. 100 is the highest
--   confidence.
--   
--   <a>$sel:collectionId:FaceSearchSettings'</a>,
--   <a>faceSearchSettings_collectionId</a> - The ID of a collection that
--   contains faces that you want to search for.
newFaceSearchSettings :: FaceSearchSettings

-- | The predicted gender of a detected face.
--   
--   Amazon Rekognition makes gender binary (male/female) predictions based
--   on the physical appearance of a face in a particular image. This kind
--   of prediction is not designed to categorize a person’s gender
--   identity, and you shouldn't use Amazon Rekognition to make such a
--   determination. For example, a male actor wearing a long-haired wig and
--   earrings for a role might be predicted as female.
--   
--   Using Amazon Rekognition to make gender binary predictions is best
--   suited for use cases where aggregate gender distribution statistics
--   need to be analyzed without identifying specific users. For example,
--   the percentage of female users compared to male users on a social
--   media platform.
--   
--   We don't recommend using gender binary predictions to make decisions
--   that impact  an individual's rights, privacy, or access to services.
--   
--   <i>See:</i> <a>newGender</a> smart constructor.
data Gender
Gender' :: Maybe GenderType -> Maybe Double -> Gender

-- | Create a value of <a>Gender</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:value:Gender'</a>, <a>gender_value</a> - The predicted gender
--   of the face.
--   
--   <a>$sel:confidence:Gender'</a>, <a>gender_confidence</a> - Level of
--   confidence in the prediction.
newGender :: Gender

-- | Information about where an object (DetectCustomLabels) or text
--   (DetectText) is located on an image.
--   
--   <i>See:</i> <a>newGeometry</a> smart constructor.
data Geometry
Geometry' :: Maybe BoundingBox -> Maybe [Point] -> Geometry

-- | Create a value of <a>Geometry</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:boundingBox:Geometry'</a>, <a>geometry_boundingBox</a> - An
--   axis-aligned coarse representation of the detected item's location on
--   the image.
--   
--   <a>$sel:polygon:Geometry'</a>, <a>geometry_polygon</a> - Within the
--   bounding box, a fine-grained polygon around the detected item.
newGeometry :: Geometry

-- | The S3 bucket that contains an Amazon Sagemaker Ground Truth format
--   manifest file.
--   
--   <i>See:</i> <a>newGroundTruthManifest</a> smart constructor.
data GroundTruthManifest
GroundTruthManifest' :: Maybe S3Object -> GroundTruthManifest

-- | Create a value of <a>GroundTruthManifest</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:s3Object:GroundTruthManifest'</a>,
--   <a>groundTruthManifest_s3Object</a> - Undocumented member.
newGroundTruthManifest :: GroundTruthManifest

-- | Shows the results of the human in the loop evaluation. If there is no
--   HumanLoopArn, the input did not trigger human review.
--   
--   <i>See:</i> <a>newHumanLoopActivationOutput</a> smart constructor.
data HumanLoopActivationOutput
HumanLoopActivationOutput' :: Maybe (NonEmpty Text) -> Maybe Text -> Maybe Text -> HumanLoopActivationOutput

-- | Create a value of <a>HumanLoopActivationOutput</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:humanLoopActivationReasons:HumanLoopActivationOutput'</a>,
--   <a>humanLoopActivationOutput_humanLoopActivationReasons</a> - Shows if
--   and why human review was needed.
--   
--   <a>$sel:humanLoopArn:HumanLoopActivationOutput'</a>,
--   <a>humanLoopActivationOutput_humanLoopArn</a> - The Amazon Resource
--   Name (ARN) of the HumanLoop created.
--   
--   
--   <a>$sel:humanLoopActivationConditionsEvaluationResults:HumanLoopActivationOutput'</a>,
--   <a>humanLoopActivationOutput_humanLoopActivationConditionsEvaluationResults</a>
--   - Shows the result of condition evaluations, including those
--   conditions which activated a human review.
newHumanLoopActivationOutput :: HumanLoopActivationOutput

-- | Sets up the flow definition the image will be sent to if one of the
--   conditions is met. You can also set certain attributes of the image
--   before review.
--   
--   <i>See:</i> <a>newHumanLoopConfig</a> smart constructor.
data HumanLoopConfig
HumanLoopConfig' :: Maybe HumanLoopDataAttributes -> Text -> Text -> HumanLoopConfig

-- | Create a value of <a>HumanLoopConfig</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:dataAttributes:HumanLoopConfig'</a>,
--   <a>humanLoopConfig_dataAttributes</a> - Sets attributes of the input
--   data.
--   
--   <a>$sel:humanLoopName:HumanLoopConfig'</a>,
--   <a>humanLoopConfig_humanLoopName</a> - The name of the human review
--   used for this image. This should be kept unique within a region.
--   
--   <a>$sel:flowDefinitionArn:HumanLoopConfig'</a>,
--   <a>humanLoopConfig_flowDefinitionArn</a> - The Amazon Resource Name
--   (ARN) of the flow definition. You can create a flow definition by
--   using the Amazon Sagemaker <a>CreateFlowDefinition</a> Operation.
newHumanLoopConfig :: Text -> Text -> HumanLoopConfig

-- | Allows you to set attributes of the image. Currently, you can declare
--   an image as free of personally identifiable information.
--   
--   <i>See:</i> <a>newHumanLoopDataAttributes</a> smart constructor.
data HumanLoopDataAttributes
HumanLoopDataAttributes' :: Maybe [ContentClassifier] -> HumanLoopDataAttributes

-- | Create a value of <a>HumanLoopDataAttributes</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:contentClassifiers:HumanLoopDataAttributes'</a>,
--   <a>humanLoopDataAttributes_contentClassifiers</a> - Sets whether the
--   input image is free of personally identifiable information.
newHumanLoopDataAttributes :: HumanLoopDataAttributes

-- | Provides the input image either as bytes or an S3 object.
--   
--   You pass image bytes to an Amazon Rekognition API operation by using
--   the <tt>Bytes</tt> property. For example, you would use the
--   <tt>Bytes</tt> property to pass an image loaded from a local file
--   system. Image bytes passed by using the <tt>Bytes</tt> property must
--   be base64-encoded. Your code may not need to encode image bytes if you
--   are using an AWS SDK to call Amazon Rekognition API operations.
--   
--   For more information, see Analyzing an Image Loaded from a Local File
--   System in the Amazon Rekognition Developer Guide.
--   
--   You pass images stored in an S3 bucket to an Amazon Rekognition API
--   operation by using the <tt>S3Object</tt> property. Images stored in an
--   S3 bucket do not need to be base64-encoded.
--   
--   The region for the S3 bucket containing the S3 object must match the
--   region you use for Amazon Rekognition operations.
--   
--   If you use the AWS CLI to call Amazon Rekognition operations, passing
--   image bytes using the Bytes property is not supported. You must first
--   upload the image to an Amazon S3 bucket and then call the operation
--   using the S3Object property.
--   
--   For Amazon Rekognition to process an S3 object, the user must have
--   permission to access the S3 object. For more information, see Resource
--   Based Policies in the Amazon Rekognition Developer Guide.
--   
--   <i>See:</i> <a>newImage</a> smart constructor.
data Image
Image' :: Maybe S3Object -> Maybe Base64 -> Image

-- | Create a value of <a>Image</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:s3Object:Image'</a>, <a>image_s3Object</a> - Identifies an S3
--   object as the image source.
--   
--   <a>$sel:bytes:Image'</a>, <a>image_bytes</a> - Blob of image bytes up
--   to 5 MBs.-- -- <i>Note:</i> This <tt>Lens</tt> automatically encodes
--   and decodes Base64 data. -- The underlying isomorphism will encode to
--   Base64 representation during -- serialisation, and decode from Base64
--   representation during deserialisation. -- This <tt>Lens</tt> accepts
--   and returns only raw unencoded data.
newImage :: Image

-- | Identifies face image brightness and sharpness.
--   
--   <i>See:</i> <a>newImageQuality</a> smart constructor.
data ImageQuality
ImageQuality' :: Maybe Double -> Maybe Double -> ImageQuality

-- | Create a value of <a>ImageQuality</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:sharpness:ImageQuality'</a>, <a>imageQuality_sharpness</a> -
--   Value representing sharpness of the face. The service returns a value
--   between 0 and 100 (inclusive). A higher value indicates a sharper face
--   image.
--   
--   <a>$sel:brightness:ImageQuality'</a>, <a>imageQuality_brightness</a> -
--   Value representing brightness of the face. The service returns a value
--   between 0 and 100 (inclusive). A higher value indicates a brighter
--   face image.
newImageQuality :: ImageQuality

-- | An instance of a label returned by Amazon Rekognition Image
--   (DetectLabels) or by Amazon Rekognition Video (GetLabelDetection).
--   
--   <i>See:</i> <a>newInstance</a> smart constructor.
data Instance
Instance' :: Maybe BoundingBox -> Maybe Double -> Instance

-- | Create a value of <a>Instance</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:boundingBox:Instance'</a>, <a>instance_boundingBox</a> - The
--   position of the label instance on the image.
--   
--   <a>$sel:confidence:Instance'</a>, <a>instance_confidence</a> - The
--   confidence that Amazon Rekognition has in the accuracy of the bounding
--   box.
newInstance :: Instance

-- | The Kinesis data stream Amazon Rekognition to which the analysis
--   results of a Amazon Rekognition stream processor are streamed. For
--   more information, see CreateStreamProcessor in the Amazon Rekognition
--   Developer Guide.
--   
--   <i>See:</i> <a>newKinesisDataStream</a> smart constructor.
data KinesisDataStream
KinesisDataStream' :: Maybe Text -> KinesisDataStream

-- | Create a value of <a>KinesisDataStream</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:arn:KinesisDataStream'</a>, <a>kinesisDataStream_arn</a> - ARN
--   of the output Amazon Kinesis Data Streams stream.
newKinesisDataStream :: KinesisDataStream

-- | Kinesis video stream stream that provides the source streaming video
--   for a Amazon Rekognition Video stream processor. For more information,
--   see CreateStreamProcessor in the Amazon Rekognition Developer Guide.
--   
--   <i>See:</i> <a>newKinesisVideoStream</a> smart constructor.
data KinesisVideoStream
KinesisVideoStream' :: Maybe Text -> KinesisVideoStream

-- | Create a value of <a>KinesisVideoStream</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:arn:KinesisVideoStream'</a>, <a>kinesisVideoStream_arn</a> -
--   ARN of the Kinesis video stream stream that streams the source video.
newKinesisVideoStream :: KinesisVideoStream

-- | The known gender identity for the celebrity that matches the provided
--   ID.
--   
--   <i>See:</i> <a>newKnownGender</a> smart constructor.
data KnownGender
KnownGender' :: Maybe KnownGenderType -> KnownGender

-- | Create a value of <a>KnownGender</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:type':KnownGender'</a>, <a>knownGender_type</a> - A string
--   value of the KnownGender info about the Celebrity.
newKnownGender :: KnownGender

-- | Structure containing details about the detected label, including the
--   name, detected instances, parent labels, and level of confidence.
--   
--   <i>See:</i> <a>newLabel</a> smart constructor.
data Label
Label' :: Maybe Double -> Maybe [Parent] -> Maybe Text -> Maybe [Instance] -> Label

-- | Create a value of <a>Label</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:confidence:Label'</a>, <a>label_confidence</a> - Level of
--   confidence.
--   
--   <a>$sel:parents:Label'</a>, <a>label_parents</a> - The parent labels
--   for a label. The response includes all ancestor labels.
--   
--   <a>$sel:name:Label'</a>, <a>label_name</a> - The name (label) of the
--   object or scene.
--   
--   <a>$sel:instances:Label'</a>, <a>label_instances</a> - If
--   <tt>Label</tt> represents an object, <tt>Instances</tt> contains the
--   bounding boxes for each instance of the detected object. Bounding
--   boxes are returned for common object labels such as people, cars,
--   furniture, apparel or pets.
newLabel :: Label

-- | Information about a label detected in a video analysis request and the
--   time the label was detected in the video.
--   
--   <i>See:</i> <a>newLabelDetection</a> smart constructor.
data LabelDetection
LabelDetection' :: Maybe Label -> Maybe Integer -> LabelDetection

-- | Create a value of <a>LabelDetection</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:label:LabelDetection'</a>, <a>labelDetection_label</a> -
--   Details about the detected label.
--   
--   <a>$sel:timestamp:LabelDetection'</a>, <a>labelDetection_timestamp</a>
--   - Time, in milliseconds from the start of the video, that the label
--   was detected.
newLabelDetection :: LabelDetection

-- | Indicates the location of the landmark on the face.
--   
--   <i>See:</i> <a>newLandmark</a> smart constructor.
data Landmark
Landmark' :: Maybe LandmarkType -> Maybe Double -> Maybe Double -> Landmark

-- | Create a value of <a>Landmark</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:type':Landmark'</a>, <a>landmark_type</a> - Type of landmark.
--   
--   <a>$sel:x:Landmark'</a>, <a>landmark_x</a> - The x-coordinate of the
--   landmark expressed as a ratio of the width of the image. The
--   x-coordinate is measured from the left-side of the image. For example,
--   if the image is 700 pixels wide and the x-coordinate of the landmark
--   is at 350 pixels, this value is 0.5.
--   
--   <a>$sel:y:Landmark'</a>, <a>landmark_y</a> - The y-coordinate of the
--   landmark expressed as a ratio of the height of the image. The
--   y-coordinate is measured from the top of the image. For example, if
--   the image height is 200 pixels and the y-coordinate of the landmark is
--   at 50 pixels, this value is 0.25.
newLandmark :: Landmark

-- | Provides information about a single type of inappropriate, unwanted,
--   or offensive content found in an image or video. Each type of
--   moderated content has a label within a hierarchical taxonomy. For more
--   information, see Content moderation in the Amazon Rekognition
--   Developer Guide.
--   
--   <i>See:</i> <a>newModerationLabel</a> smart constructor.
data ModerationLabel
ModerationLabel' :: Maybe Double -> Maybe Text -> Maybe Text -> ModerationLabel

-- | Create a value of <a>ModerationLabel</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:confidence:ModerationLabel'</a>,
--   <a>moderationLabel_confidence</a> - Specifies the confidence that
--   Amazon Rekognition has that the label has been correctly identified.
--   
--   If you don't specify the <tt>MinConfidence</tt> parameter in the call
--   to <tt>DetectModerationLabels</tt>, the operation returns labels with
--   a confidence value greater than or equal to 50 percent.
--   
--   <a>$sel:name:ModerationLabel'</a>, <a>moderationLabel_name</a> - The
--   label name for the type of unsafe content detected in the image.
--   
--   <a>$sel:parentName:ModerationLabel'</a>,
--   <a>moderationLabel_parentName</a> - The name for the parent label.
--   Labels at the top level of the hierarchy have the parent label
--   <tt>""</tt>.
newModerationLabel :: ModerationLabel

-- | Indicates whether or not the mouth on the face is open, and the
--   confidence level in the determination.
--   
--   <i>See:</i> <a>newMouthOpen</a> smart constructor.
data MouthOpen
MouthOpen' :: Maybe Bool -> Maybe Double -> MouthOpen

-- | Create a value of <a>MouthOpen</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:value:MouthOpen'</a>, <a>mouthOpen_value</a> - Boolean value
--   that indicates whether the mouth on the face is open or not.
--   
--   <a>$sel:confidence:MouthOpen'</a>, <a>mouthOpen_confidence</a> - Level
--   of confidence in the determination.
newMouthOpen :: MouthOpen

-- | Indicates whether or not the face has a mustache, and the confidence
--   level in the determination.
--   
--   <i>See:</i> <a>newMustache</a> smart constructor.
data Mustache
Mustache' :: Maybe Bool -> Maybe Double -> Mustache

-- | Create a value of <a>Mustache</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:value:Mustache'</a>, <a>mustache_value</a> - Boolean value
--   that indicates whether the face has mustache or not.
--   
--   <a>$sel:confidence:Mustache'</a>, <a>mustache_confidence</a> - Level
--   of confidence in the determination.
newMustache :: Mustache

-- | The Amazon Simple Notification Service topic to which Amazon
--   Rekognition publishes the completion status of a video analysis
--   operation. For more information, see api-video. Note that the Amazon
--   SNS topic must have a topic name that begins with
--   <i>AmazonRekognition</i> if you are using the
--   AmazonRekognitionServiceRole permissions policy to access the topic.
--   For more information, see <a>Giving access to multiple Amazon SNS
--   topics</a>.
--   
--   <i>See:</i> <a>newNotificationChannel</a> smart constructor.
data NotificationChannel
NotificationChannel' :: Text -> Text -> NotificationChannel

-- | Create a value of <a>NotificationChannel</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:sNSTopicArn:NotificationChannel'</a>,
--   <a>notificationChannel_sNSTopicArn</a> - The Amazon SNS topic to which
--   Amazon Rekognition to posts the completion status.
--   
--   <a>$sel:roleArn:NotificationChannel'</a>,
--   <a>notificationChannel_roleArn</a> - The ARN of an IAM role that gives
--   Amazon Rekognition publishing permissions to the Amazon SNS topic.
newNotificationChannel :: Text -> Text -> NotificationChannel

-- | The S3 bucket and folder location where training output is placed.
--   
--   <i>See:</i> <a>newOutputConfig</a> smart constructor.
data OutputConfig
OutputConfig' :: Maybe Text -> Maybe Text -> OutputConfig

-- | Create a value of <a>OutputConfig</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:s3KeyPrefix:OutputConfig'</a>, <a>outputConfig_s3KeyPrefix</a>
--   - The prefix applied to the training output files.
--   
--   <a>$sel:s3Bucket:OutputConfig'</a>, <a>outputConfig_s3Bucket</a> - The
--   S3 bucket where training output is placed.
newOutputConfig :: OutputConfig

-- | A parent label for a label. A label can have 0, 1, or more parents.
--   
--   <i>See:</i> <a>newParent</a> smart constructor.
data Parent
Parent' :: Maybe Text -> Parent

-- | Create a value of <a>Parent</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:name:Parent'</a>, <a>parent_name</a> - The name of the parent
--   label.
newParent :: Parent

-- | Details about a person detected in a video analysis request.
--   
--   <i>See:</i> <a>newPersonDetail</a> smart constructor.
data PersonDetail
PersonDetail' :: Maybe BoundingBox -> Maybe Integer -> Maybe FaceDetail -> PersonDetail

-- | Create a value of <a>PersonDetail</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:boundingBox:PersonDetail'</a>, <a>personDetail_boundingBox</a>
--   - Bounding box around the detected person.
--   
--   <a>$sel:index:PersonDetail'</a>, <a>personDetail_index</a> -
--   Identifier for the person detected person within a video. Use to keep
--   track of the person throughout the video. The identifier is not stored
--   by Amazon Rekognition.
--   
--   <a>$sel:face:PersonDetail'</a>, <a>personDetail_face</a> - Face
--   details for the detected person.
newPersonDetail :: PersonDetail

-- | Details and path tracking information for a single time a person's
--   path is tracked in a video. Amazon Rekognition operations that track
--   people's paths return an array of <tt>PersonDetection</tt> objects
--   with elements for each time a person's path is tracked in a video.
--   
--   For more information, see GetPersonTracking in the Amazon Rekognition
--   Developer Guide.
--   
--   <i>See:</i> <a>newPersonDetection</a> smart constructor.
data PersonDetection
PersonDetection' :: Maybe PersonDetail -> Maybe Integer -> PersonDetection

-- | Create a value of <a>PersonDetection</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:person:PersonDetection'</a>, <a>personDetection_person</a> -
--   Details about a person whose path was tracked in a video.
--   
--   <a>$sel:timestamp:PersonDetection'</a>,
--   <a>personDetection_timestamp</a> - The time, in milliseconds from the
--   start of the video, that the person's path was tracked.
newPersonDetection :: PersonDetection

-- | Information about a person whose face matches a face(s) in an Amazon
--   Rekognition collection. Includes information about the faces in the
--   Amazon Rekognition collection (FaceMatch), information about the
--   person (PersonDetail), and the time stamp for when the person was
--   detected in a video. An array of <tt>PersonMatch</tt> objects is
--   returned by GetFaceSearch.
--   
--   <i>See:</i> <a>newPersonMatch</a> smart constructor.
data PersonMatch
PersonMatch' :: Maybe [FaceMatch] -> Maybe PersonDetail -> Maybe Integer -> PersonMatch

-- | Create a value of <a>PersonMatch</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:faceMatches:PersonMatch'</a>, <a>personMatch_faceMatches</a> -
--   Information about the faces in the input collection that match the
--   face of a person in the video.
--   
--   <a>$sel:person:PersonMatch'</a>, <a>personMatch_person</a> -
--   Information about the matched person.
--   
--   <a>$sel:timestamp:PersonMatch'</a>, <a>personMatch_timestamp</a> - The
--   time, in milliseconds from the beginning of the video, that the person
--   was matched in the video.
newPersonMatch :: PersonMatch

-- | The X and Y coordinates of a point on an image. The X and Y values
--   returned are ratios of the overall image size. For example, if the
--   input image is 700x200 and the operation returns X=0.5 and Y=0.25,
--   then the point is at the (350,50) pixel coordinate on the image.
--   
--   An array of <tt>Point</tt> objects, <tt>Polygon</tt>, is returned by
--   DetectText and by DetectCustomLabels. <tt>Polygon</tt> represents a
--   fine-grained polygon around a detected item. For more information, see
--   Geometry in the Amazon Rekognition Developer Guide.
--   
--   <i>See:</i> <a>newPoint</a> smart constructor.
data Point
Point' :: Maybe Double -> Maybe Double -> Point

-- | Create a value of <a>Point</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:x:Point'</a>, <a>point_x</a> - The value of the X coordinate
--   for a point on a <tt>Polygon</tt>.
--   
--   <a>$sel:y:Point'</a>, <a>point_y</a> - The value of the Y coordinate
--   for a point on a <tt>Polygon</tt>.
newPoint :: Point

-- | Indicates the pose of the face as determined by its pitch, roll, and
--   yaw.
--   
--   <i>See:</i> <a>newPose</a> smart constructor.
data Pose
Pose' :: Maybe Double -> Maybe Double -> Maybe Double -> Pose

-- | Create a value of <a>Pose</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:yaw:Pose'</a>, <a>pose_yaw</a> - Value representing the face
--   rotation on the yaw axis.
--   
--   <a>$sel:roll:Pose'</a>, <a>pose_roll</a> - Value representing the face
--   rotation on the roll axis.
--   
--   <a>$sel:pitch:Pose'</a>, <a>pose_pitch</a> - Value representing the
--   face rotation on the pitch axis.
newPose :: Pose

-- | A description of a Amazon Rekognition Custom Labels project.
--   
--   <i>See:</i> <a>newProjectDescription</a> smart constructor.
data ProjectDescription
ProjectDescription' :: Maybe ProjectStatus -> Maybe POSIX -> Maybe Text -> ProjectDescription

-- | Create a value of <a>ProjectDescription</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:status:ProjectDescription'</a>,
--   <a>projectDescription_status</a> - The current status of the project.
--   
--   <a>$sel:creationTimestamp:ProjectDescription'</a>,
--   <a>projectDescription_creationTimestamp</a> - The Unix timestamp for
--   the date and time that the project was created.
--   
--   <a>$sel:projectArn:ProjectDescription'</a>,
--   <a>projectDescription_projectArn</a> - The Amazon Resource Name (ARN)
--   of the project.
newProjectDescription :: ProjectDescription

-- | The description of a version of a model.
--   
--   <i>See:</i> <a>newProjectVersionDescription</a> smart constructor.
data ProjectVersionDescription
ProjectVersionDescription' :: Maybe Natural -> Maybe ProjectVersionStatus -> Maybe EvaluationResult -> Maybe GroundTruthManifest -> Maybe Text -> Maybe TestingDataResult -> Maybe Text -> Maybe POSIX -> Maybe Text -> Maybe OutputConfig -> Maybe Natural -> Maybe POSIX -> Maybe TrainingDataResult -> ProjectVersionDescription

-- | Create a value of <a>ProjectVersionDescription</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:minInferenceUnits:ProjectVersionDescription'</a>,
--   <a>projectVersionDescription_minInferenceUnits</a> - The minimum
--   number of inference units used by the model. For more information, see
--   StartProjectVersion.
--   
--   <a>$sel:status:ProjectVersionDescription'</a>,
--   <a>projectVersionDescription_status</a> - The current status of the
--   model version.
--   
--   <a>$sel:evaluationResult:ProjectVersionDescription'</a>,
--   <a>projectVersionDescription_evaluationResult</a> - The training
--   results. <tt>EvaluationResult</tt> is only returned if training is
--   successful.
--   
--   <a>$sel:manifestSummary:ProjectVersionDescription'</a>,
--   <a>projectVersionDescription_manifestSummary</a> - The location of the
--   summary manifest. The summary manifest provides aggregate data
--   validation results for the training and test datasets.
--   
--   <a>$sel:kmsKeyId:ProjectVersionDescription'</a>,
--   <a>projectVersionDescription_kmsKeyId</a> - The identifer for the AWS
--   Key Management Service (AWS KMS) customer master key that was used to
--   encrypt the model during training.
--   
--   <a>$sel:testingDataResult:ProjectVersionDescription'</a>,
--   <a>projectVersionDescription_testingDataResult</a> - Contains
--   information about the testing results.
--   
--   <a>$sel:statusMessage:ProjectVersionDescription'</a>,
--   <a>projectVersionDescription_statusMessage</a> - A descriptive message
--   for an error or warning that occurred.
--   
--   <a>$sel:creationTimestamp:ProjectVersionDescription'</a>,
--   <a>projectVersionDescription_creationTimestamp</a> - The Unix datetime
--   for the date and time that training started.
--   
--   <a>$sel:projectVersionArn:ProjectVersionDescription'</a>,
--   <a>projectVersionDescription_projectVersionArn</a> - The Amazon
--   Resource Name (ARN) of the model version.
--   
--   <a>$sel:outputConfig:ProjectVersionDescription'</a>,
--   <a>projectVersionDescription_outputConfig</a> - The location where
--   training results are saved.
--   
--   <a>$sel:billableTrainingTimeInSeconds:ProjectVersionDescription'</a>,
--   <a>projectVersionDescription_billableTrainingTimeInSeconds</a> - The
--   duration, in seconds, that the model version has been billed for
--   training. This value is only returned if the model version has been
--   successfully trained.
--   
--   <a>$sel:trainingEndTimestamp:ProjectVersionDescription'</a>,
--   <a>projectVersionDescription_trainingEndTimestamp</a> - The Unix date
--   and time that training of the model ended.
--   
--   <a>$sel:trainingDataResult:ProjectVersionDescription'</a>,
--   <a>projectVersionDescription_trainingDataResult</a> - Contains
--   information about the training results.
newProjectVersionDescription :: ProjectVersionDescription

-- | Information about a body part detected by DetectProtectiveEquipment
--   that contains PPE. An array of <tt>ProtectiveEquipmentBodyPart</tt>
--   objects is returned for each person detected by
--   <tt>DetectProtectiveEquipment</tt>.
--   
--   <i>See:</i> <a>newProtectiveEquipmentBodyPart</a> smart constructor.
data ProtectiveEquipmentBodyPart
ProtectiveEquipmentBodyPart' :: Maybe [EquipmentDetection] -> Maybe Double -> Maybe BodyPart -> ProtectiveEquipmentBodyPart

-- | Create a value of <a>ProtectiveEquipmentBodyPart</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:equipmentDetections:ProtectiveEquipmentBodyPart'</a>,
--   <a>protectiveEquipmentBodyPart_equipmentDetections</a> - An array of
--   Personal Protective Equipment items detected around a body part.
--   
--   <a>$sel:confidence:ProtectiveEquipmentBodyPart'</a>,
--   <a>protectiveEquipmentBodyPart_confidence</a> - The confidence that
--   Amazon Rekognition has in the detection accuracy of the detected body
--   part.
--   
--   <a>$sel:name:ProtectiveEquipmentBodyPart'</a>,
--   <a>protectiveEquipmentBodyPart_name</a> - The detected body part.
newProtectiveEquipmentBodyPart :: ProtectiveEquipmentBodyPart

-- | A person detected by a call to DetectProtectiveEquipment. The API
--   returns all persons detected in the input image in an array of
--   <tt>ProtectiveEquipmentPerson</tt> objects.
--   
--   <i>See:</i> <a>newProtectiveEquipmentPerson</a> smart constructor.
data ProtectiveEquipmentPerson
ProtectiveEquipmentPerson' :: Maybe [ProtectiveEquipmentBodyPart] -> Maybe BoundingBox -> Maybe Double -> Maybe Natural -> ProtectiveEquipmentPerson

-- | Create a value of <a>ProtectiveEquipmentPerson</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:bodyParts:ProtectiveEquipmentPerson'</a>,
--   <a>protectiveEquipmentPerson_bodyParts</a> - An array of body parts
--   detected on a person's body (including body parts without PPE).
--   
--   <a>$sel:boundingBox:ProtectiveEquipmentPerson'</a>,
--   <a>protectiveEquipmentPerson_boundingBox</a> - A bounding box around
--   the detected person.
--   
--   <a>$sel:confidence:ProtectiveEquipmentPerson'</a>,
--   <a>protectiveEquipmentPerson_confidence</a> - The confidence that
--   Amazon Rekognition has that the bounding box contains a person.
--   
--   <a>$sel:id:ProtectiveEquipmentPerson'</a>,
--   <a>protectiveEquipmentPerson_id</a> - The identifier for the detected
--   person. The identifier is only unique for a single call to
--   <tt>DetectProtectiveEquipment</tt>.
newProtectiveEquipmentPerson :: ProtectiveEquipmentPerson

-- | Specifies summary attributes to return from a call to
--   DetectProtectiveEquipment. You can specify which types of PPE to
--   summarize. You can also specify a minimum confidence value for
--   detections. Summary information is returned in the <tt>Summary</tt>
--   (ProtectiveEquipmentSummary) field of the response from
--   <tt>DetectProtectiveEquipment</tt>. The summary includes which persons
--   in an image were detected wearing the requested types of person
--   protective equipment (PPE), which persons were detected as not wearing
--   PPE, and the persons in which a determination could not be made. For
--   more information, see ProtectiveEquipmentSummary.
--   
--   <i>See:</i> <a>newProtectiveEquipmentSummarizationAttributes</a> smart
--   constructor.
data ProtectiveEquipmentSummarizationAttributes
ProtectiveEquipmentSummarizationAttributes' :: Double -> [ProtectiveEquipmentType] -> ProtectiveEquipmentSummarizationAttributes

-- | Create a value of <a>ProtectiveEquipmentSummarizationAttributes</a>
--   with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:minConfidence:ProtectiveEquipmentSummarizationAttributes'</a>,
--   <a>protectiveEquipmentSummarizationAttributes_minConfidence</a> - The
--   minimum confidence level for which you want summary information. The
--   confidence level applies to person detection, body part detection,
--   equipment detection, and body part coverage. Amazon Rekognition
--   doesn't return summary information with a confidence than this
--   specified value. There isn't a default value.
--   
--   Specify a <tt>MinConfidence</tt> value that is between 50-100% as
--   <tt>DetectProtectiveEquipment</tt> returns predictions only where the
--   detection confidence is between 50% - 100%. If you specify a value
--   that is less than 50%, the results are the same specifying a value of
--   50%.
--   
--   
--   <a>$sel:requiredEquipmentTypes:ProtectiveEquipmentSummarizationAttributes'</a>,
--   <a>protectiveEquipmentSummarizationAttributes_requiredEquipmentTypes</a>
--   - An array of personal protective equipment types for which you want
--   summary information. If a person is detected wearing a required
--   requipment type, the person's ID is added to the
--   <tt>PersonsWithRequiredEquipment</tt> array field returned in
--   ProtectiveEquipmentSummary by <tt>DetectProtectiveEquipment</tt>.
newProtectiveEquipmentSummarizationAttributes :: Double -> ProtectiveEquipmentSummarizationAttributes

-- | Summary information for required items of personal protective
--   equipment (PPE) detected on persons by a call to
--   DetectProtectiveEquipment. You specify the required type of PPE in the
--   <tt>SummarizationAttributes</tt>
--   (ProtectiveEquipmentSummarizationAttributes) input parameter. The
--   summary includes which persons were detected wearing the required
--   personal protective equipment (<tt>PersonsWithRequiredEquipment</tt>),
--   which persons were detected as not wearing the required PPE
--   (<tt>PersonsWithoutRequiredEquipment</tt>), and the persons in which a
--   determination could not be made (<tt>PersonsIndeterminate</tt>).
--   
--   To get a total for each category, use the size of the field array. For
--   example, to find out how many people were detected as wearing the
--   specified PPE, use the size of the
--   <tt>PersonsWithRequiredEquipment</tt> array. If you want to find out
--   more about a person, such as the location (BoundingBox) of the person
--   on the image, use the person ID in each array element. Each person ID
--   matches the ID field of a ProtectiveEquipmentPerson object returned in
--   the <tt>Persons</tt> array by <tt>DetectProtectiveEquipment</tt>.
--   
--   <i>See:</i> <a>newProtectiveEquipmentSummary</a> smart constructor.
data ProtectiveEquipmentSummary
ProtectiveEquipmentSummary' :: Maybe [Natural] -> Maybe [Natural] -> Maybe [Natural] -> ProtectiveEquipmentSummary

-- | Create a value of <a>ProtectiveEquipmentSummary</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:personsWithRequiredEquipment:ProtectiveEquipmentSummary'</a>,
--   <a>protectiveEquipmentSummary_personsWithRequiredEquipment</a> - An
--   array of IDs for persons who are wearing detected personal protective
--   equipment.
--   
--   
--   <a>$sel:personsWithoutRequiredEquipment:ProtectiveEquipmentSummary'</a>,
--   <a>protectiveEquipmentSummary_personsWithoutRequiredEquipment</a> - An
--   array of IDs for persons who are not wearing all of the types of PPE
--   specified in the <tt>RequiredEquipmentTypes</tt> field of the detected
--   personal protective equipment.
--   
--   <a>$sel:personsIndeterminate:ProtectiveEquipmentSummary'</a>,
--   <a>protectiveEquipmentSummary_personsIndeterminate</a> - An array of
--   IDs for persons where it was not possible to determine if they are
--   wearing personal protective equipment.
newProtectiveEquipmentSummary :: ProtectiveEquipmentSummary

-- | Specifies a location within the frame that Rekognition checks for
--   text. Uses a <tt>BoundingBox</tt> object to set a region of the
--   screen.
--   
--   A word is included in the region if the word is more than half in that
--   region. If there is more than one region, the word will be compared
--   with all regions of the screen. Any word more than half in a region is
--   kept in the results.
--   
--   <i>See:</i> <a>newRegionOfInterest</a> smart constructor.
data RegionOfInterest
RegionOfInterest' :: Maybe BoundingBox -> RegionOfInterest

-- | Create a value of <a>RegionOfInterest</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:boundingBox:RegionOfInterest'</a>,
--   <a>regionOfInterest_boundingBox</a> - The box representing a region of
--   interest on screen.
newRegionOfInterest :: RegionOfInterest

-- | Provides the S3 bucket name and object name.
--   
--   The region for the S3 bucket containing the S3 object must match the
--   region you use for Amazon Rekognition operations.
--   
--   For Amazon Rekognition to process an S3 object, the user must have
--   permission to access the S3 object. For more information, see
--   Resource-Based Policies in the Amazon Rekognition Developer Guide.
--   
--   <i>See:</i> <a>newS3Object</a> smart constructor.
data S3Object
S3Object' :: Maybe Text -> Maybe Text -> Maybe Text -> S3Object

-- | Create a value of <a>S3Object</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:bucket:S3Object'</a>, <a>s3Object_bucket</a> - Name of the S3
--   bucket.
--   
--   <a>$sel:name:S3Object'</a>, <a>s3Object_name</a> - S3 object key name.
--   
--   <a>$sel:version:S3Object'</a>, <a>s3Object_version</a> - If the bucket
--   is versioning enabled, you can specify the object version.
newS3Object :: S3Object

-- | A technical cue or shot detection segment detected in a video. An
--   array of <tt>SegmentDetection</tt> objects containing all segments
--   detected in a stored video is returned by GetSegmentDetection.
--   
--   <i>See:</i> <a>newSegmentDetection</a> smart constructor.
data SegmentDetection
SegmentDetection' :: Maybe TechnicalCueSegment -> Maybe Natural -> Maybe Text -> Maybe Integer -> Maybe Text -> Maybe Text -> Maybe Natural -> Maybe Natural -> Maybe Integer -> Maybe SegmentType -> Maybe ShotSegment -> Maybe Natural -> SegmentDetection

-- | Create a value of <a>SegmentDetection</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:technicalCueSegment:SegmentDetection'</a>,
--   <a>segmentDetection_technicalCueSegment</a> - If the segment is a
--   technical cue, contains information about the technical cue.
--   
--   <a>$sel:endFrameNumber:SegmentDetection'</a>,
--   <a>segmentDetection_endFrameNumber</a> - The frame number at the end
--   of a video segment, using a frame index that starts with 0.
--   
--   <a>$sel:durationSMPTE:SegmentDetection'</a>,
--   <a>segmentDetection_durationSMPTE</a> - The duration of the timecode
--   for the detected segment in SMPTE format.
--   
--   <a>$sel:endTimestampMillis:SegmentDetection'</a>,
--   <a>segmentDetection_endTimestampMillis</a> - The end time of the
--   detected segment, in milliseconds, from the start of the video. This
--   value is rounded down.
--   
--   <a>$sel:startTimecodeSMPTE:SegmentDetection'</a>,
--   <a>segmentDetection_startTimecodeSMPTE</a> - The frame-accurate SMPTE
--   timecode, from the start of a video, for the start of a detected
--   segment. <tt>StartTimecode</tt> is in <i>HH:MM:SS:fr</i> format (and
--   <i>;fr</i> for drop frame-rates).
--   
--   <a>$sel:endTimecodeSMPTE:SegmentDetection'</a>,
--   <a>segmentDetection_endTimecodeSMPTE</a> - The frame-accurate SMPTE
--   timecode, from the start of a video, for the end of a detected
--   segment. <tt>EndTimecode</tt> is in <i>HH:MM:SS:fr</i> format (and
--   <i>;fr</i> for drop frame-rates).
--   
--   <a>$sel:durationMillis:SegmentDetection'</a>,
--   <a>segmentDetection_durationMillis</a> - The duration of the detected
--   segment in milliseconds.
--   
--   <a>$sel:durationFrames:SegmentDetection'</a>,
--   <a>segmentDetection_durationFrames</a> - The duration of a video
--   segment, expressed in frames.
--   
--   <a>$sel:startTimestampMillis:SegmentDetection'</a>,
--   <a>segmentDetection_startTimestampMillis</a> - The start time of the
--   detected segment in milliseconds from the start of the video. This
--   value is rounded down. For example, if the actual timestamp is
--   100.6667 milliseconds, Amazon Rekognition Video returns a value of 100
--   millis.
--   
--   <a>$sel:type':SegmentDetection'</a>, <a>segmentDetection_type</a> -
--   The type of the segment. Valid values are <tt>TECHNICAL_CUE</tt> and
--   <tt>SHOT</tt>.
--   
--   <a>$sel:shotSegment:SegmentDetection'</a>,
--   <a>segmentDetection_shotSegment</a> - If the segment is a shot
--   detection, contains information about the shot detection.
--   
--   <a>$sel:startFrameNumber:SegmentDetection'</a>,
--   <a>segmentDetection_startFrameNumber</a> - The frame number of the
--   start of a video segment, using a frame index that starts with 0.
newSegmentDetection :: SegmentDetection

-- | Information about the type of a segment requested in a call to
--   StartSegmentDetection. An array of <tt>SegmentTypeInfo</tt> objects is
--   returned by the response from GetSegmentDetection.
--   
--   <i>See:</i> <a>newSegmentTypeInfo</a> smart constructor.
data SegmentTypeInfo
SegmentTypeInfo' :: Maybe Text -> Maybe SegmentType -> SegmentTypeInfo

-- | Create a value of <a>SegmentTypeInfo</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:modelVersion:SegmentTypeInfo'</a>,
--   <a>segmentTypeInfo_modelVersion</a> - The version of the model used to
--   detect segments.
--   
--   <a>$sel:type':SegmentTypeInfo'</a>, <a>segmentTypeInfo_type</a> - The
--   type of a segment (technical cue or shot detection).
newSegmentTypeInfo :: SegmentTypeInfo

-- | Information about a shot detection segment detected in a video. For
--   more information, see SegmentDetection.
--   
--   <i>See:</i> <a>newShotSegment</a> smart constructor.
data ShotSegment
ShotSegment' :: Maybe Double -> Maybe Natural -> ShotSegment

-- | Create a value of <a>ShotSegment</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:confidence:ShotSegment'</a>, <a>shotSegment_confidence</a> -
--   The confidence that Amazon Rekognition Video has in the accuracy of
--   the detected segment.
--   
--   <a>$sel:index:ShotSegment'</a>, <a>shotSegment_index</a> - An
--   Identifier for a shot detection segment detected in a video.
newShotSegment :: ShotSegment

-- | Indicates whether or not the face is smiling, and the confidence level
--   in the determination.
--   
--   <i>See:</i> <a>newSmile</a> smart constructor.
data Smile
Smile' :: Maybe Bool -> Maybe Double -> Smile

-- | Create a value of <a>Smile</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:value:Smile'</a>, <a>smile_value</a> - Boolean value that
--   indicates whether the face is smiling or not.
--   
--   <a>$sel:confidence:Smile'</a>, <a>smile_confidence</a> - Level of
--   confidence in the determination.
newSmile :: Smile

-- | Filters applied to the technical cue or shot detection segments. For
--   more information, see StartSegmentDetection.
--   
--   <i>See:</i> <a>newStartSegmentDetectionFilters</a> smart constructor.
data StartSegmentDetectionFilters
StartSegmentDetectionFilters' :: Maybe StartTechnicalCueDetectionFilter -> Maybe StartShotDetectionFilter -> StartSegmentDetectionFilters

-- | Create a value of <a>StartSegmentDetectionFilters</a> with all
--   optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:technicalCueFilter:StartSegmentDetectionFilters'</a>,
--   <a>startSegmentDetectionFilters_technicalCueFilter</a> - Filters that
--   are specific to technical cues.
--   
--   <a>$sel:shotFilter:StartSegmentDetectionFilters'</a>,
--   <a>startSegmentDetectionFilters_shotFilter</a> - Filters that are
--   specific to shot detections.
newStartSegmentDetectionFilters :: StartSegmentDetectionFilters

-- | Filters for the shot detection segments returned by
--   <tt>GetSegmentDetection</tt>. For more information, see
--   StartSegmentDetectionFilters.
--   
--   <i>See:</i> <a>newStartShotDetectionFilter</a> smart constructor.
data StartShotDetectionFilter
StartShotDetectionFilter' :: Maybe Double -> StartShotDetectionFilter

-- | Create a value of <a>StartShotDetectionFilter</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:minSegmentConfidence:StartShotDetectionFilter'</a>,
--   <a>startShotDetectionFilter_minSegmentConfidence</a> - Specifies the
--   minimum confidence that Amazon Rekognition Video must have in order to
--   return a detected segment. Confidence represents how certain Amazon
--   Rekognition is that a segment is correctly identified. 0 is the lowest
--   confidence. 100 is the highest confidence. Amazon Rekognition Video
--   doesn't return any segments with a confidence level lower than this
--   specified value.
--   
--   If you don't specify <tt>MinSegmentConfidence</tt>, the
--   <tt>GetSegmentDetection</tt> returns segments with confidence values
--   greater than or equal to 50 percent.
newStartShotDetectionFilter :: StartShotDetectionFilter

-- | Filters for the technical segments returned by GetSegmentDetection.
--   For more information, see StartSegmentDetectionFilters.
--   
--   <i>See:</i> <a>newStartTechnicalCueDetectionFilter</a> smart
--   constructor.
data StartTechnicalCueDetectionFilter
StartTechnicalCueDetectionFilter' :: Maybe BlackFrame -> Maybe Double -> StartTechnicalCueDetectionFilter

-- | Create a value of <a>StartTechnicalCueDetectionFilter</a> with all
--   optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:blackFrame:StartTechnicalCueDetectionFilter'</a>,
--   <a>startTechnicalCueDetectionFilter_blackFrame</a> - A filter that
--   allows you to control the black frame detection by specifying the
--   black levels and pixel coverage of black pixels in a frame. Videos can
--   come from multiple sources, formats, and time periods, with different
--   standards and varying noise levels for black frames that need to be
--   accounted for.
--   
--   <a>$sel:minSegmentConfidence:StartTechnicalCueDetectionFilter'</a>,
--   <a>startTechnicalCueDetectionFilter_minSegmentConfidence</a> -
--   Specifies the minimum confidence that Amazon Rekognition Video must
--   have in order to return a detected segment. Confidence represents how
--   certain Amazon Rekognition is that a segment is correctly identified.
--   0 is the lowest confidence. 100 is the highest confidence. Amazon
--   Rekognition Video doesn't return any segments with a confidence level
--   lower than this specified value.
--   
--   If you don't specify <tt>MinSegmentConfidence</tt>,
--   <tt>GetSegmentDetection</tt> returns segments with confidence values
--   greater than or equal to 50 percent.
newStartTechnicalCueDetectionFilter :: StartTechnicalCueDetectionFilter

-- | Set of optional parameters that let you set the criteria text must
--   meet to be included in your response. <tt>WordFilter</tt> looks at a
--   word's height, width and minimum confidence. <tt>RegionOfInterest</tt>
--   lets you set a specific region of the screen to look for text in.
--   
--   <i>See:</i> <a>newStartTextDetectionFilters</a> smart constructor.
data StartTextDetectionFilters
StartTextDetectionFilters' :: Maybe [RegionOfInterest] -> Maybe DetectionFilter -> StartTextDetectionFilters

-- | Create a value of <a>StartTextDetectionFilters</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:regionsOfInterest:StartTextDetectionFilters'</a>,
--   <a>startTextDetectionFilters_regionsOfInterest</a> - Filter focusing
--   on a certain area of the frame. Uses a <tt>BoundingBox</tt> object to
--   set the region of the screen.
--   
--   <a>$sel:wordFilter:StartTextDetectionFilters'</a>,
--   <a>startTextDetectionFilters_wordFilter</a> - Filters focusing on
--   qualities of the text, such as confidence or size.
newStartTextDetectionFilters :: StartTextDetectionFilters

-- | An object that recognizes faces in a streaming video. An Amazon
--   Rekognition stream processor is created by a call to
--   CreateStreamProcessor. The request parameters for
--   <tt>CreateStreamProcessor</tt> describe the Kinesis video stream
--   source for the streaming video, face recognition parameters, and where
--   to stream the analysis resullts.
--   
--   <i>See:</i> <a>newStreamProcessor</a> smart constructor.
data StreamProcessor
StreamProcessor' :: Maybe StreamProcessorStatus -> Maybe Text -> StreamProcessor

-- | Create a value of <a>StreamProcessor</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:status:StreamProcessor'</a>, <a>streamProcessor_status</a> -
--   Current status of the Amazon Rekognition stream processor.
--   
--   <a>$sel:name:StreamProcessor'</a>, <a>streamProcessor_name</a> - Name
--   of the Amazon Rekognition stream processor.
newStreamProcessor :: StreamProcessor

-- | Information about the source streaming video.
--   
--   <i>See:</i> <a>newStreamProcessorInput</a> smart constructor.
data StreamProcessorInput
StreamProcessorInput' :: Maybe KinesisVideoStream -> StreamProcessorInput

-- | Create a value of <a>StreamProcessorInput</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:kinesisVideoStream:StreamProcessorInput'</a>,
--   <a>streamProcessorInput_kinesisVideoStream</a> - The Kinesis video
--   stream input stream for the source streaming video.
newStreamProcessorInput :: StreamProcessorInput

-- | Information about the Amazon Kinesis Data Streams stream to which a
--   Amazon Rekognition Video stream processor streams the results of a
--   video analysis. For more information, see CreateStreamProcessor in the
--   Amazon Rekognition Developer Guide.
--   
--   <i>See:</i> <a>newStreamProcessorOutput</a> smart constructor.
data StreamProcessorOutput
StreamProcessorOutput' :: Maybe KinesisDataStream -> StreamProcessorOutput

-- | Create a value of <a>StreamProcessorOutput</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:kinesisDataStream:StreamProcessorOutput'</a>,
--   <a>streamProcessorOutput_kinesisDataStream</a> - The Amazon Kinesis
--   Data Streams stream to which the Amazon Rekognition stream processor
--   streams the analysis results.
newStreamProcessorOutput :: StreamProcessorOutput

-- | Input parameters used to recognize faces in a streaming video analyzed
--   by a Amazon Rekognition stream processor.
--   
--   <i>See:</i> <a>newStreamProcessorSettings</a> smart constructor.
data StreamProcessorSettings
StreamProcessorSettings' :: Maybe FaceSearchSettings -> StreamProcessorSettings

-- | Create a value of <a>StreamProcessorSettings</a> with all optional
--   fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:faceSearch:StreamProcessorSettings'</a>,
--   <a>streamProcessorSettings_faceSearch</a> - Face search settings to
--   use on a streaming video.
newStreamProcessorSettings :: StreamProcessorSettings

-- | The S3 bucket that contains the training summary. The training summary
--   includes aggregated evaluation metrics for the entire testing dataset
--   and metrics for each individual label.
--   
--   You get the training summary S3 bucket location by calling
--   DescribeProjectVersions.
--   
--   <i>See:</i> <a>newSummary</a> smart constructor.
data Summary
Summary' :: Maybe S3Object -> Summary

-- | Create a value of <a>Summary</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:s3Object:Summary'</a>, <a>summary_s3Object</a> - Undocumented
--   member.
newSummary :: Summary

-- | Indicates whether or not the face is wearing sunglasses, and the
--   confidence level in the determination.
--   
--   <i>See:</i> <a>newSunglasses</a> smart constructor.
data Sunglasses
Sunglasses' :: Maybe Bool -> Maybe Double -> Sunglasses

-- | Create a value of <a>Sunglasses</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:value:Sunglasses'</a>, <a>sunglasses_value</a> - Boolean value
--   that indicates whether the face is wearing sunglasses or not.
--   
--   <a>$sel:confidence:Sunglasses'</a>, <a>sunglasses_confidence</a> -
--   Level of confidence in the determination.
newSunglasses :: Sunglasses

-- | Information about a technical cue segment. For more information, see
--   SegmentDetection.
--   
--   <i>See:</i> <a>newTechnicalCueSegment</a> smart constructor.
data TechnicalCueSegment
TechnicalCueSegment' :: Maybe Double -> Maybe TechnicalCueType -> TechnicalCueSegment

-- | Create a value of <a>TechnicalCueSegment</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:confidence:TechnicalCueSegment'</a>,
--   <a>technicalCueSegment_confidence</a> - The confidence that Amazon
--   Rekognition Video has in the accuracy of the detected segment.
--   
--   <a>$sel:type':TechnicalCueSegment'</a>,
--   <a>technicalCueSegment_type</a> - The type of the technical cue.
newTechnicalCueSegment :: TechnicalCueSegment

-- | The dataset used for testing. Optionally, if <tt>AutoCreate</tt> is
--   set, Amazon Rekognition Custom Labels creates a testing dataset using
--   an 80/20 split of the training dataset.
--   
--   <i>See:</i> <a>newTestingData</a> smart constructor.
data TestingData
TestingData' :: Maybe [Asset] -> Maybe Bool -> TestingData

-- | Create a value of <a>TestingData</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:assets:TestingData'</a>, <a>testingData_assets</a> - The
--   assets used for testing.
--   
--   <a>$sel:autoCreate:TestingData'</a>, <a>testingData_autoCreate</a> -
--   If specified, Amazon Rekognition Custom Labels creates a testing
--   dataset with an 80/20 split of the training dataset.
newTestingData :: TestingData

-- | Sagemaker Groundtruth format manifest files for the input, output and
--   validation datasets that are used and created during testing.
--   
--   <i>See:</i> <a>newTestingDataResult</a> smart constructor.
data TestingDataResult
TestingDataResult' :: Maybe TestingData -> Maybe TestingData -> Maybe ValidationData -> TestingDataResult

-- | Create a value of <a>TestingDataResult</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:input:TestingDataResult'</a>, <a>testingDataResult_input</a> -
--   The testing dataset that was supplied for training.
--   
--   <a>$sel:output:TestingDataResult'</a>, <a>testingDataResult_output</a>
--   - The subset of the dataset that was actually tested. Some images
--   (assets) might not be tested due to file formatting and other issues.
--   
--   <a>$sel:validation:TestingDataResult'</a>,
--   <a>testingDataResult_validation</a> - The location of the data
--   validation manifest. The data validation manifest is created for the
--   test dataset during model training.
newTestingDataResult :: TestingDataResult

-- | Information about a word or line of text detected by DetectText.
--   
--   The <tt>DetectedText</tt> field contains the text that Amazon
--   Rekognition detected in the image.
--   
--   Every word and line has an identifier (<tt>Id</tt>). Each word belongs
--   to a line and has a parent identifier (<tt>ParentId</tt>) that
--   identifies the line of text in which the word appears. The word
--   <tt>Id</tt> is also an index for the word within a line of words.
--   
--   For more information, see Detecting Text in the Amazon Rekognition
--   Developer Guide.
--   
--   <i>See:</i> <a>newTextDetection</a> smart constructor.
data TextDetection
TextDetection' :: Maybe Text -> Maybe Double -> Maybe Geometry -> Maybe Natural -> Maybe TextTypes -> Maybe Natural -> TextDetection

-- | Create a value of <a>TextDetection</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:detectedText:TextDetection'</a>,
--   <a>textDetection_detectedText</a> - The word or line of text
--   recognized by Amazon Rekognition.
--   
--   <a>$sel:confidence:TextDetection'</a>, <a>textDetection_confidence</a>
--   - The confidence that Amazon Rekognition has in the accuracy of the
--   detected text and the accuracy of the geometry points around the
--   detected text.
--   
--   <a>$sel:geometry:TextDetection'</a>, <a>textDetection_geometry</a> -
--   The location of the detected text on the image. Includes an axis
--   aligned coarse bounding box surrounding the text and a finer grain
--   polygon for more accurate spatial information.
--   
--   <a>$sel:id:TextDetection'</a>, <a>textDetection_id</a> - The
--   identifier for the detected text. The identifier is only unique for a
--   single call to <tt>DetectText</tt>.
--   
--   <a>$sel:type':TextDetection'</a>, <a>textDetection_type</a> - The type
--   of text that was detected.
--   
--   <a>$sel:parentId:TextDetection'</a>, <a>textDetection_parentId</a> -
--   The Parent identifier for the detected text identified by the value of
--   <tt>ID</tt>. If the type of detected text is <tt>LINE</tt>, the value
--   of <tt>ParentId</tt> is <tt>Null</tt>.
newTextDetection :: TextDetection

-- | Information about text detected in a video. Incudes the detected text,
--   the time in milliseconds from the start of the video that the text was
--   detected, and where it was detected on the screen.
--   
--   <i>See:</i> <a>newTextDetectionResult</a> smart constructor.
data TextDetectionResult
TextDetectionResult' :: Maybe TextDetection -> Maybe Integer -> TextDetectionResult

-- | Create a value of <a>TextDetectionResult</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:textDetection:TextDetectionResult'</a>,
--   <a>textDetectionResult_textDetection</a> - Details about text detected
--   in a video.
--   
--   <a>$sel:timestamp:TextDetectionResult'</a>,
--   <a>textDetectionResult_timestamp</a> - The time, in milliseconds from
--   the start of the video, that the text was detected.
newTextDetectionResult :: TextDetectionResult

-- | The dataset used for training.
--   
--   <i>See:</i> <a>newTrainingData</a> smart constructor.
data TrainingData
TrainingData' :: Maybe [Asset] -> TrainingData

-- | Create a value of <a>TrainingData</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:assets:TrainingData'</a>, <a>trainingData_assets</a> - A
--   Sagemaker GroundTruth manifest file that contains the training images
--   (assets).
newTrainingData :: TrainingData

-- | Sagemaker Groundtruth format manifest files for the input, output and
--   validation datasets that are used and created during testing.
--   
--   <i>See:</i> <a>newTrainingDataResult</a> smart constructor.
data TrainingDataResult
TrainingDataResult' :: Maybe TrainingData -> Maybe TrainingData -> Maybe ValidationData -> TrainingDataResult

-- | Create a value of <a>TrainingDataResult</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:input:TrainingDataResult'</a>, <a>trainingDataResult_input</a>
--   - The training assets that you supplied for training.
--   
--   <a>$sel:output:TrainingDataResult'</a>,
--   <a>trainingDataResult_output</a> - The images (assets) that were
--   actually trained by Amazon Rekognition Custom Labels.
--   
--   <a>$sel:validation:TrainingDataResult'</a>,
--   <a>trainingDataResult_validation</a> - The location of the data
--   validation manifest. The data validation manifest is created for the
--   training dataset during model training.
newTrainingDataResult :: TrainingDataResult

-- | A face that IndexFaces detected, but didn't index. Use the
--   <tt>Reasons</tt> response attribute to determine why a face wasn't
--   indexed.
--   
--   <i>See:</i> <a>newUnindexedFace</a> smart constructor.
data UnindexedFace
UnindexedFace' :: Maybe [Reason] -> Maybe FaceDetail -> UnindexedFace

-- | Create a value of <a>UnindexedFace</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:reasons:UnindexedFace'</a>, <a>unindexedFace_reasons</a> - An
--   array of reasons that specify why a face wasn't indexed.
--   
--   <ul>
--   <li>EXTREME_POSE - The face is at a pose that can't be detected. For
--   example, the head is turned too far away from the camera.</li>
--   <li>EXCEEDS_MAX_FACES - The number of faces detected is already higher
--   than that specified by the <tt>MaxFaces</tt> input parameter for
--   <tt>IndexFaces</tt>.</li>
--   <li>LOW_BRIGHTNESS - The image is too dark.</li>
--   <li>LOW_SHARPNESS - The image is too blurry.</li>
--   <li>LOW_CONFIDENCE - The face was detected with a low confidence.</li>
--   <li>SMALL_BOUNDING_BOX - The bounding box around the face is too
--   small.</li>
--   </ul>
--   
--   <a>$sel:faceDetail:UnindexedFace'</a>, <a>unindexedFace_faceDetail</a>
--   - The structure that contains attributes of a face that
--   <tt>IndexFaces</tt>detected, but didn't index.
newUnindexedFace :: UnindexedFace

-- | Contains the Amazon S3 bucket location of the validation data for a
--   model training job.
--   
--   The validation data includes error information for individual JSON
--   lines in the dataset. For more information, see Debugging a Failed
--   Model Training in the Amazon Rekognition Custom Labels Developer
--   Guide.
--   
--   You get the <tt>ValidationData</tt> object for the training dataset
--   (TrainingDataResult) and the test dataset (TestingDataResult) by
--   calling DescribeProjectVersions.
--   
--   The assets array contains a single Asset object. The
--   GroundTruthManifest field of the Asset object contains the S3 bucket
--   location of the validation data.
--   
--   <i>See:</i> <a>newValidationData</a> smart constructor.
data ValidationData
ValidationData' :: Maybe [Asset] -> ValidationData

-- | Create a value of <a>ValidationData</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:assets:ValidationData'</a>, <a>validationData_assets</a> - The
--   assets that comprise the validation data.
newValidationData :: ValidationData

-- | Video file stored in an Amazon S3 bucket. Amazon Rekognition video
--   start operations such as StartLabelDetection use <tt>Video</tt> to
--   specify a video for analysis. The supported file formats are .mp4,
--   .mov and .avi.
--   
--   <i>See:</i> <a>newVideo</a> smart constructor.
data Video
Video' :: Maybe S3Object -> Video

-- | Create a value of <a>Video</a> with all optional fields omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:s3Object:Video'</a>, <a>video_s3Object</a> - The Amazon S3
--   bucket name and file name for the video.
newVideo :: Video

-- | Information about a video that Amazon Rekognition analyzed.
--   <tt>Videometadata</tt> is returned in every page of paginated
--   responses from a Amazon Rekognition video operation.
--   
--   <i>See:</i> <a>newVideoMetadata</a> smart constructor.
data VideoMetadata
VideoMetadata' :: Maybe Double -> Maybe VideoColorRange -> Maybe Text -> Maybe Text -> Maybe Natural -> Maybe Natural -> Maybe Natural -> VideoMetadata

-- | Create a value of <a>VideoMetadata</a> with all optional fields
--   omitted.
--   
--   Use <a>generic-lens</a> or <a>optics</a> to modify other optional
--   fields.
--   
--   The following record fields are available, with the corresponding
--   lenses provided for backwards compatibility:
--   
--   <a>$sel:frameRate:VideoMetadata'</a>, <a>videoMetadata_frameRate</a> -
--   Number of frames per second in the video.
--   
--   <a>$sel:colorRange:VideoMetadata'</a>, <a>videoMetadata_colorRange</a>
--   - A description of the range of luminance values in a video, either
--   LIMITED (16 to 235) or FULL (0 to 255).
--   
--   <a>$sel:format:VideoMetadata'</a>, <a>videoMetadata_format</a> -
--   Format of the analyzed video. Possible values are MP4, MOV and AVI.
--   
--   <a>$sel:codec:VideoMetadata'</a>, <a>videoMetadata_codec</a> - Type of
--   compression used in the analyzed video.
--   
--   <a>$sel:frameHeight:VideoMetadata'</a>,
--   <a>videoMetadata_frameHeight</a> - Vertical pixel dimension of the
--   video.
--   
--   <a>$sel:durationMillis:VideoMetadata'</a>,
--   <a>videoMetadata_durationMillis</a> - Length of the video in
--   milliseconds.
--   
--   <a>$sel:frameWidth:VideoMetadata'</a>, <a>videoMetadata_frameWidth</a>
--   - Horizontal pixel dimension of the video.
newVideoMetadata :: VideoMetadata
